Artificial Intelligence  Page 1 
 DIGITAL NOTES  
ON 
ARTIFICIAL INTELLIGENCE  
B.TECH IV YR / I  SEM  
(2020 -21) 
 
 
 
 
 
 
 
 
DEPARTMENT OF INFORMATION TECHNOLOGY  
MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY  
 (Autonomous Institution – UGC, Govt. of India)  
Recognized under 2(f) and 12 (B) of UGC ACT 1956  
(Affiliated to JNTUH, Hyderabad, Approved by AICTE - Accredited by NBA & NAAC – ‘A’ Grade - ISO 9001:2015 Certified)  
Maisammaguda, Dhulapally (Post Via. Hakimpet), Secunderabad – 500100, Telangana State, India  
 
 
 
 
 
 
Artificial Intelligence  Page 2 
 MALLA REDDY COLLEGE OF ENGINEERING & TECHNOLOGY  
Department of Information Technology  
 
IV YearB.TechIT –ISem                                                                                             L    T/P/D C 
                                                                                                       4  /-/-    3 
(R17 A1204 ) ARTIFICIAL INTELLIGENCE  
Objectives:  
The students will be able:  
• To study the distinction between optimal reasoning Vs. human like reasoning  
• To understand the concepts of state space representation, exhaustive search, heuristic search 
together with the time and space complexities.  
• To get an idea on different knowledge representation techniques.  
• To understand the applications of AI, namely game playing, theorem proving  
 To  realize problems under uncertainty and acquire machine learning algorithms  
 
UNIT - I 
Problem Solvi ng by Search -I: Introduction to AI, Intelligent Agents Problem Solving by Search –II: 
Problem -Solving Agents, Searching for Solutions,Uninformed Search Strategies: Breadth -first search, 
Uniform cost search, Depth -first search,Iterative deepening Depth -first search, Bidirectional search, 
Informed (Heuristic) SearchStrategies: Greedy best -first search, A* search, Heuristic Functions, Beyond 
Classical Search: Hill -climbing search, Simulated annealing search, Local Search in Continuous Spaces, 
Searching with No n-Deterministic Actions, Searching with  Partial Observations, Online Search Agents 
and Unknown Environment .  
UNIT -II 
Problem Solving by Search -II and Propositional Logic .Adversarial Search: Games, Optimal Decisions in 
Games, Alpha –Beta Pruning, Imperfect Real -Time Decisions.  
Constraint Satisfaction Problems: Defining Constraint Satisfaction Problems, Constraint Propagation, 
Backtracking Search for CSPs, Local Search for CSPs, The Structure of Problems.  
Propositional Logic: Knowledge -Based Agents, The Wumpu s World, Logic, Propositional Logic, 
Propositional Theorem Proving: Inference and proofs, Proof by resolution, Horn clauses and definite 
clauses, Forward and backward chaining, Effective Propositional Model Checking, Agents Based on 
Propositional Logic.  
 
UNIT-III 
Logic and Knowledge Representation  
First -Order Logic: Representation, Syntax and Semantics of First -Order Logic, Using FirstOrder Logic, 
Knowledge Engineering in First -Order Logic.  
Inference in First -Order Logic: Propositional vs. First -Order Inference, Unification and Lifting, Forward 
Chaining, Backward Chaining, Resolution.  
Knowledge Representation: Ontological Engineering, Categories and Objects, Events. Mental Events and 
Mental Objects, Reasoning Systems for Categories, Reasoning with Defau lt Information.  
Artificial Intelligence  Page 3 
 UNIT -IV 
Planning  
Classical Planning: Definition of Classical Planning, Algorithms for Planning with StateSpace Search, 
Planning Graphs, other Classical Planning Approaches, Analysis of Planning approaches.  
Planning and Acting in the Real Wo rld: Time, Schedules, and Resources, Hierarchical Planning, Planning 
and Acting in Nondeterministic Domains, Multi agent Planning.  
 
UNIT -V 
Uncertain knowledge and Learning  
Uncertainty: Acting under Uncertainty, Basic Probability Notation, Inference Using F ull Joint 
Distributions, Independence, Bayes’ Rule and Its Use, Probabilistic Reasoning: Representing Knowledge 
in an Uncertain Domain, The Semantics of Bayesian Networks, Efficient Representation of Conditional 
Distributions, Approximate Inference in Baye sian Networks, Relational and First -Order Probability, Other 
Approaches to Uncertain Reasoning; Dempster -Shafer theory.  
Learning: Forms of Learning, Supervised Learning, Learning Decision Trees.Knowledge in Learning: 
Logical Formulation of Learning, Knowle dge in Learning, Explanation -Based Learning, Learning Using 
Relevance Information, Inductive Logic Programming.  
 
TEXT BOOKS  
1. Artificial Intelligence A Modern Approach, Third Edition, Stuart Russell and Peter Norvig, Pearson 
Education.  
REFERENCES:  
1. Arti ficial Intelligence, 3rd Edn., E. Rich and K. Knight (TMH)  
2. Artificial Intelligence, 3rd Edn., Patrick Henny Winston, Pearson Education.  
3. Artificial Intelligence, Shivani Goel, Pearson Education.  
4. Artificial Intelligence and Expert systems – Patterso n, Pearson Education.  
Outcomes:  
The students will be able:  
 To formulate an efficient problem space for a problem expressed in natural language.  
 To select a search algorithm for a problem and estimate its time and space complexities.  
 To possess the skill for representing knowledge using the appropriate technique for a given 
problem.  
 To apply AI techniques to solve problems of game playing  
 To solve problems uncertainty domain and apply different machine learning techniques  
 
 Artificial Intelligence  Page 4 
  
 
 
 
 
INDEX  
S.NO  Title  Page No  
1 Introduction to AI  5 
2 Uninformed Search Strategies  20 
3 A* search  42 
4 Searching with Partial Observations  45 
5 Constraint Satisfaction Problems  51 
6 Alpha –Beta Pruning  55 
7 Forward and backward chaining  66 
8 Syntax and Semantics of First -Order Logic  70 
9 Knowledge Engineering in First -Order Logic – Unification and 
Lifting  87 
10 Resolution  100 
11 Classical Planning  104 
12 Planning with State Space Search  106 
13 Acting in Nondeterministic Domains  110 
14 Multi agent Planning  114 
15 Bayes’ Rule  120 
16 Bayesian Networks  122 
17 Dempster Shafer Theory  135 
18 Forms of Learning  137 
19 Learning Decision Trees  138 
20 Knowledge in Learning  140 
 
 
 
 
 Artificial Intelligence  Page 5 
 UNIT I:  
 
Introduction:  
 Artificial Intelligence is concerned with the design of intelligence in an artificial device. The term 
was coined by John McCarthy in 1956.  
 Intelligence is the ability to acquire, understand and apply the knowledge to achieve goals in the 
world.  
 AI is the study of the mental faculties through the use of computational models  
 AI is the study of intellectual/mental processe s as computational processes.  
 AI program will demonstrate a high level of intelligence to a degree that equals or exceeds the 
intelligence required of a human in performing some task.  
 AI is unique, sharing borders with Mathematics, Computer Science, Philosophy,  
Psychology, Biology, Cognitive Science and many others.  
 Although there is no clear definition of AI or even Intelligence, it can be described as an attempt 
to build machines that like humans can think and act, able to learn and use knowledge to  solve 
problems on their own.  
History of AI : 
 
Important research that laid the groundwork for AI:  
 
 In 1931,  Goedel  layed the foundation of Theoretical Computer Science 1920 -30s: 
He published the first universal formal language and showed that math itself is either 
flawed or allows for unprovable but true statements.  
 In 1936, Turing reformulated Goedel’s result and church’s extension thereof.  Problem Solving by Search -I: Introduction to AI, Intelligent Agents Problem Solving by Search –II: Problem -
Solving Agents, Searching for Solutions,  
Uninformed Search Strategies: Breadth -first search, Uniform cost search, Depth -first search, Iterative 
deep ening Depth -first search, Bidirectional search, Informed (Heuristic) Search Strategies: Greedy best -first 
search, A* search, Heuristic Functions, Beyond Classical Search: Hill -climbing search, Simulated annealing 
search,  
Local Search in Continuous Spaces,  Searching with Non -Deterministic Actions, Searching wih Partial 
Observations, Online Search Agents and Unknown Environment  Artificial Intelligence  Page 6 
  In 1956, John McCarthy coined the term "Artificial Intelligenc e" as the topic of the  Dartmouth 
Conference , the first conference devoted to the subject.  
 In 1957, The  General Problem Solver (GPS)  demonstrated by Newell, Shaw & Simon  
 In 1958, John McCarthy (MIT) invented the Lisp language.  
 In 1959, Arthur Samuel (IBM) wrote the first game -playing program, for checkers, to achieve 
sufficient skill to challenge a world champion.   
 In 1963, Ivan Sutherland's MIT dissertation on Sketchpad introduced the idea of interactive 
graphics into computing.  
 In 1966, Ross Quillian (PhD dissertation, Carnegie Inst. of Technology; now CMU) demonstrated 
semantic nets  
 In 1967, Dendral program  (Edward Feigenbaum, Joshua Lederberg, Bruce Buchanan, Georgia 
Sutherland at Stanford) demonstrated to interpret mass spectra on organic chemical compounds. 
First successful knowledge -based program for scientific reasoning.  
 In 1967, Doug Engelbart invented the mouse at SRI  
 In 1968, Marvin Minsky & Se ymour Papert publish Perceptrons, demonstrating limits of simple 
neural nets.  
 In 1972, Prolog developed by Alain Colmerauer.  
 In Mid 80’s, Neural Networks become widely used with the Backpropagation algorithm (first 
described by Werbos in 1974).  
 1990, Major  advances in all areas of AI, with significant demonstrations in machine learning, 
intelligent tutoring, case -based reasoning, multi -agent planning, scheduling, uncertain reasoning, 
data mining, natural language understanding and translation, vision, virtu al reality, games, and 
other topics.  
 In 1997, Deep Blue beats the World Chess Champion Kasparov  
 In 2002, iRobot , founded by researchers at the MIT Artificial Intelligence Lab, introduced  Roomba , 
a vacuum cleaning robot. By 2006, two million had been sold . 
 
Foundations of Artificial Intelligence:  
 Philosophy  
e.g., foundational issues (can a machine think?), issues of knowledge and believe, mutual 
knowledge  Artificial Intelligence  Page 7 
  Psychology and Cognitive Science  
e.g., problem solving skills  
 Neuro -Science  
e.g., brain architecture  
 Computer Science And Engineering  
e.g., complexity theory, algorithms, logic and inference,  programming languages, and system 
building.  
 Mathematics and Physics  
e.g., statistical modeling, continuous mathematics,  
 Statistical Physics, and Complex Systems .   
Sub Areas of AI:  
1) Game Playing   
Deep Blue Chess program beat world champion Gary Kasparov  
2) Speech Recognition   
PEGASUS spoken language interface to American Airlines' EAASY SABRE reseration system, which 
allows us ers to obtain flight information and make reservations over the telephone. The 1990s has 
seen significant advances in speech recognition so that limited systems are now successful.  
3) Computer Vision   
Face recognition programs in use by banks, government, etc. The ALVINN system from CMU 
autonomously drove a van from Washington, D.C. to San Diego (all but 52 of 2,849 miles), averaging 
63 mph day and night, and in all weather conditions. Handwriting recognition, electronics and 
manufacturing inspection, photo  interpretation, baggage inspection, reverse engineering to 
automatically construct a 3D geometric model.  
4) Expert Systems   
Application -specific systems that rely on obtaining the knowledge of human experts in an area and 
programming that knowledge into a sy stem.  
a. Diagnostic Systems  : MYCIN system for diagnosing bacterial infections of the blood and 
suggesting treatments. Intellipath pathology diagnosis system (AMA approved). Pathfinder 
medical diagnosis system, which suggests tests and makes diagnoses. Whirlp ool customer 
assistance center.  Artificial Intelligence  Page 8 
 b. System Configuration   
DEC's XCON system for custom hardware configuration. Radiotherapy treatment planning.  
c. Financial Decision Making   
Credit card companies, mortgage companies, banks, and the U.S. government employ AI 
syste ms to detect fraud and expedite financial transactions. For example, AMEX credit 
check.  
d. Classification Systems   
Put information into one of a fixed set of categories using several sources of information. 
E.g., financial decision making systems. NASA develo ped a system for classifying very faint 
areas in astronomical images into either stars or galaxies with very high accuracy by learning 
from human experts' classifications.  
5) Mathematical Theorem Proving   
Use inference methods to prove new theorems.  
6) Natural L anguage Understanding   
AltaVista's translation  of web pages. Translation of Catepillar Truck manuals into 20 languages.  
7) Scheduling and Planning   
Automatic scheduling for manufacturing . DARPA's DART system used in Desert Storm and Desert 
Shield operations to plan logistics of people and supplies. American Airlines rerouting contingency 
planner. European space agency planning and scheduling of spacecraft assembly, integration and 
verific ation.  
8) Artificial Neural Networks:  
9) Machine Learning  
Application of AI:  
AI algorithms have attracted close attention of researchers and have also been applied 
successfully to solve problems in engineering. Nevertheless, for large and complex problems, AI 
algorithms consume considerable computation time due to stochastic feature of the search 
approaches  
1) Business; financial strategies  Artificial Intelligence  Page 9 
 2) Engineering: check design, offer suggestions to create new product, expert systems 
for all engineering problems  
3) Manufacturing: assembly, inspection and maintenance  
4)  Medicine: monitoring, diagnosing  
5) Education: in teaching  
6) Fraud detection  
7) Object identification  
8) Information retrieval  
9) Space shuttle scheduling  
Building AI Systems:  
1) Perception   
Intelligent  biological systems are physically embodied in the world and experience the world 
through their sensors (senses). For an autonomous vehicle, input might be images from a 
camera and range information from a rangefinder. For a medical diagnosis system, perce ption is 
the set of symptoms and test results that have been obtained and input to the system manually.  
2) Reasoning   
Inference, decision -making, classification from what is sensed and what the internal "model" is 
of the world. Might be a neural network, log ical deduction system, Hidden Markov Model 
induction, heuristic searching a problem space, Bayes Network inference, genetic algorithms, 
etc.  
Includes areas of knowledge representation, problem solving, decision theory, planning, game 
theory, machine learn ing, uncertainty reasoning, etc.  
3) Action   
Biological systems interact within their environment by actuation, speech, etc. All behavior is 
centered around actions in the world. Examples include controlling the steering of a Mars rover 
or autonomous vehicle, or suggesting tests and making diagnoses for a medical diagnosis 
system. Includes areas of robot actuation, natural language generation, and speech synthesis.  
The definitions of AI : Artificial Intelligence  Page 10 
 a) "The exciting new effort to make computers 
think . . . machines with minds , in the full and 
literal sense" (Haugeland, 1985)  
 
"The automation of] activities that we 
associate with human thinking, activities such 
as decision -making, problem solving, 
learning..."(Bellman, 1978)  b) "The study of mental faculties through 
the use of computational models" 
(Charniak and McDermott, 1985)  
 
"The study of the computations that 
make it possible to perceive, reason, and 
act" (Winston, 1992)  
c) "The art of creating machines that perform 
functions that require intelligence when 
performed by peopl e" (Kurzweil, 1990)  
 
"The study of how to make computers do  
              things at which, at the moment, people                    
              are better" (Rich and Knight, 1 99 1 )  d) "A field of study that seeks to explain and 
emulate intelligent behavior in terms of 
computational processes" (Schalkoff, 1 
990)  
 
"The branch of computer science that is 
concerned with the automation of 
intelligent behavior" (Luger and 
Stubblefield, 1993)  
The definitions on the top, (a) and   (b) are concerned with reasoning , whereas those on the bottom , (c) 
and (d) address behavior. The definitions on the left,  (a) and   (c) measure success in terms of human 
performance, and those on the right,  (b) and   (d) measure the ideal concept of intelligence called 
rationality  
Intelligent Systems : 
 
In order to design intelligent systems, it is important to categorize them into four categories (Luger and 
Stubberfield 1993), (Russell and Norvig, 2003)  
1. Systems that think like humans  
2. Systems that think rationally  
3. Systems that behave  like humans  
4. Systems that behave rationally  
 Human - Like Rationally  
 
 
Think:   
          Cognitive Science Approach  
 
         “Machines that think like humans”  
  
        Laws of thought Approach  
 
      “ Machines that think Rationally”  
 
 
 
Act:  
           Turing Test Approach  
 
       “Machines that behave like humans”  
 
  
        Rational Agent Approach  
 
    “Machines that behave Rationally”  
 Artificial Intelligence  Page 11 
 Scientific Goal: To determine which ideas about knowledge representation, learning, rule systems 
search, and so on, explain various sorts of real intelligence.  
Engineering Goal: To solve real world problems using AI techniques such as Knowledge representation, 
learning, rule systems, search, and so on.  
Traditionally, computer scientists and engineers have been more interested in the engineering 
goal, while psychologists, philosophers and cognitive scientists have been more interested in the 
scientific goal.  
Cognitive Science: Think Human -Like 
a. Requires a model for human cognition. Precise enough models allow simulation by  
computers.  
b. Focus is not just on behavior and I/O, but looks like reasoning process.  
c. Goal is not just to produce human -like behavior but to produce a sequence of steps of the 
reasoning process, similar to the steps followed by a human in solving the same task.  
Laws of thought: Think Rationally  
a. The study of mental faculties through the use of computational models; that it is, the study of 
computations that make i t possible to perceive reason and act.  
b. Focus is on inference mechanisms that are probably correct and guarantee an optimal solution.  
c. Goal is to formalize the reasoning process as a system of logical rules and procedures of 
inference.  
d. Develop systems of rep resentation to allow inferences to be like  
“Socrates is a man. All men are mortal. Therefore Socrates is mortal”  
Turing Test: Act Human -Like 
a. The art of creating machines that perform functions requiring intelligence when performed by 
people; that it is th e study of, how to make computers do things which, at the moment, people 
do better.  
b. Focus is on action, and not intelligent behavior centered around the representation of the world  
c. Example: Turing Test  
o 3 rooms contain: a person, a computer and an interrogator.  
o The interrogator can communicate with the other 2 by teletype (to avoid the 
machine imitate the appearance of voice of the person)  Artificial Intelligence  Page 12 
 o The interrogator tries to determine which the person is and which the machine 
is. 
o The machine tries to fool the  interrogator to believe that it is the human, and 
the person also tries to convince the interrogator that it is the human.  
o If the machine succeeds in fooling the interrogator, then conclude that the 
machine is intelligent.  
Rational agent: Act Rationally  
a. Tries to explain and emulate intelligent behavior in terms of computational process; that it is 
concerned with the automation of the intelligence.  
b. Focus is on systems that act sufficiently if not optimally in all situations.  
c. Goal is to develop systems that are rational and sufficient  
 
The difference between strong AI and weak AI:  
 
Strong AI  makes the bold claim that computers can be made to think on a level (at least) equal to 
humans.   
 
Weak AI  simply states that some "thinking -like" features can be added t o computers to make them 
more useful tools... and this has already started to happen (witness expert systems, drive -by-wire cars 
and speech recognition software).  
AI Problems : 
 
 AI problems (speech recognition, NLP, vision, automatic programming, knowledge  
representation, etc.) can be paired with techniques (NN, search, Bayesian nets, production systems, 
etc.).AI problems can be classified in two types:  
 
1. Common -place tasks(Mundane Tasks)  
2. Expert tasks  
 
Common -Place Tasks:  
1. Recognizing people, objects.  
2. Communicating (through natural language ).  
3. Navigating around obstacles on the streets.  Artificial Intelligence  Page 13 
 These tasks are done matter of factly and routinely by people and some other animals.  
Expert tasks:  
1. Medical diagnosis.  
2. Mathematical problem solving  
3. Playing games like chess  
These tasks cannot be done by all people, and can only be performed by skilled specialists.  
 Clearly tasks of the first type are easy for humans to perform, and almost all are able to 
master them. The second range of tasks require s skill development and/or intelligence and only some 
specialists can perform them well. However, when we look at what computer systems have been able to 
achieve to date, we see that their achievements include performing sophisticated tasks like medical 
diagnosis, performing symbolic integration, proving theorems and playing chess.  
 
1. Intelligent Agent’s:  
2.1 Agents andenvironments:  
 
Fig 2.1 : Agents and Environments  
2.1.1 Agent:  
An Agent is anything that can be viewed as perceiving its environment through sensors and acting 
upon that environment through actuators.  
 A human agent has eyes, ears, and other organs for sensors and hands, legs, mouth, and 
other body parts foractuators.  
 A robotic agent might have cameras and infrared range finders for sensors and various 
moto rs foractuators.  
 A software agent receives keystrokes, file contents, and network packets as sensory 
Artificial Intelligence  Page 14 
 inputs and acts on the environment by displaying on the screen, writing files, and sending 
network packets.  
 
2.1.2 Percept:  
We use the term percept to refer to the agent's perceptual inputs at any given instant.  
 
2.1.3 PerceptSequence:  
An agent's percept sequence is the complete history of everything the agent has ever perceived.  
 
2.1.4 Agent function:  
Mathematically speaking, we say that an agent's behavior is described by the agent function that 
maps any given percept sequence to an action . 
2.1.5 Agentprogram  
Internally, the agent function for an artificial agent will be implemented by an agent program. It is 
important to keep these two ideas distinct. The agent function is an ab stract mathematical 
description; the agent program is a concrete implementation, running on the agent architecture.  
To illustrate these ideas, we will use a very simple example -the vacuum -cleaner world shown in Fig 
2.1.5 . This particular world has just two locations: squares A and B. The vacuum agent perceives 
which square it is in and whether there is dirt in the square. It can choose to move left, move right, 
suck up the dirt, or do nothing. One very simple agent functi on is the following: if the current 
square is dirty, then suck, otherwise move to the other square. A partial tabulation of this agent 
function is shown in Fig 2.1.6 . 
 
 
Fig 2.1.5: A vacuum -cleaner world with just two locations.  
Artificial Intelligence  Page 15 
 Function REFLEX -VACCUM -AGENT ([location, status]) returns an action If 
status=Dirty then return Suck  
else if location = A then return Right 
else if location = B then return Left 2.1.6 Agentfunction  
 
Percept Sequ ence  Action  
[A, Clean]  Right  
[A, Dirty]  Suck  
[B, Clean]  Left 
[B, Dirty]  Suck  
[A, Clean], [A, Clean]  Right  
[A, Clean], [A, Dirty]  Suck  
…  
Fig 2.1.6 : Partial tabulation of a simple agent function for the example: vacuum -cleaner 
world shown in the Fig 2.1.5  
  
Fig 2.1.6(i) : The REFLEX -VACCUM -AGENT program is invoked for each new percept (location, status) and 
returns an action each time  
Strategies of Solving Tic -Tac-Toe Game Playing  
Tic-Tac-Toe Game Playing:  
Tic-Tac-Toe is a simple and yet an interesting board game. Researc hers have used various approaches to 
study the Tic -Tac-Toe game. For example, Fok and Ong  and Grim et al.  have  used artificial neural 
network based strategies to play it. Citrenbaum  and Yakowitz  discuss games like Go -Moku, 
Hex and Bridg -It which share some similarities with Tic -Tac-Toe. Artificial Intelligence  Page 16 
  
 
      Fig 1.  
A Formal Definition of the Game : 
The board used to play the Tic -Tac-Toe game consists of 9 cells laid out in the form of a 3x3 matrix (Fig. 
1). The game is played by 2 players and either of them can start .  Each of the two players is assigned a 
unique symbol (generally 0 and X). Each player alternately gets a turn to make a move. Making a move is 
compulsory and cannot be deferred. In each move a player places the symbol assigned to him/her in a 
hitherto bl ank cell.  
Let  a  track  be  defined  as  any  row, column  or  diagonal  on  the  board.  Since the board is a square 
matrix with 9 cells, all rows, columns and diagonals have exactly 3 cells. It can be easily observed that 
there are 3 rows, 3 columns and  2 diagonals, and hence a total of 8 tracks on the board (Fig. 1). The goal 
of the game is to fill all the three cells of any track  on the board with the symbol assigned to one before  
the  opponent  does  the  same  with  the  symbol  assigned  to  him/h er.  At any point of  the  game,  if  
there  exists  a  track  whose  all  three  cells  have  been  marked  by  the  same symbol, then the player 
to whom that symbol  have  been  assigned  wins  and  the  game  terminates. If there exist no track 
whose ce lls have been marked by the same symbol when there is no more blank cell on the board then 
the game is drawn.   
Let the priority of a cell be defined as the number of tracks passing through it. The priorities  of  the  
nine  cells  on  the  board according   to  this  definition  are  tabulated  in Table  1.  Alternatively, let the 
priority of a track be defined as the sum of the priorities of its three cells. The priorities of the eight 
tracks on the board according to this definition are tabulated in Table  2. The prioritization of the cells 
and the tracks lays the foundation of the heuristics to be used in this study.  These heuristics are 
somewhat similar to those proposed by Rich and Knight.  
Artificial Intelligence  Page 17 
  
Strategy 1:  
Algorithm:  
1. View the vector as a ternary number.  Convert it to a decimal number.  
2. Use the computed number as an index into Move -Table and access the vector stored there.  
3. Set the new board to that vector.  
Procedure:  
1) Elements of vector:  
  0: Empty  
1: X 
2: O 
→ the vector is a ternary number  
2)  Store inside the program a move -table (lookuptable):  
a)  Elements in the table: 19683 (39) 
b) Element = A vector which describes the most suitable move from the  
 
Comments:  
Artificial Intelligence  Page 18 
 1. A lot of space to store the Move -Table.  
2. A lot of work to specify all the entries in the Move -Table.  
3. Difficult to extend  
Explanation of Strategy 2 of solving Tic -tac-toe problem  
Stratergy 2 : 
Data Structure:  
1)  Use vector, called board, as Solution 1  
2) However, elements of the vector:  
 2: Empty  
 3: X 
 5: O 
3) Turn of move: indexed by integer  
 1,2,3, etc  
Function Library:  
1.Make2:  
a) Return a location on a game -board.  
IF (board[5] = 2)  
RETURN 5; //the center cell.  
ELSE  
RETURN any cell that is not at the board’s corner;  
// (cell: 2,4,6,8)  
b)  Let P represent for X or O  
c) can_win(P) :  
P has filled already at least two cells on a straight line (horizontal, vertical, or 
diagonal)  
d) cannot_win(P) = NOT(can_win(P))  
2. Posswin(P):  Artificial Intelligence  Page 19 
 IF (cannot_win(P))  
RETURN 0;  
ELSE  
RETURN index to the empty cell on the line of  
can_win(P)  
Let odd numbers are turns of X  
 Let even numbers are turns of O  
3.  Go(n): make a move  
Algorithm:  
1. Turn = 1 : (X moves)  
Go(1) //make a move at the left -top cell  
2. Turn = 2 : (O moves)  
IF board[5] is empty THEN  
Go(5)  
ELSE  
Go(1)  
3. Turn = 3:  (X moves)  
IF boa rd[9] is empty THEN  
Go(9)  
ELSE  
Go(3).  
4. Turn = 4 : (O moves)  
IF Posswin (X) <> 0 THEN  
Go (Posswin (X))  
//Prevent the opponent to win  
ELSE Go (Make2)  
5. Turn = 5:  (X moves)  Artificial Intelligence  Page 20 
 IF Posswin(X) <> 0 THEN  
Go(Posswin(X))  
//Win for X.  
ELSE IF Posswin(O) <> THEN  
Go(Posswin(O))  
//Prevent the opponent to win  
ELSE IF board[7] is empty THEN  
Go(7)  
ELSE Go(3).  
Comments:  
1. Not efficient in time, as it has to check several conditions before making each 
move.  
2. Easier to understand the program’s strategy.  
3. Hard to g eneralize.  
 
Introduction to Problem Solving, General problem solving  
Problem solving is a process of generating solutions from observed data.  
−a problem is characterized by a set of goals,  
−a set of objects, and  
−a set of operations.  
These could be ill -defined and may evolve during problem solving.  
 
Searching Solutions:  
To build a system to solve a problem:  
1. Define the problem precisely  
2. Analyze the problem  
3. Isolate and represent the task knowledge that is necessary to solve the problem  
4. Choose the best problem -solving techniques and apply it to the particular problem.  
 Artificial Intelligence  Page 21 
 Defining the problem as State Space Search : 
The state space representation forms the basis of most of the AI methods.  
 Formulate a problem as a state space search  by showing the legal probl em states, the legal 
operators, and the initial and goal states.  
 A state  is defined by the specification of the values of all attributes of interest in the world  
 An operator  changes one state into the other; it has a precondition which is the value of ce rtain 
attributes prior to the application of the operator, and a set of effects, which are the attributes 
altered by the operator  
 The initial state  is where you start  
 The goal state  is the partial description of the solution  
 
Formal Description of the problem:  
1. Define a state space that contains all the possible configurations of the relevant objects.  
2. Specify one or more states within that space that describe possible situations from which the 
problem solving process may start ( initial state)  
3. Specify on e or more states that would be acceptable as solutions to the problem . ( goal states)  
Specify a set of rules that describe the actions (operations) available  
State -Space Problem Formulation:  
 
Example: A problem is defined by four items:  
1. initial state  e.g., "at Arad―  
2. actions or successor function  :      S(x) = set of action –state pairs  
  e.g., S(Arad) = {<Arad  Zerind, Zerind>, … } 
3. goal test (or set of goal states)  
e.g., x = "at Bucharest‖, Checkmate(x)  
4. path cost (additive)  
e.g., sum of distances, number of actions executed, etc.  
c(x,a,y) is the step cost, assumed to be ≥ 0  
A solution is a sequence of actions leading from the initial state to a goal state  
 Artificial Intelligence  Page 22 
  
 
Example : 8-queens problem  
 
1. Initial State : Any arrangement of 0 to 8 queens on board.  
2. Operators : add a queen to any square.  
3. Goal Test : 8 queens on board, none attacked.  
4. Path cost : not applicable or Zero (because only the final state counts, search cost might 
be of interest).  
State Spaces versus Search Trees:  
 State Space  
o Set of valid states for a problem  
o Linked by operators  
o e.g., 20 valid states (cities) in the Romanian travel problem  
 Search Tree  
– Root node = initial state  
– Child nodes = states that can be visited from parent  
– Note that the depth of the tree can be infinite  
• E.g., via repeated st ates 
– Partial search tree  
Artificial Intelligence  Page 23 
 • Portion of tree that has been expanded so far  
– Fringe  
• Leaves of partial search tree, candidates for expansion  
    Search trees = data structure to search state -space  
 
Properties of Search Algorithms  
 
Which search algorithm one should use will generally depend on the problem domain.  
There are four important factors to consider:  
 
1. Completeness – Is a solution guaranteed to be found if at least one solution exists?  
 
2. Optimality – Is the solution found guaranteed to be the best (or lowest cost) solution if there exists 
more than one solution?  
 
3. Time Complexity – The upper bound on the time required to find a solution, as a function of the 
complexity of the problem.  
 
4. Space Complexity – The upper bound on the storage space (me mory) required at any point during the 
search, as a function of the complexity of the problem.  
 
General problem solving, Water -jug problem, 8 -puzzle problem  
General Problem Solver : 
The General Problem Solver (GPS) was the first useful AI program, written by Simon, Shaw, and Newell 
in 1959. As the name implies, it was intended to solve nearly any problem .  
Newell and Simon defined each problem as a space. At one end of the space is the starting point; on the 
other side is the goal. The problem -solving proce dure itself is conceived as a set of operations to cross 
that space, to get from the starting point to the goal state, one step at a time.  
The General Problem Solver, the program tests various actions (which Newell and Simon called 
operators)  to see which will take it closer to the goal state. An operator  is any activity that changes the Artificial Intelligence  Page 24 
 state of the system. The General Problem Solver always chooses the operation that appears to bring it 
closer to its goal.  
Example: Water Jug Problem  
 
Consider the following  problem:  
 
A Water Jug Problem: You are given two jugs, a 4 -gallon one and a 3 -gallon one, a 
pump which has unlimited water which you can use to fill the jug, and the ground on which 
water may be poured. Neither jug has any measuring markings on it. How ca n you get 
exactly 2 gallons of water in the 4 -gallon jug?  
 
State Representation and Initial State :  
We will represent a state of the problem as a tuple (x, y) where x represents the amount of 
water in the 4 -gallon jug and y represents the amount of water i n the 3 -gallon jug. Note 0 ≤x≤ 4, 
and 0 ≤y ≤3. Our initial state: (0, 0)  
 
Goal Predicate - state = (2, y) where 0≤ y≤ 3.  
 
Operators -we must defi ne a set of operators that will take us from one state to another:  
 
1. Fill 4 -gal jug  (x,y)  → (4,y)  
  x < 4  
2. Fill 3 -gal jug  (x,y)  → (x,3)  
  y < 3  
3. Empty 4 -gal jug on ground  (x,y)  → (0,y)  
  x > 0  
4. Empty 3 -gal jug on ground  (x,y)  → (x,0)  
  y > 0  
5. Pour water from 3 -gal jug  (x,y)  →! (4, y - (4 - x)) 
 to  ll 4 -gal jug  0 < x+y    4 and y > 0  
6. Pour water from 4 -gal jug  (x,y)  → (x - (3-y), 3)  
 to  ll 3 -gal-jug 0 < x+y    3 and x > 0  
7. Pour all of water from 3 -gal jug  (x,y)  → (x+y, 0)  Artificial Intelligence  Page 25 
  into 4 -gal jug  0 < x+y    4 and y    0   
8. Pour all of water from 4 -gal jug  (x,y)  → (0, x+y)  
 into 3 -gal jug 0 < x+y    3 and x    0   
 
Through Graph Search, the following solution is found :  
 
    
Gals in 4 -gal jug  Gals in 3 -gal jug  Rule Applied  
0 0   
  1. Fill 4  
4 0   
  6. Pour 4 into 3 to  ll  
1 3   
  4. Empty 3  
1 0   
  8. Pour all of 4 into 3  
0 1   
  1. Fill 4  
4 1   
  6. Pour into 3  
2 3   
 
Second Solution:  
 
 
 
 Control strategies  
Control Strategies means how to decide which rule to apply next during the process of searching for a 
solution to a problem.  
Requirement for a good Control Strategy  
 
Artificial Intelligence  Page 26 
 1. It should cause motion  
In water jug problem, if we apply a simple control strategy of starting each time from the top of 
rule list and choose the first applicable one, then we will never move towards solution.  
2. It should explore the solution space in a systematic manner  
If we choose another control strategy, let us say, choose a rule randomly from the applicable 
rules then definitely it causes motion and eventually will lead to a solution. But one may arrive 
to same state several times. This is because c ontrol strategy is not systematic.  
 
Systematic Control Strategies (Blind searches):  
 
Breadth First Search:  
 
Let us discuss these strategies using water jug problem. These may be applied to any search problem.  
 
Construct a tree with the initial state as its root.  
 
Generate all the offspring of the root by applying each of the applicable rules to the initial state . 
 
 
Now for each leaf node, generate all its successors by applying all the rules that are appropria te. 
 
8 Puzzle Problem.  
The 8 puzzle consists of eight numbered, movable tiles set in a 3x3 frame. One cell of the frame is always 
empty thus making it possible to move an adjacent numbered tile into the empty cell. Such a puzzle is 
illustrated in followin g diagram.  
 
Artificial Intelligence  Page 27 
 The program is to change the initial configuration into the goal configuration. A solution to the problem 
is an appropriate sequence of moves, such as “move tiles 5 to the right, move tile 7 to the left, move tile 
6 to the down, etc”.  
Solution:  
To solve a problem using a production system, we must specify the global database the rules, and the 
control strategy. For the 8 puzzle problem that correspond to these three components. These elements 
are the problem states, moves and goal. In t his problem each tile configuration is a state. The set of all 
configuration in the space of problem states or the problem space, there are only 3, 62,880 different 
configurations o the 8 tiles and blank space. Once the problem states have been conceptuall y identified, 
we must construct a computer representation, or description of them . this description is then used as 
the database of a production system. For the 8 -puzzle, a straight forward description is a 3X3 array of 
matrix of numbers. The initial glob al database is this description of the initial problem state. Virtually 
any kind of data structure can be used to describe states.  
 
A move transforms one problem state into another state. The 8 -puzzle is conveniently interpreted as 
having the following for  moves. Move empty space (blank) to the left, move blank up, move blank to the 
right and move blank down,. These moves are modeled by production rules that operate on the state 
descriptions in the appropriate manner.  
The rules each have preconditions that must be satisfied by a state description in order for them to be 
applicable to that state description. Thus the precondition for the rule associated with “move blank up” 
is derived from the requirement that the blank space must not already be in the top ro w. 
The problem goal condition forms the basis for the termination condition of the production system. The 
control strategy repeatedly applies rules to state descriptions until a description of a goal state is 
produced. It also keeps track of rules that hav e been applied so that it can compose them into sequence 
representing the problem solution. A solution to the 8 -puzzle problem is given in the following figure.  
Example: - Depth – First – Search traversal   and Breadth - First - Search traversal                            
                                    for 8 – puzzle problem is shown in following diagrams.  Artificial Intelligence  Page 28 
  
 
Exhaustive Searches, BFS and DFS  
Search is the systematic examination of states to find path from the start/root state to the goal state.  
Many traditional search algorithms are used in AI applications. For complex problems, the traditional 
algorithms are unable to find the solution within some practical time and space limits. Consequently, 
many special techniques are developed; using heurist ic functions. The algorithms that use heuristic 
Artificial Intelligence  Page 29 
 functions are called heuristic algorithms. Heuristic algorithms are not really intelligent; they appear to 
be intelligent because they achieve better performance.  
Heuristic algorithms aremore efficient becau se they take advantage of feedback from the data to direct 
the search path.  
Uninformed search  
Also called blind, exhaustive or brute -force search, uses no information about the problem to guide the 
search and therefore may not be very efficient.  
Informed  Search:  
Also called heuristic or intelligent search, uses information about the problem to guide the search, 
usually guesses the distance to a goal state and therefore efficient, but the search may not be always 
possible.  
Uninformed Search Methods:  
Bread th- First -Search:  
Consider the state space of a problem that takes the form of a tree. Now, if we search the goal along 
each breadth of the tree, starting from the root and continuing up to the largest depth, we call it 
breadth first search . 
 
• Algorithm:  
1. Create a variable called NODE -LIST and set it to initial state  
2. Until a goal state is found or NODE -LIST is empty do  
a. Remove the first element from NODE -LIST and call it E. If NODE -LIST was empty, 
quit 
b. For each way that each rule can match the state describe d in E do:  
i. Apply the rule to generate a new state  
ii. If the new state is a goal state, quit and return this state  
iii. Otherwise, add the new state to the end of NODE -LIST 
BFS illustrated:  
Step 1:  Initially fringe contains only one node corresponding to the source state A.  
 Artificial Intelligence  Page 30 
  
Figure 1  
FRINGE: A  
 
Step 2:  A is removed from fringe. The node is expanded, and its children B and C are generated. 
They are placed at the back of fringe.  
 
 
Figure 2  
FRINGE : B C  
 
Step 3:  Node B is removed from fringe and is expanded. Its children D, E are generated and put 
at the back of fringe.  
 
Figure 3  
FRINGE: C D E  
 
Step 4:  Node C is removed from fringe and is expanded. Its children D and G are added to the 
back of fringe.  
Artificial Intelligence  Page 31 
  
Figure 4  
FRINGE: D E D G  
 
Step 5 : Node D is removed from fringe. Its children C and F are generated and added to the back 
of fringe.  
 
Figure 5  
FRIN GE: E D G C F  
 
Step 6 : Node E is removed from fringe. It has no children.  
 
Figure 6  
FRINGE: D G C F  
 
Step 7 : D is expanded; B and F are put in OPEN.  
 
Figure 7  
FRINGE: G C F B F  
Artificial Intelligence  Page 32 
 Step 8 : G is selected for expansion. It is found to be a goal node . So the algorithm returns the 
path A C G by following the parent pointers of the node corresponding to G. The algorithm 
terminates.  
Breadth first search  is: 
 One of the simplest search strategies  
 Complete. If there is a solution, BFS is guaranteed to find it.  
 If there are multiple solutions, then a minimal solution will be found  
 The algorithm is optimal (i.e., admissible) if all operators have the same cost. Otherwise, 
breadth first search finds a solution with the shortest path length.  
 Time complexity  : O(bd ) 
 Space complexity  : O(bd ) 
 Optimality   :Yes 
b - branching factor(maximum no of successors of any node),  
d – Depth of the shallowest goal node  
Maximum length of any path ( m) in search space  
Advantages : Finds the path of minimal length to the goal.  
Disadvantages :  
 Requires the generation and storage of a tree whose size is exponential the depth of the 
shallowest goal node.  
 The breadth first search algorithm cannot be effectively used unless the search space is quite 
small.  
 
Depth - First - Search . 
We may sometimes search the goal along the largest depth of the tree, and move up only when further 
traversal along the depth is not possible. We then attempt to find alternative offspring of the parent of 
the n ode (state) last visited. If we visit the nodes of a tree using the above principles to search the goal, 
the traversal made is called depth first traversal and consequently the search strategy is called depth 
first search . 
 
• Algorithm:  
1. Create a variable called NODE -LIST and set it to initial state  Artificial Intelligence  Page 33 
 2. Until a goal state is found or NODE -LIST is empty do  
a. Remove the first element from NODE -LIST and call it E. If NODE -LIST was empty, 
quit 
b. For each way that each rule can match the state described in E do:  
i. Apply t he rule to generate a new state  
ii. If the new state is a goal state, quit and return this state  
iii. Otherwise, add the new state in front of NODE -LIST 
DFS illustrated:  
 
A State Space Graph  
Step 1 : Initially fringe contains only the node for A.  
 
 
Figure 1  
FRINGE: A  
 
Step 2:  A is removed from fringe. A is expanded and its children B and C are put in front of 
fringe.  
 
Figure 2  
FRINGE: B C  
 
Artificial Intelligence  Page 34 
 Step 3:  Node B is removed from fringe, and its children D and E are pushed in front of fringe.  
 
Figure 3  
FRINGE: D E C  
 
Step 4:  Node D is removed from fringe. C and F are pushed in front of fringe.  
 
Figure 4  
FRINGE: C F E C  
 
Step 5:  Node C is removed from fringe. Its child G is pushed in front of fringe.  
 
Figure 5  
FRINGE: G F E C  
Step 6:  Node G is expanded and found to be a goal node.  
Artificial Intelligence  Page 35 
  
Figure 6  
FRINGE: G F E C  
 
The solution path A -B-D-C-G is returned and the algorithm terminates.  
 
Depth first searchis : 
1. The algorithm takes exponential time.  
2. If N is the maximum depth of a node in the search space, in the worst case the algorithm will 
take time O(bd
).  
3. The space taken is linear in the depth of the search tree, O(bN).  
Note that the time taken by the algorithm is related to the maximum depth of the search tree. If the 
search tree has inf inite depth, the algorithm may not terminate. This can happen if the search space is 
infinite. It can also happen if the search space contains cycles. The latter case can be handled by 
checking for cycles in the algorithm. Thus Depth First Search is not co mplete.  
Exhaustive searches - Iterative Deeping DFS  
 
Description:  
 It is a search strategy resulting when you combine BFS and DFS, thus combining the advantages 
of each strategy, taking the completeness and optimality of BFS and the modest memory 
requirements of DFS.  
 IDS works by looking for the best search depth d, thus  starting with depth limit 0 and make a BFS 
and if the search failed it increase the depth limit by 1 and try a BFS again with depth 1 and so 
on – first d = 0, then 1 then 2 and so on – until a depth d is reached where a goal is found.  
Artificial Intelligence  Page 36 
 Algorithm:  
procedur e IDDFS(root)  
for depth from  0 to ∞ 
        found ← DLS(root, depth)  
if found ≠ null  
return  found  
 
procedure  DLS(node, depth)  
if depth = 0 and node is a goal  
return  node  
else if  depth > 0  
foreach  child of node  
            found ← DLS(child, depth−1)  
if found ≠ null  
return  found  
return  null 
 
Performance Measure:  
o Completeness:  IDS is like BFS, is complete when the branching factor b is finite.  
o Optimality:  IDS is also like BFS optimal when the steps are of the same cost.  
 Time Complexity:  
o One may find that it  is wasteful to generate nodes multiple times, but actually it is not 
that costly compared to BFS, that is because most of the generated nodes are always in 
the deepest level reached, consider that we are searching a binary tree and our depth 
limit reached  4, the nodes generated in last level = 24 = 16, the nodes generated in all 
nodes before last level = 20 + 21 + 22 + 23= 15  
o Imagine this scenario, we are performing IDS and the depth limit reached depth d, now 
if you remember the way IDS expands nodes, yo u can see that nodes at depth d are 
generated once, nodes at depth d -1 are generated 2 times, nodes at depth d -2 are 
generated 3 times and so on, until you reach depth 1 which is generated d times, 
we can view the total number of generated nodes in the wor st case as:  
 N(IDS) = (b)d + (d – 1)b2+ (d – 2)b3 + …. + (2)bd-1 + (1)bd = O(bd)  Artificial Intelligence  Page 37 
 o If this search were to be done with BFS, the total number of generated nodes in 
the worst case will be like:  
 N(BFS) = b + b2 + b3 + b4 + …. bd + (bd+ 1 – b) = O(bd + 1)  
o If we consider a realistic numbers, and use b = 10 and d = 5, then number of 
generated nodes in BFS and IDS will be like  
 N(IDS) = 50 + 400 + 3000 + 20000 + 100000 = 123450  
 N(BFS) = 10 + 100 + 1000 + 10000 + 100000 + 999990 = 1111100  
 BFS generates like 9  time nodes to those generated with IDS.  
 Space Complexity:  
o IDS is like DFS in its space complexity, taking O(bd) of memory.  
Weblinks:    
i. https://www.youtube.com/watch?v=7QcoJjSVT38  
ii. https://mhesham.wordpress.com/tag/iterative -deepening -depth -first-search  
Conclusion:  
 We can conclude that IDS is a hybrid search strategy between BFS and DFS inheriting their 
advantages.  
 IDS is faster than BFS and DFS.  
 It is said that “IDS is the preferred uniformed search method when there is a large search space 
and the depth of the so lution is not known”.  
 
Heuristic Searches:  
A Heuristic technique helps in solving problems, even though there is no guarantee that it will never 
lead in the wrong direction. There are heuristics of every general applicability as well as domain specific. 
The strategies are general purpose heuristics. In order to use them in a specific domain they are coupler 
with some domain specific heuristics. There are two major ways in which domain - specific, heuristic 
information can be incorporated into rule -based se arch procedure.  
A heuristic function is a function that maps from problem state description to measures desirability, 
usually represented as number weights. The value of a heuristic function at a given node in the search 
process gives a good estimate of th at node being on the desired path to solution.  Artificial Intelligence  Page 38 
 Greedy Best First Search  
 
Greedy best -first search tries to expand the node that is closest to the goal, on the: grounds that this is 
likely to lead to a solution quickly. Thus, it evaluates nodes by using jus t the heuristic function:  
f (n) = h (n) . 
 
Taking the example of Route -finding problems in Romania, the goal is to reach Bucharest starting from 
the city Arad. We need to know the straight -line distances to Bucharest from various cities as shown in 
Figure  8.1. For example, the initial state is In (Arad), and the straight line distance heuristic h SLD (In 
(Arad)) is found to be 366. Using the straight -line distance heuristic hSLD, the goal state can be reached 
faster.  
Arad                 366        Mehadia                241            Hirsova           151 
Bucharest         0            Neamt                   234             Urziceni         80 
Craiova           160         Oradea                  380             Iasi                 226  
Drobeta          242          Pitesti                    100             Vaslui            199 
Eforie             161          Rimnicu Vilcea    193              Lugoj            244  
Fagaras          176          Sibiu                     253              Zerind           374 
Giurgiu          77            Timisoara            329 
Figure 8.1: Values of h SLD-straight -line distances to B u c h a r e s t.  
The Initial State                             
 
After Expanding Arad                
 
After Expanding Sibiu  
Artificial Intelligence  Page 39 
  
After Expanding Fagaras  
 
Figure 8.2:  Stages in a greedy best -first search for Bucharest using the straight -line distance heuristic 
hSLD. Nodes are labeled with their h -values.  
 
Figure 8.2  shows the progress of greedy best -first search using h SLD to find a path from Arad to 
Bucharest. The first node to be expanded from Arad will be Sibiu, because it is closer to 
Bucharest than either Zerind or Timisoara. The next node to be expanded will be Fagaras, 
because it is closest.  
Fagaras in turn generates  Bucharest, which is the goal.  
 
Evaluation Criterion of Greedy Search  
 
 Complete: NO [can get stuck in loops, e.g., Complete in finite space with repeated -
state checking ] 
 Time Complexity: O (bm)  [but a good heuristic can give dramatic improvement ] 
 Space Complexity: O (bm) [keeps all nodes in memory ] 
Artificial Intelligence  Page 40 
  Optimal: NO  
 
Greedy best -first search is not optimal, and it is incomplete. The worst -case time and space 
complexity is O (bm), where m is the maximum depth of the search space.  
 
HILL CLIMBING PROCEDURE:  
Hill Climbing Algorithm  
We will assume we are trying to maximize a function. That is, we are trying to find a point in the search 
space that is better than all the others. And by "better" we mean that the evaluation is higher. We might 
also say that the solution is of better quality than all the others.  
The idea behind hill climbing is as follows.  
1. Pick a random point in the search space.  
2. Consider all the neighbors of the current state.  
3. Choose the neighbor with the best quality and move to that state.  
4. Repeat 2 thru 4 until all the neighboring states are of lower quality.  
5. Return the current state as the solution state.  
We can also present this algorithm as follows (it is taken from the AIMA book (Russell, 1995) and follows 
the conventions we have been us ing on this course when looking at blind and heuristic searches).  
Artificial Intelligence  Page 41 
 Algorithm:  
Function  HILL -CLIMBING( Problem ) returns  a solution state  
Inputs:  Problem ,  problem  
Local variables:  Current , a node  
 Next, a node  
Current  = MAKE -NODE(INITIAL -STATE[ Problem ]) 
Loop do  
Next = a highest -valued successor of Current  
If VALUE[Next] < VALUE[Current] then return Current  
Current  = Next 
End 
 
Also, if two neighbors have the same evaluation and they are both the best quali ty, then the algorithm 
will choose between them at random.  
Problems with Hill Climbing  
The main problem with hill climbing (which is also sometimes called gradient descent ) is that we are not 
guaranteed to find the best solution. In fact, we are not offered any guarantees about the solution. It 
could be abysmally bad.  
You can see that we will eventually reach a state that has no better neighbours but there are better 
solutions elsewhere in the search space. The problem we have just described is called  a local maxima . 
Simulated annealing search  
A hill -climbing algorithm that never makes “downhill” moves towards states with lower value (or higher 
cost) is guaranteed to be incomplete, because it can stuck on a local maximum. In contrast, a purely 
random w alk –that is, moving to a successor chosen uniformly at random from the set of successors – is 
complete, but extremely inefficient. Simulated annealing is an algorithm that combines hill -climbing 
with a random walk in some way that yields both efficiency a nd completeness.  
Figure 10.7  shows simulated annealing algorithm. It is quite similar to hill climbing. Instead of picking the 
best move, however, it picks the random move. If the move improves the situation, it is always 
accepted. Otherwise, the algorithm  accepts the move with some probability less than 1. The probability 
decreases exponentially with the “badness” of the move – the amount E by which the evaluation is Artificial Intelligence  Page 42 
 worsened. The probability also decreases as the "temperature" T goes down: "bad moves are  more 
likely to be allowed at the start when temperature is high, and they become more unlikely as T 
decreases. One can prove that if the schedule lowers T slowly enough, the algorithm will find a global 
optimum with probability approaching 1.  
Simulated an nealing was first used extensively to solve VLSI layout problems. It has been applied widely 
to factory scheduling and other large -scale optimization tasks . 
function S I M U L A T E D - A N NEALING ( problem, schedule) returns a solution state  
inputs: problem, a problem  
schedule, a mapping from time to "temperature"  
local variables: current, a node  
next, a node  
T, a "temperature" controlling the probability of downward steps  
current MAKE -NODE(INITIAL -STATE[ problem ]) 
for tl to ∞ do 
T schedule[t]  
if T = 0 then return current  
next a randomly selected successor of current  
EVALUE [next] – VALUE [current]  
if E> 0 then current  next 
else current  next only with probability eE /T 
 
 
LOCAL  SEARCH IN CONTINUOUS SPACES  
 
 We have considered algorithms that work only in discrete environments, but real -world 
environment are continuous.  
 Local search amounts to maximizing a continuous objective function in a multi -dimensional 
vector space.  
 This is hard to do in general.  
 Can immediately retreat  
 Discretize  the sp ace near each state  
 Apply a discrete local search strategy (e.g., stochastic hill climbing, simulated annealing)  Artificial Intelligence  Page 43 
  Often resists a closed -form solution  
 Fake up an empirical gradient  
 Amounts to greedy hill climbing in discretized state space  
 Can employ Newton -Raphson Method to find maxima.  
 Continuous problems have similar problems: plateaus, ridges, local maxima, etc.  
 
Best First Search : 
 A combination of depth first and breadth first searches.  
 Depth first is good because a solution can be found without computing all nodes and breadth 
first is good because it does not get trapped in dead ends.  
 The best first search allows us to switch between paths thus gaining the benefit of both 
approaches. At  each step the most promising node is chosen. If one of the nodes chosen 
generates nodes that are less promising it is possible to choose another at the same level and in 
effect the search changes from depth to breadth. If on analysis these are no better t han this 
previously unexpanded node and branch is not forgotten and the search method reverts to the  
OPEN  is a priorityqueue of nodes that have been evaluated by the heuristic function but which have not 
yet been expanded into successors. The most promisin g nodes are at the front.  
CLOSED  are nodes that have already been generated and these nodes must be stored because a graph is 
being used in preference to a tree.  
Algorithm :  
1. Start with OPEN holding the initial state  
2. Until a goal is found or there are no  nodes left on open do.  
 Pick the best node on OPEN  
 Generate its successors  
 For each successor Do  
• If it has not been generated before ,evaluate it ,add it to OPEN and record its 
parent  Artificial Intelligence  Page 44 
 • If it has been generated before change the parent if this new path is better and 
in that case update the cost of getting to any successor nodes.  
3. If a goal is found or  no more nodes left in OPEN, quit, else return to 2.  
Example: 
 
1. It is not optimal.  
2. It is incomplete because it can start down an infinite path and never return to try other 
possibilities.  
Artificial Intelligence  Page 45 
 3. The worst -case time complexity for greedy search is O (bm), where m is the maximum depth of 
the search space.  
4. Because greedy search retains all nodes in memory, its space complexity is the same as its time 
complexity  
A* Algorithm  
The Best First algorithm is a simplified form of the A* algorithm.  
The A* search algorithm  (pronounced "Ay -star") is a tree search algorithm  that finds a path from a given 
initial node to a given goal node (or one passing a given goal test). It employs a "heuristic estimate" 
which ranks each node by an estimate of the best route that goes through that node. It visits the nodes 
in order of this heuristic estimate.  
Similar to greedy best -first search but is more accurate because A* takes into account the nodes that 
have already been traversed.  
From A* we note that f = g + h where   
g is a measure of the distance/cost to go from the initial node to the current node  
his an estimate of the distance/cost to solution from the current node.  
Thus fis an estimate of how long it takes to go from the initial node to the solution  
Algorithm:  
1. Initialize  :   Set  OPEN = (S);   CLOSED = ( )  
    g(s)= 0,  f(s)=h(s)  
2. Fail   :   If  OPEN = ( ), Terminate and fail.  
3. Select   :  select the minimum cost state, n, from OPEN,  
save n in CLOSED  
4. Terminate   :  If n €G, Terminate with success and return f(n) 
5. Expand  :   for each successor, m, of n  Artificial Intelligence  Page 46 
      a) If m € *OPEN U CLOSED+  
      Set g(m) = g(n) + c(n , m)  
      Set f(m) = g(m) + h(m)  
      Insert m in OPEN  
     b) If m € *OPEN U CLOSED+  
      Set g(m) = min { g(m) , g(n) + c(n , m)}  
      Set f(m) = g(m) + h(m)  
      If f(m) has decreased and m € CLOSED  
      Move m to OPEN.  
Description:  
 A* begins at a selected node . Applied to this node is the "cost" of entering this node (usually 
zero for the initial node). A* then estimates the distance to the goal node from the current 
node. This estimate and the cost added together are the heuristic which is assigned to the path  
leading to this node. The node is then added to a priority queue , often called "open".  
 The algorithm then removes the next node from the priority queue (because of the w ay a 
priority queue works, the node removed will have the lowest heuristic). If the queue is empty, 
there is no path from the initial node to the goal node and the algorithm stops. If the node is the 
goal node, A* constructs and outputs the successful path  and stops.  
 If the node is not the goal node, new nodes are created for all admissible adjoining nodes; the 
exact way of doing this depends on the problem at hand. For each successive node, A* 
calculates the "cost" of entering the node and saves it with t he node. This cost is calculated from 
the cumulative sum of costs stored with its ancestors, plus the cost of the operation which 
reached this new node.  
 The algorithm also maintains a 'closed' list of nodes whose adjoining nodes have been checked. 
If a ne wly generated node is already in this list with an equal or lower cost, no further 
processing is done on that node or with the path associated with it. If a node in the closed list 
matches the new one, but has been stored with a higher  cost, it is removed from the closed list, 
and processing continues on the new node.  Artificial Intelligence  Page 47 
  Next, an estimate of the new node's distance to the goal is added to the cost to form the 
heuristic for that node. This is then added to the 'open' priority queue, unless an identical node 
is found there.  
 Once the above three steps have been repeated for each new adjoining node, the original node 
taken from the priority queue is added to the 'closed' list. The next node is then popped from 
the priority queue and the process is repeated The heu ristic costs from each city to Bucharest:  
 
 
Artificial Intelligence  Page 48 
 
Artificial Intelligence  Page 49 
 A* search properties:  
 The algorithm A* is admissible. This means that provided a solution exists, the first solution 
found by A* is an optimal solution. A* is admissible under the following conditions:  
 Heuristic function: for every node n ,  h(n) ≤ h*(n) .  
 A* is also complete.  
 A* is optimally efficient for a given heuristic.  
 A* is much more efficient that uninformed search.  
Iterative Deeping A* Algorithm:  
Algorithm:  
Let L be the list of visited but not ex panded node, and  
C the maximum depth  
1) Let C=0  
2) Initialize Lto the initial state (only)  
3) If List empty increase C and goto 2),  
else  
extract a node  n  from the  front  of  L  
4) If  n is a goal node,  
SUCCEED and return the path from the initial state to n  
5) Remove n from L.  If the level is smaller than C, insert at the  front of  L all the  children n' of  n 
with f(n') ≤ C  
6) Goto 3)  
 
Artificial Intelligence  Page 50 
  IDA* is complete & optimal Space usage is linear in the depth of solution. Each iteration is depth 
first search, and thus it d oes not require a priority queue.  
 Iterative deepening A* (IDA*) eliminates the memory constraints of A* search algorithm 
without sacrificing solution optimality.  
 Each iteration of the algorithm is a depth -first search that keeps track of the cost, f(n) = g(n) + 
h(n), of each node generated.  
 As soon as a node is generated whose cost exceeds a threshold for that iteration, its path is cut 
off, and the search backtracks before continuing.  
 The cost threshold is initialized to the heuristic estimate of the init ial state, and in each 
successive iteration is increased to the total cost of the lowest -cost node that was pruned during 
the previous iteration.  
 The algorithm terminates when a goal state is reached whose total cost dees not exceed the 
current threshold.  
UNIT II  
Constraint Satisfaction Problems  
https ://www.cnblogs.com/RDaneelOlivaw/p/8072603.html  
Sometimes a problem is not embedded in a long set of action sequences but requires picking the best 
option from available choices. A good general -purpose problem solving technique is to list the 
constraints o f a situation (either negative constraints, like limitations, or positive elements that you 
want in the final solution). Then pick the choice that satisfies most of the constraints.  
Formally speaking, a constraint satisfaction problem (or CSP ) is defined by a set of variables, X1;X2; : : : 
;Xn, and a set of constraints, C1;C2; : : : ;Cm. Each variable Xi has anonempty domain Di of possible 
values. Each constraint Ci involves some subset of tvariables and specifies the allowable combinations of  
values for that subset. A state of theproblem is defined by an assignment of values to some or all of the 
variables, {Xi = vi;Xj =vj ; : : :} An assignment that does not violate any constraints is called a consistent or Problem Solving by Search -II and Propositional Logic .Adversarial Search: Games, Optimal Decisions in Games, 
Alpha –Beta Pruning, Imperfect Real -Time Decisions.  
Constraint Satisfaction Problems: Defining Constraint Satisfaction Problems, Constraint Propa gation, 
Backtracking Search for CSPs, Local Search for CSPs, The Structure of Problems.  
Propositional Logic: Knowledge -Based Agents, The Wumpus World, Logic, Propositional Logic, Propositional 
Theorem Proving: Inference and proofs, Proof by resolution, Horn clauses and definite clauses, Forward and 
backward chaining, Effective Propositional Model Checking, Agents Based on Propositional Logic.  Artificial Intelligence  Page 51 
 legalassignment. A complete assignm ent is one in which every variable is mentioned, and a solution to a 
CSP is a complete assignment that satisfies all the constraints. Some CSPs also require a solution that 
maximizes an objectivefunction.  
CSP can be given an incremental formulation as a st andard search problem as follows:  
1. Initial state : the empty assignment fg, in which all variables are unassigned.  
2. Successor function : a value can be assigned to any unassigned variable, provided that it does not 
conflict with previously assigned variables.  
3. Goal test : the current assignment is complete.  
4. Path cost : a constant cost for every step  
Examples:  
1. The best -known category of continuous -domain CSPs is that of linear 
programming problems, where constraints must be linear inequalities 
forming a convex region. 
2. Crypt arithmetic puzzles.  
 
Example: The map coloring problem .  
The task of coloring each region red, green or blue in such a way that no neighboring regions 
have the same color.  
We are given the task of coloring each region red, green, or blue in such  a way that the 
neighboring regions must not have the same color.  
To formulate this as CSP, we define the variable to be the regions: WA, NT, Q, NSW, V, SA, and 
T. The domain of each variable is the set {red, green, blue}. The constraints require 
Artificial Intelligence  Page 52 
 neighbori ng regions to have distinct colors: for example, the allowable combinations for WA 
and NT are the pairs {(red,green),(red,blue),(green,red),(green,blue),(blue,red),(blue,green)}. 
(The constraint can also be represented as the inequality WA ≠ NT). There are  many possible 
solutions, such as   {WA = red, NT = green, Q = red, NSW = green, V = red, SA = blue, T = 
red}.Map of Australia showing each of its states and territories  
 
Constraint Graph: A CSP is usually represented as an undirected graph, called con straint graph 
where the nodes are the variables and the edges are the binaryconstraints.  
The map -coloring problem represented as a constraint graph.  
CSP can be viewed as a standard search problem as follows:  
> Initial   state   :   the   empty   assignment   {},in  which   all  variables   are unassigned.  
> Successor function:  a value can be assigned to any unassigned variable, provided that it 
does not conflict with previously assigned variables.  
> Goal test:  the current assignment is complete.  
 
Artificial Intelligence  Page 53 
 > Path cost:  a cons tant cost(E.g.,1) for every step.  
 
Game Playing  
Adversarial search , or game -tree search , is a technique for analyzing an adversarial game in order to try 
to determine who can win the game and what moves the players should make in order to win. 
Adversarial  search is one of the oldest topics in Artificial Intelligence. The original ideas for adversarial 
search were developed by Shannon in 1950 and independently by Turing in 1951, in the context of the 
game of chess —and their ideas still form the basis for th e techniques used today.  
2-Person Games:  
o Players : We call them Max and Min.  
o Initial State : Includes board position and whose turn it is.  
o Operators : These correspond to legal moves.  
o Terminal Test : A test applied to a board position which determines wheth er the game is 
over. In chess, for example, this would be a checkmate or stalemate situation.  
o Utility Function : A function which assigns a numeric value  to a terminalstate . For 
example, in chess the outcome is win (+1), lose ( -1) or draw (0). Note that by  
convention, we always measure utility relative to Max .  
 
MiniMax Algorithm:  
1. Generate the whole game tree.  
2. Apply the utility function to leaf nodes to get their values.  
3. Use the utility of nodes at level n to derive the utility of nodes at level n -1.  
4. Continue backing up values towards the root (one layer at a time).  
5. Eventually the backed up values reach the top of the tree, at which point Max chooses the move 
that yields the highest value. This is called the minimax decision because it maximises the u tility 
for Max on the assumption that Min will play perfectly to minimise it.  Artificial Intelligence  Page 54 
  
 
 
Example:  
 
Properties of minimax:  
Artificial Intelligence  Page 55 
  Complete   :  Yes (if tree is finite)  
 Optimal   : Yes (against an optimal opponent)  
 Time complexity  :  O(bm) 
 Space complexity  :  O(bm) (depth -first exploration)  
 For chess, b ≈ 35, m ≈100 for "reasonable" games  
 exact solution completely infeasible.  
Limitations  
– Not always feasible to traverse entire tree  
– Time limitations  
 
Alpha -beta pruning algorithm:  
• Pruning:  eliminating a branch of the  search tree from consideration without exhaustive 
examination of each node  
• - Pruning: the basic idea is to prune portions of the search tree that cannot improve the 
utility value of the max or min node, by just considering the values of nodes seen so far. 
• Alpha -beta pruning  is used on top of minimax search to detect paths that do not need to be 
explored. The intuition is:  
• The MAX player is always trying to maximize the score. Call this .  
• The MIN player is always trying to minimize the score. Call this  .  
• Alpha cutoff : Given a Max node n, cutoff the search below n (i.e., don't generate or examine any 
more of n's children) if alpha(n) >= beta(n)  
(alpha increases and passes beta from below ) 
• Beta cutoff.:  Given a Min node n, cutoff the search below n (i.e., don't generate or examine any 
more of n's children) if beta(n) <= alpha(n)  
(beta decreases and passes alpha from above)  
• Carry alpha and beta values down during search Pruning occurs when ever alpha >= beta  Artificial Intelligence  Page 56 
 Algorithm:
Artificial Intelligence  Page 57 
 Example:  
 
1) Setup phase: Assign to each left -most (or right -most) internal node of the tree,  
                             variables: alpha = -infinity, beta = +infinity  
 
 
 
2) Look at first computed final configuration value.  It’s a 3. Parent is a min node, so  
set the beta (min) value to 3.  
 
Artificial Intelligence  Page 58 
 3) Look at next value, 5.  Since parent is a min node, we want the minimum of  
3 and 5 which is 3.  Parent min node is done – fill alpha (max) value of its parent max  
node. Always  set alpha for max nodes and beta for min nodes. Copy the state of the max 
parent node into the second unevaluated min child.  
 
 
4) Look at next value, 2.  Since parent node is min with b=+inf, 2 is smaller, change b.  
 
Artificial Intelligence  Page 59 
 5) Now, the min parent node has a m ax value of 3 and min value of 2.  The value of the  
2nd child does not matter.  If it is >2, 2 will be selected for min node.  If it is <2, it will be 
selected for min node, but since it is <3 it will not get selected for the parent max node.  
Thus, we pru ne the right subtree of the min node.  Propagate max value up the tree.  
 
 
6) Max node is now done and we can set the beta value of its parent and propagate node                                               
     state to sibling subtree’s left -most path.  
 
Artificial Intelligence  Page 60 
 7) The next node is 10.  10 is not smaller than 3, so state of parent does not change.  We still 
have to look at the 2nd child since alpha is still –inf. 
 
 
8) The next node is 4.  Smallest value goes to the parent min node.  Min subtree is done, so 
the parent max node gets the alpha (max) value from the child.  Note that if the max node 
had a 2nd subtree, we can prune it since a>b.  
 
Artificial Intelligence  Page 61 
 9) Continue propagating value up the tree, modifying the corresponding alpha/beta values.  
Also propagate the state of r oot node down the left -most path of the right subtree.  
 
 
10) Next value is a 2.  We set the beta (min) value of the min parent to 2.  Since no other 
children exist, we propagate the value up the tree.  
 
Artificial Intelligence  Page 62 
 11) We have a value for the 3rd level max node, now  we can modify the beta (min) value of 
the min parent to 2.  Now, we have a situation that a>b and thus the value of the rightmost 
subtree of the min node does not matter, so we prune the whole subtree.  
 
12) Finally, no more nodes remain, we propagate values up the tree.  The root has a value 
of 3 that comes from the left -most child.  Thus, the player should choose the left -most 
child’s move in order to maximize his/her winnings.  As you can see, the result is the same 
as with the mini -max example, but we did not visit all nodes of the tree.  
 
Artificial Intelligence  Page 63 
 UNIT III  
Knowledge Based Agents  A knowledge -based agent needs a KB and an inference 
mechanism. It operates by storing sentences in its knowledge base, inferring new sentences 
with the inference mechanism, and usin g them to deduce which actions to take. ... The 
interpretation of a sentence is the fact to which it refers.  
 
 
 
Knowledge base = set of sentences in a formal language Declarative approach to building an 
agent (or other system): Tell it what it needs toknow  - Thenitcan Askitselfwhattodo —
answersshouldfollowfromtheKB Agents can be viewed at the knowledge leveli.e., wh at they 
know, regardless of howimplemented or at the implementation leveli.e.,data 
structuresinKBand algorithmsthatmanipulatethem. The Wumpus World:  
 
A variety of "worlds" are being used as examples for Knowledge Representation, Reasoning, 
and Planning. A mong them the Vacuum World, the Block World, and the Wumpus World. 
The Wumpus World was introduced by Genesereth, and is discussed in Russell -Norvig. The 
Wumpus World is a simple world (as is the Block World) for which to represent knowledge 
and to reason.  It is a cave with a number of rooms, represented as a 4x4 square  
Artificial Intelligence  Page 64 
  
Rules of the Wumpus World The neighborhood of a node consists of the four squares north, 
south, east, and west of the given square. In a square the agent gets a vector of percepts, with 
components Stench, Breeze, Glitter, Bump, Scream For example [Stench, None, Glitter, 
None, None]  Stench is perceived at a square iff the wumpus is at this square or in its 
neighborhood.  Breeze is perceived at a square iff a pit is in the neighborhood of this 
square.  Glitter is perceived at a square iff gold is in this square  Bump is perceived at a 
square iff the agent goes Forward into a wall  Scream is perceived at a square iff the 
wumpus is killed anywhere in the cave An agent can do the following actions (one at a time): 
Turn (Right), Turn (Left), Forward, Shoot, Grab, Release, Climb  The agent can go forward 
in the direction it is currently facing, or Turn Right, or Turn Left. Going forward into a wall 
will generate a Bump percept.  The agent ha s a single arrow that it can shoot. It will go 
straight in the direction faced by the agent until it hits (and kills) the wumpus, or hits (and is 
absorbed by) a wall.  The agent can grab a portable object at the current square or it can 
Release an object that it is holding.  The agent can climb out of the cave if at the Start 
square.The Start square is (1,1) and initially the agent is facing east. The agent dies if it is in 
the same square asthe wumpus. The objective of the game is to kill the wumpus, to pick up 
the gold, and to climb out with it. Representing our Knowledge about the Wumpus World 
Percept(x, y) Where x must be a percept vector and y must be a situation. It means that at 
situation y theagentperceives x.For convenience we introduce the follow ing definitions:  
Artificial Intelligence  Page 65 
 Percept([Stench,y,z,w,v],t) = > Stench(t)  Percept([x,Breeze,z,w,v],t) = > Breeze(t)  
Percept([x,y,Glitter,w,v],t) = > AtGold(t) Holding(x, y)  
Where x is an object and y is a situation. It means that the agent is holding the object x i n 
situation y. Action(x, y) Where x must be an action (i.e. Turn (Right), Turn (Left), Forward,) 
and y must be a situation. It means that at situation y the agent takes action x. At(x,y,z) 
Where x is an object, y is a Location, i.e. a pair [u,v] with u and  v in {1, 2, 3, 4}, and z is a 
situation. It means that the agent x in situation z is at location y. Present(x,s) Means that 
object x is in the current room in the situation s. Result(x, y) It means that the result of 
applying action x to the situation y i s the situation Result(x,y).Notethat Result(x,y) is a term, 
not a statement. For example we can say  Result(Forward, S0) = S1  
Result(Turn(Right),S1) = S2 These definitions could be made more general. Since in the 
Wumpus World there is a single agent, th ere is no reason for us to make predicates and 
functions relative to a specific agent. In other"worlds" we should change things 
appropriately.  
Validity And Satisfiability  
A sentence is valid  
if it is true in all models, e.g.,True,A ∨¬A,   A⇒A,(A∧(A⇒B)) ⇒B Validity is connected to 
inference via the Deduction Theorem: KB |=  αif and onlyif(KB ⇒α) isvalid 
Asentenceissatisfiableifitistrue insome model e.g., A ∨B,  C Asentence 
isunsatisfiableifitistrueinnomodels e.g., A ∧¬A Satisfiability is connected to inferenc e via the 
following: KB|=α iff(KB ∧¬α)isunsatisfiable i.e., prove α by reductionandabsurdum  
 Proof Methods  
Proof methods divide into (roughly)two kinds:  
 Application of inference rules – Legitimate(sound)generationofnewsentencesfromold – 
Proof=asequenceofinferenceruleapplicationscanuseinferencerulesasoperatorsinastand 
ardsearch algorithm – Typicallyrequiretranslationofsentencesintoanormalform Model 
checking – Truthtableenumeration(alwaysexponentialinn) – Artificial Intelligence  Page 66 
 Improvedbacktracking,e.g.,Davis –Putnam–Loge Mann –Loveland – Heuristic 
searchinmodelspace(soundbutincomplete) e.g.,min -conflicts -likehillclimbingalgorithms  
 Forward and Backward Chaining  
Horn Form (restricted) KB = conjunction of Horn clauses Horn clause = – proposition 
symbol;or – (conjun ctionofsymbols) ⇒ symbol Example KB: C ∧(B ⇒ A) ∧ (C∧D  ⇒ B) 
Modus Ponens (for Horn Form): complete for Horn KBs  
 α1,...,αn,α1 ∧···∧α⇒ β β  
Canbeusedwithforwardchaining orbackwardchaining. These algorithms 
areverynaturalandruninlineartime.,  
ForwardChaining  
Idea: If anyrulewhosepremisesaresatisfiedintheKB, 
additsconclusiontotheKB,untilqueryisfound  
ForwardChaining Algorithm  
 
Artificial Intelligence  Page 67 
  
 
Proof of Completeness  
 FC derives every atomic sentence that is entailed by KB  
Artificial Intelligence  Page 68 
 1. FCreachesafixedpointwherenonewatomicsentences arederived  
2. Considerthefinalstateasamodelm,assigningtrue/falsetosymbols  
3. Every clause in the original KB is true inm i. Proof:Supposeaclausea1 ∧...∧ak⇒bisfalsei 
nm Then a1 ∧. . . ∧akis true in m and b is false in m 
Thereforethealgorithmhasnotreachedafi xedpoint ! 4. Hence m is a model ofKB 5. 
IfKB|=q,thenqistrueineverymodelofKB,includingm a. Generalidea: 
constructanymodelofKBby soundinference,checkα  
 
Backward Chaining  
Idea:workbackwardsfromthequeryq: to prove q byBC, check if q is known already, or prov e 
by BC all premises of some rule concluding q Avoidloops: 
checkifnewsubgoalisalreadyonthegoalstack Avoid repeated work: check if new subgoal 1. 
has already been proved true, or 2. has alreadyfailed  
 Artificial Intelligence  Page 69 
  
 
Forward vs Backward Chaining  
FC is data -driven, cf . automatic, unconscious processing, 
e.g.,objectrecognition,routinedecisions Maydolotsofworkthatisirrelevanttothegoal BC is goal -
driven, appropriate forproblem -solving, e.g., Where are my keys? How do I get into a PhD 
program? Complexity of BC can be much less than linear in size of KB  
FIRST ORDER LOGIC:  
Artificial Intelligence  Page 70 
 PROCEDURAL LANGUAGES AND PROPOSITIONAL LOGIC:  
Drawbacks of Procedural Languages  
 Programming languages (such as C++ or Java or Lisp) are by far the largest class of formal 
languages in common use. Progra ms themselves represent  only computational processes. Data 
structures within programs can represent facts.  
 For example, a program could use a 4 × 4 array to represent the contents of the wumpus world. 
Thus, the programming language statement World*2,2+←  Pit is a fairly natural way to assert that 
there is a pit in square [2,2].  
What programming languages lack is any general mechanism for deriving facts from other facts; 
each update to a data structure is done by a domain -specific procedure whose details are derived by 
the programmer from his or her own knowledge of the domain.  
 A second drawback of is the lack the expressiveness required to handle partial information . For 
example data structures in programs lack the  easy way to say, “There is a pit in  *2,2+ or *3,1+” or “If 
the wumpus is in *1,1+ then he is not in *2,2+.”   
Advantages of Propositional Logic  
 The declarative nature of propositional logic, specify that  knowledge and inference are separate, 
and inference is entirely domain -independent.   Propositional logic is a declarative language 
because its semantics is based on a truth relation between sentences and possible worlds.   It also 
has sufficient expressive power to deal with partial information, using disjunction and negation.  
 Propo sitional logic has a third COMPOSITIONALITY property that is desirable in representation 
languages, namely, compositionality. In a compositional language, the meaning of a sentence is a 
function of the meaning of its parts. For example, the meaning of “S1, 4∧ S1,2” is related to the 
meanings of “S1,4” and “S1,2.  
Drawbacks of  Propositional Logic   Propositional logic lacks the expressive power to concisely 
describe an environment with many objects.   
For example, we were forced to write a separate rule about breezes and pits for each square, such 
as B1,1⇔ (P1,2 ∨ P2,1) .  Artificial Intelligence  Page 71 
  In English,  it seems easy enough to say, “Squares adjacent to pits are breezy.”   The syntax and 
semantics of English somehow make it possible to describe the environment concisely  
SYNTAX AND SEMANTICS OF FIRST -ORDER LOGIC  
 Models for first -order logic :  
The models of a logical language are the formal structures that constitute the possible worlds under 
consideration. Each model links the vocabulary of the logical sentences to elemen ts of the possible 
world, so that the truth of any sentence can be determined.  Thus, models for propositional logic 
link proposition symbols to predefined truth values.  Models for first -order logic have objects. The 
domain of a model is the set of object s or domain elements it contains. The domain is required to be 
nonempty —every possible world must contain at least one object.  
 A relation is just the set of tuples of objects that are related.   Unary Relation: Relations relates 
to single Object  Binary Relation: Relation Relates to multiple objects Certain kinds of relationships 
are best considered as functions, in that a given object must be related to exactly one object.  
 For Example:  
Richard the Lionheart, King of England from 1189 to 1199;  His younger brother, the evil King John, 
who ruled from 1199 to 1215;  the left legs of Richard and John;  crown  
 
Artificial Intelligence  Page 72 
  
Unary Relation : John is a king Binary Relation :crown is on head of john  , Richard is brother ofjohn 
The  unary "left leg" function incl udes the following mappings: (Richard the Lionheart) ->Richard's 
left leg (King John) ->Johns left Leg  
Symbols and interpretations  
 Symbols are the basic syntactic elements of first -order logic.  Symbols stand for objects, 
relations, and functions.  
 The symbols are of three kinds:   Constant symbols which stand for objects; Example:  John, 
Richard  Predicate symbols, which stand for relations;  Example:  OnHead, Person, King, and Crown 
 Function symbols, which stand for functions.  Example:  left leg  
 Symbols will begin with uppercase letters.  
Interpretation The semantics must relate sentences to models in order to determine truth. For this 
to happen, we need an interpretation that specifies exactly which objects, relations and functions 
are referred to by the constant, predicate, and function symbols.  
 For Example:   
 Richard refers to Richard the Lionheart and John refers to the evil king John.  Brother refers to 
the brotherhood relation  OnHead refers to the "on head relation that holds between the crown 
and King John;   Person, King, and Crown refer to the sets of objects that are persons, kings, and 
crowns.  LeftLeg refers to the "left leg" function,   
 The truth of any sentence is determined by a model and an interpretation for the sentence' s 
symbols. Therefore, entailment, validity, and so on are defined in terms of all possiblemodels and all 
possible interpretations. The number of domain elements in each model may be unbounded -for 
example, the domain elements may be integers or real numbers . Hence, the number of possible 
models is anbounded, as is the number of interpretations.  
 Term  Artificial Intelligence  Page 73 
  A term is a logical expression that refers to an object.  Constant symbols are therefore terms. 
Complex Terms A complex term is just a complicated kind of na me. A complex term is formed by a 
function symbol followed by a parenthesized list of terms as arguments to the function symbol For 
example: "King John's left leg"  Instead of using a constant symbol, we use LeftLeg(John). The formal 
semantics of terms :  
Consider a term f (tl,. . . , t,). The function symbol  frefers to some function in the model (F); the 
argument terms refer to objects in the domain (call them d1….dn); and the term as a whole refers to 
the object that is the value of the function Fapplied to dl, . . . , d,. For  example,: the LeftLeg 
function symbol refers to the function “ (King John) -+ John's left leg”  and John refers to King John, 
then LeftLeg(John) refers to King John's left leg. In this way, the interpretation fixes the referent of 
every term.  
 Atomic sentences  
 An atomic sentence is formed from a predicate symbol followed by a parenthesized list of terms: 
For Example:  Brother(Richard, John).  
 Atomic  sentences can have complex terms as arguments.  For Example: Married (Father(Rich ard), 
Mother( John)).  
 An atomic sentence is true in a given model, under a given interpretation, if the relation referred to 
by the predicate symbol holds among the objects referred to by the arguments  
 Complex sentences Complex sentences can be constru cted using logical Connectives, just as in 
propositional calculus.  For Example:   Artificial Intelligence  Page 74 
  
Thus, the sentence says, ―For all x, if x is a king, then x is a person.‖  The symbol x is called 
a variable. Variables are lowercase letters.  A variable is a term all by  itself, and can also 
serve as the argument of a function A term with no variables is called a ground term.  
 Assume we can extend the interpretation in different ways: x→ Richard the Lionheart, x→ King 
John, x→ Richard’s left leg, x→ John’s left leg, x→ t he crown  
 The universally quantified sentence ∀x King(x) ⇒Person(x) is true in the original model if the 
sentence King(x) ⇒Person(x) is true under each of the five extended interpretations. That is, the 
universally quantified sentence is equivalent to ass erting the following five sentences:  
 Richard the Lionheart is a king ⇒Richard the Lionheart is a person. King John is a king ⇒King John is 
a person. Richard’s left leg is a king ⇒Richard’s left leg is a person. John’s left leg is a king ⇒John’s 
left leg is a person. The crown is a king ⇒the crown is a person.  
Existential quantification ( ∃)  
Universal quantification makes statements about every object. Similarly, we can make a statement 
about some object in the universe without naming it, by using an existential quantifier.  
Artificial Intelligence  Page 75 
  “The sentence ∃x P says that P is true for at least one object x. Mo re precisely, ∃x P is true in a given 
model if P is true in at least one extended interpretationthat assigns x to a domain element.” ∃x is 
pronounced “There exists an x such that . . .” or “For some x . . .”.  
 For example, that King John has a crown on hi s head, we write ∃xCrown(x) ∧OnHead(x, John)   
Given assertions:   
 Richard the Lionheart is a crown ∧Richard the Lionheart is on John’s head; King John is a crown 
∧King John is on John’s head; Richard’s left leg is a crown ∧Richard’s left leg is on John’s  head; John’s 
left leg is a crown ∧John’s left leg is on John’s head; The crown is a crown ∧the crown is on John’s 
head. The fifth assertion is true in the model, so the original existentially quantified sentence is true 
in the model. Just as ⇒appears to b e the natural connective to use with ∀, ∧is the natural 
connective to use with ∃.  
 Nested quantifiers  
 One can express more complex sentences using multiple quantifiers.  
 For example, “Brothers are siblings” can be written as ∀x∀y Brother (x, y) ⇒Siblin g(x, y). 
Consecutive quantifiers of the same type can be written as one quantifier with several variables.  
 For example, to say that siblinghood is a symmetric relationship,   
 we can write ∀x, y Sibling(x, y) ⇔Sibling(y, x).  
 In other cases we will have mixtures.  
 
For example:  1.  “Everybody loves somebody” means that for every person, there is someone that 
person loves: ∀x∃y Loves(x, y) . 2. On the other hand, to say “There is someone who is loved by 
everyone,” we write ∃y∀x Loves(x, y) .  
 Connections  between ∀and ∃ 
Universal and Existential quantifiers are actually intimately connected with each other, through 
negation.  Artificial Intelligence  Page 76 
 Example assertions:  1. “ Everyone dislikes medicine”  is the same as asserting “ there does not exist 
someone who likes medicine” ,  and vice versa: “ ∀x ￢Likes(x, medicine)” is equivalent to “ ￢∃x 
Likes(x, medicine)”. 2.  “Everyone likes ice cream” means that “ there is no one who does not like ice 
cream” : ∀xLikes(x, IceCream) is equivalent to ￢∃x ￢Likes(x, IceCream) .  
Because ∀is really a conjunction over the universe of objects and ∃is a disjunction that they obey De 
Morgan’s rules. The De Morgan rules for quantified and unquantified sentences are as follows:  
 
Equality  
First -order logic includes one more way to make atomic se ntences, other than using a predicateand 
terms .We can use the equality symbol to signify that two terms refer to the same object.   
For example,  
“Father(John) =Henry” says that the object referred to by Father (John) and the object referred to by 
Henry a re the same.   
 Because an interpretation fixes the referent of any term, determining the truth of an equality 
sentence is simply a matter of seeing that the referents of the two terms are the same object.The 
equality symbol can be used to state facts abou t a given function.It can also be used with negation 
to insist that two terms are not the same object.   
 For example,    
 “Richard has at least two brothers” can be written as, ∃x, y Brother (x,Richard ) ∧Brother (y,Richard 
) ∧￢(x=y) .  
Artificial Intelligence  Page 77 
  The sentence  
∃x, y Brother (x,Richard ) ∧Brother (y,Richard ) does not have the intended meaning.  In particular, it 
is true only in the model where Richard has only one brother considering the extended 
interpretation in which both x and y are assigned to King John. The a ddition of ￢(x=y) rules out such 
models.  
 
USING FIRST ORDER LOGIC  Assertions and queries in first -order logic  
 Assertions:  
 Sentences are added to a knowledge base using TELL, exactly as in propositional logic. Such 
sentences are called assertions.   
 For example,   
 John is a king, TELL (KB, King (John)). Richard is a person. TELL (KB, Person (Richard)). All kings are 
persons: TELL (KB, ∀x King(x) ⇒Person(x)).  
 Asking Queries:   
 We can ask questions of the knowledge base using ASK. Questions asked w ith ASK are called 
queries or goals.   
Artificial Intelligence  Page 78 
  For example,  
 ASK (KB, King (John))   returns true.   
 Anyquery that is logically entailed by the knowledge base should be answered affirmatively.   
 Forexample, given the two preceding assertions, the query:  
 “ASK  (KB, Person (John))” should also return true.  
 Substitution or binding list  
 We can ask quantified queries, such as ASK (KB, ∃x Person(x)) .  
 The answer is true, but this is perhaps not as helpful as we would like. It is rather like answering 
“Can you tell me the time?” with “Yes.”   
 If we want to know what value of x makes the sentence true, we will need a different function, 
ASKVARS, which we call with ASKVARS (KB, Person(x)) and which yields a stream of answers.  
  In this case there will be two ans wers: {x/John} and {x/Richard}. Such an answer is called a 
substitution or binding list.   
 ASKVARS is usually reserved for knowledge bases consisting solely of Horn clauses, because in such 
knowledge bases every way of making the query true will bind the variables to specific values.  
 The kinship domain  
 The objects in Kinship domain are people.   
 We have two unary predicates, Male and Female.   
 Kinship relations —parenthood, brotherhood, marriage, and so on —are represented by binary 
predicates: Parent,  Sibling, Brother,Sister,Child, Daughter, Son, Spouse, Wife, Husband, 
Grandparent,Grandchild, Cousin, Aunt, and Uncle.   
 We use functions for Mother and Father, because every person has exactly one of each of these.  
 We can represent each function and pr edicate, writing down what we know in termsof the other 
symbols.   Artificial Intelligence  Page 79 
  For example: - 1. one’s mother is one’s female parent: ∀m, c Mother (c)=m ⇔Female(m) ∧Parent(m, 
c) .  
2. One’s husband is one’s male spouse:  ∀w, h Husband(h,w) ⇔Male(h) ∧Spouse(h,w) .  
3. Male and female are disjoint categories: ∀xMale(x) ⇔￢Female(x) .  
4. Parent and child are inverse relations: ∀p, c Parent(p, c) ⇔Child (c, p) .  
5. A grandparent is a parent of one’s parent: ∀g, c Grandparent (g, c) ⇔∃p Parent(g, p) ∧Parent(p, c) 
.  
6. A sibling is another child of one’s parents: ∀x, y Sibling(x, y) ⇔x _= y ∧∃p Parent(p, x) ∧Parent(p, 
y) .  
Axioms:  
Each of these sentences can be viewed as an axiom of the kinship domain. Axioms are commonly 
associated with purely mathematical domains.   They provide the basic factual information from 
which useful conclusions can be derived.   
Kinship axioms are also definitions; they have the form ∀x, y P(x, y) ⇔. . ..  
The axioms define the Mother function, Husband, Male, Parent, Grandparent, and Sibli ng predicates 
in terms of other predicates.   
Our definitions “bottom out” at a basic set of predicates (Child, Spouse, and Female) in terms of 
which the others are ultimately defined. This is a natural way in which to build up the representation 
of a doma in, and it is analogous to the way in which software packages are built up by successive 
definitions of subroutines from primitive library functions.   
Theorems:   
Not all logical sentences about a domain are axioms.  Some are theorems —that is, they are en tailed 
by the axioms.   
For example, consider the assertion that siblinghood is symmetric: ∀x, y Sibling(x, y) ⇔Sibling(y, x) .  Artificial Intelligence  Page 80 
 It is a theorem that follows logically from the axiom that defines siblinghood.  If we ASK the 
knowledge base this sentence, it  should return true. From a purely logical point of view, a 
knowledge base need contain only axioms and no theorems, because the theorems do not increase 
the set of conclusions that follow from the knowledge base.  From a practical point of view, 
theorems are essential to reduce the computational cost of deriving new sentences. Without them, 
a reasoning system has to start from first principles every time.  
Axioms :Axioms without Definition  
Not all axioms are definitions. Some provide more general informat ion about certain predicates 
without constituting a definition. Indeed, some predicates have no complete definition because we 
do not know enough to characterize them fully.   
For example, there is no obvious definitive way to complete the sentence  
∀xPers on(x) ⇔. . .  
Fortunately, first -order logic allows us to make use of the Person predicate without completely 
defining it. Instead, we can write partial specifications of properties that every person has and 
properties that make something a person:  
∀xPers on(x) ⇒. . . ∀x . . . ⇒Person(x) .  
Axioms can also be “just plain facts,” such as Male (Jim) and Spouse (Jim, Laura).Such facts form the 
descriptions of specific problem instances, enabling specific questions to be answered. The answers 
to these questions  will then be theorems that follow from the axioms  
Numbers, sets, and lists  
Number theory  
Numbers are perhaps the most vivid example of how a large theory can be built up from NATURAL 
NUMBERS a tiny kernel of axioms. We describe here the theory of natura l numbers or non -negative 
integers. We need:  
 predicate NatNum that will be true of natural numbers;   one PEANO AXIOMS constant symbol, 0;  
 One function symbol, S (successor).   The Peano axioms define natural numbers and addition.  Artificial Intelligence  Page 81 
 Natural numbers are defined recursively:  NatNum(0) .  ∀n NatNum(n) ⇒ NatNum(S(n)) .   
That is, 0 is a natural number, and for every object n, if n is a natural number, then S(n) is a natural 
number.   
So the natural numbers are 0, S(0), S(S(0)), and so on.  We also need axioms to constrain the 
successor function:  ∀n 0 != S(n) .  ∀m, n m != n ⇒ S(m) != S(n) .  
 Now we can define addition in terms of the successor function:  ∀m NatNum(m) ⇒ + (0, m) = m .  
∀m, n NatNum(m) ∧ NatNum(n) ⇒ + (S(m), n) = S(+(m, n))  
The first of  these axioms says that adding 0 to any natural number m gives m itself. Addition is 
represented using the binary function symbol “+” in the term + (m, 0);   
To make our sentences about numbers easier to read, we allow the use of infix notation.  We can 
also write S(n) as n + 1, so the second axiom becomes :  
∀m, n NatNum (m) ∧ NatNum(n) ⇒ (m + 1) + n = (m + n)+1 .  
This axiom reduces addition to repeated application of the successor function. Once we have 
addition, it is straightforward to define multiplication as repeated addition, exponentiation as 
repeated multiplication, integer division and remainders, prime num bers, and so on. Thus, the 
whole of number theory (including cryptography) can be built up from one constant, one function, 
one predicate and four axioms.  
Sets  
The domain of sets is also fundamental to mathematics as well as to commonsense reasoning. Set s 
can be represented as individualsets, including empty sets.   
Sets can be built up by:   adding an element to a set or   Taking the union or intersection of two 
sets.   
Operations that can be performed on sets are:  To know whether an element is a mem ber of a set  
 Distinguish sets from objects that are not sets.  
Vocabulary of set theory:   Artificial Intelligence  Page 82 
 The empty set is a constant written as { }.  There is one unary predicate, Set, which is true of sets.  
The binary predicates are   
 x∈ s (x is a member of set s )   s1 ⊆ s2 (set s1 is a subset, not necessarily proper, of set s2).   
The binary functions are   
 s1 ∩ s2 (the intersection of two sets),   s1 ∪ s2 (the union of two sets), and   ,x|s- (the set 
resulting from adjoining element x to set s).   
 
One poss ible set of axioms is as follows:  
  The only sets are the empty set and those made by adjoining something to a set: ∀sSet(s) ⇔(s={}) 
∨(∃x, s2 Set(s2) ∧s={x|s2}) .  The empty set has no elements adjoined into it. In other words, there 
is no way to decomp ose {} into a smaller set and an element: ￢∃x, s {x|s}={} .  Adjoining an 
element already in the set has no effect: ∀x, s x∈s ⇔s={x|s} .  The only members of a set are the 
elements that were adjoined into it. We express this recursively, saying that x is a member of s if and 
only if s is equal to some set s2 adjoined with some element y, where either y is the same as x or x is 
a member of s2: ∀x, s x∈s ⇔∃y, s2 (s={y|s2} ∧(x=y ∨x∈s2))   A set is a subset of another set if and 
only if all of the first set’s members are members of the second set: ∀s1, s2 s1 ⊆s2 ⇔(∀x x∈s1 
⇒x∈s2)   Two sets are equal if and only if each is a subset of the othe r: ∀s1, s2 (s1 =s2) ⇔(s1 ⊆s2 
∧s2 ⊆s1)   
 
 An object is in the intersection of two sets if and only if it is a member of both sets: ∀x, s1, s2 
x∈(s1 ∩ s2) ⇔(x∈s1 ∧x∈s2)   An object is in the union of two sets if and only if it is a member of 
either set: ∀x, s1, s2 x ∈(s1 ∪s2) ⇔(x∈s1 ∨x∈s2)   
Lists : are similar to sets. The differences are that lists are ordered and the same element canappear 
more than once in a list. We can use the vocabulary of Lisp for lists:   
 Nil is the constant list with no elements ;   Cons, Append, First, and Rest are functions;  Find is 
the predicate that does for lists what Member does for sets.   List? is a predicate that is true only of 
lists.   The empty list is * +.   The term Cons(x, y), where y is a nonempty list, is wr itten [x|y].   The Artificial Intelligence  Page 83 
 term Cons(x, Nil) (i.e., the list containing the element x) is written as [x].   A list of several elements, 
such as [A,B,C], corresponds to the nested term   Cons(A, Cons(B, Cons(C, Nil))).  
The wumpus world  
Agents Percepts and Acti ons  
The wumpus agent receives a percept vector with five elements. The corresponding first -order 
sentence stored in the knowledge base must include both the percept and the time at which it 
occurred; otherwise, the agent will get confused about when it sa w what.We use integers for time 
steps. A typical percept sentence would be  
Percept ([Stench, Breeze, Glitter,None, None], 5).  
 
Here, Percept is a binary predicate, and Stench and so on are constants placed in a list. The actions 
in the wumpus world can b e represented by logical terms:  
Turn (Right), Turn (Left), Forward,Shoot,Grab, Climb.  
To determine which is best, the agent program executes the query:  
ASKVARS ( ∃a BestAction (a, 5)), which returns a binding list such as {a/Grab}.   
The agent program ca n then return Grab as the action to take.  
The raw percept data implies certain facts about the current state.   
For example: ∀t, s, g, m, c Percept ([s, Breeze, g,m, c], t) ⇒Breeze(t) , ∀t, s, b, m, c Percept ([s, b, 
Glitter,m, c], t) ⇒Glitter (t) ,  
 
UNIT III – Knowledge and Reasoning  
 These rules exhibit a trivial form of the reasoning process called perception.  
 Simple ―reflex‖ behavior can also be implemented by quantified implication sentences.  Artificial Intelligence  Page 84 
  For example, we have ∀tGlitter (t) ⇒BestAction(Grab , t) .  
 Given the percept and rules from the preceding paragraphs, this would yield the desired 
conclusion Best Action (Grab, 5) —that is, Grab is the right thing to do.  
 Environment Representation  
 Objects are squares, pits, and the wumpus.  Each square  could be named —Square1,2and so 
on—but then the fact that Square1,2and Square1,3 are adjacent would have to be an ―extra‖ 
fact, and this needs  one suchfact for each pair of squares. It is better to use a complex term in 
which the row and columnappear as i ntegers;   
 For example, we can simply use the list term [1, 2].   
 Adjacency of any two squares can be defined as:  
∀x, y, a, b Adjacent ([x, y], [a, b]) ⇔ (x = a ∧(y = b − 1 ∨y = b + 1)) ∨(y = b ∧(x = a − 1 ∨x 
= a + 1)).  
 Each pit need not be distinguis hed with each other. The unary predicate Pit is true of squares 
containing pits.   
 Since there is exactly one wumpus, a constant Wumpus is just as good as a unary predicate. 
The agent’s location changes over time, so we write At (Agent, s, t) to mean that  theagent is 
at square s at time t.   
 To specify the Wumpus location (for example) at [2, 2] we can write ∀t At (Wumpus, [2, 2], 
t).  
 Objects can only be at one location at a time: ∀x, s1, s2, t At(x, s1, t) ∧At(x, s2, t) ⇒s1 = s2 .  
 Given its current l ocation, the agent can infer properties of the square from properties of its 
current percept.   
 For example, if the agent is at a square and perceives a breeze, then that square is breezy:  
∀s, t At(Agent, s, t) ∧Breeze(t) ⇒Breezy(s) .  Artificial Intelligence  Page 85 
  It is useful to k now that a square is breezy because we know that the pits cannot move about.  
Breezy has no time argument.  
Having discovered which places are breezy (or smelly) and, very importantly, not breezy (or 
not smelly), the agent can deduce where the pits =e (and  where the wumpus is).   
There are two kinds of synchronic rules that could allow such deductions:  
Diagnostic rules:  
Diagnostic rules lead from observed effects to hidden causes.  For finding pits, the obvious 
diagnostic rules say that if a square is bre ezy, some adjacent square must contain a pit, or  
∀s Breezy(s) ⇒∃r Adjacent (r, s) ∧Pit(r) ,  
and that if a square is not breezy, no adjacent square contains a pit:  ∀s￢Breezy (s) ⇒￢∃r 
Adjacent (r, s) ∧ Pit (,r) .Combining these two, we obtain the biconditional sentence ∀s 
Breezy ( s ) ⇔∃r Adjacent(r, s) ∧  Pit (r) .  
Causal rules:  
Causal rules reflect the assumed direction of causality in the world: some hidden property of 
the world causes certain perce pts to be generated.  For example, a pit causes all adjacent 
squares to be breezy:  
 and if all squares adjacent to a given square are pitless, the square will not be breezy: ∀s[∀r 
Adjacent (r, s) ⇒￢Pit (r)] ⇒￢Breezy ( s ) .  
 
It  is possible to show that these two sentences together are logically equivalent to the 
biconditional sentence ― ∀s Breezy ( s ) ⇔∃r Adjacent(r, s) ∧  Pit (r)‖  .  
 The biconditional itself can also be thought of as causal, because it states how the truth value 
of Breezy is generated  from the world state.  Artificial Intelligence  Page 86 
  Systems that reason with causal rules are called model -based reasoning systems, because the 
causal rules form a model of how the environment operates.  
 Whichever kind of representation the agent uses, ifthe axioms correctly and co mpletely 
describe the way the world works and the way that percepts are produced, then any complete 
logical inference procedure will infer the strongest possible description of the world state, 
given the available percepts. Thus, the agent designer can con centrate on getting the 
knowledgeright, without worrying too much about the processes of deduction.  
 Inference in First -Order Logic  
Propositional Vs First Order Inference  
Earlier inference in first order logic is performed with Propositionalization which is a process of 
converting the Knowledgebase present in First Order logic into Propositional logic and on that 
using any inference mechanisms of propositional logic are used to check inference.  
Inference rules for quantifiers:             
There are some In ference rules that can be applied to sentences with quantifiers to obtain 
sentences without quantifiers . These rules will lead us to make the conversion.  
Universal Instantiation (UI):  
The rule says that we can infer any sentence obtained by substituting a ground term (a term 
without variables) for the variable. Let SUBST (θ ) denote the result of applying the substitution 
θto the sentence a. Then the rule is written  
 
For any variable v and ground term g. 
For example, there is a sentence in knowledge base stating that all greedy kings are Evils  
 
For the variable x, with the substitutions like {x/John},{x/Richard}the following sentences can 
be inferred.  
 
Artificial Intelligence  Page 87 
 Thus a universally quantified sentence can be replaced by the set of all possible instantiations.  
Existential Instantiation (EI) :  
The existential sentence says there is some object satisfying a condition, and the instantiation 
process is just giving a name to that object, that name must not already belong to another object. 
This new name is called a Skolem constant. Existential Instantiation is a special case of a more 
general process called “skolemization”.  
 For any sentence a, variable v, and constant symbol k that does not appear elsewhere in the 
knowledge base,  
 
For example, from the sentence   
 
So, we can infer the sentence  
 
As long as C1 does not appear elsewhere in the knowledge base. Thus an existentially quantified 
sentence can be replaced by one instantiation  
 Elimination of Universal and Existential quantifiers should give new knowledge ba se which can 
be shown to be inferentially equivalent to old in the sense that it is satisfiable exactly when the 
original knowledge base is satisfiable.  
 
Reduction to propositional inference:  
Once we have rules for inferring non quantified sentences from quantified sentences, it becomes 
possible to reduce first -order inference to propositional inference. For example, suppose our 
knowledge base contains just the sentences  
 
Artificial Intelligence  Page 88 
 Then we apply UI to the first sentence using all possible ground term  substitutions  from the 
vocabulary of the knowledge base -in this case, {xl John) and {x/Richard). We obtain  
 
We discard the universally quantified sentence. Now, the knowledge base is essentially 
propositional if we view the ground atomic sentences -King (John), Greedy (John), and Brother 
(Richard , John ) as proposition symbols. Therefore, we can apply any of the complete 
propositional algorithms to obtain conclusions such as Evil (John).  
 
Disadvantage:  
If the knowledge base includes a function symbol, the set of possible  ground term substitutions is 
infinite. Propositional algorithms will have difficulty with an infinitely large set of sentences.  
NOTE:  
Entailment for first -order logic is semi decidable  which means algorithms exist that say yes to 
every entailed sentence, but no algorithm exists that also says no to every non entailed sentence  
 
2. Unification and Lifting  
 
Consider the above discussed example, if we add Siblings (Peter, Sharon) to the knowledge base 
then it will be  
 
Removing Universal Quantifier will add new sentences to the knowledge base which are not 
necessary for the query Evil (John)?  
 
Hence we need to teach the computer to make better inferences. For this purpose Inference rules 
were used.  
Artificial Intelligence  Page 89 
  
First Order Inferen ce Rule:  
The key advantage of lifted inference rules over propositionalization  is that they make only those 
substitutions which are required to allow particular inferences to proceed.  
 
Generalized Modus Ponens:  
If there is some substitution θ that makes the premise of the implication identical to sentences 
already in the knowledge base, then we can assert the conclusion of the implication, after 
applying θ. This inference process can be captured as a single inference rule called Generalized 
Modus Ponens  which is a lifted version of Modus Ponens -it raises Modus Ponens from 
propositional to first -order logic  
For atomic sentences pi, pi ', and q, where there is a substitution θ such that SUBST( θ , pi ) = 
SUBST(θ , pi '), for all i,  
 
p1 ', p2 ', … , pn ', (p1 ∧ p2 ∧ … ∧ pn ⇒ q) 
 
SUBST (θ, q)  
There are N + 1 premises to this rule, N atomic sentences + one implication.  
Applying SUBST (θ, q) yields the conclusion we seek. It is a sound inference rule.  
Suppose that instead of knowing Greedy (John) in our example we know that everyone is 
greedy:  
∀y Greedy(y)  
We would conclude that Evil(John).  
Applying the substitution {x/John, y / John) to the implication premises King ( x ) and Greedy ( 
x ) and the knowledge base sentences King(John) and Greedy(y) will make them identical. Thus, 
we can infer the conclusion of the implication.  
 
 For our example,  
Artificial Intelligence  Page 90 
  
Unification:  
It is the process used to find substitutions that make different logical expressions look identical.  
Unification  is a key component of all first -order Inference algorithms.  
 UNIFY (p, q) = θ where SUBST (θ, p) = SUBST (θ, q) θ is our unifier value (if one exists).  
Ex:  ―Who does John know?‖  
UNIFY (Knows (John, x), Knows (John, Jane)) = {x/ Jane}.  
UNIFY (Knows (Jo hn, x), Knows (y, Bill)) = {x/Bill, y/ John}.  
UNIFY (Knows (John, x), Knows (y, Mother(y))) = {x/Bill, y/ John}  
UNIFY (Knows (John, x), Knows (x, Elizabeth)) = FAIL  
 
 The last unification fails because both use the same variable, X.  X can’t equal both John 
and Elizabeth. To avoid this change the variable X to Y (or any other value) in Knows(X, 
Elizabeth)    
Knows(X, Elizabeth) → Knows(Y, Elizabeth)  
 
           Still me ans the same. This is called standardizing apart.  
 sometimes it is possible for more than one unifier returned:  
UNIFY (Knows (John, x), Knows(y, z)) =???  
 
This can return two possible unifications: {y/ John, x/ z} which means   Knows (John, z) OR {y/ 
John,  x/ John, z/ John}. For each unifiable pair of expressions there is a single most general 
unifier (MGU) , In this case it is {y/ John, x/z) . 
 
 An algorithm for computing most general unifiers is shown below.  
Artificial Intelligence  Page 91 
  
Figure 2.1 The unification algorithm. The algorithm works by comparing the structures of the 
inputs, element by element. The substitution 0 that is the argument to UNIFY is built up along the 
way and is used to make sure that later comparisons are consistent with bindings that were 
established earlier. In a compound expression, such as F (A, B), the function OP picks out the 
function symbol F and the function ARCS picks ou t the argument list (A, B).  
 
The process is very simple: recursively explore the two expressions simultaneously "side by 
side," building up a unifier along the way, but failing if two corresponding points in the 
structures do not match. Occur check  step m akes sure same variable isn’t used twice.  
 
Storage and retrieval  
 STORE(s) stores a sentence s into the knowledge base  
Artificial Intelligence  Page 92 
  FETCH(s) returns all unifiers such that the query q unifies with some sentence in the 
knowledge base.  
Easy way to implement these functio ns is Store all sentences in a long list, browse list one 
sentence at a time with UNIFY on an ASK query. But this is inefficient.  
To make FETCH more efficient by ensuring that unifications are attempted only with sentences 
that have some chance of unifying . (i.e. Knows(John, x) vs. Brother(Richard, John) are not 
compatible for unification)  
 To avoid this, a simple scheme called predicate indexing puts all the Knows facts in one 
bucket and all the Brother facts in another.  
 The buckets can be stored in a hash table for efficient access. Predicate indexing is useful 
when there are many predicate symbols but only a few clauses for each symbol.  
 
But if we have many clauses for a given predicate symbol, facts can be stored under multiple 
index keys.  
For the fact Employs (AIMA.org, Richard), the queries are  
Employs (A IMA. org, Richard) Does AIMA.org employ Richard?  
Employs (x, Richard) who employs Richard?  
Employs (AIMA.org, y) whom does AIMA.org employ?  
Employs Y(x), who employs whom?  
 
We can arrange this into a subsumption lattice, as shown below . 
 
Figure 2.2 (a) The subsumption lattice whose lowest node is the sentence Employs (AIMA.org, 
Richard). (b) The subsumption lattice for the sentence Employs (John, John).  
 
A subsumption lattice has the following properties:  
 child of any node obtained from its parents by one substitution  
 the ―highest‖ common descendant of any two nodes is the result of applying their most 
general unifier  
Artificial Intelligence  Page 93 
  predicate with n arguments contains  O(2n ) nodes (in our example, we have two 
arguments, so our lattice has four nodes)  
 Repeated constants = slightly different lattice.  
 
3. Forward Chaining  
 
First -Order Definite Clauses:  
A definite clause either is atomic or is an implication whose antecedent is a conjunction of 
positive literals and whose consequent is a single positive literal. The following are first -order 
definite clauses:  
 
Unlike propositional literals, first -order literals can include variables, in which case those 
variables a re assumed to be universally quantified.  
Consider the following problem;  
“The law says that it is a crime for an American to sell weapons to hostile nations. The 
country Nono, an enemy of America, has some missiles, and all of its missiles were sold to it 
by Colonel West, who is American.”  
We will represent the facts as first -order definite clauses  
". . . It is a crime for an American to sell weapons to hostile nations":  
 
---------  (1) 
"Nono . . . has some missiles." The sentence 3 x Owns (Nono, .rc) A Missile (x) is transformed 
into two definite clauses by Existential Elimination, introducing a new constant M1: 
Owns (Nono, M1) -----------------  (2) 
Missile (Ml) -------------------------  (3) 
"All of its missiles were sold to it by Colonel West":  
Missil e (x) A Owns (Nono, x) =>Sells (West, z, Nono) -----------------  (4) 
We will also need to know that missiles are weapons:  
                                    Missile (x) =>Weapon (x) ----------  (5) 
Artificial Intelligence  Page 94 
 We must know that an enemy of America counts as "hostile":  
Enemy (x, America) =>Hostile(x) -----------  (6) 
"West, who is American":  
American (West) ---------------  (7) 
"The country Nono, an enemy of America ":  
      Enemy (Nono, America) ------------  (8) 
 
A simple forward -chaining algorithm:  
 Starting from the kno wn facts, it triggers all the rules whose premises are satisfied, 
adding their conclusions lo the known facts  
 The process repeats until the query is answered or no new facts are added. Notice that a 
fact is not "new" if it is just renaming of a known fact.  
 
We will use our crime problem to illustrate how FOL -FC-ASK works. The implication sentences 
are (1), (4), (5), and (6). Two iterations are required:  
 On the first iteration, rule (1) has unsatisfied premises.  
      Rule (4) is satisfied with {x/Ml), and Sells (West, M1, Nono) is added.  
      Rule (5) is satisfied with {x/M1) an d Weapon (M1) is added.  
      Rule (6) is satisfied with {x/Nono}, and Hostile (Nono) is added.  
 On the second iteration, rule (1) is satisfied with {x/West, Y/MI, z /Nono), and Criminal 
(West) is added.  
It is sound , because every inference is just an application of Generalized Modus Ponens, it is 
complete for definite clause knowledge bases; that is, it answers every query whose answers are 
entailed by any knowledge base of defini te clauses  Artificial Intelligence  Page 95 
  
Figure 3.1 A conceptually straightforward, but very inefficient, forward -chaining 
algorithm. On each iteration, it adds to KB all the atomic sentences that can be inferred 
in one step from the implication sentences and the atomic sentences al ready in KB. 
 
 
Figure 3.2 The proof tree generated by forward chaining on the crime example. The initial 
facts appear at the bottom level, facts inferred on the first iteration in the middle level, and 
facts inferred on the second iteration at the top level.  
 
Efficient forward chaining:  
The above given forward chaining algorithm was lack with efficiency due to the the three 
sources of complexities:  
 Pattern Matching  
Artificial Intelligence  Page 96 
  Rechecking of every rule on every iteration even a few additions are made to rules  
 Irrelevant facts  
 
1. Matching rules against known facts:  
For example, consider this rule,  
Missile(x) A Owns (Nono, x) =>Sells (West, x, Nono) . 
 
The algorithm will check all the objects owned by Nono in and then for each object, it could 
check whether it is a missile. This is the conjunct ordering problem:  
―Find an ordering to solve the conjuncts of the rule premise so that the total cost is minimized‖. 
The most constrained variable heuristic used for CSPs would suggest ordering the conjuncts to 
look for missiles first if there are fewer missiles than object s that are owned by Nono.  
The connection between pattern matching and constraint satisfaction is actually very close. We 
can view each conjunct as a constraint on the variables that it contains -for example, Missile(x) is 
a unary constraint on x. Extending this idea, we can express everyfinite -domain CSP as a single 
definite clause together with some associated ground facts. Matching a definite clause against a 
set of facts is NP -hard 
 
2. Incremental forward chaining:  
On the second iteration, the rule                       Missile (x) =>Weapon (x)  
Matches against Missile (M1) (again), and of course the conclusion Weapon(x/M1) is already 
known so nothing happens. Such redundant rule matching can be avoided if we make the 
following observation:  
―Every new fact  inferred on iteration t must be derived from at leastone new fact inferred on 
iteration t – 1‖. 
This observation leads naturally to an incremental forward chaining algorithm where, at iteration 
t, we check a rule only if its premise includes a conjunct p, that unifies with a fact p: newly 
inferred at iteration t - 1. The rule matching step then fixes p, to match with p’, but allows the 
other conjuncts of the rule to match with facts from any previous iteration.  
 
3. Irrelevant facts:  Artificial Intelligence  Page 97 
  One way to avoid drawing irrelevant conclusions is to use backward chaining.  
 Another solution is to restrict forward chaining to a selected subset of rules  
  A third approach, is to rewrite the rule set, using information from the goal.so that only 
relevant variable binding s-those belonging to a so -called magic set-are considered during 
forward inference.  
 For example, if the goal is Criminal (West), the rule that concludes Criminal (x) will be 
rewritten to include an extra conjunct that constrains the value of x: 
 
Magic(x) A American(z) A Weapon(y) A Sells(x, y, z) A Hostile(z) =>Criminal(x )  
 
The fact Magic (West) is also added to the KB. In this way, even if the knowledge base contains 
facts about millions of Americans, only Colonel West will be considered during the forward 
inference process.  
 
4. Backward Chaining  
This algorithm work backward from the goal, chaining through rules to find known facts that 
support the proof. It is called with a list of goals containing the original query, and returns the set 
of all substitutions satisfying the query. The algorithm tak es the first goal in the list and finds 
every clause in the knowledge base whose head, unifies with the goal. Each such clause creates a 
new recursive call in which  body, of the clause is added to the goal stack .Remember that facts 
are clauses with a hea d but no body, so when a goal unifies with a known fact, no new sub goals 
are added to the stack and the goal is solved. The algorithm for backward chaining and proof tree 
for finding criminal (West) using backward chaining are given below.   Artificial Intelligence  Page 98 
  
Figure 4.1 A simple backward -chaining algorithm.  
. 
 
 
Figure 4.2 Proof tree constructed by backward chaining to prove that West is a criminal. The 
tree should be read depth first, left to right. To prove Criminal (West), we have to prove the four 
conjuncts below it. Some of these are in the knowledge base, and others require further 
backward chaining. Bindings for each successful unification are shown next to the 
corresponding sub goal. Note that once one sub goal in a con junction succeeds, its substitution 
is applied to subsequent sub goals.  
Logic programming:  
 Prolog is by far the most widely used logic programming language.  
 Prolog programs are sets of definite clauses written in a notation different from standard 
first-order logic.  
Artificial Intelligence  Page 99 
  Prolog uses uppercase letters for variables and lowercase for constants.  
 Clauses are written with the head preceding the body; " : -" is used for left implication, 
commas separate literals in the body, and a period marks the end of a sentenc e 
 
Prolog includes "syntactic sugar" for list notation and arithmetic. Prolog program for append (X, 
Y, Z), which succeeds if list Z is the result of appending lists x and Y  
 
 
For example, we can ask the query append (A, B, [1, 2]): what two lists can be  appended to give 
[1, 2]? We get back the solutions  
 
 The execution of Prolog programs is done via depth -first backward chaining  
 Prolog allows a form of negation called negation as failure. A negated goal not P is 
considered proved if the system fails to p rove p. Thus, the sentence  
    Alive (X) : - not dead(X)   can be read as "Everyone is alive if not        provably dead."  
 Prolog has an equality operator, =, but it lacks the full power of logical equality. An 
equality goal succeeds if the two terms are unifiable and fails otherwise. So X+Y=2+3 
succeeds with x bound to 2 and Y bound to 3, but Morningstar=evening star fails.  
 The occur check is omitted from Prolog's unification algorithm.  
 
Efficient implementation of logic programs:  
The execution of a Prolog program can happen in two modes: interpreted and compiled.  
 Interpretation essentially amounts to running the FOL -BC-ASK algorithm, with the 
program as the knowledge base. These are designed to maximize speed.  
First, instead of constructing the list of all possible answers for each sub goal before 
continuing to the next, Prolog interpreters generate one answer and a "promise" to generate 
the rest when the current answer has been fully explored. This promise is called a choice 
point. FOL -BC-ASK spends a good d eal of time in generating and composing substitutions 
Artificial Intelligence  Page 100  
 when a path in search fails. Prolog will backup to previous choice point and unbind some 
variables. This is called ―TRAIL‖. So, new variable is bound by UNIFY -VAR and it is 
pushed on to trail.  
 
 Prolog Compilers compile into an intermediate language i.e., Warren Abstract Machine 
or WAM named after David. H. D. Warren who is one of the implementers of first prolog 
compiler. So, WAM is an abstract instruction set that is suitable for prolog and can be 
either translated or interpreted into machine language.  
Continuations are used to implement choice point’scontinuation as packaging up a procedure 
and a list of arguments that together define what should be done next whenever the current goal 
succeeds.  
 Paralle lization can also provide substantial speedup. There are two principal sources of  
parallelism  
1. The first, called OR-parallelism, comes from the possibility of a goal unifying with 
many different clauses in the knowledge base. Each gives rise to an independ ent branch 
in the search space that can lead to a potential solution, and all such branches can be 
solved in parallel.  
2. The second, called AND -parallelism, comes from the possibility of solving each 
conjunct in the body of an implication in parallel. AND -parallelism is more difficult to 
achieve, because solutions for the whole conjunction require consistent bindings for all 
the variables.  
Redundant inference and infinite loops:  
Consider the following logic program that decides if a path exists between two po ints on a 
directed graph.  
 
A simple three -node graph, described by the facts link (a, b) and link (b, c)  
 
It generates the query path (a, c)  
Hence each node is connected to two random successors in the next layer.  
Artificial Intelligence  Page 101  
  
Figure 4.3 (a) Proof that a path exists from A to C. (b) Infinite proof tree generated when the 
clauses are in the "wrong" order.  
 
 
Constraint logic programming:  
The Constraint Satisfaction problem can be solved in prolog as same like backtracking 
algorithm.  
Because it works only for fin ite domain CSP’s in prolog terms there must be finite number of 
solutions for any goal with unbound variables.  
 
 
 If we have a query, triangle (3, 4, and 5) works fine but the query like, triangle (3, 4, Z) 
no solution.  
 The difficulty is variable in prolog  can be in one of two states i.e., Unbound or bound.  
 Binding a variable to a particular term can be viewed as an extreme form of constraint 
namely ―equality‖.CLP allows variables to be constrained rather than bound.  
The solution to triangle (3, 4, Z) is Constraint 7>=Z>=1.  
 
 
5. Resolution  
Artificial Intelligence  Page 102  
 As in the propositional case, first -order resolution requires that sentences be in conjunctive 
normal form (CNF) that is, a conjunction of clauses, where each clause is a disjunction 
ofliterals.  
 
 Literals can contain variables, which are assumed to be universally quantified.  Every sentence of 
first-order logic can be converted into an inferentially equivalent CNF sentence.  We will 
illustrate the procedure by translating the sentence  
 "Everyone who  loves all animals is loved by someone," or  
 
The steps are as follows:  
 Eliminate implications:  
 
 Move Negation inwards: In addition to the usual rules for negated connectives, we need 
rules for negated quantifiers. Thus, we have  
 
 
  Our sentence goes through the following transformations:  
 
 
 Standardize variables:  For sentences like which use the 
same variable name twice, change   the name of one of the variables. This avoids 
confusion later when we drop the quantifiers. Thus, we have  
 
 Skolemize: Skolemization is the process of removing existential quantifiers by 
elimination. Translate 3 x P(x) into P(A), where A is a new constant. If we apply this rule 
to our sample sentence, however, we obtain  
Artificial Intelligence  Page 103  
  
Which has the wrong meaning entirely: it says that everyone either fails to love a particular 
animal A or is loved by some particular entity B. In fact, our original sentence allows each person 
to fail to love a different animal or to be loved by a differe nt person.  
Thus, we want the Skolem entities to depend on x: 
 
 
Here F and G are Skolem functions. The general rule is that the arguments of the Skolem 
function are all the universally quantified variables in whose scope the existential quantifier 
appears.  
 
 Drop universal quantifiers: At this point, all remaining variables must be universally 
quantified. Moreover, the sentence is equivalent to one in which all the universal 
quantifiers have been moved to the left. We can therefore drop the universal quantifiers  
 
 Distribute V over A  
 
 
This is the CNF form of given sentence.  
The resolution inference rule:  
The resolution rule for first -order clauses is simply a lifted version of the propositional resolution 
rule. Propositional literals are complementary if one is the negation of the other; first -order 
literals are complementary if one unifies with the negati on of the other. Thus we have  
 
 
Where UNIFY (l i, m  j) == θ.  
For example, we can resolve the two clauses  
 
Artificial Intelligence  Page 104  
 By eliminating the complementary literals Loves (G(x), x) and ¬ Loves (u, v), with unifier  
θ = {u/G(x), v/x), to produce the resolvent clause  
 
Exam ple proofs:  
Resolution proves that KB /= a by proving KB A la unsatisfiable, i.e., by deriving the empty 
clause. The sentences in CNF are  
 
 
The resolution proof is shown in below figure;  
 
Figure 5.1 A resolution proof that West is a criminal.  
 
Notice the structure: single "spine" beginning with the goal clause, resolving against clauses 
from the knowledge base until the empty clause is generated. Backward chaining is really just a 
Artificial Intelligence  Page 105  
 special case of resolution with a particular control strategy to decide which resolution to perform 
next.  
 
 
UNIT -IV 
Planning Classical Planning :   AI as the study of rational action, which means that planning —devising a 
plan of action to achieve one’s goals —is a critical part of AI. We have seen two examples of planning 
agents so far the search -based problem -solving agent.  
DEFINITION OF CLASSICAL PLANNING : The problem -solving agent can find sequences o f actions that 
result in a goal state. But it deals with atomic representations of states and thus needs good domain -
specific heuristics to perform well. The hybrid propositional logical agent can find plans without domain -
specific heuristics because it us es domain -independent heuristics based on the logical structure of the 
problem but it relies on ground (variable -free) propositional 
inference,whichmeansthatitmaybeswampedwhentherearemanyactionsandstates.For 
example,inthe world ,thesimpleactionofmovingastepf orwardhadtoberepeated for all four agent 
orientations, T time steps, and n2 currentlocations.  
 In response to this, planning researchers have settled on a factored representation — one in which a 
state of the world is represented by a collection of variable s. We use a language called PDDL , the 
Planning Domain Definition Language that allows us to express all 4 Tn2 actions with one action schema. 
There have been several versions of PDDL.we select a simple version and alter its syntax to be consistent 
with the rest of the book. We now show how PDDL describes the four things we need to define a search 
problem: the initial state, the actions that are available in a state, the result of applying an action, and 
the goal test.  
Each state is represented as a conjuncti on of flaunts that are ground, functionless atoms. For example, 
Poor ∧ Unknown might represent the state  of a hapless  agent,  and a state in a package delivery 
problem might be At(Truck 1, Melbourne) ∧ At(Truck 2, Sydney ). Database semantics is used: th e Planning  
Classical Planning : Definition of Classical Planning, Algorithms for Planning with StateSpace Search, Planning 
Graphs, other Classical Planning Approaches, Analysis of Planning approaches.  
Planning and  Acting in the Real World : Time, Schedules, and Resources, Hierarchical Planning, Planning and 
Acting in Nondeterministic Domains, Multi agent Planning  Artificial Intelligence  Page 106  
 closed -world assumption means that any flaunts that are not mentioned are false, and the unique 
names assumption means that Truck 1 and Truck 2 are distinct.  
A set of ground (variable -free) actions can be represented by a single action schema. The schema  is a 
lifted representation —it lifts the level of reasoning from propositional logic to a restricted subset of 
first-order logic. For example, here is an action schema for flying a plane from one location to another:  
Action(Fly (p, from, to),  
PRECOND:At(p, from) ∧ Plane(p) ∧ Airport (from) ∧ Airport (to)  
EFFECT:¬At(p, from) ∧ At(p, to))  
 
The schema consists of the action name, a list of all the variables used in the schema, a precondition and 
an effect.    
A set of action schemas serves as a de finition of a planning domain. A specific problem within the 
domain is defined with the addition of an initial state and a goal.   
state is a conjunction of ground atoms. (As with all states, the closed -world assumption is used, which 
means that any atoms that are not mentioned are false.) The goal is just like a precondition: a 
conjunction of literals (positive or negative) that may contain variables, such as At(p, SFO ) ∧ Plane(p). 
Any variables are treated as existentially quantified, so this goal is to have any plane at SFO. The 
problem is solved when we can find a sequence of actions that end in a states that entails the goal.  
Example: Air cargo transport  
An air cargo transport problem involving loading and unloading cargo and flying it from place to pl ace. 
The problem can be defined with three actions: Load , Unload , and Fly . The actions affect two 
predicates: In(c, p) means that cargo c is inside plane p, and At(x, a) means that object x (either plane or 
cargo) is at airport a. Note that some care mu st be taken to make sure the At predicates are maintained 
properly. When a plane flies from one airport to another, all the cargo inside the plane goes with it. In 
first-order logic it would be easy to quantify over all objects that are inside the plane. B ut basic PDDL 
does not have a universal quantifier, so we need a different solution. The approach we use is to say that 
a piece of cargo ceases to be At anywhere when it is In a plane; the cargo only becomes At the new 
airport when it is unloaded. So At re ally means “available for use at a given location.”  
The complexity of classical planning  : 
We consider the theoretical complexity of planning and distinguish two decision problems. PlanSAT is 
the question of whether there exists any plan that solves a plan ning problem. Bounded PlanSAT asks 
whether there is a solution of length k or less; this can be used to find an optimal plan.  Artificial Intelligence  Page 107  
 The first result is that both decision problems are decidable for classical planning. The proof follows 
from the fact that the num ber of states is finite. But if we add function symbols to the language, then 
the number of states becomes infinite, and PlanSAT becomes only semi decidable: an algorithm exists 
that will terminate with the correct answer for any solvable problem, but may not terminate on 
unsolvable problems. The Bounded PlanSAT problem remains decidable even in the presence of 
function symbols.  
Both PlanSAT and Bounded PlanSAT are in the complexity class PSPACE, a class that is larger (and hence 
more difficult) than NP and  refers to problems that can be solved by a deterministic Turing machine with 
a polynomial amount of space. Even if we make some rather severe restrictions, the problems remain 
quite difficult.  
 
Algorithms for Planning with State Space Search  
Forward (pro gression) state -space search:  
Now that we have shown how a planning problem maps into a search problem, we can solve planning 
problems with any of the heuristic search algorithms from Chapter 3 or a local search algorithm from 
Chapter 4 (provided we keep t rack of the actions used to reach the goal). From the earliest days of 
planning research (around 1961) until around 1998 it was assumed that forward state -space search was 
too inefficient to be practical. It is not hard to come up with reasons why .  
 First , forward search is prone to exploring irrelevant actions. Consider the noble task of buying a copy of 
AI: A Modern Approach from an online bookseller. Suppose there is an action schema Buy(isbn) with 
effect Own(isbn). ISBNs are 10 digits, so this action s chema represents 10 billion ground actions. An 
uninformed forward -search algorithm would have to start enumerating these 10 billion actions to find 
one that leads to the goal.  
Second, planning problems often have large state spaces. Consider an air cargo p roblem with 10 
airports, where each airport has 5 planes and 20 pieces of cargo. The goal is to move all the cargo at 
airport A to airport B. There is a simple solution to the problem: load the 20 pieces of cargo into one of 
the planes at A, fly the plane to B, and unload the cargo. Finding the solution can be difficult because the 
average branching factor is huge: each of the 50 planes can fly to 9 other airports, and each of the 200 
packages  can be either unloaded (if  it is loaded) or loaded into any pl ane at its airport (if it is unloaded).  
So in any state there  is a minimum of 450 actions (when all the packages are at airports with no planes) 
and a maximum of 10,450 (when all packages and planes are at the same airport). On average, let’s say 
there a re about 2000 possible actions per state, so the search graph up to the depth of the obvious 
solution has about 2000 nodes.  Artificial Intelligence  Page 108  
  
 
Backward (regression) relevant -states search : 
In regression search we start at the goal and apply the actions backward until we f ind a sequence of 
steps that reaches the initial state. It is called relevant -states search because we only consider actions 
that are relevant to the goal (or current state). As in belief -state search (Section 4.4), there is a set of 
relevant states to con sider at each step, not just a single state.  
We start with the goal, which is a conjunction of literals forming a description of a set of states —for 
example, the goal ¬Poor ∧ Famous describes those states in which Poor is false, Famous is true, and any 
other fluent can have any value. If there are n ground flaunts in a domain, then there are 2n ground 
states (each fluent can be true or false), but 3n descriptions of sets of goal states (each fluent can be 
positive, negative, or not mentioned).  
In general, backward search works only when we know how to regress from a state description to the 
predecessor state description. For example, it is hard to search backwards for a solution to the n -queens 
Artificial Intelligence  Page 109  
 problem because there is no easy way to describe the states tha t are one move away from the goal. 
Happily, the PDDL representation was designed to make it easy to regress actions —if a domain can be 
expressed in PDDL, then we can do regression search on it.  
The final issue is deciding which actions are candidates to regress over. In the forward direction we 
chose actions that were applicable —those actions that could be the next step in the plan. In backward 
search we want actions that are relevant —those actions that could be the last step in a plan leading up 
to the c urrent goal state.  
Heuristics for planning:  
Neither forward nor backward search is efficient without a good heuristic function. Recall from Chapter 
3 that a heuristic function h(s) estimates the distance from a state s to the goal and that if we can derive  
an admissible heuristic for this distance —one that does not overestimate —then we can use A ∗ search to 
find optimal solutions. An admissible heuristic can be derived by defining a relaxed problem that is 
easier to solve. The exact cost of a solution to this easier problem then becomes the heuristic for the 
original problem.  
By definition, there i s no way to analyze an atomic state, and thus it it requires some ingenuity by a 
human analyst to define good domain -specific heuristics for search problems with atomic states. 
Planning uses a factored representation for states and action schemas. That mak es it possible to define 
good domain -independent heuristics and for programs to automatically apply a good domain -
independent heuristic for a given problem.  
Planning Graphs : 
All of the heuristics we have suggested can suffer from inaccuracies. This section  shows how a special 
data structure called a planning graph can be used to give better heuristic estimates. These heuristics 
can be applied to any of the search techniques we have seen so far. Alternatively, we can search for a 
solution over the space form ed by the planning graph, using an algorithm called GRAPHPLAN.  
A planning problem asks if we can reach a goal state from the initial state. Suppose we are given a tree 
of all possible actions from the initial state to successor states, and their successors , and so on. If we 
indexed this tree appropriately, we could answer the planning question “can we reach state G from 
state S0” immediately, just by looking it up. Of course, the tree is of exponential size, so this approach is 
impractical. A planning graph  is polynomial - size approximation to this tree that can be constructed 
quickly. The planning graph can’t answer definitively whether G is reachable from S0, but it can estimate 
how many steps it takes to reach G. The estimate is always correct when it rep orts the goal is not 
reachable, and it never overestimates the number of steps, so it is an admissible heuristic.  Artificial Intelligence  Page 110  
 A planning graph is a directed graph organized into levels: first a level S0 for the initial state, consisting 
of nodes representing each flue nt that holds in S0; then a level A0 consisting of nodes for each ground 
action that might be applicable in S0; then alternating levels Si followed by Ai; until we reach a 
termination condition (to be discussed later).  
Roughly speaking, Si contains all the  literals that could hold at time i, depending on the actions executed 
at preceding time steps. If it is possible that either P or ¬P could hold, then both will be represented in 
Si. Also roughly speaking, Ai contains all the actions that could have their preconditions satisfied at time 
i. We say “roughly speaking” because the planning graph records only a restricted subset of the possible 
negative interactions among actions; therefore, a literal might show up at level Sj when actually it could 
not be true until a later level, if at all. (A literal will never show up too late.) Despite the possible error, 
the level j at which a literal first appears is a good estimate of how difficult it is to achieve the literal 
from the initial state.  
We now define mutex l inks for both actions and literals. A mutex relation holds between two actions at 
a given level if any of the following three conditions holds:  
• Inconsistent effects: one action negates an effect of the other. For example, Eat(Cake) and the 
persistence of  Have(Cake) have inconsistent effects because they disagree on the effect Have(Cake).  
• Interference: one of the effects of one action is the negation of a precondition of the other. For 
example Eat(Cake) interferes with the persistence of Have(Cake) by it s precondition.  
• Competing needs: one of the preconditions of one action is mutually exclusive with a 
precondition of the other. For example, Bake(Cake) and Eat(Cake) are mutex because they compete on 
the value of the Have(Cake) precondition.  
A mutex rela tion holds between two literals at the same level if one is the negation of the other or if 
each possible pair of actions that could achieve the two literals is mutually exclusive. This condition is 
called inconsistent support. For example, Have(Cake) and Eaten(Cake) are mutex in S1 because the only 
way of achieving Have(Cake), the persistence action, is mutex with the only way of achieving 
Eaten(Cake), namely Eat(Cake). In S2 the two literals are not mutex, because there are new ways of 
achieving them, suc h as Bake(Cake) and the persistence of Eaten(Cake), that are not mutex.  
other Classical Planning Approaches:  
Currently the most popular and effective approaches to fully automated planning are:  
• Translating to a Boolean satisfiability (SAT) problem  
• Forw ard state -space search with carefully crafted heuristics  
• Search using a planning graph (Section 10.3)  Artificial Intelligence  Page 111  
 These three approaches are not the only ones tried in the 40 -year history of automated planning. Figure 
10.11 shows some of the top systems in the Inte rnational Planning Competitions, which have been held 
every even year since 1998. In this section we first describe the translation to a satisfiability problem 
and then describe three other influential approaches: planning as first -order logical deduction;  as 
constraint satisfaction; and as plan refinement.  
Classical planning as Boolean satisfiability :  
we saw how SATPLAN solves planning problems that are expressed in propositional logic. Here we show 
how to translate a PDDL description into a form that can  be processed by SATPLAN. The translation is a 
series of straightforward steps:  
• Proposition Alize the actions: replace each action schema with a set of ground actions formed by 
substituting constants for each of the variables. These ground actions are no t part of the translation, but 
will be used in subsequent steps.  
• Define the initial state: assert F 0 for every fluent F in the problem’s initial state, and ¬F for every 
fluent not mentioned in the initial state.  
• Proposition Alize the goal: for every v ariable in the goal, replace the literals that contain the 
variable with a disjunction over constants. For example, the goal of having block A on another block, 
On(A, x) ∧ Block (x) in a world with objects A, B and C, would be replaced by the goal  
(On(A, A ) ∧ Block (A)) ∨ (On(A, B) ∧ Block (B)) ∨ (On(A, C) ∧ Block (C)) .  
• Add successor -state axioms: For each fluent F , add an axiom of the form  
F t+1 ⇔ ActionCausesF t ∨ (F t ∧ ¬ActionCausesNotF t) ,  
where Action CausesF is a disjunction of all the ground actions that have F in their add list, and Action 
CausesNotF is a disjunction of all the ground actions that have F in their delete list.  
Analysis of Planning approaches:  
Planning combines the two major areas of AI we have covered so far: search and logic.  A planner can be 
seen either as a program that searches for a solution or as one that (constructively) proves the existence 
of a solution. The cross -fertilization of ideas from the two areas has led both to improvements in 
performance amounting to several  orders of magnitude in the last decade and to an increased use of 
planners in industrial applications. Unfortunately, we do not yet have a clear understanding of which 
techniques work best on which kinds of problems. Quite possibly, new techniques will em erge that 
dominate existing methods.  
Planning is foremost an exercise in controlling combinatorial explosion. If there are n propositions in a 
domain, then there are 2n states. As we have seen, planning is PSPACE - hard. Against such pessimism, Artificial Intelligence  Page 112  
 the identifi cation of independent sub problems can be a powerful weapon. In the best case —full 
decomposability of the problem —we get an exponential speedup.  
Decomposability is destroyed, however, by negative interactions between actions. GRAPHPLAN records 
mutexes to point out where the difficult interactions are. SATPLAN rep - resents a similar range of mutex 
relations, but does so by using the general CNF form rather than a specific data structure. Forward 
search addresses the problem heuristically by trying to find p atterns (subsets of propositions) that cover 
the independent sub problems. Since this approach is heuristic, it can work even when the sub problems 
are not completely independent.  
Sometimes it is possible to solve a problem efficiently by recognizing that negative interactions can be 
ruled out. We say that a problem has serializable sub goals if there exists an order of sub goals such that 
the planner can achieve them in that order without having to undo any of the previously achieved sub 
goals. For example , in the blocks world, if the goal is to build a tower (e.g., A on B, which in turn is on C, 
which in turn is on the Table, as in Figure 10.4 on page 371), then the sub goals are serializable bottom 
to top: if we first achieve C on Table, we will never hav e to undo it while we are achieving the other sub 
goals. Planners such as GRAPHPLAN, SATPLAN, and FF have moved the field of planning forward, by 
raising the level of performance of planning systems.  
 
Planning and Acting in the Real World:  
This allows huma n experts to communicate to the planner what they know about how to solve the 
problem. Hierarchy also lends itself to efficient plan construction because the planner can solve a 
problem at an abstract level before delving into details. Presents agent archi tectures that can handle 
uncertain environments and interleave deliberation with execution, and gives some examples of real -
world systems.  
Time, Schedules, and Resources:  
The classical planning representation talks about what to do, and in what order, but the representation 
cannot talk about time: how long an action takes and when it occurs. For example, the planners of 
Chapter 10 could produce a schedule for an airline that says which planes are assigned to which flights, 
but we really need to know departu re and arrival times as well. This is the subject matter of scheduling. 
The real world also imposes many resource constraints; for example, an airline has a limited number of 
staff —and staff who are on one flight cannot be on another at the same time. This  section covers 
methods for representing and solving planning problems that include temporal and resource 
constraints.  Artificial Intelligence  Page 113  
 The approach we take in this section is “plan first, schedule later”: that is, we divide the overall problem 
into a planning phase in whi ch actions are selected, with some ordering constraints, to meet the goals of 
the problem, and a later scheduling phase, in which temporal information is added to the plan to ensure 
that it meets resource and deadline constraints.  
 
This approach is common  in real -world manufacturing and logistical settings, where the planning phase 
is often performed by human experts. The automated methods of Chapter 10 can also be used for the 
planning phase, provided that they produce plans with just the minimal ordering  constraints required 
for correctness. G RAPHPLAN (Section 10.3), SATPLAN (Section 10.4.1), and partial -order planners 
(Section 10.4.4) can do this; search -based methods (Section 10.2) produce totally ordered plans, but 
these can easily be converted to pla ns with minimal ordering constraints.  
Hierarchical Planning  : 
The problem -solving and planning methods of the preceding chapters all operate with a fixed set of 
atomic actions. Actions can be strung together into sequences or branching networks; state -of-the-art 
algorithms can generate solutions containing thousands of actions.  
For plans executed by the human brain, atomic actions are muscle activations. In very round numbers, 
we have about 103 muscles to activate (639, by some counts, but many of them have  multiple subunits); 
we can modulate their activation perhaps 10 times per second; and we are alive and awake for about 
109 seconds in all. Thus, a human life contains about 1013 actions, give or take one or two orders of 
Artificial Intelligence  Page 114  
 magnitude. Even if we restrict our selves to planning over much shorter time horizons —for example, a 
two-week vacation in Hawaii —a detailed motor plan would contain around 1010 actions. This is a lot 
more than 1000.  
To bridge this gap, AI systems will probably have to do what humans appear to do: plan at higher levels 
of abstraction. A reasonable plan for the Hawaii vacation might be “Go to San Francisco airport; take 
Hawaiian Airlines flight 11 to Honolulu; do vacation stuff for two weeks; take Hawaiian Airlines flight 12 
back to San Franci sco; go home.” Given such a plan, the action “Go to San Francisco airport” can be 
viewed as a planning task in itself, with a solution such as “Drive to the long -term parking lot; park; take 
the shuttle to the terminal.” Each of these actions, in turn, can  be decomposed further, until we reach 
the level of actions that can be executed without deliberation to generate the required motor control 
sequence.  
Planning and Acting in Nondeterministic Domains: While the basic concepts are the same as in Chapter 
4, there are also significant differences. These arise because planners deal with factored representations 
rather than atomic representations. This affects the way we represent the agent’s capability for action 
and observation and the way we represent belief states —the sets of possible physical states the agent 
might be in —for unobservable and partially observable environments. We can also take ad - vantage of 
many of the domain -independent methods given in Chapter 10 for calculating search heuristics.  
Consider  this problem: given a chair and a table, the goal is to have them match —have the same color. 
In the initial state we have two cans of paint, but the colors of the paint and the furniture are unknown. 
Only the table is initially in the agent’s field of vie w:  
Init(Object(Table) ∧ Object(Chair ) ∧ Can(C1) ∧ Can(C2) ∧ InView (Table)) Goal (Color (Chair , c) ∧ Color 
(Table, c))  
There are two actions: removing the lid from a paint can and painting an object using the paint from an 
open can. The action schemas a re straightforward, with one exception: we now allow preconditions and 
effects to contain variables that are not part of the action’s variable list. That is, Paint(x, can) does not 
mention the variable c, representing the color of the paint in the can. In the fully observable case, this is 
not allowed —we would have to name the action Paint(x, can, c). But in the partially observable case, we 
might or might not know what color is in the can. (The variable c is universally quantified, just like all the 
other variables in an action schema.)  
Action(RemoveLid (can),  
PRECOND:Can(can)  
EFFECT:Open(can))  Artificial Intelligence  Page 115  
 Action(Paint(x , can),  
PRECOND:Object(x) ∧ Can(can) ∧ Color (can, c) ∧ Open(can)  
EFFECT:Color (x, c))  
 
To solve a partially observable problem, the agent will have  to reason about the percepts it will obtain 
when it is executing the plan. The percept will be supplied by the agent’s sensors when it is actually 
acting, but when it is planning it will need a model of its sensors. In Chapter 4, this model was given by a  
function, PERCEPT(s). For planning, we augment PDDL with a new type of schema, the percept schema:  
 
Multi agent Planning:  
we have assumed that only one agent is doing the sensing, planning, and acting. When there are 
multiple agents in the environment, e ach agent faces a multi agent planning problem in which it tries to 
achieve its own goals with the help or hindrance of others.  
Between the purely single -agent and truly multi agent cases is a wide spectrum of problems that exhibit 
various degrees of decom position of the monolithic agent. An agent with multiple effectors that can 
operate concurrently —for example, a human who can type and speak at the same time —needs to do 
multi effector planning to manage each effector while handling positive and negative i nteractions 
among the effectors. When the effectors are physically decoupled into detached units —as in a fleet of 
delivery robots in a factory — multi effector planning becomes multibody planning. A multibody problem 
is still a “standard” single -agent probl em as long as the relevant sensor information collected by each 
body can be pooled —either centrally or within each body —to form a common estimate of the world 
state that then informs the execution of the overall plan; in this case, the multiple bodies act as a single 
body.  
When a single entity is doing the planning, there is really only one goal, which all the bodies necessarily 
share. When the bodies are distinct agents that do their own planning, they may still share identical 
goals; for example, two huma n tennis players who form a doubles team share the goal of winning the 
match. Even with shared goals, however, the multibody and multi agent cases are quite different. In a 
multibody robotic doubles team, a single plan dictates which body will go where on the court and which 
body will hit the ball. In a multi - agent doubles team, on the other hand, each agent decides what to do; 
without some method for coordination, both agents may decide to cover the same part of the court and 
each may leave the ball for t he other to hit.  
Planning with multiple simultaneous actions  Artificial Intelligence  Page 116  
 For the time being, we will treat the multi effector, multibody, and multi agent settings in the same way, 
labeling them generically as multi actor settings, using the generic term actor to cove cover effectors, 
bodies, and agents. The goal of this section is to work out how to define transition models, correct plans, 
and efficient planning algorithms for the multi actor setting.  
 A correct plan is one that, if executed by the actors, achieves the  goal. (In the true multi agent setting, 
of course, the agents may not agree to execute any particular plan, but at least they will know what 
plans would work if they did agree to execute them.) For simplicity, we assume perfect synchronization: 
each actio n takes the same amount of time and actions at each point in the joint plan are simultaneous.  
 
 
 
Having put the actors together into a multi actor system with a huge branching factor, the principal 
focus of research on multi actor planning has been to dec ouple  the actors to the extent possible, so that 
the complexity of the problem grows linearly with n rather than exponentially. If the actors have no 
interaction with one another —for example, n actors each playing a game of solitaire —then we can 
simply so lve n separate problems. If the actors are loosely coupled, can we attain something close to 
this exponential improvement? This is, of course, a central question in many areas of AI.  
The standard approach to loosely coupled problems is to pretend the probl ems are completely 
decoupled and then fix up the interactions. For the transition model, this means writing action schemas 
as if the actors acted independently. Let’s see how this works for the doubles tennis problem. Let’s 
Artificial Intelligence  Page 117  
 suppose that at one point in the  game, the team has the goal of returning the ball that has been hit to 
them and ensuring that at least one of them is covering the net.  
Planning with multiple agents Cooperation andcoordination:  
Now let us consider the true multi agent setting in which each agent makes its own plan. To start with, 
let us assume that the goals and knowledge base are shared. One might think   that this reduces to the 
multibody case —each agent simply computes the joint solution and executes its own part of that 
solution. Al as, the “the” in “the joint solution” is misleading. For our doubles team, more than one joint 
solution exists:  
If both agents can agree on either plan 1 or plan 2, the goal will be achieved. But if A chooses plan 2 and 
B chooses plan 1, then nobody will r eturn the ball. Conversely, if A chooses 1 and B chooses 2, then they 
will both try to hit the ball.  
One option is to adopt a convention before engaging in joint activity. A convention is any constraint on 
the selection of joint plans. For example, the con vention “stick to your side of the court” would rule out 
plan 1, causing the doubles partners to select plan 2. Drivers on a road face the problem of not colliding 
with each other; this is (partially) solved by adopting the convention “stay on the right si de of the road” 
in most countries; the alternative, “stay on the left side,” works equally well as long as all agents in an 
environment agree. Similar considerations apply to the development of human language, where the 
important thing is not which languag e each individual should speak, but the fact that a community all 
speaks the same language. When conventions are widespread, they are called social laws.  
Conventions can also arise through evolutionary processes. For example, seed -eating harvester ants are  
social creatures that evolved from the less social wasps. Colonies of ants execute very elaborate joint 
plans without any centralized control —the queen’s job is to re - produce, not to do centralized 
planning —and with very limited computation,  
 Communicati on, and memory capabilities in each ant (Gordon, 2000, 2007). The colony has many roles, 
including interior workers, patrollers, and foragers. Each ant chooses to perform a role ac - cording to the 
local conditions it observes. One final example of cooperat ive multi agent behavior appears in the 
flocking behavior of birds.  
 We can obtain a reasonable simulation of a flock if each bird agent (sometimes called a boid) observes 
the positions of its nearest neighbors and then chooses the heading and acceleration  that maximizes the 
weighted sum of these three components.  
 
 Artificial Intelligence  Page 118  
  
 
1. Cohesion: a posit ive score for getting closer to the average position of the neighbors  
2. Separation: a negative score for getting too close to any one neighbor  
3. Alignment: a positive score for getting closer to the average heading of the neighbors  
If all the boids execu te this policy, the flock exhibits the emergent behavior of flying as a pseudo rigid 
body with roughly constant density that does not disperse over time, and that occasionally makes 
sudden swooping motions. You can see a still images in Figure 11.11(a) and  compare it to an actual flock 
in (b). As with ants, there is no need for each agent to possess a joint plan that models the actions of 
other agents. The most difficult multi agent problems involve both cooperation with members of one’s 
own team and compet ition against members of opposing teams, all without centralized control.  
 
 
UNIT -V 
 
Figure 11.11 (a) A simulated flock of birds, using Reynold’s boids model. Image courtesy 
Giuseppe Randazzo, novastructura.net. (b) An actual flock of starlings. Image by Eduardo 
(pastaboy sleeps on flickr). (c) Two competitive teams of agents attempting to capture the 
towers in the N ERO game. Image courtesy Risto Miikkulainen.  (c) (b) (a) 
Uncertainty: Acting under Uncertainty, Basic Probability Notation, Inference Using Full Joint Distributions, 
Independence, Bayes’ Rule and Its Use, Probabilistic Reasoning: Repr esenting Knowledge in an Uncertain 
Domain, The Semantics of Bayesian Networks, Efficient Representation of Conditional Distributions, 
Approximate Inference in Bayesian Networks, Relational and First -Order Probability, Other Approaches to 
Uncertain Reasonin g; Dempster -Shafer theory.  
Learning: Forms of Learning, Supervised Learning, Learning Decision Trees.Knowledge in Learning: Logical 
Formulation of Learning, Knowledge in Learning, Explanation -Based Learning, Learning Using Relevance 
Information, Inductive Logic Programming  Artificial Intelligence  Page 119  
  
 
Uncertain knowledge and Learning  
Core vs. Probabilistic AI •  
 Knowledge Reasoning : work with facts/assertions; develop rules of logical inference  
 Planning: work with applicability/effects of actions; develop searches for actions which achieve 
goals/avert disasters.  
 Expert Systems: develop by hand a set of rules for examining inputs, updating internal states 
and generating outputs  
 Learning approach : use probabilistic models to tune performance based on many data 
examples.  
 Probabilistic AI: emphasis on noisy measurements, approximation in hard cases, learning, 
algorithmic issues.  
o logical assertions ⇒ probability distributions  
o logical inference ⇒ conditional probability distributions  
o logical operators ⇒ probabilistic generative models  
 
Probabilistic reasoning  
Causes of uncertainty:  
Following are some leading causes of uncertainty to occur in the real world.  
 
 Information occurred from unreliable sources.  
 Experimental Errors  
 Equipment fault  
 Temperature variation  
 Climate change.  
 
Probabilistic reasoning is a way of knowledge representation where we apply the concept of probability 
to indicate the uncertainty in knowledge. In probabilistic reasoning,  we combine probability theory with 
logic to handle the uncertainty.  
 
We use probability in probabilistic reasoning because it provides a way to handle the uncertainty that is 
the result of someone's laziness and ignorance.  
 
In the real world, there are lo ts of scenarios, where the certainty of something is not confirmed, such as 
"It will rain today," "behavior of someone for some situations," "A match between two teams or two 
players." These are probable sentences for which we can assume that it will happe n but not sure about 
it, so here we use probabilistic reasoning.  
 
Probability: Probability can be defined as a chance that an uncertain event will occur. It is the numerical 
measure of the likelihood that an event will occur. The value of probability alway s remains between 0 
and 1 that represent ideal uncertainties.  
 
0 ≤ P(A) ≤ 1,   where P(A) is the probability of an event A.   
P(A) = 0,  indicates total uncertainty in an event A.    
P(A) =1, indicates total certainty in an event A.     
 Artificial Intelligence  Page 120  
 We can find the probability of an uncertain event by using the below formula.  
P(¬A) = probability of a not happening event.  
P(¬A) + P(A) = 1.  
Event:  Each possible outcome of a variable is called an event.  
Sample space : The collection of all possible events is called sampl e space.  
Random variables : Random variables are used to represent the events and objects in the real world.  
Prior probability : The prior probability of an event is probability computed before observing new 
information.  
Posterior Probability : The probability that is calculated after all evidence or information has taken into 
account. It is a combination of prior probability and new information.  
Conditional probability : 
Conditional probability is a probability of occurring an event when another even t has already happened.  
 
Let's suppose, we want to calculate the event A when event B has already occurred, "the probability of A 
under the conditions of B", it can be written as:  
Where P(A ⋀B)= Joint probability of a and B  
P(B)= Marginal probability of B.  
If the probability of A is given and we need to find the probability of B, then it will be given as:  
 
It can be explained by using the below Venn diagram, where B is occurred event, so sample space will be 
reduced to set B, and now we can only calculate ev ent A when event B is already occurred by dividing 
the probability of P(A ⋀B) by P( B ).  
 
Example:  
In a class, there are 70% of the students who like English and 40% of the students who likes English and 
mathematics, and then what is the percent of students  those who like English also like mathematics?  
Solution:  
Let, A is an event that a student likes Mathematics  
B is an event that a student likes English.  
Hence, 57% are the students who like English also like Mathematics.  
Why Reason Probabilistically?  
 In many problem domains it isn't possible to create complete, consistent models of the world. 
Therefore agents (and people) must act in uncertain worlds (which the real world is).  
 Want an agent to make rational decisions even when there is not enough informat ion to prove 
that an action will work.  
 Some of the reasons for reasoning under uncertainty:  
o True uncertainty . E.g., flipping a coin.  
o Theoretical ignorance . There is no complete theory which is known about the problem 
domain. E.g., medical diagnosis.  
o Laziness . The space of relevant factors is very large, and would require too much work to 
list the complete set of antecedents and consequents. Furthermore, it would be too hard 
to use the enormous rules that resulted.  
o Practical ignorance . Uncertain about a particular individual in the domain because all of 
the information necessary for that individual has not been collected.  Artificial Intelligence  Page 121  
  Probability theory will serve as the formal language for representing and reasoning with 
uncertain knowledge.  
Bayes' theorem:  
Bayes' theorem is also known as  Bayes' rule, Bayes' law , or Bayesian reasoning , which determines the 
probability of an event with uncertain knowledge.  
In probability theory, it relates the conditional probability and marginal probabilities of two random 
events.  
Bayes' theorem was named after the British mathematician  Thomas Bayes . The  Bayesian inference  is an 
application of Bayes' theorem, which is fundamental to Bayesian statistics  
It is a way to calculate the value of P(B|A) with the knowledge of P(A|B).  
Bayes'  theorem allows updating the probability prediction of an event by observing new 
information of the real world.  
 
Example : If cancer corresponds to one's age then by using Bayes' theorem, we can 
determine the probability of cancer more accurately with the help of age.  
Bayes' theorem can be derived using product rule and conditional probability of event A with 
known event B:  
As from product rule we can write:  
1. P(A ⋀ B)= P(A|B)  P(B) or   
Similarly, the probability of event B with known event A:  
1. P(A ⋀ B)= P(B|A)  P(A)   
Equating right hand side of both the equations, we will get:  
 
The above equation (a) is called as  Bayes' rule  or Bayes' theorem . This equation is basic of most modern 
AI systems for  probabilistic inference . 
It shows t he simple relationship between joint and conditional probabilities. Here,  
P(A|B) is known as  posterior , which we need to calculate, and it will be read as Probability of hypothesis 
A when we have occurred an evidence B.  
P(B|A) is called the likelihood, in which we consider that hypothesis is true, then we calculate the 
probability of evidence.  
P(A) is called the  prior probability , probability of hypothesis before considering the evidence  
P(B) is called  marginal probability , pure probability of an evidence.  
Artificial Intelligence  Page 122  
 In the equation (a), in general, we can write P (B) = P(A)*P(B|Ai), hence the Bayes' rule can be written 
as: 
 
Applying Bayes' rule:  
Bayes' rule allows us to compute the single term P(B|A) in terms of P(A|B), P( B), and P(A). This is very 
useful in cases where we have a good probability of these three terms and want to determine the fourth 
one.  
Suppose we want to perceive the effect of some unknown cause, and want to compute that cause, then 
the Bayes' rule become s: 
 
Example -1: 
Question: what is the probability that a patient has diseases meningitis with a stiff 
neck?  
Given Data:  
A doctor is aware that disease meningitis causes a patient to have a stiff neck, and it occurs 
80% of the time. He is also aware of some  more facts, which are given as follows:  
o The Known probability that a patient has meningitis disease is 1/30,000.  
o The Known probability that a patient has a stiff neck is 2%.  
Let a be the proposition that patient has stiff neck and b be the proposition that patient has 
meningitis. , so we can calculate the following as:  
P(a|b) = 0.8  
P(b) = 1/30000  
P(a)= .02  
 
Artificial Intelligence  Page 123  
 Hence, we can assume that 1 patient out of 750 patients has meningitis disease 
with a stiff neck.  
Bayesian Network can be used for building models from data and experts opinions, and it 
consists of two parts:  
o Directed Acyclic Graph  
o Table of conditional probabilities.  
The generalized form of Bayesian network that represents and solve decision problems 
under uncertain knowledge is known as an  Influence  diagram . 
A Bayesian network graph is made up of nodes and Arcs (directed links), where:  
 
o Each  node  corresponds to the random variables, and a variable can be  continuous  or discrete . 
o Arc or directed arrows  represent the causal relationship or conditional probabilities between 
random variables. These directed links or arrows connect the pair of nodes in the graph.  
These links represent that one node directly influence the other node, and if there is no directed 
link that means that nodes are independent wit h each other  
o In the above diagram, A, B, C, and D are random variables represented by the nodes of 
the network graph.  
o If we are considering node B, which is connected with node A by a directed arrow, 
then node A is called the parent of Node B.  
o Node C is in dependent of node A.  
The Bayesian network has mainly two components:  
Artificial Intelligence  Page 124  
 o Causal Component  
o Actual numbers  
Each node in the Bayesian network has condition probability distribution  P(X i |Parent(X i) ), which 
determines the effect of the parent on that node.  
Representing Belief about Propositions  
 Rather than reasoning about the truth or falsity of a proposition, reason about the belief that a 
proposition or event is true or false  
 For each primitive proposition or event, attach a  degree of belief  to the sentence  
 Use probability theory  as a formal means of manipulating degrees of belief  
 Given a proposition, A, assign a probability, P(A), such that 0 <= P(A) <= 1, where if A is true, 
P(A)=1, and if A is false, P(A)=0. Proposition A must be either true or fa lse, but P(A) summarizes 
our degree of belief in A being true/false.  
o Examples  
o P(Weather=Sunny) = 0.7 means that we believe that the weather will be Sunny with 70% 
certainty. In this case Weather is a random variable that can take on values in a domain suc h 
as {Sunny, Rainy, Snowy, Cloudy}.  
o P(Cavity=True) = 0.05 means that we believe there is a 5% chance that a person has a cavity. 
Cavity is a Boolean random variable since it can take on possible values  True  and False . 
o Example: P(A=a ^ B=b) = P(A=a, B=b) = 0.2, where A=My_Mood, a=happy, B=Weather, and 
b=rainy, means that there is a 20% chance that when it's raining my mood is happy.  
 
 We will assume that in a given problem domain, the programmer and expert identify all of the 
relevant propositional variables that are needed to reason about the domain.  
 Each of these will be represented as a  random variable , i.e., a variable that can take on values 
from a set of mutually exclusive and exhaustive values called the  sample space  or partition  of 
the random variable . Usually this will mean a sample space { True , False }.  
 For example, the proposition  Cavity  has possible values  True  and False  indicating whether a 
given patient has a cavity or not. A random variable that has True and False as its possible values 
is calle d a Boolean random variable . 
More generally, propositions can include the equality predicate with random variables and the 
possible values they can have.  
For example, we might have a random variable  Color  with possible values  red, green , blue , 
and other .  
Then P(Color=red) indicates the likelihood that the color of a given object is red.  
 Similarly, for Boolean random variables we can ask P(A=True), which is abbreviated to P(A), and 
P(A=False), which is abbreviated to P(~A).  
Axioms of Probability Theory  Artificial Intelligence  Page 125  
 Probability Theory provides us with the formal mechanisms and rules for manipulating propositions 
represented probabilistically. The following are the three axioms of probability theory:  
 0 <= P(A=a) <= 1 for all  a in sample space of A  
 P(True)=1, P(False)=0  
 P(A v B) = P(A) + P(B) - P(A ^ B)  
From these axioms we can show the following properties also hold:  
 P(~A) = 1 - P(A)  
 P(A) = P(A ^ B) + P(A ^ ~B)  
 Sum{P(A=a)} = 1, where the sum is over all possible values  a in the sample space of A  
Joint Probability Distrib ution  
Given an application domain in which we have determined a sufficient set of random variables to 
encode all of the relevant information about that domain, we can completely specify all of the possible 
probabilistic information by constructing the  full joint probability distribution , 
 P(V1=v1, V2=v2, ..., Vn=vn), which assigns probabilities to all possible combinations of values to all 
random variables . 
For example, consider a domain described by three Boolean random variables, Bird, Flier, and Young. 
Then we can enumerate a table showing all possible interpretations and associated probabilities:  
Bird  Flier  Young  Probability  
T T T 0.0 
T T F 0.2 
T F T 0.04  
T F F 0.01  
F T T 0.01  
F T F 0.01  
F F T 0.23  
F F F 0.5 
Notice that there are 8 rows in the above table representing the fact that there are 23 ways to assign 
values to the three Boolean variables. More generally, with  n Boolean variables the table will be of size 
2n. And if  n variables each had  k possible values, then the table would be size  kn. 
Also notice that the sum of the probabilities in the right column must equal 1 since we know that the set 
of all possible values for each variable are known. This means that for  n Boolean random variables, the 
table has 2n-1 values that must be determined  to completely fill in the table.  Artificial Intelligence  Page 126  
 If all of the probabilities are known for a full joint probability distribution table, then we can 
compute  any probabilistic statement about the domain. For example, using the table above, we can 
compute  
 P(Bird=T) = P(B) =  0.0 + 0.2 + 0.04 + 0.01 = 0.25  
 P(Bird=T, Flier=F) = P(B, ~F) = P(B, ~F, Y) + F(B, ~F, ~Y) = 0.04 + 0.01 = 0.05  
Conditional Probabilities  
 Conditional probabilities are key for reasoning because they formalize the process of 
accumulating evidence and updating probabilities based on new evidence.  
 For example, if we know there is a 4% chance of a person having a cavity, we can represent this 
as the  prior  (aka unconditional) probability P(Cavity)=0.04.  
 Say that person now has a symptom of a toothache, we'd like to know what is 
the posterior  probability of a Cavity given this new evidence. That is, compute P(Cavity | 
Toothache).  
 If P(A|B) = 1, this is equivalent to the sentence in Propositional Logic B => A. Similarly, if P(A|B) 
=0.9, then this is like saying B => A with 90% certainty.  
 In other words, we've made implication fuzzy because it's not absolutely certain.  
 Given several  measurements and other "evidence", E1, ..., Ek, we will formulate queries as P(Q | 
E1, E2, ..., Ek) meaning "what is the degree of belief that Q is true given that we know E1, ..., 
Ek and nothing else ." 
Conditional probability is defined as: P(A|B) = P(A ^ B)/P(B) = P(A,B)/P(B)  
One way of looking at this definition is as a normalized (using P(B)) joint probability (P(A,B)).  
 Example Computing Conditional Probability from the Joint Probability Distribution  
Say we want to compute P(~Bird | Flier) and we know the full joint probability distribution 
function given above.  
 We can do this as follows:  
    P(~B|F) = P(~B,F) / P(F)  
     = (P(~B,F,Y) + P(~B,F,~Y)) / P(F)  
            = (.01 + .01)/P(F)  
Next, we could either compute the marginal probability P(F) from the full joint probability 
distribution, or, as is more commonly done, we could do it by using a process 
called  normalization , which first requires computing  
   P(B|F) = P(B,F) / P(F)  
   = (P(B,F,Y) + P(B,F,~Y)) / P(F)  
   = (0.0 + 0.2)/P(F)  
Now we also know th at P(~B|F) + P(B|F) = 1, so substituting from above and solving for P(F) we 
get P(F) = 0.22. Hence, P(~B|F) = 0.02/0.22 = 0.091.  Artificial Intelligence  Page 127  
 While this is an effective procedure for computing conditional probabilities, it is intractable in 
general because it means tha t we must compute and store the full joint probability distribution 
table, which is exponential in size.  
 Some important rules related to conditional probability are:  
o Rewriting the definition of conditional probability, we get the  Product Rule : P(A,B) = P(A|B)P(B)  
o Chain Rule : P(A,B,C,D) = P(A|B,C,D)P(B|C,D)P(C|D)P(D), which generalizes the product rule for a 
joint probability of an arbitrary number of variables. Note that ordering the variables results in a 
different expression, but all have the same resu lting value.  
o Conditionalized version of the Chain Rule : P(A,B|C) = P(A|B,C)P(B|C)  
o Bayes's Rule : P(A|B) = (P(A)P(B|A))/P(B), which can be written as follows to more clearly emphasize 
the "updating" aspect of the rule: P(A|B) = P(A) * [P(B|A)/P(B)] Note: The  terms P(A) and P(B) are 
called the  prior  (or marginal ) probabilities. The term P(A|B) is called the  posterior  probability 
because it is derived from or depends on the value of B.  
o Conditionalized version of Bayes's Rule : P(A|B,C) = P(B|A,C)P(A|C)/P(B|C)  
o Conditioning (aka Addition) Rule : P(A) = Sum{P(A|B=b)P(B=b)} where the sum is over all possible 
values  b in the sample space of B.  
o P(~B|A) = 1 - P(B|A)  
Assuming conditional independence of B and C given A, we can simplify 
Bayes's Rule for two pieces of evi dence B and C:  
 P(A | B,C) = (P(A)P(B,C | A))/P(B,C)  
            = (P(A)P(B|A)P(C|A))/(P(B)P(C|B))  
            = P(A) * [P(B|A)/P(B)] * [P(C|A)/P(C|B)]  
            = (P(A) * P(B|A) * P(C|A))/P(B,C)  
Naive Bayes Classifier : 
Say we have a random variable, C, which represents the possible ways to classify an input pattern of 
features that have been measured.  
The domain of C is the set of possible classifications, e.g., it might be the possible diagnoses in a 
medical domain.  
 Say the possible values for C are {a,b,c}, and the features we have measured are  
 E1=e1, E2=e2, ..., En=en.  
 Then we can compute  
P(C=a | E1=e1, ..., En=en),  
 P(C=b | E1=e1, ..., En=en) and  Artificial Intelligence  Page 128  
  P(C=c | E1=e1, ..., En=en) assuming E1, ..., En are conditionally independent given C.  
Since for each value of C the denominators are the same above, they can be ignored.  
So, for example  
P(C=a | E1=e1, ..., En=en) = P(C=a) * P(E1=e1 | C=a) * P(E2=e2 | C=a) * ... * P(En=en | C=a)  
 Choose the value for C that gives the maximum probability.  
Finally, since only relative values are needed and probabilities are often very small, it is common to 
compute the sum of logarithms of the probabilities:  
log P(C=a |  E1=e1, ..., En=en) = log P(C=a) + log P(E1=e1 | C=a) + ... + log P(En=en | C=a).  
If B and C are (unconditionally) independent, then P(C|B) = P(C), so  
P(A | B,C) = P(A) * [P(B|A)/P(B)] * [P(C|A)/P(C)]  
 Exam ple 
Consider the medical domain consisting of three  Boolean variables: PickledLiver, Jaundice, 
Bloodshot, where the first indicates if a given patient has the "disease" PickledLiver, and the 
second and third describe symptoms of the patient. We'll assume that Jaundice and Bloodshot 
are independent.  
The doc tor wants to determine the likelihood that the patient has a PickledLiver.  
 Based on no other information, she knows that the  prior  probability P(PickledLiver) = 10-17. So, 
this represents the doctor's initial belief in this diagnosis. However, after examination, she 
determines that the patient has jaundice. She knows that P(Jaundice) = 2-10 and P(Jaundice | 
PickledLiver) = 2-3, so she computes the new updated probability in the patient having 
PickledLiver as:  
P(PickledLiver | Jaundice) = P(P)P(J|P)/P(J)  
                           = (2-17 * 2-3)/2-10 
                           = 2-10 
So, based on this new evidence, the doctor increases her belief in this diagnosis from 2-17 to 2-10. 
 Next, she determines that the patient's eyes are bloodsho t, so now we need to add this new 
piece of evidence and update the probability of PickledLiver given Jaundice and Bloodshot.  
Say, P(Bloodshot) = 2-6 and P(Bloodshot | PickledLiver) = 2-1. Then, she computes the new 
conditional probability:  Artificial Intelligence  Page 129  
 P(PickledLiver | Jaundice, Bloodshot) = (P(P)P(J|P)P(B|P))/(P(J)P(B))  
                                      = 2-10 * [2-1 / 2-6] 
                                      = 2-5 
So, after taking both symptoms into account, the doctor's belief that the patient has a 
PickledLiv er is 2-5. 
Bayesian Networks (aka Belief Networks)  
 Bayesian Networks, also known as Bayes Nets, Belief Nets, Causal Nets, and Probability Nets, are 
a space -efficient data structure for encoding all of the information in the  full joint probability 
distribut ion for the set of random variables defining a domain. That is, from the Bayesian Net 
one can compute any value in the full joint probability distribution of the set of random 
variables.  
 Represents all of the direct causal relationships between variables  
 Intuitively, to construct a Bayesian net for a given set of variables, draw arcs from cause 
variables to immediate effects.  
 Space efficient because it exploits the fact that in many real -world problem domains the 
dependencies between variables are generall y local, so there are a lot of conditionally 
independent variables  
 Captures both qualitative and quantitative relationships between variables  
 Can be used to reason  
o Forward (top -down) from causes to effects -- predictive reasoning  (aka  causal 
reasoning ) 
o Backward (bottom -up) from effects to causes -- diagnostic reasoning  
 Formally, a Bayesian Net is a  directed, acyclic graph (DAG) , where there is a node for each 
random variable, and a directed arc from A to B whenever A is a direct causal influence on B. 
Thus the arcs represent direct causal relationships and the nodes represent states of affairs. The 
occurrence of A provides support for B, and vice versa. The backward influence is call 
"diagnostic" or "evidential" support for A due to the occurrence of B.  
 Each  node A in a net is conditionally independent of any subset of nodes that are not 
descendants of A given the parents of A.  
Net Topology Reflects Conditional Independence Assumptions  
 Conditional independence defines local net structure. For example, if B an d C are conditionally 
independent given A, then by definition P(C|A,B) = P(C|A) and, symmetrically, P(B|A,C) = P(B|A). 
Intuitively, think of A as the direct cause of both B and C. In a Bayesian Net this will be 
represented by the local structure:  Artificial Intelligence  Page 130  
  
For exa mple, in the dentist example in the textbook, having a Cavity causes both a Toothache 
and the dental probe to Catch, but these two events are conditionally independent given Cavity. 
That is, if we know nothing about whether or not someone has a Cavity, the n Toothache and 
Catch are dependent. But as soon as we definitely know the person has a cavity or not, then 
knowing that the person has a Toothache as well has no effect on whether Catch is true. This 
conditional independence relationship will be reflected  in the Bayesian Net topology as:  
 
 In general, we will construct the net so that given its parents, a node is conditionally 
independent of the rest of the net variables. That is,  
P(X1=x1, ..., Xn=xn) = P(xi | Parents(Xi)) * ... * P(xn | Parents(Xn))  
Hence, we don't need the full joint probability distribution, only conditionals relative to the 
parent variables.  
 Example (From ( Charniak, 1991 )) 
Consider the problem domain in which when I go home I want to know if someone in my family 
is home before I go in. Let's say I know the following information:  
(1) Why my wife leaves the house, she often (but not always) turns on the outside light. (She 
also sometimes turns the light on when she's expecting a guest.)  
 (2) When nobody is home, the dog is often left outside.  
(3) If the dog has bowel -troubles, it is also often left outside.  
(4) If the dog is outside, I will probably hear it barking (though i t might not bark, or I might hear 
a different dog barking and think it's my dog).  
Artificial Intelligence  Page 131  
 Given this information, define the following five Boolean random variables:  
O: Everyone is Out of the house  
L: The Light is on  
D: The Dog is outside  
B: The dog has Bowel tro ubles  
H: I can Hear the dog barking  
From this information, the following direct causal influences seem appropriate:  
1. H is only directly influenced by D. Hence H is conditionally independent of L, O and B given D.  
2. D is only directly influenced by O and B. Hence D is conditionally independent of L given O and B.  
3. L is only directly influenced by O. Hence L is conditionally independent of D, H and B given O.  
4. O and B are independent.  
Based on the above, the following is a Bayesian Net that represents these dire ct causal relationships 
(though it is important to note that these causal connections are not absolute, i.e., they are not 
implications):  
 
Next, the following quantitative information is added to the net; this information is usually given 
by an expert or determined empirically from training data.  
o For each root node (i.e., node without any parents), the prior probability of the random 
variable associated with the node is determined and stored there  
o For each non -root node, the conditional probabilities of th e node's variable given all 
possible combinations of its immediate parent nodes are determined. This results in 
a conditional probability table  (CPT) at each non -root node.  
Doing this for the above example, we get the following Bayesian Net:  
Artificial Intelligence  Page 132  
  
Notice that in this example, a total of 10 probabilities are computed and stored in the net, 
whereas the full joint probability distribution would require a table containing 25 = 32 
probabilities. The reduction is due to the conditional independence of many variables.  
Two variables that are not directly connected by an arc can still affect each other. For example, 
B and H are  not (unconditionally) independent, but H does not directly depend on B.  
Given a Bayesian Net, we can easily read off the conditional independence  relations that are 
represented. Specifically,  each node, V, is conditionally independent of all nodes that are not 
descendants of V, given V's parents . For example, in the above example H is conditionally 
independent of B, O, and L given D. So, P(H | B,D, O,L) = P(H | D).  
Building a Bayesian Net  
Intuitively, "to construct a Bayesian Net for a given set of variables, we draw arcs from cause variables to 
immediate effects. In almost all cases, doing so results in a Bayesian network [whose conditional 
independence implications are accurate]." (Heckerman, 1996)  
More formally, the following algorithm constructs a Bayesian Net:  
1. Identify a set of random variables that describe the given problem domain  
2. Choose an ordering for them: X1, ..., Xn  
3. for i=1 to n do 
a. Add a new node for Xi to the net  
b. Set Parents(Xi) to be the minimal set of already added nodes such that we have 
conditional independence of Xi and all other members of {X1, ..., Xi -1} given Parents(Xi)  
c. Add a directed arc from each node in Parents(Xi) to X i 
d. If Xi has at least one parent, then define a conditional probability table at Xi: P(Xi=x | 
possible assignments to Parents(Xi)). Otherwise, define a prior probability at Xi: P(Xi)  
Artificial Intelligence  Page 133 
  There is not, in general, a unique Bayesian Net for a given set of random variables. But all 
represent the same information in that from any net constructed every entry in the joint 
probability distribution can be computed.  
 The "best" net is constructed if in Step 2 the variables are topologically sorted first. That is, each 
variable comes before all of its children. So, the first nodes should be the roots, then the nodes 
they directly influence, and so on.  
 The algorithm will not construct a net that is illegal in the sense of violating the rules of 
probability . 
Computing Joint Probabilities from a Bayesian Net  
To illustrate how a Bayesian Net can be used to compute an arbitrary value in the joint probability 
distribution, consider the Bayesian Net shown above for the "home domain."  
Goal: Compute P(B,~O,D,~L,H)  
P(B,~O,D,~L,H) = P(H,~L,D,~O,B)  
     = P(H | ~L,D,~O,B) * P(~L,D,~O,B)            by Product Rule  
     = P(H|D) * P(~L,D,~O,B)                      by Conditional Independence of H and  
                                                       L,O, and B given D  
     = P(H|D) P(~L | D,~O,B) P(D,~O,B)            by Product Rule  
     = P(H|D) P(~L|~O) P(D,~O,B)                  by Conditional Independence of L and D,  
                                                       and L and B, given O  
     = P(H|D) P(~L|~O) P(D | ~O,B) P(~ O,B)        by Product Rule  
     = P(H|D) P(~L|~O) P(D|~O,B) P(~O | B) P(B)   by Product Rule  
     = P(H|D) P(~L|~O) P(D|~O,B) P(~O) P(B)       by Independence of O and B  
     = (.3)(1 - .6)(.1)(1 - .6)(.3)  
     = 0.00144  
where all of the numeric values are available directly in the Bayesian Net (since 
P(~A|B) = 1 - P(A|B)).  
APPROXIMATE INFERENCE IN BAYESIAN NETWORKS  
 Direct sampling methods  
The simplest kind of random sampling process for Bayesian networks generates events from a 
network that has no evide nce associated with it. The idea is to sample each variable in turn, in 
topological order. The probability distribution from which the value is sampled is conditioned on the 
values already assigned to the variable’s parents.  
 Artificial Intelligence  Page 134  
  
 Likelihood weighting  
Likelihood weighting avoids the inefficiency of rejection sampling by generating only eventsthat are 
consistent with the evidence e. It is a particular instance of the general statisticaltechnique of 
importance sampling, tailored for inference in Bayesian networks . 
 
 
 Inference by Markov chain simulation  
 
Markov chainMonte Carlo (MCMC) MARKOV CHAIN algorithms work quite differently from rejection 
sampling and likelihood weighting. Instead of generating each sample from scratch, MCMC 
algorithms generate each  sample by making a random change to the preceding sample. It is 
therefore helpful to think of an MCMC algorithm as being in a particular current state specifying a 
value for every variable and generating a next state by making random changes to the  curren t state.  
 
FIRST-ORDER PROBABILITY MODELS  
 Possible worlds  
For Bayesian networks, the possible worlds are assignmentsof values to variables; for the Boolean 
case in particular, the possible worlds areidentical to those of propositional logic. For a first -order 
probability model, then, it seemswe need the possible worlds to be those of first -order logic —that 
is, a set of objects withrelations among them and an interpretation that maps constant symbols to 
objects, predicatesymbols to relations, and function sym bols to functions on those objects . 
 
 Relational probability models  
Like first -order logic, RPMs have constant, function, and predicate symbols.We can refine the model 
by introducing a context -specific independence . 
Artificial Intelligence  Page 135  
 A context -specific independence allows a variable to be independent of some of its parents given certain 
values of others . 
 
 Open -universe probability models  
 A vision system doesn’t know what exists, if anything, around the next corner, and may not know if 
the object it sees now is the same one it  saw a few minutes ago.  
 A text -understanding system does not know in advance the entities that will be featured in a text, 
and must reason about whether phrases such as “Mary,” “Dr. Smith,” “she,”“his cardiologist,” “his 
mother,” and so on refer to the sam e object.  
 An intelligence analyst hunting for spies never knows how many spies there really are and can only 
guess whether various pseudonyms, phone numbers, and sightings belong to the same individual.  
Representing ignorance: Dempster –Shafer theory  
The Dempster –Shafer theory DEMPSTER –SHAFER is designed to deal with the distinction between 
uncertainty  and ignorance. Rather than computing the probability of a proposition, it computes 
theprobability that the evidence supports the proposition. This measure o f belief is called abelief 
function, written Bel(X).  
The mathematical formulation  of Dempster –Shafer theory  is similar tothose of probability theory; the 
main difference is that, instead of assigning probabilities  to possible worlds, the theory assigns masses 
to sets of possible world, that is, to events.  
The masses still must add to 1 over all possible events. Bel(A) is defined to be the sum ofmasses for all 
events that are subsets of (i.e., that entail) A, including A itself. With thisdefinition, Bel(A ) and Bel(¬A) 
sum to at most 1, and the gap —the interval between Bel(A)and 1 − Bel(¬A) —is often interpreted as 
bounding the probability of A.  
As with default reasoning, there is a problem in connecting beliefs to actions. Wheneverthere is a gap in 
the beli efs, then a decision problem can be defined such that a Dempster –Shafer system is unable to 
make a decision.  
Bel(A) should be interpretednot as a degree of belief in A but as the probability assigned to all the 
possible worlds (nowinterpreted as logical th eories) in which A is provable.  
 
For eg: - 
let us consider a room where four person are presented A, B, C, D(lets say) And suddenly lights out and 
when the lights come back B has been died due to stabbing in his back with the help of a knife. No one 
came in to the room and no one has leaved the room and B has not committed suicide. Then we have to 
find out who is the murdrer?  
 
 Either {A} or{C} or {D} has killed him.  
 Either {A, C} or {C, D} or {A, C} have killed him.  Artificial Intelligence  Page 136  
  Or the three of them kill him i.e; {A, C, D } 
 None of the kill him {o}(let us say).  
These will be the possible evidences by which we can find the murderer by measure of plausiblity.  
Using the above example we can say :  
Set of possible conclusion (P): ,p1, p2….pn-where P is set of possible conclusion  and cannot be 
exhaustive means at least one (p)i must be true.(p)i must be mutually exclusive.Power Set will contain 
2n elements where n is number of elements in the possible set.  
For eg: - 
If P = { a, b, c}, then Power set is given as  
{o, {a}, {b}, {c}, { a, b}, {b, c}, {a, c}, {a, b, c}}= 23 elements.  
 
Mass function m(K): It is an interpretation of m({K or B}) i.e; it means there is evidence for {K or B} which 
cannot be divided among more specific beliefs for K and B.  
 
Belief in K: The belief in element K of Power Set is the sum of masses of element which are subsets of K. 
This can be explained through an example  
Lets say K = {a, b, c}  
Bel(K) = m(a) + m(b) + m(c) + m(a, b) + m(a, c) + m(b, c) + m(a, b, c)  
Plaausiblity in K: It is the sum of masses of set th at intersects with K.  
i.e; Pl(K) = m(a) + m(b) + m(c) + m(a, b) + m(b, c) + m(a, c) + m(a, b, c)  
 
Characteristics of Dempster Shafer Theory:  
It will ignorance part such that probability of all events aggregate to 1.Ignorance is reduced in this 
theory by ad ding more and more evidences.Combination rule is used to combine various types of 
possiblities.  
Advantages:  
 Ucertainty interval reduces.  
 DST has much lower level of ignorance.  
 Diagnose Hierarchies can be represented using this.  
 Person dealing with such pro blems is free to think about evidences.  
Disadvantages:  
 In this computation effort is high, as we have to deal with 2n of sets.  
 
Learning  
An agent is learning if it improves its performance on future tasks after making observations  
about the world.  
Forms Of  Learning  
Any component of an agent can be improved by learning from data.It depends upon 4 factors:  
 Which component is to be improved  
o direct mapping from conditions on the current state to actions  
o infer relevant properties of the world  
o results of possible  actions  
o Action -value information  
o Goals that describe classes of states  
 What prior knowledge the agent already has.  Artificial Intelligence  Page 137  
  What representation is used for the data and the component.  
o representations: propositional and first -order logical sentences  
o Bayesian netwo rks for the inferential components  
o factored representation —a vector of attribute values —and outputs that can be either a 
continuous numerical value or a discrete value  
 What feedback is available to learn from :  types of feedback that determine the three ma in types of 
learning  
o In unsupervised learning the agent learns patterns in the input even though no explicit 
feedback is supplied  
o reinforcement learning the agent learns from a series of reinforcements —rewards or 
punishments.  
o supervised learning the agent observes some example input –output pairs and learns a function 
that maps from input to output  
o semi -supervised learning we are given a few labeled examples and must make what we can of 
a large collection of unlabelled examples  
 
SUPERVISED LEARNING  
Given a training set of N example input –output pairs (x1, y1), (x2, y2), . . . (xN, yN) , where each yj was 
generated by an unknown function y = f(x), discover a function h that approximates the true function f. 
The function h is a hypothesis. To measure the accuracy of a hypothesis we  give it a test set of examples 
that are distinct from the training set.  
Conditional Probability Distribution  : the function f is stochastic —it is not strictly a function of x, and 
what we have to learn is a , P(Y | x) 
Classific ation   :When the output y is one of a finite set of values  the learning problem is called 
classification  
Regression : When y is a number (such as tomorrow’s temperature), the learning problem is called 
regression  
Hypothesis space , H, can be a set of polyn omials. A polynomial is fitting a function of a single variable to 
some data points.  
Ockham’s razor   :how do we choose a function or a polynomial from among multiple consistent 
hypotheses? One answer is to prefer the simplest hypothesis consistent with the  data. This principle is 
called Ockham’s razor  
Realizable :  a learning problem is realizable if the hypothesis space contains the true function. 
Unfortunately, we cannot always tell whether a given learning problem is realizable, because the true 
function is not known.  
Supervised learning can be done by choosing the hypothesis ” h”that is most probable one for the  given 
data:  Artificial Intelligence  Page 138  
  
 
There is a tradeoff between the expressiveness of a hypothesis space and the complexity of finding a 
good hypothesis within that  space.  
 
LEARNING DECISION TREES  
Decision tree induction is one of the simplest and yet most successful forms of machine learning.  
The decision tree representation : The aim here is to learn a definition for the goal predicate . 
A decision tree represents a function that takes as input a vector of attribute values and returns a 
“decision” —a single output value. The input and output values can be discrete or continuous  
o A decision tree reaches its decision by performing a sequence of tests.  
o Each internal node in the tree corresponds to a test of the value of one of the input attributes, Ai,  
o the branches from the node are labeled with the possible values of the attribute, Ai =vik.  
o Each leaf node in the tree specifies a value to be returned by  the function.  
 
Decision Tree Algorithm:  
The D ECISION -TREE-LEARNING algorithm adopts a greedy divide -and-conquer strategy. This test divides 
the problem up into smaller subproblems that can then be solved recursively.  
 
function DECISION -TREE -LEARNING(examp les, attributes, parent examples) returns  
a tree  
if examples is empty then return PLURALITY -VALUE(parent examples)  
else if all examples have the same classification then return the classification  
else if attributes is empty then return PLURALITY -VALUE(examples)  
else 
A←argmaxa ∈ attributes IMPORTANCE(a, examples)  
tree←a new decision tree with root test A  
for each value vk of A do 
exs ←,e : e ∈examples and e.A = vk}  
subtree←DECISION -TREE -LEARNING(exs, attributes −A, examples)  
add a branch to tree with label (A = vk) and subtree subtree  
return tree 
 
Expressiveness of decision trees  
A Boolean decision tree is logically equivalent to the assertion that the goal attribute is true if and 
only if the input attributes satisfy one of the paths leading to a leaf with value true.  
Goal ⇔(Path 1 ∨Path 2 ∨・・・ ) , where each Path is a conjunction of attribute -value tests required 
to follow that path.  A tree consists of just tests on attributes in the interior nodes, values of 
Artificial Intelligence  Page 139  
 attributes on the branches,  and output values on the leaf nodes. For a wide variety of problems, the 
decision tree format yields a nice, concise result. But some functions cannot be represented 
concisely. We can evaluate the accuracy of a learning algorithm with a learning curve.  
Choosing attribute tests  
The greedy search used in decision tree learning is designed to approximately minimize the depth of the 
final tree. The idea is to pick the attribute that goes as far as possible toward providing an exact 
classification of the exampl es. A perfect attribute divides the examples into sets, each of which are all 
positive or all negative and thus will be leaves of the tree.  
 
Entropy is a measure of the uncertainty of a random variable; acquisition of information corresponds to 
a reduction  in entropy.  
 
We can check that the entropy of a fair coin flip is indeed 1 bit:  
H(Fair) = −(0.5 log2 0.5 + 0.5 log2 0.5) = 1 .  
 
The information gain  from the attribute INFORMATION GAIN test on A is the expected reduction in 
entropy:  
 
 
Pruning  
In decision trees, a technique called decision tree pruning combats overfitting. Pruning works by 
eliminating nodes that are not clearly relevant.  
 
Issues in decision trees:  
 Missing data  
 Multivalued attributes  
 Continuous and integer -valued input attributes  
 Continuous -valued output attributes  
 
LEARNING  
A LOGICAL FORMULATION OF LEARNING  
Current -best -hypothesis search  
The idea behind current -best -hypothesis search is to maintain a single hypothesis, and to adjust it as 
new examples arrive in order to maintain consistency.  
The extension of the hypothesis must be increased to include new examples. This is called 
generalization.  
function CURRENT -BEST -LEARNING(examples, h) returns a hypothesis or fail  
Artificial Intelligence  Page 140  
 if examples is empty then  
return h  
e←FIRST(examples)  
if e is con sistent with h then  
return CURRENT -BEST -LEARNING(REST(examples), h)  
else if e is a false positive for h then  
for each hin specializations of h consistent with examples seen so far do  
h←CURRENT -BEST -LEARNING(REST(examples), h)  
if h = fail then return h  
else if e is a false negative for h then  
for each hin generalizations of h consistent with examples seen so far do  
h←CURRENT -BEST -LEARNING(REST(examples), h)  
if h = fail then return h  
return fail  
The extension of the hypothesis must be decreased to exclude the  example. This is called s pecialization.  
Least -commitment search  
Backtracking arises because the current -best -hypothesis approach has to choose a particular hypothesis 
as its best guess even though it does not have enough data yet to be sure of the choice.  What we can do 
instead is to keep around all and only those hypotheses that are consistent with all the data so far. Each 
new example will either have no effect or will get rid of some of the hypotheses.  
One important property of this approach is that it is incremental: one never has to go back and 
reexamine the old examples.  
 
Boundary Set :  
We also have an ordering on the hypothesis space, namely, generalization/specialization. This is a partial 
ordering, which means that each boundary will not be a point  but rather a set of hypotheses called a 
boundary set.  
The great thing is that we can represent the entire G -SET version space using just two boundary sets: a 
most general boundary (the G -set) and a most S -SET specific boundary (the S -set). Everything in 
between is guaranteed to be consistent with the examples.  
The members Si and Gi of the S - and G -sets.  
For each one, the new example may be a false positive or a false negative.  
1. False positive for Si: This means Si is too general, but there are no consis tent specializations of Si (by 
definition), so we throw it out of the S -set. 
2. False negative for Si: This means Si is too specific, so we replace it by all its immediate generalizations, 
provided they are more specific than some member of G.  
3. False pos itive for Gi: This means Gi is too general, so we replace it by all its immediate specializations, 
provided they are more general than some member of S.  
4. False negative for Gi: This means Gi is too specific, but there are no consistent generalizations of  Gi 
(by definition) so we throw it out of the G -set 
 Artificial Intelligence  Page 141  
 EXPLANATION -BASED LEARNING  
Explanation -based learning is a method for extracting general rules from individual observations.  
Memoization  
The technique of memoization has long been used in computer science to speed up programs by saving 
the results of computation. The basic idea of memo functions is to accumulate a database of input –
output pairs; when the function is called, it first checks the database to see whether it can avoid solving 
the problem  from scratch.  
Explanation -based learning takes this a good deal further, by creating general rules that cover an entire 
class of cases.  
Basic EBL process works as follows:  
1. Given an example, construct a proof that the goal predicate applies to the examp le using the available 
background knowledge  
2. In parallel, construct a generalized proof tree for the variabilized goal using the same inference steps 
as in the original proof.  
3. Construct a new rule whose left -hand side consists of the leaves of the pro of tree and whose right -
hand side is the variabilized goal (after applying the necessary bindings from the generalized proof).  
4. Drop any conditions from the left -hand side that are true regardless of the values of the variables in 
the goal.  
 
Three factors involved in the analysis of efficiency gains from EBL:  
1. Adding large numbers of rules can slow down the reasoning process, because the inference 
mechanism must still check those rules even in cases where they do not yield a solution. In other wor ds, 
it increases the branching factor in the search space.  
2. To compensate for the slowdown in reasoning, the derived rules must offer significant increases in 
speed for the cases that they do cover. These increases come about mainly because the derived r ules 
avoid dead ends that would otherwise be taken, but also because they shorten the proof itself.  
3. Derived rules should be as general as possible, so that they apply to the largest possible set of cases.  
 
LEARNING USING RELEVANCE INFORMATION  
The learni ng algorithm is based on a straightforward attempt to find the simplest determination 
consistent with the observations.  
 A determination P ' Q says that if any examples match on P, then they must also match on Q. A 
determination is therefore consistent wit h a set of examples if every pair that matches on the predicates 
on the left -hand side also matches on the goal predicate.  
 
An algorithm for finding a minimal consistent determination  
 
function MINIMAL -CONSISTENT -DET(E,A) returns a set of attributes  
inputs : E, a set of examples  
A, a set of attributes, of size n  
for i = 0 to n do Artificial Intelligence  Page 142  
 for each subset A i of A of size i do 
if CONSISTENT -DET?(A i,E) then return Ai 
 
function CONSISTENT -DET?(A,E) returns a truth value  
inputs : A, a set of attributes  
E, a set of examples  
local variables : H, a hash table  
for each example e in E do 
if some example in H has the same values as e for the attributes A  
but a different classification then return false  
store the class of e in H, indexed by the values for attributes A of the example e  
return true  
 
Given an algorithm for learning determinations, a learning agent has a way to construct a minimal 
hypothesis within which to learn the target predicate. For example, we can combine MINIMAL -
CONSISTENT -DET with the DECISION -TREE -LEARNI NG algorithm.  
This yields a relevance -based decision -tree learning algorithm RBDTL that first identifies a minimal set of 
relevant attributes and then passes this set to the decision tree algorithm for learning.  
 
INDUCTIVE LOGIC PROGRAMMING  
Inductive logic programming (ILP) combines inductive methods with the power of first -order 
representations, concentrating in particular on the representation of hypotheses as logic programs.  
It has gained popularity for three reasons.  
1. ILP offers a rigorous approach to the general knowledge -based inductive learning problem.  
2. It offers complete algorithms for inducing general, first -order theories from examples, which can 
therefore learn successfully in domains where attribute -based algorithms are hard to apply.  
3. Induc tive logic programming produces hypotheses that are (relatively) easy for humans to read  
The object of an inductive learning program is to come up with a set of sentences for the Hypothesis 
such that the entailment constraint is satisfied. Suppose, for the  moment, that the agent has no 
background knowledge: Background is empty. Then one possible solution we would need to make pairs 
of people into objects.  
Top-down inductive learning methods  
The first approach to ILP works by starting with a very general rul e and gradually specializing it so that it 
fits the data.  
This is essentially what happens in decision -tree learning, where a decision tree is gradually grown until 
it is consistent with the observations.  
 To do ILP we use first -order literals instead of a ttributes, and the hypothesis is a set of clauses instead 
of a decision tree.  
Three kinds of literals  
1. Literals using predicates  
2. Equality and inequality literals  
3. Arithmetic comparisons  Artificial Intelligence  Page 143  
 Inductive learning with inverse deduction  
The second major approach to ILP involves inverting the normal deductive proof process.  
Inverse resolution is based INVERSE on the observation.  
Recall that an ordinary resolution step takes two clauses C1 and C2 and resolves them to produce the 
resolvent C.  
An inverse resolution step takes a resolvent C and produces two clauses C1 and C2, such that C is the 
result of resolving C1 and C2.  
Alternatively, it may take a resolvent C and clause C1 and produce a clause C2 such that C is the result of 
resolving C1 and C2.  
A number of approache s to taming the search implemented in ILP systems  
1. Redundant choices can be eliminated  
2. The proof strategy can be restricted  
3. The representation language can be restricted  
4. Inference can be done with model checking rather than theorem proving  
5. Inference can be done with ground propositional clauses rather than in first -order 
logic.  
 
 

 
 
 
20 © MAT Journals 2024. All Rights Reserved  
 
Journal of Information Technology and Sciences  
 
 
e-ISSN: 2581 -849X, Vol. 10, Issue 1 (January – April, 2024) pp:  (20-27) 
  
 
Advancements and Applications of Generative Artificial Intelligence  
 
Dattatray G. Takale1*, Parikshit N. Mahalle2, Bipin Sule3 
1Assistant Professor, Department of Computer Engineering,  Vishwakarma institute of information 
Technology, Pune , Maharashtra, India  
2Professor , Department of AI & DS, Vishwakarma Institute of Information Technology, SPPU Pune  
3Senior  Professor, Department of Engineering, Sciences (Computer Prg) and Humanities, 
Vishwakarma Institute of Technology, Pune, Maharashtra, India  
*Corresponding Author : dattatray.takale@viit.ac.in  
 
Received Date:  March 0 4, 2024 ; Published Date:  March 14, 2024  
 
Abstract  
As a transformative technology, generative artificial intelligence (AI) has emerged in a variety of 
fields such as image synthesis, text generation, music composition and creative design with diverse 
applications. The purpose of this paper is to provide a comprehensive overview of recent advances 
in generative AI techniques.  To begin with, we examine the evolution of generative models from 
traditional methods to state -of-the-art deep learning approaches like Generative Adversarial 
Networks (GANs), Variational Autoencoders (VAEs), and Transformers. During the following part 
of the paper, we discuss generative AI and its wide -ranging applications across a wide range of 
sectors, such as creating realistic images, writing na tural language texts, composing music, and 
enabling creative design tasks.  Our discussion also includes potential future research and 
development directions along with the challenges and ethical considerations associated with 
generative AI. A comprehensive  overview of generative AI is provided in this review for 
researchers, practitioners, and enthusiasts.  
 
Keywords - Deep learning, Generative Adversarial Networks (GANs), Generative Artificial 
Intelligence ( GAI), Transformer , Variational Autoencoders (VAEs)  
 
INTRODUCTION  
 
The ability of Generative Artificial 
Intelligence (AI) to create new content, simulate 
human creativity, and produce realistic output 
has revolutionized various fields. Generative AI, 
unlike traditional AI systems that concentrate on 
classification and prediction, aims to replicate 
the creative process observed in humans by 
understa nding and replicating it  [1]. From image 
and text generation to music composition and 
creative design, generative AI makes significant 
strides through advanced algorithms and deep 
learning techniques.  
A major contribution to artificial 
intelligence and tec hnology is the development 
of creative and innovative ways to use it  [2]. 
Entertainment, healthcare, marketing, and 
education all benefit from the ability to generate 
new content autonomously.  The entertainment 
industry, for example, employs generative AI in 
the creation of lifelike characters, virtual worlds, 
and immersive virtual reality experiences. A medical image synthesis tool, a drug discovery 
tool, and a personalized treatment plan can b e 
used in healthcare  [3]. A generative AI platform 
also enables personalized ads tailored to 
individual preferences in marketing and 
advertising.  From its historical roots to the 
current state -of-the-art models and their varied 
applications, this review pr ovides a 
comprehensive overview of generative AI 
advancements and applications. The goal of our 
study is to examine the evolution of generative 
models from early rules -based systems to 
sophisticated deep learning architectures and to 
analyse  their impact o n different fields.  We will 
explore the strengths, limitations, and real -world 
applications of generative AI techniques, 
including Generative Adversarial Networks 
(GANs), Variational Autoencoders (VAEs), and 
Transformer -based models  [4]. 
As generative AI r epresents a paradigm 
shift in how we interact with technology and 
create content, researchers, practitioners, and 
enthusiasts alike must understand its importance Advancements and Applications of Generative Artificial Intelligence                              Dattatray G. Takale  et al.  
 
21 © MAT Journals 2024. All Rights Reserved  
 and potential. The purpose of this review is to 
inspire research, innovation, and collaborati on in 
this rapidly evolving field by providing insights 
into the capabilities and challenges of generative 
AI [5]. Our goal is to shed light on the 
transformative potential of generative AI and 
pave the way for new advancements in artificial 
intelligence a nd beyond through a 
comprehensive analysis of the current landscape 
and prospects  [6]. 
 
EVOLUTION OF GENERATIVE MODELS  
 
The idea of generative models (Fig. 1) 
can be traced back to the early days of research 
into artificial intelligence. During those early  
days, the primary emphasis was on designing 
algorithms that were capable of producing new 
data points that were similar to those that were 
found in a certain dataset  [7]. The Markov chain, 
which was suggested by Andrey Markov, a 
Russian mathematician, in the late 19th century, is considered to be one of the first instances of 
generative models. Markov chains are stochastic 
processes that describe a series of events in 
which the likelihood of each occurrence relies 
solely on the state of the event that came  before 
it. Because of this, Markov chains are useful for 
creating sequences of data such as text and voice  
[8]. 
During the middle of the 20th century, 
the area of artificial intelligence saw 
considerable improvements on account of the 
development of rule -based expert systems and 
symbolic artificial intelligence  [9]. The 
simulation of human intellect and the resolution 
of difficult issues were accomplished by these 
systems via the use of explicit programming and 
logical reasoning. The capacity to produce fr esh 
material or learn from data was not one of their 
strengths, even though they were effective in 
particular fields, such as expert systems for 
medical diagnosis and theorem proving  [10].  
 
 
 
Figure 1:  Evolution of generative AI . 
 
In the realm of artificial intelligence, a 
notable breakthrough occurred when the switch 
from conventional methodologies to deep 
learning -based approaches in generative 
modelling were made  [11]. The traditional 
generative models, such as Markov models and 
Hidden Markov Models (HMMs), have 
limitations due to their rudimentary designs and 
their inability to recognise complicated patterns 
in the data. Profound lea rning -based 
frameworks, then again, utilize brain networks 
that have various layers of connected hubs to 
learn progressive portrayals of information. This 
enables these approaches to model complex connections and provide very realistic  outputs 
[12].  
The i nvention of Restricted Boltzmann 
Machines (RBMs) by Geoffrey Hinton and his 
colleagues in the 2000s is considered to be one 
of the pioneering efforts in the field of deep 
learning -based generative modelling  [13]. RBMs 
are probabilistic graphical models tha t learn the 
underlying structure of data by using a layer of 
visible units and a layer of hidden units together 
in a hierarchical construction. They were crucial 
in laying the groundwork for more sophisticated 
generative models, such as Variational 
Autoenc oders (VAEs) and Generative 
J. of Infor. Tech. and Sci.                                                                                                                    Vol. 10, Issue 1  
 
22 © MAT Journals 2024. All Rights Reserved  
 Adversarial Networks (GANs)  [14], which came 
into being in the years that followed  [15].  
 
Overview of Key Generative AI Techniques  
 
 Variational Autoencoders (VAEs): The 
VAE is a probabilistic generative model 
that learns to encode and decode data. The 
encoder networks translate input data into 
latent space representations, while the 
decoder networks reconstruct the original 
data from the latent space representations. 
To maximize the likelihood of generating 
input data while m inimizing divergence 
between the latent distribution and a 
predefined prior distribution, variational 
inference techniques are employed in 
training VAEs  [16]. 
 Generative Adversarial Networks (GANs):  
As generative models, GANs train two 
neural networks simu ltaneously: a generator 
network and a discriminator network, to 
generate data. Networks of generators 
generate synthetic data samples, and 
networks of discriminators differentiate real 
data from fake data. The two networks are 
trained adversarial, with the  generator 
aiming to fool the discriminator and the 
discriminator attempting to distinguish 
between real and fake data  [17]. 
 Transformer -Based Models:  There have 
been revolutions in generative modelling in natural language processing tasks because 
of transformer -based models, such as the 
Transformer architecture introduced by 
Vaswani et al. in 2017. To generate 
coherent text that is contextually relevant to 
the user, transformers rely on self -attention 
mechanisms to capture long -range 
dependencies in s equential data. Several 
variants of the Transformer architecture 
have achieved state -of-the-art performance 
in tasks such as text generation, language 
translation, and document summarization. 
These include GPT (Generative Pretrained 
Transformer) and BERT ( Bidirectional 
Encoder Representations from 
Transformers)  [18]. 
 
APPLICATIONS OF GENERATIVE AI  
 
Many applications of generative 
Artificial Intelligence (AI) techniques have been 
found across a wide range of domains, utilizing 
their ability to generate data instances similar to 
given datasets  which is shown in Fig. 2 . Several 
prominent generative AI applications are 
discussed in this section including image 
synthesis and manipulation with Generative 
Adversarial Networks (GANs), text generation 
and natural lan guage processing with 
Transformers, music generation, design, and 
healthcare applications.  
 
 
 
Figure 2:  Application of generative AI. 
Advancements and Applications of Generative Artificial Intelligence                              Dattatray G. Takale  et al.  
 
23 © MAT Journals 2024. All Rights Reserved  
 Image  Synthesis and Manipulation using 
GANs  
 
There is a lot of attention being paid to 
Generational Adversarial Networks (GANs) due 
to the remarkable ability they possess to create 
high-quality and realistic images. In these 
models, a generator network is used to generate 
images from random noise, and a discriminator 
network is used to distinguish between images 
that are real and those that are not. GANs have 
been applied to a variety of domains, including:  
 Image Generation:  A genetic algorithm 
(GAN) is designed  to generate realistic 
images of faces, landscapes, animals, and 
objects, amongst other things. They have 
been used to create artwork and create 
synthetic images for training datasets in 
various fields, as well as in generating 
artwork.  
 Image Translation a nd Style Transfer:  For 
example, GANs can be used to translate 
images from one domain to another, such as 
for converting day scenes into night scenes 
(for example: converting a day scene into a 
night scene) or for applying an artistic style 
to a photograph (example: applying an 
artistic style to a photograph)  
 Image Editing and Manipulation:  With 
training on GANs, users can edit images in 
a variety of ways. For example, they can 
change the facial expressions, the hair 
Color, or the background scenery of a 
photograph with very high precision.  
 
Text Generation and Natural Language 
Processing with Transformers  
 
Several natural language processing 
(NLP) tasks, including text generation, 
translation, summarization , and sentiment 
analysis, has  been revolutionized b y 
transformer -based models. As these models 
utilize self -attention mechanisms to capture 
long-range relationships in text data, they are 
highly effective in creating coherent and 
contextually relevant text. Applications for 
Transformers in Natural Language  Processing 
include:  
 Text Generation:  Several applications have 
been created that use transformers to 
generate human -like text, such as stories, 
articles, dialogue, and poetry. They have 
been adopted in applications such as chatbots, virtual assistants, co ntent creation 
platforms, and creative writing platforms.  
 Machine Translation:  There are several 
translation services and applications 
powered by transformers that provide high -
quality, accurate and fluent translation 
services, allowing our customers to 
communicate across linguistic barriers 
seamlessly.  
 Text Summarization:  There are currently 
several transformations built for document 
summarization, news aggregation, and the 
curation of content that can be used to 
create concise summaries of long articles or 
documents, extracting the most important 
information from them while preserving 
their original contexts.  
 
Music Generation and Creative Design 
Applications  
 
This text summarizes several 
applications of generative AI to creative 
domains such as music generation and graphic 
design. These applications relied on generative 
models to generate engaging and engaging 
content in many forms:  
 Music Generation:  Several models can 
generate original music pieces, melody, 
harmony, and rhythms, and even mimic the 
style of famous composers. These models 
can be found in music composition 
software, interactive platforms for music 
generation, and entertainment applications.  
 Creative Design:  As generative AI is being 
used across a wide range of creative design 
applications , such as generating digital 
artwork, graphics, and animations, it allows 
artists and designers to discover new 
creative possibilities, automate repetitive 
tasks, and generate several designs in a 
short amount of time.  
 
Other Emerging Applications  
 
Generat ive AI techniques are 
continuously being explored and applied in 
emerging domains, including healthcare, 
gaming, and entertainment:  
 Healthcare:  Several tasks can be 
accomplished with the help of generative 
models in medical imaging, including 
reconstructio n, segmentation, and anomaly 
detection. By generating synthetic medical J. of Infor. Tech. and Sci.                                                                                                                    Vol. 10, Issue 1  
 
24 © MAT Journals 2024. All Rights Reserved  
 images for training deep learning models, 
as well as simulating medical scenarios for 
training healthcare professionals, these 
models assist in generating medical images 
for training.  
 Gaming:  As a result of the use of genetic 
algorithms procedural content generation 
techniques have become more and more 
common in creating environments, 
characters, levels and narratives for video 
games. These techniques enhance the 
realism, variety and re plicability of video 
games, which results in immersive gaming 
experiences for players.  
 Entertainment:  Virtual reality (VR) 
experiences, interactive storytelling and 
content generation for movies, music videos 
and ads use generative models. With these 
apps, audiences get immersed and engaged 
in entertainment content with generative AI.  
 
CHALLENGES AND ETHI CAL 
CONSIDERATIONS  
 
While generative artificial intelligence 
(AI) has achieved significant breakthroughs in 
various applications, it also poses unique ethical 
challenges and concerns that are unique to the 
field. The purpose of this chapter is to examine a  
few of the key challenges facing generative AI 
models, such as mode collapse and evaluation 
metrics. Additionally, it discusses some of the 
ethical implications of generative AI, 
particularly regarding privacy issues and deep 
fakes.  
 
Addressing Challenges  
 
Mode Collapse:  Generated Adversarial 
Networks (GAN) often suffers  from mode 
collapse, when their generator fails to capture 
the entire distribution of data. As a consequence, 
diverse and realistic samples are generated. It is 
necessary to design more robust training 
strategies to address mode collapse, such as 
modifying  GAN architecture, incorporating 
regularization methods, or using alternative loss 
functions.  
Evaluation Metrics:  A generative AI model's 
performance is difficult to assess due to its 
subjective nature. The quality and diversity of 
generated samples are no t always accurately 
measured by traditional metrics such as 
Inception Score or Frechet Inception Distance. A major research area in generative AI is the 
development of comprehensive evaluation 
metrics that take into account realism, diversity, 
and semantic  coherence.  
 
Ethical Implications  
 
Deepfakes:  There are significant ethical 
concerns about deepfakes, AI -generated 
synthetic media depicting individuals acting in 
ways they never did. In addition to face 
swapping and voice synthesis, generative AI also 
poses misinformation, defamation, and 
manipulation risks. Creating fake videos or 
audio recordings can lead to harm, political 
unrest, and reputation damage, as well as 
deceiving viewers.  
Privacy Concerns:  In the case of generative AI 
models trained on large datasets of images or 
text, privacy concerns are significant. As a result 
of these models, a highly realistic image or text 
of an individual can potentially be generated, 
thereby violating their privacy rights and 
damaging their reputation. Furthermore, th e use 
of generative AI in surveillance systems or 
social media platforms raises serious concerns 
about unauthorized data collection, surveillance, 
and manipulation of content created by users 
without their permission.  
 
Mitigating Ethical Risks  
 
Detection and Authentication:  As a means of 
reducing the negative effects of deepfakes, 
robust detection methods must be developed for 
identifying them. To authenticate the 
authenticity of media content and detect 
manipulated or synthetic content, researchers are 
experimenting with technologies such as digital 
watermarking, cryptographic signatures, and 
forensic analysis.  
Regulation and Policy:  A crucial role is played 
by policymakers and regulatory agencies when it 
comes to addressing the ethical challenges 
associat ed with generative AI. To prevent the 
misuse and abuse of AI -generated content, it is 
necessary to implement regulations and 
guidelines for the responsible use of generative 
AI, such as data privacy laws, transparency 
requirements, and content moderation p olicies.  
Education and Awareness:  Developing media 
literacy and critical thinking skills requires 
raising public awareness about deepfakes as well 
as other artificial intelligence -generated content Advancements and Applications of Generative Artificial Intelligence                              Dattatray G. Takale  et al.  
 
25 © MAT Journals 2024. All Rights Reserved  
 and the potential risks associated with them. By 
educating  individuals about the capabilities and 
limitations of generative AI models, they can be 
able to discern between manipulated and 
authentic content and mitigate the spread of 
misinformation as a result.  
 
FUTURE DIRECTIONS AND 
OPPORTUNITIES  
 
As Generative Ar tificial Intelligence 
(AI) continues to evolve, several promising 
research directions and opportunities emerge. 
This section explores potential avenues for 
innovation, interdisciplinary collaboration, and 
real-world deployment of generative AI 
technologies . 
 
Potential Research Directions  
 
Improving Model Robustness:  Future research 
efforts may concentrate on improving the 
resilience of generative artificial intelligence 
models, especially in terms of tackling typical 
issues such as mode collapse, training instability, 
and sensitivity to perturbations in input setting s. 
Increasing the stability and dependability of 
generative models might be accomplished via 
the development of innovative training 
algorithms, regularisation approaches, and 
architectural changes.  
Incorporating Contextual Information:  The 
ability of gener ative AI models to generate 
coherent and contextually relevant content can 
be improved by integrating contextual 
information and prior knowledge into these 
models. Conditional generation, attention 
mechanisms, reinforcement learning, and other 
techniques c an allow models to leverage context 
to produce more personalized and adaptive 
outputs.  
Exploring Hybrid Approaches:  It is possible to 
advance the capabilities of generative models by 
integrating different generative AI techniques, 
such as combining Generat ive Adversarial 
Networks (GANs) with Variational 
Autoencoders (VAEs) or transformer -based 
models with convolutional neural networks 
(CNNs). By combining different techniques with 
hybrid approaches, it would be possible to 
capitalize on the strengths of eac h technique 
while minimizing their respective shortcomings, 
resulting in more versatile and effective 
generative artificial intelligence systems.  Semantic Understanding and Control:  The 
ability to manipulate and guide the generation 
process intuitively can  be achieved by 
empowering generative AI models with semantic 
understanding and control capabilities. In this 
field, it might be possible for researchers to 
develop interpretable and controllable generative 
models that are capable of generating content 
based on the inputs of the user by specifying the 
desired attributes, styles, and semantics.  
 
Opportunities for Interdisciplinary 
Collaboration  
 
Human -Computer Interaction (HCI):  Research 
in artificial intelligence and human -computer 
interaction can be facili tated by collaborating 
with experts in human -computer interaction to 
design user -friendly interfaces and interactive 
tools that can be used to generate creative 
content. To be effective at interacting with 
generative AI systems, HCI principles can be 
used to develop intuitive controls, feedback 
mechanisms, and collaborative workflows.  
Cognitive Science:  As a result of cognitive 
science insights, generative AI models can be 
designed to mimic the creative and cognitive 
abilities of humans. In the future, coll aborative 
efforts between AI researchers and cognitive 
scientists may enable the creation of generative 
models that exhibit human -like reasoning, 
imagination, and problem -solving abilities, 
leading to new opportunities for creative AI 
applications.  
Arts an d Humanities:  The arts and humanities 
can provide new approaches to generative AI by 
involving artists, designers, and scholars, and 
fostering interdisciplinary creativity within it. To 
create an enriched diversity of AI applications 
that explore new forms  of artistic expression, 
cultural representation, and aesthetic innovation, 
AI researchers and practitioners in creative fields 
should collaborate to enrich the diversity of 
generative AI applications.  
 
Real -World Deployment and Applications  
 
Creative Industries:  There is tremendous 
potential for generative AI technologies to 
revolutionize creative industries such as 
entertainment, advertising, design, and fashion 
in the years to come. The real -world application 
of generative artificial intelligence in these 
domains can enhance the efficiency and J. of Infor. Tech. and Sci.                                                                                                                    Vol. 10, Issue 1  
 
26 © MAT Journals 2024. All Rights Reserved  
 creativity of creative professionals by enhancing 
content generation, personalized experiences, 
and innovative storytelling formats.  
Healthcare and Education:  Generative AI 
models can be applied in healthcare fo r tasks 
such as medical image synthesis, drug discovery, 
and personalized treatment planning. In 
education, generative AI can facilitate 
interactive learning experiences, personalized 
tutoring systems, and educational content 
generation, catering to indivi dual learning styles 
and preferences.  
Environmental Sustainability:  For climate 
modelling and ecological conservation, 
generative AI techniques can be used to optimize 
resource utilization, design sustainable products, 
and simulate environmental scenarios.  As a 
result of generative artificial intelligence, we can 
contribute to the development of sustainable 
development and environmental stewardship by 
generating insights and solutions to complex 
environmental challenges.  
 
CONCLUSION  
 
There has been a surge in creativity and 
innovation in the field of image generation due 
to the advancements in generative AI models, 
with methodologies like Generative Adversarial 
Networks, Variational Autoencoders, 
Transformer -based models, and more driving 
advances in a varie ty of fields. Despite the 
remarkable achievements, there remain 
challenges, including mode collapse, evaluation 
metrics, and ethical considerations. In terms of 
future research directions, model robustness 
must be improved, contextual information must 
be i ncorporated, and interdisciplinary 
collaborations must be fostered. It would be 
helpful if feature work focused on developing 
robust evaluation metrics, improving model 
interpretability and control, and addressing 
ethical issues associated with the respons ible 
deployment of generative AI systems.  There is 
no doubt that the future of generative AI is going 
to unlock new frontiers of creativity, transform 
industries  and enrich the lives of people by 
tackling these challenges and seizing 
opportunities for coll aboration and innovation.  
 
REFERENCES  
 
1. Baidoo -Anu, D., & Ansah, L. O. (2023). 
Education in the era of generative artificial intelligence (AI): Understanding the 
potential benefits of ChatGPT in promoting 
teaching and learning.  Journal of AI , 7(1), 
52-62. https://doi.org/10.61969/jai.1337500  
2. Harshvardhan, G. M., Gourisaria, M. K., 
Pandey, M., & Rautaray, S. S. (2020). A 
comprehensive survey and analysis of 
generative models in machine 
learning.  Computer Sc ience Review , 38, 
100285. 
https://doi.org/10.1016/j.cosrev.2020.10028
5 
3. Noy, S., & Zhang, W. (2023). Experimental 
evidence on the productivity effects of 
generative artificial 
intelligence.  Scienc e, 381(6654), 187 -192. 
https://www.science.org/doi/abs/10.1126/sc
ience.adh2586  
4. Wach, K., Duong, C. D., Ejdys, J., 
Kazlauskaitė, R., Korzynski, P., Mazurek, 
G., ... & Ziemba, E. (2023).  The dark side 
of generative artificial intelligence: A 
critical analysis of controversies and risks 
of ChatGPT.  Entrepreneurial Business and 
Economics Review , 11(2), 7 -30. 
https://www. ceeol.com/search/article -
detail?id=1205845  
5. Castelli, M., & Manzoni, L. (2022). 
Generative models in artificial intelligence 
and their applications.  Applied 
Sciences , 12(9), 4127. 
https://doi.org/10.3390/a pp12094127  
6. Bahroun, Z., Anane, C., Ahmed, V., & 
Zacca, A. (2023). Transforming education: 
A comprehensive review of generative 
artificial intelligence in educational settings 
through bibliometric and content 
analysis.  Sustainability , 15(17), 12983.  
https://doi.org/10.3390/su151712983  
7. Kanbach, D. K., Heiduk, L., Blueher, G., 
Schreiter, M., & Lahmann, A. (2023). The 
GenAI is out of the bottle: generative 
artificial intelligence from a business model 
innovation perspective.  Review of 
Managerial Science , 1 -32. 
https://doi.org/10.1007/s11846 -023-00696 -
z 
8. Sujata, P., Takale, D. G., Tyagi, S., 
Bhalerao, S., Tiwari, M., & Dhanraj, J. A. 
(2024). New Perspectives, Challenges, and 
Advances in Data Fusion in 
Neuroimaging.  Human Cancer Diagnosis 
and Detection Using Exascale 
Computing , 185, 185. Advancements and Applications of Generative Artificial Intelligence                              Dattatray G. Takale  et al.  
 
27 © MAT Journals 2024. All Rights Reserved  
 https://books.google .co.in/books?hl=en&lr=
&id=HNf2EAAAQBAJ&oi=fnd&pg=PA1
85&dq=New+Perspectives,+Challenges,+a
nd+Advances+in+Data+Fusion+in+Neuroi
maging&ots=ouXsSqYTjU&sig=yK__UP
OSPOR35zi8Fzp6jkqMnWI&redir_esc=y#
v=onepage&q=New%20Perspectives%2C
%20Challenges%2C%20and%20Advance s
%20in%20Data%20Fusion%20in%20Neur
oimaging&f=false  
9. Santhakumar, G., Takale, D. G., Tyagi, S., 
Anitha, R., Tiwari, M., & Dhanraj, J. A. 
(2024). Analysis of Multimodality Fusion 
of Medical Image Segmentation Employing 
Deep Learning.  Human Cancer Diagnosis 
and Detection Using Exascale 
Computing , 171, 171. 
https://books.google.co.in/books?hl=en&lr=
&id=HNf2EAAAQBAJ&oi=fnd&pg=PA1
71&dq=Analysis+of+Mu ltimodality+Fusio
n+of+Medical+Image+Segmentation+Empl
oying+Deep+Learning&ots=ouXsSqYUdQ
&sig=HrHBAUwiiX_66N2KjHAqUuHxO
BU&redir_esc=y#v=onepage&q=Analysis
%20of%20Multimodality%20Fusion%20of
%20Medical%20Image%20Segmentation%
20Employing%20Deep%20Learning&f=fal
se 
10. Takale, D. G., Mahalle, P. N., Sakhare, S. 
R., Gawali, P. P., Deshmukh, G., Khan, V., 
... & Maral, V. B. (2023, August). Analysis 
of Clinical Decision Support System in 
Healthcare Industry Using Machine 
Learning Approach. In  International 
Conference on ICT for Sustainable 
Development  (pp. 571 -587). Singapore: 
Springer Nature Singapore. 
https://doi.org/10.1007/978 -981-99-5652 -
4_51  
11. Kadam, S. U., Dhede, V. M., Khan, V. N., 
Raj, A., & Takale, D. G.  (2022). Machine 
learning methode for automatic potato 
disease 
detection.  NeuroQuantology , 20(16), 2102 -2106.  
12. Takale, D. G., Gunjal, S. D., Khan, V. N., 
Raj, A., & Guja, S. N. (2022). Road 
accident prediction model using data 
mining 
techniques.  NeuroQuant ology , 20(16), 
2904.  
13. Bere, S. S., Shukla, G. P., Khan, V. N., 
Shah, A. M., & Takale, D. G. (2022). 
Analysis of students performance prediction 
in online courses using machine learning 
algorithms.  NeuroQuantology , 20(12), 13 -
19.  
14. Raut, R., Borole, Y., Pati l, S., Khan, V., & 
Takale, D. G. (2022). Skin disease 
classification using machine learning 
algorithms.  NeuroQuantology , 20(10), 
9624 -9629.  
15. Kadam, S. U., Khan, V. N., Singh, A., 
Takale, D. G., & Galhe, D. S. (2022). 
Improve the performance of non -intrusiv e 
speech quality assessment using machine 
learning 
algorithms.  NeuroQuantology , 20(10), 
12937. 
https://www.proquest.com/openview/6782d
8326 7a975d8a2aaf2d5c2530d79/1?pq -
origsite=gscholar&cbl=2035897  
16. Takale, D. G. (2019). A review on 
implementing energy efficient clustering 
protocol for wireless sensor network.  J 
Emerg Technol Innov Res (JETIR) , 6(1), 
310-315.  
17. Takale, D. G. (2019). A review on  QoS 
aware routing protocols for wireless sensor 
networks.  International Journal of 
Emerging Technologies and Innovative 
Research , 6(1), 316 -320.  
18. Takale, D. G. (2019). A Review on 
Wireless Sensor Network: its Applications 
and challenges.  Journal of Emerg ing 
Technologies and Innovative Research 
(JETIR) , 6(1), 222 -226.  
 
 
CITE THIS ARTICLE  
 
Dattatray G. Takale  et al. (2024 ). Advancements and Applications of Generative Artificial 
Intelligence , Journal of Information Technology and Sciences , 10(1), 20-27. 
 
 

1 
 LECTURE NOTES  
ON 
ARTIFICIAL INTELLIGENCE  
 
 
 
 
 
PREPARED BY  
DR. PRASHANTA KUMAR PATRA  
COLLEGE OF ENGINEERING AND TECHNOLOGY , 
BHUBANESWAR  
         
 
 
 2 
  
ARTIFICIAL INTELLIGENCE  SYLLABUS  
Module 1                                                                                                                                                            12Hrs  
What is Artificial Intelligence? AI Technique, Level of the Model,Problem Spaces, and Search: Defining 
the Problem as a State Space Search, Production Systems, Problem Characteristics, Production System 
Characteristics, Issues in the Design of Searc h  Programs. Heuristic Search Techniques: Generate -and-
Test, Hill Climbing, Best -first Search, Problem Reduction, Constraint Satisfaction, Means -ends     
Analysis, Knowledge Representation: Representations and Mappings, Approaches to Knowledge 
Representati on, Using Predicate Logic: Representing Simple Facts in Logic, Representing Instance and 
ISA Relationships, Computable Functions and Predicates, Resolution, Natural Deduction.Using Rules: 
Procedural Versus Declarative Knowledge, Logic Programming, Forward Versus Backward Reasoning,  
Matching, Control Knowledge.Symbolic Reasoning Under Uncertainty: Introduction to Nonmonotonic 
Reasoning, Logics for Nonmonotonic Reasoning, Implementation Issues, Augmenting a Problem -solver, 
Depth -first Search, Breadthfirst Sea rch.Weak and Strong Slot -and-Filler Structures: Semantic Nets, 
Frames, Conceptual Dependency  Scripts, CYC.  
Module 2 10Hrs  
Game Playing: The Minimax Search Procedure, Adding Alpha -beta Cutoffs, Iterative Deepening.Planning: 
The Blocks World, Components of a Planning System, Goal Stack Planning, Nonlinear Planning Using 
Constraint Posting, Hierarchical PlanningOther Planning Techniques.Understanding: What is 
Understanding, What Makes Understanding Hard?, Understanding as Constraint Satisfaction.Natural 
Langu age Processing: Introduction, Syntactic Processing, Semantic Analysis, Discourse and Pragmatic 
Processing, Statistical Natural Language Processing, Spell Checking.  
Module 3                                                                                                                                                 8Hrs  
Learning: Rote Learning, learning by Taking Advice, Learning in Problem -solving, Learning from 
Examples: Induction, Explanation -based Learning, Discovery, Analogy, Formal Learning Theory, Ne ural 
Net Learning and Genetic Learning. Expert Systems: Representing and Using Domain Knowledge, Expert 
System Shells, Explanation, Knowledge Acquisition.  
Text Book:  
1. Elaine Rich, Kevin Knight, & Shivashankar B Nair, Artificial Intelligence,  McGraw Hill,  3rd ed.,2009  
References:  
1) Introduction to Artificial Intelligence & Expert Systems, Dan W Patterson,  PHI.,2010  
2) S Kaushik, Artificial Intelligence, Cengage Learning, 1st ed.2011  
 
 
 3 
 Module 1                                                                                                                                       
ARTIFICIAL INTELLIGENCE  
What is Artificial Intelligence?  
It is a branch of Computer Science that  pursues creating the computers or machines as intelligent 
as human beings.   
It is the s cience and engineering of making intelligent machines, especially intelligent computer 
programs.  
It is related to the similar task of using computers to understand human intelligence, but  AI does 
not have to confine itself to methods that are biologically  observable  
Definition:  Artificial Intelligence  is the study of how to make computers do things, which, at the 
moment, people do better.  
According to the father of Artificial Intelligence, John McCarthy, it is  “The science and 
engineering of making intelli gent machines, especially intelligent computer programs”.  
Artificial Intelligence is a way of  making a computer, a computer -controlled robot, or a 
software think intelligently , in the similar manner the intelligent humans think.  
AI is accomplished by study ing how human brain thinks and how humans learn, decide, and 
work while trying to solve a problem, and then using the outcomes of this study as a basis of 
developing intelligent software and systems.  
It has gained prominence recently due, in part, to  big d ata, or the increase in speed, size and 
variety of data businesses are now collecting. AI can perform tasks such as identifying patterns 
in the data more efficiently than humans, enabling businesses to gain more insight out of 
their data.  
From a business perspective AI is a set of very powerful tools, and  methodologies for using 
those tools to solve business problems.  
From a programming perspective, AI includes the study of symbolic  programming, problem 
solving, and search.  
AI Vocabulary  
Intelligence relate s to tasks involving higher mental processes, e.g. creativity, solving problems, 
pattern recognition, classification, learning, induction, deduction, building analogies, 
optimization, language processing, knowledge and many more. Intelligence is the comput ational 
part of the ability to achieve goals.  
Intelligent behaviour is depicted by perceiving one’s environment, acting in complex 
environments, learning and understanding from experience, reasoning to solve problems and 
discover hidden knowledge, applying  knowledge successfully in new situations, thinking 
abstractly, using analogies, communicating with others and more.  4 
  
Science based goals of AI pertain to developing concepts, mechanisms and understanding 
biological intelligent behaviour. The emphasis is o n understanding intelligent behaviour.  
 
Engineering based goals of AI relate to developing concepts, theory and practice of building 
intelligent machines. The emphasis is on system building.  
 
AI Techniques depict how we represent, manipulate and reason wit h knowledge in order to 
solve problems. Knowledge is a collection of ‘facts’. To manipulate these facts by a program, a 
suitable representation is required. A good representation facilitates problem solving.  
 
Learning means that programs learn from what fa cts or behaviour can represent. Learning 
denotes changes in the systems that are adaptive in other words, it enables the system to do the 
same task(s) more efficiently next time.  
 
Applications of AI refers to problem solving, search and control strategies,  speech recognition, 
natural language understanding, computer vision, expert systems, etc.  
 
Problems of AI:  
Intelligence does not imply perfect understanding; every intelligent being has limited perception, 
memory and computation. Many points on the spectr um of intelligence versus cost are viable, 
from insects to humans. AI seeks to understand the computations required from intelligent 
behaviour and to produce computer systems that exhibit intelligence. Aspects of intelligence 
studied by AI include percepti on, communicational using human languages, reasoning, planning, 
learning and memory.  
 
The following questions are to be considered before we can step forward:  
1. What are the underlying assumptions about intelligence?  
2. What kinds of techniques will be us eful for solving AI problems?  
3. At what level human intelligence can be modelled?  
4. When will it be realized when an intelligent program has been built?  
 
Branches of AI : 
 
A list of branches of AI is given below. However some branches are surely missing,  because no 
one has identified them yet. Some of these may be regarded as concepts  or topics rather than full 
branches.  
 
Logical AI  — In general the facts of the specific situation in which it must act, and its goals are 
all represented by sentences of some  mathematical logical language. The program  decides what 
to do by inferring that certain actions are appropriate for achieving its  goals.  
 5 
 Search  — Artificial Intelligence programs often examine large numbers of possibilities – for 
example, moves in a ches s game and inferences by a theorem proving program.  Discoveries are 
frequently made about how to do this more efficiently in various  domains.  
 
Pattern Recognition  — When a program makes observations of some kind, it is often planned 
to compare  what it sees  with a pattern. For example, a vision program may try to match a  pattern 
of eyes and a nose in a scene in order to find a face. More complex patterns  are like a natural 
language text, a chess position or in the history of some event.  These more complex pa tterns 
require quite different methods than do the simple  patterns that have been studied the most.  
 
Representation  — Usually languages of mathematical logic are used to represent the facts about 
the world.  
 
Inference  — Others can be inferred from some fac ts. Mathematical logical deduction is 
sufficient  for some purposes, but new methods of non-monotonic inference have been added  to 
the logic since the 1970s. The simplest kind of non -monotonic reasoning is default  reasoning in 
which a conclusion is to be in ferred by default. But the conclusion can  be withdrawn if there is 
evidence to the divergent. For example, when we hear of a  bird, we infer that it can fly, but this 
conclusion can be reversed when we hear that  it is a penguin. It is the possibility that a  
conclusion may have to be withdrawn that  constitutes the non -monotonic character of the 
reasoning. Normal logical reasoning  is monotonic, in that the set of conclusions can be drawn 
from a set of premises, i.e.  monotonic increasing function of the premise s. Circumscription is 
another form of  non-monotonic reasoning.  
 
Common sense knowledge and Reasoning  — This is the area in which AI is farthest from the 
human level, in spite of the fact that  it has been an active research area since the 1950s. While 
there  has been considerable  progress in developing systems of non-monotonic reasoning and 
theories of action,  yet more new ideas are needed.  
 
Learning from experience  — There are some rules expressed in logic for learning. Programs 
can only learn what  facts or behaviour their formalisms can represent, and unfortunately learning 
systems  are almost all based on very limited abilities to represent information.  
 
Planning  — Planning starts with general facts about the world (especially facts about the effects  
of acti ons), facts about the particular situation and a statement of a goal. From  these, planning 
programs generate a strategy for achieving the goal. In the most  common cases, the strategy is 
just a sequence of actions.  
 
Epistemology  — This is a study of the kin ds of knowledge that are required for solving 
problems in  the world.  
 
Ontology  — Ontology is the study of the kinds of things that exist. In AI the programs and  
sentences deal with various kinds of objects and we study what these kinds are and  what their 
basic properties are. Ontology assumed importance from the 1990s.  
 6 
 Heuristics  — A heuristic is a way of trying to discover something or an idea embedded in a  
program. The term is used variously in AI. Heuristic functions are used in some  approaches to 
searc h or to measure how far a node in a search tree seems to be from  a goal. Heuristic 
predicates that compare two nodes in a search tree to see if one is  better than the other, i.e. 
constitutes an advance toward the goal, and may be more  useful.  
 
Genetic prog ramming  — Genetic programming is an automated method for creating a working 
computer  program from a high -level problem statement of a problem. Genetic programming  
starts from a high -level statement of ‘what needs to be done’ and automatically  creates a 
computer program to solve the problem.  
 
Applications of AI 
 
AI has applications in all fields of human study, such as finance and economics, environmental 
engineering, chemistry, computer science, and so on. Some of the applications of AI are listed 
below:  
 Perception  
■ Machine vision  
■ Speech understanding  
■ Touch ( tactile or haptic ) sensation  
 Robotics  
 Natural Language Processing  
■ Natural Language Understanding  
■ Speech Understanding  
■ Language Generation  
■ Machine Translation  
 Planning  
 Expert Systems  
 Machine Learning  
 Theorem Proving  
 Symbolic Mathematics  
 Game Playing  
 
AI Technique:  
 
Artificial Intelligence research during the last three decades has concluded that  Intelligence 
requires knowledge . To compensate overwhelming quality, knowledge  possesses less desirable 
properties.  
A. It is huge.  
B. It is difficult to characterize correctly.  
C. It is constantly varying.  
D. It differs from data by being organized in a way that corresponds to its  application.  
E. It is complicated.  
 
 7 
 An AI technique is a method that exploits kn owledge that is represented so that:  
 The knowledge captures generalizations that share properties, are grouped  
together, rather than being allowed separate representation.  
 
 It can be understood by people who must provide it —even though for many  
programs bu lk of the data comes automatically from readings.  
 
 In many AI domains, how the people understand the same people must supply  the 
knowledge to a program.  
 
 It can be easily modified to correct errors and reflect changes in real conditions.  
 
 It can be widely used even if it is incomplete or inaccurate.  
 
 It can be used to help overcome its own sheer bulk by helping to narrow the  range 
of possibilities that must be usually considered.  
 
In order to characterize an AI technique let us consider initially OXO or tic -tac-toe and use a 
series of different approaches to play the game.  
The programs increase in complexity, their use of generalizations, the clarity  of their 
knowledge and the extensibility of their approach. In this way they move  towards being 
representatio ns of AI techniques.  
 
Example -1: Tic-Tac-Toe 
 
1.1 The first approach (simple)  
 
The Tic -Tac-Toe game consists of a nine element vector called BOARD; it represents the 
numbers 1 to 9 in three rows.  
 
An element contains the value 0 for blank, 1 for X and 2 for O . A MOVETABLE vector consists 
of 19,683 elements (39) and is needed where each element is a nine element vector. The contents 
of the vector are especially chosen to help the algorithm.  
The algorithm makes moves by pursuing the following:  
1. View the vector as  a ternary number. Convert it to a decimal number.  
2. Use the decimal number as an index in MOVETABLE and access the vector.  
3. Set BOARD to this vector indicating how the board looks after the move. This approach is 
capable in time but it has several disadvanta ges. It takes more space and requires stunning 8 
 effort to calculate the decimal numbers. This method is specific to this game and cannot be 
completed.  
 
1.2 The second approach  
 
The structure of the data is as before but we use 2 for a blank, 3 for an X and 5 fo r an O. 
A variable called TURN indicates 1 for the first move and 9 for the last. The algorithm consists 
of three actions:  
MAKE2 which returns 5 if the centre square is blank; otherwise it returns any blank non -
corner square, i.e. 2, 4, 6 or 8. POSSWIN (p)  returns 0 if player p cannot win on the next move 
and otherwise returns the number of the square that gives a winning move.  
 It checks each line using products 3*3*2 = 18 gives a win for X, 5*5*2=50 gives a win 
for O, and the winning move is the holder of  the blank. GO (n) makes a move to square n setting 
BOARD[n] to 3 or 5.  
This algorithm is more involved and takes longer but it is more efficient in storage which 
compensates for its longer time. It depends on the programmer’s skill.  
 
1.3 The final approa ch 
 
The structure of the data consists of BOARD which contains a nine element vector, a list of 
board positions that could result from the next move and a number representing an estimation of how 
the board position leads to an ultimate win for the player t o move.  
This algorithm looks ahead to make a decision on the next move by deciding which the most 
promising move or the most suitable move at any stage would be and selects the same.  
Consider all possible moves and replies that the program can make. Conti nue this process for 
as long as time permits until a winner emerges, and then choose the move that leads to the computer 
program winning, if possible in the shortest time.  
Actually this is most difficult to program by a good limit but it is as far that the  technique can 
be extended to in any game. This method makes relatively fewer loads on the programmer in terms 
of the game technique but the overall game strategy must be known to the adviser.  
 
Example -2: Question Answering  
 
Let us consider Question Answer ing systems that accept input in English and provide 
answers also in English. This problem is harder than the previous one as it is more difficult to 
specify the problem properly. Another area of difficulty concerns deciding whether the answer 
obtained is correct, or not, and further what is meant by ‘correct’. For example, consider the 
following situation:  
 
2.1 Text  
 
Rani went shopping for a new Coat. She found a red one she really liked.  
When she got home, she found that it went perfectly with her favouri te dress.  
 
2.2 Question  
 
1. What did Rani go shopping for?  9 
 2. What did Rani find that she liked?  
3. Did Rani buy anything?  
 
Method 1  
 
2.3 Data Structures  
 
A set of templates that match common questions and produce patterns used to match  
against inputs. Tem plates and patterns are used so that a template that matches a  given question 
is associated with the corresponding pattern to find the answer in the  input text. For example, the 
template who did x y generates x y z if a match occurs  and z is the answer to the question. The 
given text and the question are both stored  as strings.  
 
2.4 Algorithm  
 
Answering a question requires the following four steps to be followed:  
 
 Compare the template against the questions and store all successful matches to produce a 
set of text patterns.  
 
 Pass these text patterns through a substitution process to change the person  or voice and 
produce an expanded set of text patterns.  
 
 Apply each of these patterns to the text; collect all the answers and then print  the 
answers.  
 
2.5 Exampl e 
 
In question 1 we use the template WHAT DID X Y which generates  Rani go shopping for z and 
after substitution we get  Rani goes shopping for z and Rani went shopping for z giving z 
[equivalence] a  new coat  
 
In question 2 we need a very large number of tem plates and also a scheme to allow  the insertion 
of ‘find’ before ‘that she liked’; the insertion of ‘really’ in the text; and  the substitution of ‘she’ 
for ‘Rani’ gives the answer ‘a red one’.  
 
Question 3 cannot be answered.  
 
2.6 Comments  
 
This is a very p rimitive approach basically not matching the criteria we set for  
intelligence and worse than that, used in the game. Surprisingly this type of technique  was 
actually used in ELIZA which will be considered later in the course.  
 
 
 10 
 Method 2  
2.7 Data Structure s 
 
A structure called English consists of a dictionary, grammar and some semantics  about 
the vocabulary we are likely to come across. This data structure provides the  knowledge to 
convert English text into a storable internal form and also to convert  the r esponse back into 
English. The structured representation of the text is a processed  form and defines the context of 
the input text by making explicit all references such  as pronouns. There are three types of such 
knowledge representation systems:  productio n rules of the form ‘if x then y’, slot and filler 
systems and statements in  mathematical logic. The system used here will be the slot and filler 
system.  
 
Take,  for example sentence:   
‘She found a red one she really liked’.  
 
Event2         Event2  
instance:    finding      instance:    liking  
tense:    past      tense:     past 
agent:    Rani      modifier:    much  
object:    Thing1     object:   Thing1  
 
Thing1  
instance:   coat 
colour:   red 
 
The question is stored in two forms: as input and in the above form.  
 
2.8 Algorithm  
 
 Convert the question to a structured form using English know how, then use  a marker to 
indicate the substring (like ‘who’ or ‘what’) of the structure, that  should be returned as an 
answer. If a slot and filler system is used a special  marke r can be placed in more than one 
slot. 
 The answer appears by matching this structured form against the structured  text. 
 The structured form is matched against the text and the requested segments  of the 
question are returned.  
 
2.9 Examples  
 
Both questions 1  and 2 generate answers via a new coat and a red coat respectively.  
Question 3 cannot be answered, because there is no direct response.  
 
2.10 Comments  
 
This approach is more meaningful than the previous one and so is more effective.  The 
extra power given m ust be paid for by additional search time in the knowledge  bases. A warning 11 
 must be given here: that is – to generate unambiguous  English  knowledge base is a complex task 
and must be left until later in the course. The  problems of handling pronouns are dif ficult.  
 
For example:  
Rani walked up to the salesperson: she asked where the toy department was.  
Rani walked up to the salesperson: she asked her if she needed any help.  
 
Whereas in the original text the linkage of ‘she’ to ‘Rani’ is easy, linkage of  ‘she’ in each of the 
above sentences to Rani and to the salesperson requires additional  knowledge about the context 
via the people in a shop.  
 
Method 3  
 
2.11 Data Structures  
 
World model contains knowledge about objects, actions and situations that are  describ ed 
in the input text. This structure is used to create integrated text from input  text. The diagram 
shows how the system’s knowledge of shopping might be  represented and stored. This 
information is known as a script and in this case is a  shopping script.  (See figure 1.1  next page )  
 
1.8.2.12 Algorithm  
 
Convert the question to a structured form using both the knowledge contained in Method 
2 and the World model, generating even more possible structures, since even more knowledge is 
being used. Sometimes filte rs are introduced to prune the possible answers.  
To answer a question, the scheme followed is:  Convert the question to a structured form 
as before but use the world model to resolve any ambiguities that may occur. The structured 
form is matched against th e text and the requested segments of the question are returned.  
 
2.13 Example  
 
Both questions 1 and 2 generate answers, as in the previous program. Question 3 can now 
be answered. The shopping script is instantiated and from the last sentence the path thro ugh step 
14 is the one used to form the representation. ‘M’ is bound to the red coat -got home. ‘ Rani buys 
a red coat’  comes from step 10 and the integrated text generates that she bought a red coat.  
 
2.14 Comments  
 
This program is more powerful than both t he previous programs because it has more 
knowledge. Thus, like the last game program it is exploiting AI techniques. However, we are not 
yet in a position to handle any English question. The major omission is that of a general 
reasoning mechanism known as inference to be used when the required answer is not explicitly 
given in the input text. But this approach can handle, with some modifications, questions of the 
following form with the answer —Saturday morning Rani went shopping. Her brother tried to 
call h er but she did not answer.  
 
Question :  Why couldn’t Rani’s brother reach her?  12 
 Answer :  Because she was not in.  
 
This answer is derived because we have supplied an additional fact that a person cannot 
be in two places at once. This patch is not sufficiently  general so as to work in all cases and does 
not provide  the type of solution we are really looking for.  
 
 
 13 
  
LEVEL OF THE AI MODEL  
 
‘What is our goal in trying to produce programs that do the intelligent things that people do?’  
 
Are we trying to produce p rograms that do the tasks the same way that people do?  
OR 
Are we trying to produce programs that simply do the tasks the easiest way that is 
possible?  
 
Programs in the first class attempt to solve problems that a computer can easily solve and 
do not usual ly use AI techniques. AI techniques usually include a search, as no direct method is 
available, the use of knowledge about the objects involved in the problem area and abstraction on 
which allows an element of pruning to occur, and to enable a solution to be found in real time; 
otherwise, the data could explode in size. Examples of these trivial problems in the first class, 
which are now of interest only to psychologists are EPAM (Elementary Perceiver and 
Memorizer) which memorized garbage syllables.  
 
The s econd class of problems attempts to solve problems that are non -trivial for a computer and 
use AI techniques. We wish to model human performance on these:  
 
1. To test psychological theories of human performance. Ex. PARRY [Colby, 1975] – a 
program to simulate  the conversational behavior of a paranoid person.  
2. To enable computers to understand human reasoning – for example, programs that 
answer questions based upon newspaper articles indicating human behavior.  
3. To enable people to understand computer reasoning. S ome people are reluctant to accept 
computer results unless they understand the mechanisms involved in arriving at the 
results.  
4. To exploit the knowledge gained by people who are best at gathering information. This 
persuaded the earlier workers to simulate h uman behavior in the SB part of AISB 
simulated behavior . Examples  of this type of approach led to GPS (General Problem 
Solver) . 
 
Questions  for Practice : 
 
1. What is intelligence ? How do we measure it? Are these measurements useful?  
2. When the temperature falls and the thermostat turns the heater on, does it act because it 
believes the room to be too cold? Does it feel cold? What sorts of things can have beliefs 
or feelings? Is this related to the idea of consciousness?  
3. Some people believe that the relationship b etween your mind (a non -physical thing) and 
your brain (the physical thing inside your skull) is exactly like the relationship between a 
computational process (a non -physical thing) and a physical computer. Do you agree?  
4. How good are machines at playing ch ess? If a machine can consistently beat all the best 
human chess players, does this prove that the machine is intelligent ? 
5. What is AI Technique? Explain Tic -Tac-Toe Problem using AI Technique.  
 14 
  
PROBLEMS, PROBLEM SPACES AND SEARCH  
 
 
To solve the problem of  building a system you should take the following steps:  
1. Define the problem accurately including detailed specifications and what constitutes a 
suitable solution.  
2. Scrutinize the problem carefully, for some features may have a central affect on the 
chosen method of solution.  
3. Segregate and represent the background knowledge needed in the solution of the 
problem.  
4. Choose the best solving techniques for the problem to solve a solution.  
 
Problem solving is a process of generating solutions from observe d data.  
• a ‘problem’ is characterized by a set of goals , 
• a set of objects , and  
• a set of operations . 
These could be ill -defined and may evolve during problem solving.  
• A ‘problem space ’ is an abstract space.  
 A problem space encompasses all valid state s that can be generated by the 
application of any combination of operators on any combination of objects . 
 The problem space may contain one or more solutions . A solution is a 
combination of operations and objects that achieve the goals . 
• A ‘ search ’ refers  to the search for a solution in a problem space.  
 Search proceeds with different types of ‘ search control strategies ’. 
 The depth -first search and breadth -first search are the two common search  
strategies.  
 
2.1 AI - General Problem Solving  
 
Problem solving has been the key area of concern for Artificial Intelligence.  
 
Problem solving is a process of generating solutions from observed or given data. It is however 
not always possible to use direct methods (i.e. go directly from data to solution). Instead, 
problem solving often needs to use indirect or modelbased methods.  
 
General Problem Solver (GPS) was a computer program created in 1957 by Simon and Newell 
to build a universal problem solver machine. GPS was based on Simon and Newell’s theoretical 
work on log ic machines. GPS in principle can solve any formalized symbolic problem, such as 
theorems proof and geometric problems and chess playing. GPS solved many simple problems, 
such as the Towers of Hanoi, that could be sufficiently formalized, but GPS could not  solve any 
real-world problems . 
 
To build a system to solve a particular problem, we need to:  
 Define the problem precisely – find input situations as well as final situations for an 
acceptable solution to the problem  15 
  Analyze  the problem – find few importan t features that may have impact on the 
appropriateness of various possible techniques for solving the problem  
 Isolate and represent task knowledge necessary to solve the problem  
 Choose the best problem -solving technique(s) and apply to the particular probl em 
 
 
Problem definitions  
A problem is defined by its ‘ elements ’ and their ‘ relations ’. To provide a formal  description of a 
problem, we need to do the following:  
 
a. Define a state space that contains all the possible configurations of the relevant objects, 
including some impossible ones.  
b. Specify one or more states that describe possible situations, from which the problem -
solving process may start. These states are called initial states . 
c. Specify one or more states that would be acceptable solution to the probl em. 
 
These states are called goal states . 
 
Specify a set of rules that describe the actions ( operators ) available.  
 
The problem can then be solved by using the rules , in combination with an  appropriate control 
strategy , to move through the problem space until a path from  an initial state to a goal state is 
found. This process is known as ‘search’ . Thus:  
 
 Search is fundamental to the problem -solving process.  
 Search is a general mechanism that can be used when a more direct 
method is not  known.  
 Search provid es the framework into which more direct methods for 
solving subparts of     a problem can be embedded. A very large number of 
AI problems are formulated as    search problems.  
 Problem space  
 
A problem space is represented by a directed graph, where nodes represent  search state and paths 
represent the operators applied to change the state .  
 
To simplify search algorithms, it is often convenient to logically and  programmatically represent 
a problem space as a tree. A tree usually decreases  the complexity of a  search at a cost. Here, the 
cost is due to duplicating some  nodes on the tree that were linked numerous times in the graph, 
e.g. node B and node D. 
 
A tree is a graph in which any two vertices are connected by exactly one  path. Alternatively, any 
connecte d graph with no cycles is a tree . 
 
 16 
 
 
 
• Problem solving : The term, Problem Solving relates to analysis in AI. Problem solving may be  
characterized as a systematic search through a range of possible actions to  reach some predefined 
goal or solution. Proble m-solving methods are  categorized as special purpose and general 
purpose . 
 
• A special -purpose method is tailor -made for a particular problem, often  exploits very specific 
features of the situation in which the problem is  embedded.  
 
• A general -purpose met hod is applicable to a wide variety of problems. One  General -purpose 
technique used in AI is ‘means -end analysis’: a step -bystep,  or incremental, reduction of the 
difference between current state and  final goal.  
 
 
 
 
 
 
 
 17 
 2.3 DEFINING PROBLEM AS A STATE SPAC E SEARCH  
 
To solve the problem of playing a game, we require the rules of the game and  targets for winning 
as well as representing positions in the game. The opening position  can be defined as the initial 
state and a winning position as a goal state. Moves  from  initial state to other states leading to the 
goal state follow legally. However, the  rules are far too abundant in most games — especially in 
chess, where they exceed  the number of particles in the universe. Thus, the rules cannot be 
supplied accurate ly and computer programs cannot handle easily. The storage also presents 
another  problem but searching can be achieved by hashing.  
 
The number of rules that are used must be minimized and the set can be  created by expressing 
each rule in a form as possible . The representation of games  leads to a state space representation 
and it is common for well -organized games  with some structure. This representation allows for 
the formal definition of a problem  that needs the movement from a set of initial positions to one 
of a set of target  positions. It means that the solution involves using known techniques and a 
systematic  search. This is quite a common method in Artificial Intelligence.  
 
 
 2.3.1 State Space Search  
A state space represents a problem in terms of state s and operators that change  states.  
A state space consists of:  
 A representation of the states the system can be in. For example, in a 
board game, the board represents the current state of the game.  
 A set of operators that can change one state into another state. In a board 
game, the operators are the legal moves from any given state. Often the 
operators are represented as programs that change a state representation to 
represent the new state.  
 An initial state . 
 A set of final states ; some of these may be des irable, others undesirable. 
This set is often represented implicitly by a program that detects terminal 
states.  
 
2.3.2 The Water Jug Problem  
 
In this problem, we use two jugs called four and three; four holds a maximum of  four gallons of 
water and three a maximum of three gallons of water. How can we  get two gallons of water in 
the four jug? 
 
The state space is a set of prearranged pairs giving the number of gallons of  water in the pair of 
jugs at any time, i.e., ( four, three ) where four = 0, 1, 2, 3 or 4  and three = 0, 1, 2 or 3.  
 
The start state is (0, 0) and the goal state is (2, n) where n may be any but it is  limited to three 
holding from 0 to 3 gallons of water or empty. Three and four  shows the name and numerical 
number shows the amount of water in ju gs for solving  the water jug problem. The major 
production rules for solving this problem are  shown below:  
 18 
 Initial condition        Goal comment  
1. (four, three) if four < 4      (4, three) fill four from tap  
2. (four, three) if three< 3      (four, 3) fil l three from tap  
3. (four, three) If four > 0      (0, three) empty four into drain  
4. (four, three) if three > 0      (four, 0) empty three into drain  
5. (four, three) if four + three<4  (four + three, 0) empty three into 
four 
6. (four, three) if four + t hree<3  (0, four + three) empty four into 
three  
7. (0, three) If three > 0      (three, 0) empty three into four  
8. (four, 0) if four > 0       (0, four) empty four into three  
9. (0, 2)        (2, 0) empty three into four  
10. (2, 0)        (0, 2) empty fou r into three  
11. (four, three) if four < 4      (4, three -diff) pour diff, 4 -four, into  
four from three  
12. (three, four) if three < 3      (four -diff, 3) pour diff, 3 -three, into  
three from four and a solution is 
given  below four three rule  
(Fig. 2.2 Prod uction Rules for the Water Jug Problem ) 
 
 
Gallons in Four Jug   Gallons in Three Jug    Rules Applied  
0      0      - 
0      3      2 
3      0      7 
3      3      2 
4      2      11 
0      2      3 
2      0      10 
(Fig. 2.3 One Solution to the Water Jug P roblem ) 
 
The problem solved by using the production rules in combination with an appropriate  control 
strategy, moving through the problem space until a path from an initial state  to a goal state is 
found. In this problem solving process, search is the fund amental  concept. For simple problems 
it is easier to achieve this goal by hand but there will  be cases where this is far too difficult.  
 
2.4 PRODUCTION SYSTEMS  
 
Production systems provide appropriate structures for performing and describing  search 
processe s. A production system has four basic components as enumerated  below.  
 
 A set of rules each consisting of a left side that determines the applicability of the 
rule and a right side that describes the operation to be performed if the rule is 
applied.  
 A datab ase of current facts established during the process of inference.  19 
  A control strategy that specifies the order in which the rules will be compared 
with facts in the database and also specifies how to resolve conflicts in selection 
of several rules or select ion of more facts.  
 A rule firing module.  
 
The production rules operate on the knowledge database. Each rule has a  precondition —that is, 
either satisfied or not by the knowledge database. If the  precondition is satisfied, the rule can be 
applied. Applicatio n of the rule changes  the knowledge database. The control system chooses 
which applicable rule should  be applied and ceases computation when a termination condition on 
the knowledge  database is satisfied.  
 
Example: Eight puzzle (8 -Puzzle)  
 
The 8 -puzzle is a 3 × 3 array containing eight square pieces, numbered 1 through 8,  and 
one empty space. A piece can be moved horizontally or vertically into the empty  space, in effect 
exchanging the positions of the piece and the empty space. There  are four possible move s, UP 
(move the blank space up), DOWN, LEFT and RIGHT.  The aim of the game is to make a 
sequence of moves that will convert the board  from the start state into the goal state:  
 
 
This example can be solved by the operator sequence UP, RIGHT, UP, LEFT, DOWN . 
 
Example: Missionaries and Cannibals  
 
The Missionaries and Cannibals problem illustrates the use of state space search for  planning 
under constraints:  
Three missionaries and three cannibals wish to cross a river using a two person boat. If 
at any time th e cannibals outnumber the missionaries on either side of the river, they will eat the 
missionaries. How can a sequence of boat trips be performed that will get everyone to the other 
side of the river without any missionaries being eaten?  
 
State representat ion: 
 
1. BOAT position: original (T) or final (NIL) side of the river.  
2. Number of Missionaries and Cannibals on the original side of the river.  
3. Start is (T 3 3); Goal is (NIL 0 0).  
 
Operators:  20 
 
 
 21 
 2.4.1 Control Strategies  
 
The word ‘ search ’ refers to the search for a solution in a problem space . 
• Search proceeds with different types of ‘search control strategies ’. 
• A strategy is defined by picking the order in which the nodes expand.  
The Search strategies are evaluated along the following dimensions:  Completeness, Time 
complexity, Space complexity, Optimality (the search - related  terms are first explained, and then 
the search algorithms and control strategies are  illustrated next).  
 
Search -related terms  
• Algorithm’s performance and complexity  
Ideally  we want a common measure so that we can compare approaches in  order to select 
the most appropriate algorithm for a given situation.  
 Performance of an algorithm depends on internal and external factors.  
 
Internal factors/ External factors  
  Time required to  run 
  Size of input to the algorithm  
  Space (memory) required to run  
  Speed of the computer  
  Quality of the compiler  
 Complexity is a measure of the performance of an algorithm. Complexity 
measures the internal factors, usually in time than space.  
• Computa tional complexity \ 
It is the measure of resources in terms of Time and Space . 
 
 If A is an algorithm that solves a decision problem f, then run -time of A is the number of 
steps taken on the input of length n. 
 Time Complexity T(n) of a decision problem f is the run -time of the ‘best’ algorithm A 
for f. 
 Space Complexity S(n) of a decision problem f is the amount of memory used by the 
‘best’ algorithm A for f. 
 
• ‘Big - O’ notation  
The Big-O, theoretical measure of the execution of an algorithm , usually  indicat es the time or the 
memory needed, given the problem size n, which is  usually the number of items.  
• Big-O notation  
The Big-O notation is used to give an approximation to the run-time- efficiency  of an algorithm; 
the letter ‘ O’ is for order of magnitude of operations or  space at run -time.  
• The Big-O of an Algorithm A 
 If an algorithm A requires time proportional to f(n), then algorithm A is said to be 
of order f(n), and it is denoted as O(f(n)) . 
 If algorithm A requires time proportional to n2, then the order  of the algorithm is 
said to be O(n2).  
 If algorithm A requires time proportional to n, then the order of the algorithm is 
said to be O(n).  22 
 The function f(n) is called the algorithm’s growth -rate function . In other words, if  an algorithm 
has performance com plexity O(n) , this means that the run -time t should  be directly proportional 
to n, ie t • n or t = k n where k is constant of proportionality.  
Similarly, for algorithms having performance complexity O(log2(n)), O(log N),  O(N log N), 
O(2N) and so on.  
 
Examp le 1: 
Determine the Big-O of an algorithm:  
 
Calculate the sum of the n elements in an integer array a[0..n -1]. 
 
Line no.   Instructions    No of execution steps  
line 1    sum     1 
line 2    for (i = 0; i < n; i++)   n + 1  
line 3    sum += a[i]    n 
line 4     print sum    1 
Total     2n + 3  
 
Thus, the polynomial (2n + 3) is dominated by the 1st term as n while the number  of elements in 
the array becomes very large.  
 
• In determining the Big-O, ignore constants such as 2 and 3. So the algorithm  is of order n. 
• So the Big-O of the algorithm is O(n) . 
• In other words the run -time of this algorithm increases roughly as the size of  the input data n, 
e.g., an array of size n. 
 
Tree structure  
Tree is a way of organizing objects, related in a hierarchical fashion.  
• Tree is a type of data structure in which each element is attached to one or  more 
elements directly beneath it.  
• The connections between elements are called branches . 
• Tree is often called inverted trees because it is drawn with the root at the top.  
• The elements that have no elements below them are called leaves . 
• A binary tree is a special type: each element has only two branches below it.  
Properties  
• Tree is a special case of a graph . 
• The topmost node in a tree is called the root node . 
• At root  node all operations on the tree begin.  
• A node has at most one parent.  
• The topmost node (root node) has no parents.  
• Each node has zero or more child nodes , which are below it .  
• The nodes at the bottommost level of the tree are called leaf nodes . 
Since leaf nodes are at the bottom most level, they do not have children.  
• A node that has a child is called the child’s parent node . 
• The depth of a node n is the length of the path from the root to the node.  
• The root node is at depth zero.  23 
 • Stacks and  Queues  
 
The Stacks and Queues are data structures that maintain the order of last-in, first-out and first-in, 
first-out respectively. Both stacks and queues are often  implemented as linked lists, but that is  
not the only possible implementation.  
Stack - Last In First Out (LIFO) lists  
 An ordered list; a sequence of items, piled one on top of the other.  
 The insertions and deletions are made at one end only, called Top. 
 If Stack S = (a[1], a[2], . . . . a[n]) then a[1] is bottom most element  
 Any intermediate element (a[i]) is on top of element a[i-1], 1 < i <= n.  
 In Stack all operation take place on Top. 
 
The Pop operation removes item from top of the stack.  
The Push operation adds an item on top of the stack.  
 
Queue - First In First Out (FIFO) lists  
 
• An ord ered list; a sequence of items; there are restrictions about how items  can be added to and 
removed from the list. A queue has two ends.  
• All insertions (enqueue ) take place at one end, called Rear or Back  
• All deletions (dequeue) take place at other end , called Front . 
• If Queue has a[n] as rear element then a[i+1] is behind a[i] , 1 < i <= n.  
• All operation takes place at one end of queue or the other.  
 
The Dequeue operation removes the item at Front of the queue.  
The Enqueue operation adds an item to the Rear of the queue.  
Search  
 
Search is the systematic examination of states to find path from the start / root state  to the goal 
state . 
 
• Search usually results from a lack of knowledge.  
• Search explores knowledge alternatives to arrive at the best ans wer. 
• Search algorithm output is a solution, that is, a path from the initial state to a  state that satisfies 
the goal test.  
 
For general -purpose problem -solving – ‘Search’ is an approach . 
 
• Search deals with finding nodes having certain properties in a graph that represents search 
space.  
• Search methods explore the search space ‘intelligently’, evaluating  possibilities without 
investigating every single possibility.  
Examples:  
• For a Robot this might consist of PICKUP, PUTDOWN, MOVEFORWARD,  MOVEBACK, 
MOVELEFT, and MOVERIGHT —until the goal is reached.  
• Puzzles and Games have explicit rules: e.g., the ‘ Tower of Hanoi ’ puzzle  24 
 
 
This puzzle involves a set of rings of different sizes that can be placed on  three different pegs.  
• The puzzle starts with the ri ngs arranged as shown in Figure 2.4(a)  
• The goal of this puzzle is to move them all as to Figure 2.4(b)  
• Condition: Only the top ring on a peg can be moved, and it may only be  placed on a smaller 
ring, or on an empty peg.  
 
In this Tower of Hanoi puzzle:  
• Situations encountered while solving the problem are described as states . 
• Set of all possible configurations of rings on the pegs is called ‘problem space ’. 
• States  
A State is a representation of elements in a given moment.  
A problem is defined by its  elements and their relations . 
At each instant of a problem, the elements have specific descriptors and  relations; the descriptors 
indicate how to select elements?  
Among all possible states, there are two special states called:  
  Initial state – the start p oint 
 Final state – the goal state  
• State Change: Successor Function  
A ‘successor function ’ is needed for state change. The Successor Function  moves one state to 
another state.  
Successor Function:  
 It is a description of possible actions; a set of operators . 
 It is a transformation function on a state representation, which converts that state into 
another state.  
 It defines a relation of accessibility among states.  
 It represents the conditions of applicability of a state and corresponding transformation 
functi on. 
 
• State space  
A state space is the set of all states reachable from the initial state . 
  A state space forms a graph (or map) in which the nodes are states and the arcs between 
nodes are actions.  
  In a state space , a path is a sequence of states connec ted by a sequence of actions.  
  The solution of a problem is part of the map formed by the state space . 25 
 • Structure of a state space  
The structures of a state space are trees and graphs . 
  A tree is a hierarchical structure in a graphical form.  
  A graph is a non-hierarchical structure.  
• A tree has only one path to a given node;  
i.e., a tree has one and only one path from any point to any other point.  
• A graph consists of a set of nodes (vertices) and a set of edges (arcs). Arcs  establish 
relationships (conn ections) between the nodes; i.e., a graph has  several paths to a given node.  
• The Operators are directed arcs between nodes.  
A search process explores the state space . In the worst case, the search explores  all possible 
paths between the initial state and the goal state . 
• Problem solution  
In the state space , a solution is a path from the initial state to a goal state or, sometimes, just a 
goal state . 
 A solution cost function assigns a numeric cost to each path; it also gives the cost of 
applying the opera tors to the states . 
 A solution quality is measured by the path cost function ; and an optimal solution has the 
lowest path cost among all solutions.  
 The solutions can be any or optimal or all . 
 The importance of cost depends on the problem and the type of so lution asked  
 
• Problem description  
A problem consists of the description of:  
 The current state of the world,  
 The actions that can transform one state of the world into another,  
 The desired state of the world.  
The following action one taken to describe the  problem:  
  State space is defined explicitly or implicitly  
A state space should describe everything that is needed to solve a problem  and nothing that is 
not needed to solve the problem.  
  Initial state is start state  
 Goal state is the conditions it has to fulfill.  
The description by a desired state may be complete or partial.  
 Operators are to change state  
 Operators do actions that can transform one state into another;  
 Operators consist of: Preconditions and Instructions;  
 
Preconditions provide partial descr iption of the state of the world that  must be true in order to 
perform the action, and  
Instructions tell the user how to create the next state.  
 Operators should be as general as possible, so as to reduce their number.  
 Elements of the domain has relevance t o the problem  
 Knowledge of the starting point.  
 Problem solving is finding a solution  
 Find an ordered sequence of operators that transform the current (start) state 
into a goal state.  26 
  Restrictions are solution quality any, optimal, or all  
 Finding the shorte st sequence, or  
 finding the least expensive sequence defining cost, or  
 finding any sequence as quickly as possible.  
This can also be explained with the help of algebraic function as given below.  
 
 
PROBLEM CHARACTERISTICS  
 
Heuristics cannot be generalized, as they are domain specific. Production systems  provide ideal 
techniques for representing such heuristics in the form of IF -THEN  rules. Most problems 
requiring simulation of intelligence use heuristic search  extensively. Some heuristics are used to 
define the control structure that guides the  search process, as seen in the example described 
above. But heuristics can also be  encoded in the rules to represent the domain knowledge. Since 
most AI problems  make use of knowledge and guided search through the know ledge, AI can be  
described as the study of techniques for solving exponentially hard problems in  polynomial time 
by exploiting knowledge about problem domain . 
 
To use the heuristic search for problem solving, we suggest analysis of the  problem for the 
following considerations:  
 Decomposability of the problem into a set of independent smaller subproblems  
 Possibility of undoing solution steps, if they are found to be unwise  
 Predictability of the problem universe  
 Possibility of obtaining an obvious solution to a problem without comparison of all other 
possible solutions  
 Type of the solution: whether it is a state or a path to the goal state  
 Role of knowledge in problem solving  
 Nature of solution process: with or without interacting with the user  
 
The general cla sses of engineering problems such as planning, classification,  diagnosis, 
monitoring and design are generally knowledge intensive and use a large  amount of heuristics. 
Depending on the type of problem, the knowledge  representation schemes and control strat egies 
for search are to be adopted. Combining  heuristics with the two basic search strategies have been 
discussed above. There are  a number of other general -purpose search techniques which are 
essentially heuristics  based. Their efficiency primarily depend s on how they exploit the domain -
specific  knowledge to abolish undesirable paths. Such search methods are called ‘weak  
methods’, since the progress of the search depends heavily on the way the domain  knowledge is 
exploited. A few of such search techniques which form the centre of  many AI systems are briefly 
presented in the following sections.  
 
Problem Decomposition  
 
Suppose to solve the expression is: + (X³ + X² + 2X + 3sinx)  dx 27 
 
 
This problem can be solved by breaking it into smaller problems, each of  which we can solve by 
using a small collection of specific rules. Using this technique  of problem decomposition, we can 
solve very large problems very easily. This can  be considered as an intelligent behaviour.  
 
Can Solution Steps be Ignored?  
 
Suppose we are  trying to prove a mathematical theorem: first we proceed considering  that 
proving a lemma will be useful. Later we realize that it is not at all useful. We  start with another 
one to prove the theorem. Here we simply ignore the first method.  
Consider the 8 -puzzle problem to solve: we make a wrong move and realize  that mistake. But 
here, the control strategy must keep track of all the moves, so that  we can backtrack to the initial 
state and start with some new move.  
Consider the problem of playing chess. Her e, once we make a move we never recover  from that 
step. These problems are illustrated in the three important classes of  problems mentioned below:  
1. Ignorable, in which solution steps can be ignored.  Eg: Theorem Proving  
2. Recoverable, in which solution s teps can be undone.  Eg: 8 -Puzzle  
3. Irrecoverable, in which solution steps cannot be undone.  Eg: Chess  
 
Is the Problem Universe Predictable?  
 
Consider the 8 -Puzzle problem. Every time we make a move, we know exactly  what will happen. 
This means that it is possible to plan an entire sequence of moves  and be confident what the 
resulting state will be. We can backtrack to earlier moves  if they prove unwise.  
 
Suppose we want to play Bridge. We need to plan before the first play, but we  cannot play with 
certaint y. So, the outcome of this game is very uncertain. In case  of 8-Puzzle, the outcome is 
very certain. To solve uncertain outcome problems, we  follow the process of plan revision as the 
plan is carried out and the necessary  feedback is provided. The disadvan tage is that the planning 
in this case is often very  expensive.  
 
Is Good Solution Absolute or Relative?  
Consider the problem of answering questions based on a database of simple facts  such as the 
following:  28 
 1. Siva was a man.  
2. Siva was a worker in a comp any. 
3. Siva was born in 1905.  
4. All men are mortal.  
5. All workers in a factory died when there was an accident in 1952.  
6. No mortal lives longer than 100 years.  
Suppose we ask a question: ‘Is Siva alive?’  
By representing these facts in a formal languag e, such as predicate logic, and  then using formal 
inference methods we can derive an answer to this question easily.  
There are two ways to answer the question shown below:  
Method I:  
1. Siva was a man.  
2. Siva was born in 1905.  
3. All men are mortal.  
4. Now  it is 2008, so Siva’s age is 103 years.  
5. No mortal lives longer than 100 years.  
Method II:  
1. Siva is a worker in the company.  
2. All workers in the company died in 1952.  
Answer: So Siva is not alive. It is the answer from the above methods.  
 
We are int erested to answer the question; it does not matter which path we  follow. If we follow 
one path successfully to the correct answer, then there is no  reason to go back and check another 
path to lead the solution.  
 
CHARACTERISTICS OF PRODUCTION SYSTEMS  
Produc tion systems provide us with good ways of describing the operations that can  be 
performed in a search for a solution to a problem.  
At this time, two questions may arise:  
1. Can production systems be described by a set of characteristics? And how can they be 
easily implemented?  
2. What relationships are there between the problem types and the types of production 
systems well suited for solving the problems?  
To answer these questions, first consider the following definitions of classes  of production 
systems:  
1. A mono tonic production system is a production system in which the application of a 
rule never prevents the later application of another rule that could also have been 
applied at the time the first rule was selected.  
2. A non -monotonic production system is one in wh ich this is not true.  
3. A partially communicative production system is a production system with the 
property that if the application of a particular sequence of rules transforms state P into 
state Q, then any combination of those rules that is allowable also  transforms state P 
into state Q.  
4. A commutative production system is a production system that is both monotonic and 
partially commutative.  
 29 
 
 
 
Is there any relationship between classes of production systems and classes  of problems? 
For any solvable problem s, there exist an infinite number of production  systems that show how 
to find solutions. Any problem that can be solved by any  production system can be solved by a 
commutative one, but the commutative one is  practically useless. It may use individual state s to 
represent entire sequences of  applications of rules of a simpler, non -commutative system. In the 
formal sense,  there is no relationship between kinds of problems and kinds of production systems  
Since all problems can be solved by all kinds of systems.  But in the practical sense,  there is 
definitely such a relationship between the kinds of problems and the kinds  of systems that lend 
themselves to describing those problems.  
 
Partially commutative, monotonic productions systems are useful for solving  ignorable 
problems. These are important from an implementation point of view  without the ability to 
backtrack to previous states when it is discovered that an  incorrect path has been followed. Both 
types of partially commutative production  systems are signific ant from an implementation point; 
they tend to lead to many  duplications of individual states during the search process.  Production 
systems that are not partially commutative are useful for many  problems in which permanent 
changes occur.  
 
Issues in the Des ign of Search Programs  
 
Each search process can be considered to be a tree traversal. The object of the  search is to find a 
path from the initial state to a goal state using a tree. The number  of nodes generated might be 
huge; and in practice many of the n odes would not be  needed. The secret of a good search 
routine is to generate only those nodes that are  likely to be useful, rather than having a precise 
tree. The rules are used to represent  the tree implicitly and only to create nodes explicitly if they 
are actually to be of  use. 
 
The following issues arise when searching:  
• The tree can be searched forward from the initial node to the goal state or  backwards from the 
goal state to the initial state.  
• To select applicable rules, it is critical to have an efficient procedure for  matching rules against 
states.  
• How to represent each node of the search process? This is the knowledge  representation 
problem or the frame problem. In games, an array suffices; in  other problems, more complex 
data structures are n eeded.  
 
Finally in terms of data structures, considering the water jug as a typical  problem do we use a 
graph or tree? The breadth -first structure does take note of all  nodes generated but the depth -first 
one can be modified.  30 
 Check duplicate nodes  
 
1. Obse rve all nodes that are already generated, if a new node is present.  
2. If it exists add it to the graph.  
3. If it already exists, then  
a. Set the node that is being expanded to the point to the already existing  node 
corresponding to its successor rather th an to the new one. The new  one can be thrown 
away.  
 
b. If the best or shortest path is being determined, check to see if this path is  better or 
worse than the old one. If worse, do nothing.  
 
Better save the new path and work the change in length through th e chain of successor  nodes if 
necessary.  
 
Example: Tic -Tac-Toe 
 
State spaces are good representations for board games such as Tic -Tac-Toe. The  position of a 
game can be explained by the contents of the board and the player  whose turn is next. The board 
can be represented as an array of 9 cells, each of  which may contain an X or O or be empty.  
• State:  
 Player to move next: X or O.  
 Board configuration:  
 
 
• Operators: Change an empty cell to X or O.  
• Start State: Board empty; X’s turn.  
• Terminal States: Three X’s in a row; Three O’s in a row; All cells full.  
 
Search Tree  
 
The sequence of states formed by possible moves is called a search tree . Each level  of the tree is 
called a ply. 
 
Since the same state may be reachable by different sequences of moves, the  state space may in 
general be a graph. It may be treated as a tree for simplicity, at  the cost of duplicating states.  31 
 
 
Solving problems using search  
• Given an informal description of the problem, construct a formal description  as a state space:  
 Define a data structure to represent the state . 
 Make a representation for the initial state from the given data.  
 Write programs to represent operators that change a given state representation to a new 
state representation.  
 Write a program to detect terminal states . 
 
• Choose an appropriate search technique:  
 How large is the search space?  
 How well structured is the domain?  
 What knowledge about the domain can be used to guide the search?  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 32 
  
HEURISTIC SEARCH TECHNIQUES:  
 
Search Algorithms  
Many traditiona l search algorithms are used in AI applications. For complex  problems, the 
traditional algorithms are unable to find the solutions within some  practical time and space 
limits. Consequently, many special techniques are  developed, using heuristic functions.  
The algorithms that use heuristic functions are called heuristic algorithms . 
 
• Heuristic algorithms are not really intelligent; they appear to be intelligent  because they 
achieve better performance.  
• Heuristic algorithms are more efficient because they t ake advantage of  feedback from the data 
to direct the search path.  
• Uninformed search algorithms or Brute -force algorithms , search through  the search space all 
possible candidates for the solution checking whether  each candidate satisfies the problem’s 
statement.  
• Informed search algorithms use heuristic functions that are specific to the  problem, apply 
them to guide the search through the search space to try to  reduce the amount of time spent in 
searching.  
 
A good heuristic will make an informed search d ramatically outperform any  uninformed search: 
for example, the Traveling Salesman Problem (TSP), where the  goal is to find is a good solution 
instead of finding the best solution.  
 
In such problems, the search proceeds using current information about the  problem to predict 
which path is closer to the goal and follow it, although it does  not always guarantee to find the 
best possible solution. Such techniques help in  finding a solution within reasonable time and 
space (memory). Some prominent  intelligent sea rch algorithms are stated below:  
1. Generate and Test Search  
2. Best -first Search  
3. Greedy Search  
4. A* Search  
5. Constraint Search  
6. Means -ends analysis  
 
There are some more algorithms. They are either improvements or combinations of  these.  
• Hierarc hical Representation of Search Algorithms: A Hierarchical  representation of most 
search algorithms is illustrated below. The  representation begins with two types of search:  
• Uninformed Search: Also called blind, exhaustive or brute -force search, it  uses n o 
information about the problem to guide the search and therefore may  not be very efficient.  
• Informed Search: Also called heuristic or intelligent search, this uses  information about the 
problem to guide the search —usually guesses the  distance to a goal state and is therefore 
efficient, but the search may not be  always possible.  
 33 
 
 
 
The first requirement is that it causes motion , in a game playing program, it moves  on the board 
and in the water jug problem, filling water is used to fill jugs. It means  the control strategies 
without the motion will never lead to the solution.  
The second requirement is that it is systematic , that is, it corresponds to the  need for global 
motion as well as for local motion. This is a clear condition that  neither would it be r ational to 
fill a jug and empty it repeatedly, nor it would be  worthwhile to move a piece round and round 
on the board in a cyclic way in a  game. We shall initially consider two systematic approaches for 
searching. Searches  can be classified by the order i n which operators are tried: depth -first, 
breadth -first, bounded  depth -first. 
 34 
 
 
Breadth -first search  
A Search strategy, in which the highest layer of a decision tree is searched completely  before 
proceeding to the next layer is called Breadth -first search  (BFS).  
• In this strategy, no viable solutions are omitted and therefore it is guaranteed  that an optimal 
solution is found.  
• This strategy is often not feasible when the search space is large.  
Algorithm  
1. Create a variable called LIST and set it to be the starting state.  
2. Loop until a goal state is found or LIST is empty, Do  
a. Remove the first element from the LIST and call it E. If the LIST is empty,  quit. 
b. For every path each rule can match the state E, Do  
(i) Apply the rule to generate a new sta te. 
(ii) If the new state is a goal state, quit and return this state.  
(iii) Otherwise, add the new state to the end of LIST.  
 
 35 
 Advantages  
1. Guaranteed to find an optimal solution (in terms of shortest number of steps  
to reach the goal).  
2. Can always fin d a goal node if one exists (complete).  
Disadvantages  
1. High storage requirement: exponential with tree depth.  
Depth -first search  
A search strategy that extends the current path as far as possible before backtracking  to the last 
choice point and trying th e next alternative path is called Depth -first search (DFS).  
• This strategy does not guarantee that the optimal solution has been found.  
• In this strategy, search reaches a satisfactory solution more rapidly than breadth  first, an 
advantage when the searc h space is large.  
Algorithm  
Depth -first search applies operators to each newly generated state, trying to drive  directly toward 
the goal.  
1. If the starting state is a goal state, quit and return success.  
2. Otherwise, do the following until success or fai lure is signalled:  
a. Generate a successor E to the starting state. If there are no more successors,  then signal failure.  
b. Call Depth -first Search with E as the starting state.  
c. If success is returned signal success; otherwise, continue in the loop.  
Advantages  
1. Low storage requirement: linear with tree depth.  
2. Easily programmed: function call stack does most of the work of maintaining  state of the 
search.  
Disadvantages  
1. May find a sub -optimal solution (one that is deeper or more costly than the  best solution).  
2. Incomplete: without a depth bound, may not find a solution even if one exists.  
2.4.2.3 Bounded depth -first search  
Depth -first search can spend much time (perhaps infinite time) exploring a very  deep path that 
does not contain a solution, w hen a shallow solution exists. An easy  way to solve this problem is 
to put a maximum depth bound on the search. Beyond  the depth bound, a failure is generated 
automatically without exploring any deeper.  
Problems:  
1. It’s hard to guess how deep the solution  lies. 
2. If the estimated depth is too deep (even by 1) the computer time used is  dramatically 
increased, by a factor of bextra . 
3. If the estimated depth is too shallow, the search fails to find a solution; all  that computer time 
is wasted.  
 
Heuristics  
A heuristic is a method that improves the efficiency of the search process. These are  like tour 
guides. There are good to the level that they may neglect the points in  general interesting 
directions; they are bad to the level that they may neglect points  of interest to particular 
individuals. Some heuristics help in the search process without  sacrificing any claims to entirety 
that the process might previously had. Others may  occasionally cause an excellent path to be 
overlooked. By sacrificing entirety it  increases efficiency. Heuristics may not find the best 36 
 solution every time but  guarantee that they find a good solution in a reasonable time. These are 
particularly  useful in solving tough and complex problems, solutions of which would require  
infinite time , i.e. far longer than a lifetime for the problems which are not solved in  any other 
way.  
Heuristic search  
To find a solution in proper time rather than a complete solution in unlimited time  we use 
heuristics. ‘A heuristic function is a function that maps from problem state  descriptions to 
measures of desirability, usually represented as numbers’. Heuristic  search methods use 
knowledge about the problem domain and choose promising  operators first. These heuristic 
search methods use heuristic functions to ev aluate  the next state towards the goal state. For 
finding a solution, by using the heuristic  technique, one should carry out the following steps:  
1. Add domain —specific information to select what is the best path to continue  searching along.  
2. Define a he uristic function h(n) that estimates the ‘goodness’ of a node n.  
Specifically, h(n) = estimated cost(or distance) of minimal cost path from n     to a goal state.  
3. The term, heuristic means ‘serving to aid discovery’ and is an estimate, based  on domain 
specific information that is computable from the current state  description of how close we are to 
a goal.  
Finding a route from one city to another city is an example of a search problem in  which 
different search orders and the use of heuristic knowledge are easily  understood.  
1. State: The current city in which the traveller is located.  
2. Operators: Roads linking the current city to other cities.  
3. Cost Metric: The cost of taking a given road between cities.  
4. Heuristic information: The search could be gui ded by the direction of the  goal city from the 
current city, or we could use airline distance as an estimate  of the distance to the goal.  
Heuristic search techniques  
For complex problems, the traditional algorithms, presented above, are unable to  find the 
solution within some practical time and space limits. Consequently, many  special techniques are 
developed, using heuristic functions.  
• Blind search is not always possible, because it requires too much time or  Space (memory).  
 
Heuristics are rules of thumb ; they do not guarantee a solution to a problem.  
• Heuristic Search is a weak technique but can be effective if applied correctly;  it requires 
domain specific information.  
 
Characteristics of heuristic search  
• Heuristics are knowledge about domain, which help search and reasoning in  its domain.  
• Heuristic search incorporates domain knowledge to improve efficiency over  blind search.  
• Heuristic is a function that, when applied to a state, returns value as estimated  merit of state, 
with respect to goal.  
 Heuristics might (for reasons) underestimate or overestimate the merit of a state with 
respect to goal.  
 Heuristics that underestimate are desirable and called admissible.  
• Heuristic evaluation function estimates likelihood of given state leading to  goal sta te. 
• Heuristic search function estimates cost from current state to goal, presuming  function is 
efficient.  
 37 
  
Heuristic search compared with other search  
The Heuristic search is compared with Brute force or Blind search techniques below:  
 
Comparison of Alg orithms  
 
Brute force / Blind search     Heuristic search  
Can only search what it has knowledge   Estimates ‘distance’ to goal state  
about already       through explored nodes  
 
No knowledge about how far a node    Guides search process toward goal  
node from  goal state  
Prefers states (nodes) that lead  
close to and not away from goal  
state 
 
Example: Travelling salesman  
A salesman has to visit a list of cities and he must visit each city only once. There  are different 
routes between the cities. The problem is t o find the shortest route  between the cities so that the 
salesman visits all the cities at once.  
 
Suppose there are N cities, then a solution would be to take N! possible  combinations to find the 
shortest distance to decide the required route. This is not  efficient as with N=10 there are 
36,28,800 possible routes. This is an example of  combinatorial explosion . 
 
There are better methods for the solution of such problems: one is called  branch and bound . 
First, generate all the complete paths and find the dist ance of the first complete  path. If the next 
path is shorter, then save it and proceed this way avoiding the path  when its length exceeds the 
saved shortest path length, although it is better than the  previous method.  
 
Generate a nd Test Strategy  
 
Generate -And-Test Algorithm  
Generate -and-test search algorithm is a very simple algorithm that guarantees to find a solution if 
done systematically and there exists a solution.  
Algorithm: Generate -And-Test 
1.Generate a possible solution.  
2.Test to see if this is th e expected solution.  
3.If the solution has been found quit else go to step 1.  
Potential solutions that need to be generated vary depending on the kinds of problems. For some 
problems the possible solutions may be particular points in the problem space and for some 
problems, paths from the start state.  38 
 
 
Figure: Generate And Test  
Generate -and-test, like depth -first search, requir es that complete solutions be generated for 
testing. In its most systematic form, it is only an exhaustive search of the problem space. 
Solutions can also be generated randomly but solution is not guaranteed. This approach is what is 
known as British Museu m algorithm: finding an object in the British Museum by wandering 
randomly.  
Systematic Generate -And-Test 
While generating complete solutions and generating random solutions are the two extremes there 
exists another approach that lies in between. The approa ch is that the search process proceeds 
systematically but some paths that unlikely to lead the solution are not considered. This 
evaluation is performed by a heuristic function.  
Depth -first search tree with backtracking can be used to implement systematic generate -and-test 
procedure. As per this procedure, if some intermediate states are likely to appear often in the 
tree, it would be better to modify that procedure to traverse a graph rather than a tree.  
Generate -And-Test And Planning  
Exhaustive generate -and-test is very useful for simple problems. But for complex problems even 
heuristic generate -and-test is not very effective technique. But this may be made effective by 
combining with other techniques in such a way that the space in which to search is rest ricted. An 
AI program DENDRAL, for example, uses plan -Generate -and-test technique. First, the planning 
process uses constraint -satisfaction techniques and creates lists of recommended and 
contraindicated substructures. Then the generate -and-test procedure uses the lists generated and 
required to explore only a limited set of structures. Constrained in this way, generate -and-test 
proved highly effective. A major weakness of planning is that it often produces inaccurate 
solutions as there is no feedback from the world. But if it is used to produce only pieces of 
solutions then lack of detailed accuracy becomes unimportant.  
 39 
 Hill Climbing  
Hill Climbing is heuristic search used for mathematical optimization problems in the field of 
Artificial Intelligence .  
Give n a large set of inputs and a good heuristic function, it tries to find a sufficiently good 
solution to the problem. This solution may not be the global optimal maximum.  
 In the above definition,  mathematical optimization problems  implies that hill climbing  
solves the problems where we need to maximize or minimize a given real function by 
choosing values from the given inputs. Example -Travelling salesman problem  where we 
need to  minimize the distance traveled by salesman.  
 ‘Heuristic search’ means that this search algorithm may not find the optimal solution to 
the problem. However, it will give a good solution in  reasonable time.  
 A heuristic function  is a function that will rank a ll the possible alternatives at any 
branching step in search algorithm based on the available information. It helps the 
algorithm to select the best route out of possible routes.  
Features of Hill Climbing  
1. Variant of generate and test algorithm :  It is a va riant of generate and test algorithm. The 
generate and test algorithm is as follows :  
 
1. Generate a possible solutions.  
2. Test to see if this is the expected solution.  
3. If the solution has been found quit else go to step 1.  
Hence we call Hill climbing as a variant of generate and test algorithm as it takes the feedback 
from test procedure. Then this feedback is utilized by the generator in deciding the next move in 
search space.  
2. Uses the  Greedy approach  : At any point in state space, the search moves in  that direction 
only which optimizes the cost of function with the hope of finding the optimal solution at 
the end.  
Types of Hill Climbing  
 
 
 
1. Simple Hill climbing :  It examines the neighboring nodes one by one and selects the first 
neighboring node which o ptimizes the current cost as next node.  
Algorithm for Simple Hill climbing  : 
Step 1 :  Evaluate the initial state. If it is a goal state then stop and return success. Otherwise, 
make initial state as current state.  
Step 2 :  Loop until the solution state is found or there are no new operators present which can be 
applied to current state.  
a) Select a state that has not been yet applied to the current state and apply it to produce a new 
state.  
b) Perform these to evaluate new state  
    i. If the current state is a goal state, then stop and return success.  
    ii. If it is better than the current state, then make it current state and proceed further.  
    iii. If it is not better than the current state, then continue in the loop until a solution is found.  
 
Step 3  : Exit. 40 
 2. Steepest -Ascent Hill climbing :  It first examines all the neighboring nodes and then 
selects the node closest to the solution state as next node.  
 
Step 1 :  Evaluate the initial state. If it is goal state then exit else make the current state as in itial 
state  
Step 2 :  Repeat these steps until a solution is found or current state does not change  
i. Let ‘target’ be a state such that any successor of the current state will be better than it;  
ii. for each operator that applies to the current state  
     a. apply the new operator and create a new state  
     b. evaluate the new state  
     c. if this state is goal state then quit else compare with ‘target’  
     d. if this state is better than ‘target’, set this state as ‘target’  
     e. if target is better t han current state set current state to Target  
Step 3 :  Exit 
3. Stochastic hill climbing :  It does not examine all the neighboring nodes before deciding 
which node to select .It just selects a neighboring node at random, and decides (based on 
the amount of imp rovement in that neighbor) whether to move to that neighbor or to 
examine another.  
State Space diagram for Hill Climbing  
State space diagram is a graphical representation of the set of states our search algorithm can 
reach vs the value of our objective fun ction(the function which we wish to maximize).  
X-axis :  denotes the state space ie states or configuration our algorithm may reach.  
Y-axis :  denotes the values of objective function corresponding to to a particular state.  
The best solution will be that sta te space where objective function has maximum value(global 
maximum).  
 
Different regions in the State Space Diagram  
1. Local maximum :  It is a state which is better than its neighbori ng state however there 
exists a state which is better than it(global maximum). This state is better because here 
value of objective function is higher than its neighbors.  41 
 2. Global maximum :  It is the best possible state in the state space diagram. This becau se at 
this state, objective function has highest value.  
3. Plateua/flat local maximum :  It is a flat region of state space where neighboring states 
have the same value.  
4. Ridge :  It is region which is higher than its neighbours but itself has a slope. It is a s pecial 
kind of local maximum.  
5. Current state :  The region of state space diagram where we are currently present during 
the search.  
6. Shoulder :  It is a plateau that has an uphill edge.  
Problems in different regions in Hill climbing  
Hill climbing cannot reach the optimal/best state(global maximum) if it enters any of the 
following regions :  
1. Local maximum :  At a local maximum all neighboring states have a values which is 
worse than than the current state. Since hill climbing uses greedy approach, it will not 
move to the worse state and terminate itself. The process will end even though a better 
solution may exist.  
To overcome local maximum problem :  Utilize  backtracking technique . Maintain a list of 
visited states. If the search reaches an undesirable state, it c an backtrack to the previous 
configuration and explore a new path.  
2. Plateau :  On plateau all neighbors have same value . Hence, it is not possible to select the 
best direction.  
To overcome plateaus :  Make a big jump. Randomly select a state far away from cu rrent state. 
Chances are that we will land at a non -plateau region  
3. Ridge :  Any point on a ridge can look like peak because movement in all possible 
directions is downward. Hence the algorithm stops when it reaches this state.  
To overcome Ridge :  In this ki nd of obstacle, use two or more rules before testing. It 
implies moving in several directions at once.  
 
Best First Search (Informed Search)  
 
In BFS and DFS, when we are at a node, we can consider any of the adjacent as next 
node. So both BFS and DFS blindl y explore paths without considering any cost function. The 
idea of Best First Search is to use an evaluation function to decide which adjacent is most 
promising and then explore. Best First Search falls under the category of Heuristic Search or 
Informed Se arch.  
We use a priority queue to store costs of nodes. So the implementation is a variation of BFS, we 
just need to change Queue to PriorityQueue.  
 
Algorithm:  
Best-First-Search (Grah g, Node start)  
    1) Create an empty PriorityQueue  
       PriorityQueue pq; 
    2) Insert "start" in pq.  
       pq.insert(start)  
    3) Until PriorityQueue is empty  
          u = PriorityQueue.DeleteMin  42 
           If u is the goal  
             Exit 
          Else 
             Foreach neighbor v of u  
                If v "Unvisit ed" 
                    Mark v "Visited"                     
                    pq.insert(v)  
             Mark v "Examined"                     
End procedure  
Let us consider below example.  
 
 
 
 
We start from source "S" and search for  
goal "I" using given costs and Best  
First search.  
 
pq initially contains S  
We remove s from and process unvisited  
neighbors of S to pq.  
pq now contains {A, C, B} (C is put  
before B because C has lesser cost)  
 
We remove A from pq and process unvisited  
neighbors of A to pq.  
pq now contains {C, B, E, D}  43 
  
We remove C from pq and process unvisite d 
neighbors of C to pq.  
pq now contains {B, H, E, D}  
 
We remove B from pq and process unvisited  
neighbors of B to pq.  
pq now contains {H, E, D, F, G}  
 
We remove H from pq.  Since our goal  
"I" is a neighbor of H, we return.  
Analysis :  
 The worst case time co mplexity for Best First Search is O(n * Log n) where n is number 
of nodes. In worst case, we may have to visit all nodes before we reach goal. Note that 
priority queue is implemented using Min(or Max) Heap, and insert and remove 
operations take O(log n) ti me. 
 Performance of the algorithm depends on how well the cost or evaluation function is 
designed.  
 
A* Search Algorithm  
 
A* is a type of search algorithm. Some problems can be solved by representing the world in the 
initial state, and then for each action w e can perform on the world we generate states for what the 
world would be like if we did so. If you do this until the world is in the state that we specified as 
a solution, then the route from the start to this goal state is the solution to your problem.  
 
In this tutorial I will look at the use of state space search to find the shortest path between two 
points (pathfinding), and also to solve a simple sliding tile puzzle (the 8 -puzzle). Let's look at 
some of the terms used in Artificial Intelligence when de scribing this state space search.  
 
Some terminology  
 
A node  is a state that the problem's world can be in. In pathfinding a node would be just a 2d 
coordinate of where we are at the present time. In the 8 -puzzle it is the positions of all the tiles.  
Next a ll the nodes are arranged in a  graph  where links between nodes represent valid steps in 
solving the problem. These links are known as  edges . In the 8 -puzzle diagram the edges are 
shown as blue lines. See figure 1 below.  
State space search , then, is solving  a problem by beginning with the start state, and then for each 
node we expand all the nodes beneath it in the graph by applying all the possible moves that can 
be made at each point.  
 
Heuristics and Algorithms  
 
At this point we introduce an important conc ept, the  heuristic . This is like an algorithm, but with 
a key difference. An algorithm is a set of steps which you can follow to solve a problem, which 
always works for valid input. For example you could probably write an algorithm yourself for 44 
 multiplying  two numbers together on paper. A heuristic is not guaranteed to work but is useful in 
that it may solve a problem for which there is no algorithm.  
We need a heuristic to help us cut down on this huge search problem. What we need is to use our 
heuristic at  each node to make an estimate of how far we are from the goal. In pathfinding we 
know exactly how far we are, because we know how far we can move each step, and we can 
calculate the exact distance to the goal.  
But the 8 -puzzle is more difficult. There is no known algorithm for calculating from a given 
position how many moves it will take to get to the goal state. So various heuristics have been 
devised. The best one that I know of is known as the Nilsson score which leads fairly directly to 
the goal most o f the time, as we shall see.  
 
Cost 
 
When looking at each node in the graph, we now have an idea of a heuristic, which can estimate 
how close the state is to the goal. Another important consideration is the cost of getting to where 
we are. In the case of pa thfinding we often assign a movement cost to each square. The cost is 
the same then the cost of each square is one. If we wanted to differentiate between terrain types 
we may give higher costs to grass and mud than to newly made road. When looking at a nod e we 
want to add up the cost of what it took to get here, and this is simply the sum of the cost of this 
node and all those that are above it in the graph.  
 
8 Puzzle  
Let's look at the 8 puzzle in more detail. This is a simple sliding tile puzzle on a 3*3 g rid where 
one tile is missing and you can move the other tiles into the gap until you get the puzzle into the 
goal position. See figure 1.  
 
Figure 1 : The 8 -Puzzle state space for a very simple example   
 
There are 362,880 different states that the puzzle can be in, and to find a solution the search has 
to find a route through them. From most positions of the search the number of edges (that's the 45 
 blue lines) is two. That means that the number of nodes you have in each level of the search is 
2^d where d is the depth. If the number of steps to solve a particular state is 18, then that �s 
262,144 nodes just at that level.  
 
The 8 puzzle game state is as simple as representing a list of the 9 squares and what's in them. 
Here are two states for example; the last one is the GOAL state, at which point we've found the 
solution. The first is a jumbled up example that you may start from.  
 
Start state SPACE, A, C, H, B, D, G, F, E  
Goal state A, B, C, H, SPACE, D, G, F, E  
The rules that you can apply to the puzzle are also simple. If there is a blank tile above, below, to 
the left or to the right o f a given tile, then you can move that tile into the space. To solve the 
puzzle you need to find the path from the start state, through the graph down to the goal state.  
 
There is example code to to solve the 8 -puzzle on the  github  site. 
 
 
Pathfinding  
In a video game, or some other pathfinding scenario, you want to search a state space and find 
out how to get from somewhere you are to somewhere you want to be, without bu mping into 
walls or going too far. For reasons we will see later, the A* algorithm will not only find a path, if 
there is one, but it will find the shortest path. A state in pathfinding is simply a position in the 
world. In the example of a maze game like Pacman you can represent where everything is using 
a simple 2d grid. The start state for a ghost say, would be the 2d coordinate of where the ghost is 
at the start of the search. The goal state would be where pacman is so we can go and eat him. 
There is al so example code to do pathfinding on the  github  site. 
 
Figure 2 : The first three steps of  a pathfinding state space  
 
 46 
 Implementing A*  
We are now ready to look at the operation of the A* algorithm. What we need to do is start with 
the goal state and then generate the graph downwards from there. Let's take the 8 -puzzle in 
figure 1. We ask how ma ny moves can we make from the start state? The answer is 2, there are 
two directions we can move the blank tile, and so our graph expands.  
If we were just to continue blindly generating successors to each node, we could potentially fill 
the computer's memo ry before we found the goal node. Obviously we need to remember the best 
nodes and search those first. We also need to remember the nodes that we have expanded 
already, so that we don't expand the same state repeatedly.  
Let's start with the OPEN list. This  is where we will remember which nodes we haven't yet 
expanded. When the algorithm begins the start state is placed on the open list, it is the only state 
we know about and we have not expanded it. So we will expand the nodes from the start and put 
those o n the OPEN list too. Now we are done with the start node and we will put that on the 
CLOSED list. The CLOSED list is a list of nodes that we have expanded.  
 
f = g + h  
 
Using the OPEN and CLOSED list lets us be more selective about what we look at next in t he 
search. We want to look at the best nodes first. We will give each node a score on how good we 
think it is. This score should be thought of as the cost of getting from the node to the goal plus 
the cost of getting to where we are. Traditionally this has  been represented by the letters f, g and 
h. 'g' is the sum of all the costs it took to get here, 'h' is our heuristic function, the estimate of 
what it will take to get to the goal. 'f' is the sum of these two. We will store each of these in our 
nodes.  
Using the f, g and h values the A* algorithm will be directed, subject to conditions we will look 
at further on, towards the goal and will find it in the shortest route possible.  
 
So far we have looked at the components of the A*, let's see how they all fit together to make the 
algorithm :  
 
Pseudocode  
 
 
Hopefully the ideas we looked at in the preceding paragraphs will now click into place as we 
look at the A* algorithm pseudocode. You may find it helpful t o print this out or leave the 
window open while we discuss it.  
 
To help make the operation of the algorithm clear we will look again at the 8 -puzzle problem in 
figure 1 above. Figure 3 below shows the f,g and h scores for each of the tiles.  47 
 
 
Figure 3 : 8 -Puzzle state space showing f,g,h scores   
 
First of all look at the g score for each node. This is the cost of what it took to get from the start 
to that node. So in the picture the  center number is g. As you can see it increases by one at each 
level. In some problems the cost may vary for different state changes. For example in 
pathfinding there is sometimes a type of terrain that costs more than other types.  
Next look at the last n umber in each triple. This is h, the heuristic score. As I mentioned above I 
am using a heuristic known as Nilsson's Sequence, which converges quickly to a correct solution 
in many cases. Here is how you calculate this score for a given 8 -puzzle state :  
 
Advantages:  
  
It is complete and optimal.  
  
It is the best one from other techniques. It is used to solve very complex problems.  
  
It is optimally efficient, i.e. there is no other optimal algorithm guaranteed to expand fewer nodes 
than A*.  
  
Disadvantages:  
  
This algorithm is complete if the branching factor is finite and every action has fixed cost.  
  
  
  
The speed execution of A* search is highly dependant on the accuracy of the heuristic algorithm 
that is used to compute h (n).  48 
  
 
AO* Search: (And -Or) Gra ph 
  
The Depth first search and Breadth first search given earlier for OR trees or graphs can be easily 
adopted by AND -OR graph. The main difference lies in the way termination conditions are 
determined, since all goals following an AND nodes must be reali zed; where as a single goal 
node following an OR node will do. So for this purpose we are using AO* algorithm.  
  
Like A* algorithm here we will use two arrays and one heuristic function.  
  
OPEN:  
  
It contains the nodes that has been traversed but yet not b een marked solvable or unsolvable.  
  
CLOSE : 
  
It contains the nodes that have already been processed.  
  
6 7:The distance from current node to goal node.  
  
  
  
Algorithm:  
  
Step 1:  Place the starting node into OPEN.  
  
Step 2:  Compute the most promising solu tion tree say T0.  
  
Step 3:  Select a node n that is both on OPEN and a member of T0. Remove it from OPEN and 
place it in  
  
CLOSE  
  
Step 4:  If n is the terminal goal node then leveled n as solved and leveled all the ancestors of n 
as solved.  If the starting  node is marked as solved then success and exit.  
  
Step 5:  If n is not a solvable node, then mark n as unsolvable. If starting node is marked as 
unsolvable,  then return failure and exit.  
  
Step 6:  Expand n. Find all its successors and find their h (n) valu e, push them into OPEN.  
  
Step 7:  Return to Step 2.  
  
Step 8:  Exit.  
  49 
 Implementation:  
  
Let us take the following example to implement the AO* algorithm.  
 
  
Step 1:  
  
In the above graph, the solvable nodes are A, B, C, D, E, F and the unsolvable nodes are G, H. 
Take A as the starting node. So place A into OPEN.  
  
 
 
 50 
 
 
 51 
 
 
 52 
 
 
 
 
  
Advantages:  
  
It is an optimal algorithm.  
  
If traverse according to the ordering of nodes. It can be used for both OR and AND graph.  
  
Disadvantages:  
  
Sometimes for unsolvable nodes, it can’t find the optimal path. Its complexity is than other 
algorithms.  
 
PROBLEM REDUCTION   
 
Problem Reduction with AO* Algorithm.  
 
When a problem can be divided into a set of sub problems, where each sub problem can be 
solved separately and a combination of these will be a solution, AND -OR graphs or AND - OR 
trees are used for representing the solution. The decomposition of the problem or problem 
reduction generates AND arcs. One AND are may point to any number of successor nodes. All 53 
 these must be solved so that the arc will rise to many arcs, indicating several po ssible solutions. 
Hence the graph is known as AND - OR instead of AND. Figure shows an AND - OR graph.  
 
 
An algorithm to find a solution in an AND - OR graph must handle AND area appropriately. A* 
algorithm can not search AND - OR graphs efficiently. Thi s can be understand from the give 
figure.  
 
FIGURE : AND - OR graph  
 
In figure (a) the top node A has been expanded producing two area one leading to B and leading 
to C-D . the numbers at each node represent the value of f ' at that node (cost of getting to the 
goal state from current state). For simplicity, it is assumed that every operation(i.e. applying a 
rule) has unit cost, i.e., each are with single successor will have a cost of 1 and each of its 
components. With the available information till now , it appears that C is the most promising 
node to expand since its f ' = 3 , the lowest but going through B would be better since to use C 
we must also use D' and the cost would be 9(3+4+1+1). Through B it would be 6(5+1).  
 
Thus the choice of the next node t o expand depends not only n a value but also on whether that 
node is part of the current best path form the initial mode. Figure (b) makes this clearer. In figure 
the node G appears to be the most promising node, with the least f ' value. But G is not on t he 
current beat path, since to use G we must use GH with a cost of 9 and again this demands that 
arcs be used (with a cost of 27). The path from A through B, E -F is better with a total cost of 
(17+1=18). Thus we can see that to search an AND -OR graph, the following three things must 
be done.  
1. traverse the graph starting at the initial node and following the current best path, and 
accumulate the set of nodes that are on the path and have not yet been expanded.  
 
2. Pick one of these unexpanded nodes and exp and it. Add its successors to the graph and 
computer f ' (cost of the remaining distance) for each of them.  
 54 
 3. Change the f ' estimate of the newly expanded node to reflect the new information produced 
by its successors. Propagate this change backward thr ough the graph. Decide which of the 
current best path.  
 
The propagation of revised cost estimation backward is in the tree is not necessary in A* 
algorithm. This is because in AO* algorithm expanded nodes are re -examined so that the current 
best path can b e selected. The working of AO* algorithm is illustrated in figure as follows:  
 
Referring the figure. The initial node is expanded and D is Marked initially as promising node. D 
is expanded producing an AND arc E -F. f ' value of D is updated to 10. Going backwards we can 
see that the AND arc B -C is better . it is now marked as current best path. B and C have to be 
expanded next. This process continues until a solution is found or all paths have led to dead ends, 
indicating that there is no solution. An A* algorithm the path from one node to the other is 
always that of the lowest cost and it is independent of the paths through other nodes.  
 
The algorithm for performing a heuristic search of an AND - OR graph is given below. Unlike 
A* algorithm which used two  lists OPEN and CLOSED, the AO* algorithm uses a single 
structure G. G represents the part of the search graph generated so far. Each node in G points 
down to its immediate successors and up to its immediate predecessors, and also has with it the 
value of h' cost of a path from itself to a set of solution nodes. The cost of getting from the start 
nodes to the current node "g" is not stored as in the A* algorithm. This is because it is not 
possible to compute a single such value since there may be many paths  to the same state. In AO* 
algorithm serves as the estimate of goodness of a node. Also a there should value called 
FUTILITY is used. The estimated cost of a solution is greater than FUTILITY then the search is 
abandoned as too expansive to be practical.  
For representing above graphs AO* algorithm is as follows  
 
AO* ALGORITHM:  
1. Let G consists only to the node representing the initial state call this node INTT. Compute   
    h' (INIT).  
 
2. Until INIT is labeled SOLVED or hi (INIT) becomes greater than FUTIL ITY, repeat the   
    following procedure.  55 
  
(I)     Trace the marked arcs from INIT and select an unbounded node NODE.  
 
(II)  Generate the successors of NODE . if there are no successors then assign FUTILITY as   
        h' (NODE). This means that NODE is no t solvable. If there are successors then for each 
one    
        called SUCCESSOR, that is not also an ancester of NODE do the following  
 
 
            (a) add SUCCESSOR to graph G  
 
            (b) if successor is not a terminal node, mark it solved and ass ign zero to its h ' value.  
 
            (c) If successor is not a terminal node, compute it h' value.  
 
(III) propagate the newly discovered information up the graph by doing the following . let S be a   
        set of nodes that have been marked SOLVED. Ini tialize S to NODE. Until S is empty 
repeat   
        the following procedure;  
 
           (a) select a node from S call if CURRENT and remove it from S.  
 
          (b) compute h' of each of the arcs emerging from CURRENT , Assign minimum h' to      
               CURRENT.  
 
          (c) Mark the minimum cost path a s the best out of CURRENT.  
 
          (d) Mark CURRENT SOLVED if all of the nodes connected to it through the new marked   
               are have been labeled SOLVED.  
 
          (e) If CURRENT has been marked SOLVED or its h ' has just changed, its new status 
must   
               be propagate backwards up the graph . hence all the ancestors of CURRENT are added   
               to S. 
(Refered From Artificial Intelligence TMH)  
 
AO*  Search Procedure.  
 
1. Place the start node on open.  
 
2. Using the search tree, compute the most promising solution tree TP .  
 
3. Select node n that is both on open and a part of tp, remove n from open and place it no closed.  
 
4. If n is a goal node, label n as solved. If th e start node is solved, exit with success where tp is 
the solution tree, remove all nodes from open with a solved ancestor.  56 
  
5. If n is not solvable node, label n as unsolvable. If the start node is labeled as unsolvable, exit 
with failure. Remove all node s from open ,with unsolvable ancestors.  
 
6. Otherwise, expand node n generating all of its successor compute the cost of for each newly 
generated node and place all such nodes on open.  
 
7. Go back to step(2)  
 
Note: AO* will always find minimum cost solutio n. 
 
CONSTRAINT SATISFACTION: - 
 
Many problems in AI can be considered as problems of constraint satisfaction, in which the goal 
state satisfies a given set of constraint. constraint satisfaction problems can be solved by using 
any of the search strategies. The general form of the constraint satisfaction procedure is as 
follows:  
 
Until a complete solution is found or until all paths have led to lead ends, do  
 
1. select an unexpanded node of the search graph.  
 
2. Apply the constraint inference rules to the sel ected node to generate all possible new 
constraints.  
 
3. If the set of constraints contains a contradiction, then report that this path is a dead end.  
 
4. If the set of constraints describes a complete solution then report success.  
 
5. If neither a constra int nor a complete solution has been found then apply the rules to generate 
new partial solutions. Insert these partial solutions into the search graph.  
 
Example: consider the crypt arithmetic problems.  
 
    SEND  
+ MORE  
----------  
MONEY  
----------  
 
Assign decimal digit to each of the letters in such a way that the answer to the problem is correct 
to the same letter occurs more than once , it must be assign the same digit each time . no two 
different letters may be assigned the same digit. Consider the crypt  arithmetic problem.  
 
 
 57 
    SEND  
+ MORE  
-----------  
MONEY  
-----------  
 
CONSTRAINTS: - 
 
1. no two digit can be assigned to same letter.  
 
2. only single digit number can be assign to a letter.  
 
1. no two letters can be assigned same digit.  
 
2. Assumption can b e made at various levels such that they do not contradict each other.  
 
3. The problem can be decomposed into secured constraints. A constraint satisfaction approach 
may be used.  
 
4. Any of search techniques may be used.  
 
5. Backtracking may be performed as  applicable us applied search techniques.  
 
6. Rule of arithmetic may be followed.  
 
Initial state of problem.  
D=? 
E=? 
Y=? 
N=? 
R=? 
O=? 
S=? 
M=?  
C1=?  
C2=?  
C1 ,C 2, C3 stands for the carry variables respectively.  
 
Goal State: the digits to the letters must be a ssigned in such a manner so that the sum is satisfied.  
 
 
Solution Process:  
 
We are following the depth -first method to solve the problem.  
 
1. initial guess m=1 because the sum of two single digits can generate at most a carry '1'.  58 
  
2. When n=1 o=0 or 1 bec ause the largest single digit number added to m=1 can generate the 
sum of either 0 or 1 depend on the carry received from the carry sum. By this we conclude that 
o=0 because m is already 1 hence we cannot assign same digit another letter(rule no.)  
 
3. We h ave m=1 and o=0 to get o=0 we have s=8 or 9, again depending on the carry received 
from the earlier sum.  
 
The same process can be repeated further. The problem has to be composed into various 
constraints. And each constraints is to be satisfied by guessing  the possible digits that the letters 
can be assumed that the initial guess has been already made . rest of the process is being shown 
in the form of a tree, using depth -first search for the clear understandability of the solution 
process.  59 
  
 
            D>6(Controduction)  60 
 
 
  
    
 
 61 
 MEANS - ENDS ANALYSIS: - 
 
Most of the search strategies either reason forward of backward however, often a  mixture o the 
two directions is appropriate. Such mixed strategy would make it possible to solve the major 
parts of problem first and solve the smaller problems the arise when combining them together. 
Such a technique is called "Means - Ends Analysis".  
 
The means -ends analysis process centers around finding the difference between current state and 
goal state. The problem space of means - ends analysis has an initial state and one or more goal 
state, a set of operate with a set of preconditions their appli cation and difference functions that 
computes the difference between two state a(i) and s(j). A problem is solved using means - ends 
analysis by  
 
 
1. Computing the current state s1 to a goal state s2 and computing their difference D12.  
 
2. Satisfy the prec onditions for some recommended operator op is selected, then to reduce the 
difference D12.  
 
3. The operator OP is applied if possible. If not the current state is solved a goal is created and 
means - ends analysis is applied recursively to reduce the sub go al. 
 
4. If the sub goal is solved state is restored and work resumed on the original problem.  
 
( the first AI program to use means - ends analysis was the GPS General problem solver)  
 
means - ends analysis I useful for many human planning activities. Consid er the example of 
planing for an office worker. Suppose we have a different table of three rules:  
 
1. If in out current state we are hungry , and in our goal state we are not hungry , then either the 
"visit hotel" or "visit Canteen " operator is recommende d. 
 
2. If our current state we do not have money , and if in your goal state we have money, then the 
"Visit our bank" operator or the "Visit secretary" operator is recommended.  
 
3. If our current state we do not know where something is , need in our goal s tate we do know, 
then either the "visit office enquiry" , "visit secretary" or "visit co worker " operator is 
recommended.  
 
 
 
 
 
 
 
 62 
 KNOWLEDGE REPRESENTATION  
 
KNOWLEDGE REPRESENTATION: - 
 
For the purpose of solving complex problems c \encountered in AI, we nee d both a large amount 
of knowledge and some mechanism for manipulating that knowledge to create solutions to new 
problems. A variety of ways of representing knowledge (facts) have been exploited in AI 
programs. In all variety of knowledge representations ,  we deal with two kinds of entities.  
 
A. Facts: Truths in some relevant world. These are the things we want to represent.  
 
B. Representations of facts in some chosen formalism . these are things we will  
 
actually be able to manipulate.  
 
One way to think of  structuring these entities is at two levels : (a) the knowledge level, at which 
facts are described, and (b) the symbol level, at which representations of objects at the 
knowledge level are defined in terms of symbols that can be manipulated by programs.  
 
The facts and representations are linked with two -way mappings. This link is called 
representation mappings. The forward representation mapping maps from facts to 
representations. The backward representation mapping goes the other way, from representation s 
to facts.  
 
One common representation is natural language (particularly English) sentences. Regardless of 
the representation for facts we use in a program , we may also need to be concerned with an 
English representation of those facts in order to facilit ate getting information into and out of the 
system. We need mapping functions from English sentences to the representation we actually use 
and from it back to sentences.  
Representations and Mappings  
 In order to solve complex problems encountered in artific ial intelligence, one needs both 
a large amount of knowledge and some mechanism for manipulating that knowledge to 
create solutions.  
 Knowledge and Representation are two distinct entities. They play central but 
distinguishable roles in the intelligent syst em. 
 Knowledge is a description of the world. It determines a system’s competence by what it 
knows.  
 Moreover, Representation is the way knowledge is encoded. It defines a system’s 
performance in doing something.  
 Different types of knowledge require differen t kinds of representation.  63 
 
 
Fig: Mapping between Facts and Representations  
The Knowledge Representation m odels/mechanisms are often based on:  
 Logic  
 Rules  
 Frames  
 Semantic Net  
Knowledge is categorized into two major types:  
1. Tacit corresponds to “informal” or “implicit“  
 Exists within a human being;  
 It is embodied.  
 Difficult to articulate formally.  
 Difficult to co mmunicate or share.  
 Moreover, Hard to steal or copy.  
 Drawn from experience, action, subjective insight  
2. Explicit formal type of knowledge, Explicit  
 Explicit knowledge  
 Exists outside a human being;  
 It is embedded.  
 Can be articulated formally.  
 Also, Can be sh ared, copied, processed and stored.  
 So, Easy to steal or copy  
 Drawn from the artifact of some type as a principle, procedure, process, concepts.  
A variety of ways of representing knowledge have been exploited in AI programs.  
There are two different kinds o f entities, we are dealing with.  
1. Facts: Truth in some relevant world. Things we want to represent.  
2. Also, Representation of facts in some chosen formalism. Things we will actually be able 
to manipulate.  
These entities structured at two levels:  
1. The knowledge  level, at which facts described.  
2. Moreover, The symbol level, at which representation of objects defined in terms of 
symbols that can manipulate by programs  
Framework of Knowledge Representation  
 The computer requires a well -defined problem description to p rocess and provide a well -
defined acceptable solution.  64 
  Moreover, To collect fragments of knowledge we need first to formulate a description in 
our spoken language and then represent it in formal language so that computer can 
understand.  
 Also, The computer can then use an algorithm to compute an answer.  
So, This process illustrated as,  
 
Fig: Knowledge  Representation Framework  
The steps are:  
 The informal formalism of the problem takes place first.  
 It then represented formally and the computer produces an output.  
 This output can then represented in an informally described solution that user understands 
or checks for consistency.  
The Problem solving requires,  
 Formal knowledge representation, and  
 Moreover, Conversion of informal knowledge to a formal knowledge that is the 
conversion of implicit knowledge to explicit knowledge.  
Mapping between Facts and Repr esentation  
 Knowledge is a collection of facts from some domain.  
 Also, We need a representation of “facts“ that can manipulate by a program.  
 Moreover, Normal English is insufficient, too hard currently for a computer program to 
draw inferences in natural la nguages.  
 Thus some symbolic representation is necessary.  
A good knowledge representation enables fast and accurate access to knowledge and 
understanding of the content.  
A knowledge representation system should have following properties.  
1. Representational Ad equacy  
 The ability to represent all kinds of knowledge that are needed in that domain.  
2. Inferential Adequacy  
 Also, The ability to manipulate the representational structures to derive new 
structures corresponding to new knowledge inferred from old.  
3. Inferenti al Efficiency  
 The ability to incorporate additional information into the knowledge structure that 
can be used to focus the attention of the inference mechanisms in the most 
promising direction.  
4. Acquisitional Efficiency  
 Moreover, The ability to acquire new knowledge using automatic methods 
wherever possible rather than reliance on human intervention.  65 
 Knowledge Representation Schemes  
Relational Knowledge  
 The simplest way to represent declarative facts is a set of relations of the same sort used 
in the databas e system.  
 Provides a framework to compare two objects based on equivalent attributes. o Any 
instance in which two different objects are compared is a relational type of knowledge.  
 The table below shows a simple way to store facts.  
 Also, The facts about a s et of objects are put systematically in columns.  
 This representation provides little opportunity for inference.  
 
 Given the facts, it is not possible to answer a simple question such as: “Who is the 
heaviest player?”  
 Also, But if a procedure for finding the heaviest player is provided, then these facts will 
enable that procedure to compute an answer.  
 Moreover, We can  ask things like who “bats – left” and “throws – right”.  
Inheritable Knowledge  
 Here the knowledge elements inherit attributes from their parents.  
 The knowledge embodied in the design hierarchies found in the functional, physical and 
process domains.  
 Within  the hierarchy, elements inherit attributes from their parents, but in many cases, not 
all attributes of the parent elements prescribed to the child elements.  
 Also, The inheritance is a powerful form of inference, but not adequate.  
 Moreover, The basic KR ( Knowledge Representation) needs to augment with inference 
mechanism.  
 Property inheritance: The objects or elements of specific classes inherit attributes and 
values from more general classes.  
 So, The classes organized in a generalized hierarchy.  66 
 
 
 Boxed nodes — objects and values of attributes of objects.  
 Arrows — the point from object to its value.  
 This structure is  known as a slot and filler structure, semantic network or a collection of 
frames.  
The steps to retrieve a value for an attribute of an instance object:  
1. Find the object in the knowledge base  
2. If there is a value for the attribute report it  
3. Otherwise look fo r a value of an instance, if none fail  
4. Also, Go to that node and find a value for the attribute and then report it  
5. Otherwise, search through using is until a value is found for the attribute.  
Inferential Knowledge  
 This knowledge generates new information f rom the given information.  
 This new information does not require further data gathering form source but does 
require analysis of the given information to generate new knowledge.  
 Example: given a set of relations and values, one may infer other values or re lations. A 
predicate logic (a mathematical deduction) used to infer from a set of attributes. 
Moreover, Inference through predicate logic uses a set of logical operations to relate 
individual data.  
 Represent knowledge as formal logic: All dogs have tails   ∀x: dog(x) → hastail(x)  
 Advantages:  
 A set of strict rules.  
 Can use to derive more facts.  
 Also, Truths of new statements can be verified.  
 Guaranteed correctness.  
 So, Many inference procedures available to implement standard rules of logic popular in 
AI syst ems. e.g Automated theorem proving.  67 
 Procedural Knowledge  
 A representation in which the control information, to use the knowledge, embedded in the 
knowledge itself. For example, computer programs, directions, and recipes; these indicate 
specific use or impl ementation;  
 Moreover, Knowledge encoded in some procedures, small programs that know how to do 
specific things, how to proceed.  
 Advantages:  
 Heuristic or domain -specific knowledge can represent.  
 Moreover, Extended logical inferences, such as default reasoni ng facilitated.  
 Also, Side effects of actions may model. Some rules may become false in time. 
Keeping track of this in large systems may be tricky.  
 Disadvantages:  
 Completeness — not all cases may represent.  
 Consistency — not all deductions may be correct. e.g If we know that Fred is a 
bird we might deduce that Fred can fly. Later we might discover that Fred is an 
emu.  
 Modularity sacrificed. Changes in knowledge base might have far -reaching 
effects.  
 Cumbersome control information.  
 
USING PREDICATE LOGIC   
Representation of Simple Facts in Logic  
Propositional logic is useful because it is simple to deal with and a decision procedure for it 
exists.  
Also, In order to draw conclusions, facts are represented in a more convenient way as,  
1. Marcus is a man.  
 man(Marcus)  
2. Plato is a man.  
 man(Plato)  
3. All men are mortal.  
 mortal(men)  
But propositional logic fails to capture the relationship between an individual being a man and 
that individual being a mortal.  
 How can these sentences be represented so that we can infer the thir d sentence from the 
first two?  
 Also, Propositional logic commits only to the existence of facts that may or may not be 
the case in the world being represented.  
 Moreover, It has a simple syntax and simple semantics. It suffices to illustrate the process 
of inference.  
 Propositional logic quickly becomes impractical, even for very small worlds.  
Predicate logic  
First-order Predicate logic (FOPL) models the world in terms of  
 Objects, which are things with individual identities  
 Properties of objects that distingu ish them from other objects  
 Relations that hold among sets of objects  68 
  Functions, which are a subset of relations where there is only one “value” for any given 
“input”  
First-order Predicate logic (FOPL) provides  
 Constants: a, b, dog33. Name a specific objec t. 
 Variables: X, Y. Refer to an object without naming it.  
 Functions: Mapping from objects to objects.  
 Terms: Refer to objects  
 Atomic Sentences: in(dad -of(X), food6) Can be true or false, Correspond to propositional 
symbols P, Q.  
A well -formed formula ( wff) is a sentence containing no “free” variables. So, That is, all 
variables are “bound” by universal or existential quantifiers.  
(∀x)P(x, y) has x bound as a universally quantified variable, but y is free.  
Quantifiers  
Universal quantification  
 (∀x)P(x) means that P holds for all values of x in the domain associated with that variable  
 E.g., (∀x) dolphin(x)  →  mammal(x)  
Existential quantification  
 (∃ x)P(x) means that P holds for some value of x in the domain associated with that 
variable  
 E.g., (∃ x) mammal(x)  ∧  lays-eggs(x)  
Also, Consider the following example that shows the use of predicate logic as a way of 
representing knowledge.  
1. Marcus was a man.  
2. Marcus was a Pompeian.  
3. All Pompeians were Romans.  
4. Caesar was a ruler.  
5. Also, All Pompeians were either loyal to Ca esar or hated him.  
6. Everyone is loyal to someone.  
7. People only try to assassinate rulers they are not loyal to.  
8. Marcus tried to assassinate Caesar.  
The facts described by these sentences can be represented as a set of well -formed formulas ( wffs) 
as follows:  
1. Marcus was a man.  
 man(Marcus)  
2. Marcus was a Pompeian.  
 Pompeian(Marcus)  
3. All Pompeians were Romans.  
 ∀x: Pompeian(x)  → Roman(x)  
4. Caesar was a ruler.  
 ruler(Caesar)  
5. All Pompeians were either loyal to Caesar or hated him.  
  inclusive -or 
 ∀x: Roman(x)  → loyalto(x, Ca esar)  ∨ hate(x, Caesar)  
 exclusive -or 
 ∀x: Roman(x)  → (loyalto(x, Caesar)  ∧¬ hate(x, Caesar))  ∨ 
 (¬loyalto(x, Caesar)  ∧ hate(x, Caesar))  69 
 6. Everyone is loyal to someone.  
 ∀x: ∃y: loyalto(x, y)  
7. People only try to assassinate rulers they are not loyal to.  
 ∀x: ∀y: person(x)  ∧ ruler(y)  ∧ tryassassinate(x, y)  
 →¬loyalto(x, y)  
8. Marcus tried to assassinate Caesar.  
 tryassassinate(Marcus, Caesar)  
Now suppose if we want to use these statements to answer the question:  Was Marcus loyal to 
Caesar?  
Also, Now let’s try to produce a formal proof, reasoning backward from the desired goal: ¬ 
Ioyalto(Marcus, Caesar)  
In order to prove the goal, we need to use the rules of inference to transform it into another goal 
(or possibly a set of goals) that can, in turn, transformed, and so on, until there are no unsatisfied 
goals remaining.  
 
Figure: An attempt to prove ¬loyalto(Marcus, Caesar).   
 The prob lem is that, although we know that Marcus was a man, we do not have any way 
to conclude from that that Marcus was a person. Also, We need to add the representation 
of another fact to our system, namely : ∀ man(x) → person(x)  
 Now we can satisfy the last goal  and produce a proof that Marcus was not loyal to 
Caesar.  
 Moreover, From this simple example, we see that three important issues must be 
addressed in the process of converting English sentences into logical statements and then 
using those statements to ded uce new ones:  
1. Many English sentences are ambiguous (for example, 5, 6, and 7 above). 
Choosing the correct interpretation may be difficult.  
2. Also, There is often a choice of how to represent the knowledge. Simple 
representations are desirable, but they may e xclude certain kinds of reasoning.  
3. Similalry, Even in very simple situations, a set of sentences is unlikely to contain 
all the information necessary to reason about the topic at hand. In order to be able 
to use a set of statements effectively. Moreover, I t is usually necessary to have 
access to another set of statements that represent facts that people consider too 
obvious to mention.  
 
 70 
 Representing Instance and ISA Relationships  
 
 Specific attributes  instance  and isa play an important role particularly in a useful form of 
reasoning called property inheritance.  
 The predicates instance and isa explicitly captured the relationships they used to express, 
namely class membership and class inclusion.  
 4.2 shows the first five sentences of the last section represen ted in logic in three different 
ways.  
 The first part of the figure contains the representations we have already discussed. In 
these representations, class membership represented with unary predicates (such as 
Roman), each of which corresponds to a class.  
 Asserting that P(x) is true is equivalent to asserting that x is an instance (or element) of P.  
 The second part of the figure contains representations that use the  instance  predicate 
explicitly.  
 
Figure: Three ways of representing class membership:  ISA Relationships  
 The predicate  instance  is a binary one, whose first argument is an object and wh ose 
second argument is a class to which the object belongs.  
 But these representations do not use an explicit  isa predicate.  
 Instead, subclass relationships, such as that between Pompeians and Romans, described 
as shown in sentence 3.  
 The implication rule s tates that if an object is an instance of the subclass Pompeian then it 
is an instance of the superclass Roman.  
 Note that this rule is equivalent to the standard set -theoretic definition of the subclass -
superclass relationship.  
 The third part contains repr esentations that use both the  instance  and isa predicates 
explicitly.  
 The use of the  isa predicate simplifies the representation of sentence 3, but it requires that 
one additional axiom (shown here as number 6) be provided.  71 
 Computable Functions and Predica tes 
 To express simple facts, such as the following greater -than and less -than relationships: 
gt(1,O) It(0,1) gt(2,1)   It(1,2) gt(3,2)   It( 2,3)  
 It is often also useful to have computable functions as well as computable predicates. 
Thus we might want to be able to evaluate the truth of   gt(2 + 3,1)  
 To do so requires that we first compute the value of the plus function given the arguments 
2 and 3, and then send the arguments 5 and 1 to gt.  
Consider the following set of facts, again involving Marcus:  
1) Marcus  was a man.  
            man(Marcus)  
2) Marcus was a Pompeian.  
            Pompeian(Marcus)  
3) Marcus was born in 40 A.D.  
            born(Marcus, 40)  
4) All men are mortal.  
            x: man(x) → mortal(x)  
5) All Pompeians died when the volcano erupted in 79 A.D.  
         erupted(volcano, 79) ∧ ∀ x : [Pompeian(x) → died(x, 79)]  
6) No mortal lives longer than 150 years.  
             x: t1: At2:  mortal(x)   born(x, t1)   gt(t2 – t1,150) → died(x, t2)  
7) It is now 1991.  
            now = 1991  
So, Above example shows how these ideas of computable functions and predicates can be useful.  
It also makes use of the notion of equality and allows equal objects to be substituted for each 
other whenever it appears helpful to do so during a proof.  
 So, Now suppose we want to answer the question “Is Marcus alive?”  
 The statements suggested here, there may be two ways of deducing an answer.  
 Either we can show that Marcus is dead because he was ki lled by the volcano or we can 
show that he must be dead because he would otherwise be more than 150 years old, 
which we know is not possible.  
 Also, As soon as we attempt to follow either of those paths rigorously, however, we 
discover, just as we did in th e last example, that we need some additional knowledge. For 
example, our statements talk about dying, but they say nothing that relates to being alive, 
which is what the question is asking.  
So we add the following facts:  
8) Alive means not dead.  
            x: t: [alive(x, t) → ¬ dead(x, t)]  [¬ dead(x, t) → alive(x, t)]  
9) If someone dies, then he is dead at all later times.  
            x: t1: At2 : died(x, t1)   gt(t2, t1) → dead(x, t2)  
So, Now let’s attempt to answer the question “Is Marcus alive?” by prov ing: ¬ alive(Marcus, 
now)  
 
 
 
 
 72 
  
Resolution  
Propositional Resolution  
1. Convert all the propositions of F to clause form.  
2. Negate P and convert the result to clause form. Add it to the set of clauses obtained in 
step 1.  
3. Repeat until either a contradiction is fo und or no progress can be made:  
1. Select two clauses. Call these the parent clauses.  
2. Resolve them together. The resulting clause, called the resolvent, will be the 
disjunction of all of the literals of both of the parent clauses with the following 
exception:  If there are any pairs of literals L and  ¬ L such that one of the parent 
clauses contains  L and the other contains  ¬L, then select one such pair and 
eliminate both  L and ¬ L from the resolvent.  
3. If the resolvent is the empty clause, then a contradiction ha s been found. If it is 
not, then add it to the set of classes available to the procedure.  
The Unification Algorithm  
 In propositional logic, it is easy to determine that two literals cannot both be true at the 
same time.  
 Simply look for L and ¬L in predicat e logic, this matching process is more complicated 
since the arguments of the predicates must be considered.  
 For example, man(John) and ¬man(John) is a contradiction, while the man(John) and 
¬man(Spot) is not.  
 Thus, in order to determine contradictions, we  need a matching procedure that compares 
two literals and discovers whether there exists a set of substitutions that makes them 
identical.  
 There is a straightforward recursive procedure, called the unification algorithm, that does 
it. 
Algorithm: Unify(L1, L2) 
1. If L1 or L2 are both variables or constants, then:  
1. If L1 and L2 are identical, then return NIL.  
2. Else if L1 is a variable, then if L1 occurs in L2 then return {FAIL}, else return 
(L2/L1).  
3. Also, Else if L2 is a variable, then if L2 occurs in L1 then retu rn {FAIL}, else 
return (L1/L2). d. Else return {FAIL}.  
2. If the initial predicate symbols in L1 and L2 are not identical, then return {FAIL}.  
3. If LI and L2 have a different number of arguments, then return {FAIL}.  
4. Set SUBST to NIL. (At the end of this procedu re, SUBST will contain all the 
substitutions used to unify L1 and L2.)  
5. For I ← 1 to the number of arguments in L1 :  
1. Call Unify with the ith argument of L1 and the ith argument of L2, putting the 
result in S.  
2. If S contains FAIL then return {FAIL}.  
3. If S is n ot equal to NIL then:  
2. Apply S to the remainder of both L1 and L2.  
3. SUBST: = APPEND(S, SUBST).  
6. Return SUBST.  73 
 Resolution in Predicate Logic  
We can now state the resolution algorithm for predicate logic as follows, assuming a set of given 
statements F and a st atement to be proved P:  
Algorithm: Resolution  
1. Convert all the statements of F to clause form.  
2. Negate P and convert the result to clause form. Add it to the set of clauses obtained in 1.  
3. Repeat until a contradiction found, no progress can make, or a predete rmined amount of 
effort has expanded.  
1. Select two clauses. Call these the parent clauses.  
2. Resolve them together. The resolvent will the disjunction of all the literals of both 
parent clauses with appropriate substitutions performed and with the following 
exception: If there is one pair of literals T1 and ¬T2 such that one of the parent 
clauses contains T2 and the other contains T1 and if T1 and T2 are unifiable, then 
neither T1 nor T2 should appear in the resolvent. We call T1 and T2 
Complementary literals. Use the substitution produced by the unification to create 
the resolvent. If there is more than one pair of complementary literals, only one 
pair should omit from the resolvent.  
3. If the resolvent is an empty clause, then a contradiction has found. Moreover,  If it 
is not, then add it to the set of classes available to the procedure.  
 
Resolution Procedure  
 Resolution is a procedure, which gains its efficiency from the fact that it operates on 
statements that have been converted to a very convenient standard for m. 
 Resolution produces proofs by refutation.  
 In other words,  to prove a statement (i.e., to show that it is valid), resolution attempts to 
show that the negation of the statement produces a contradiction with the known 
statements (i.e., that it is unsatisf iable).  
 The resolution procedure is a simple iterative process: at each step, two clauses, called 
the parent clauses, are compared (resolved), resulting in a new clause that has inferred 
from them. The new clause represents ways that the two parent clauses  interact with each 
other. Suppose that there are two clauses in the system:  
winter  V summer  
             ¬ winter  V cold 
 Now we observe that precisely one of winter and ¬ winter will be true at any point.  
 If winter is true, then cold must be true to guara ntee the truth of the second clause. If ¬ 
winter is true, then summer must be true to guarantee the truth of the first clause.  
 Thus we see that from these two clauses we can deduce   summer V cold  
 This is the deduction that the resolution procedure will mak e. 
 Resolution operates by taking two clauses that each contains the same literal, in this 
example,  winter . 
 Moreover, The literal must occur in the positive form in one clause and in negative form 
in the other. The resolvent obtained by combining all of the  literals of the two parent 
clauses except the ones that cancel.  
 If the clause that produced is the empty clause, then a contradiction has found.  
For example, the two clauses  
            winter  74 
             ¬ winter  
will produce the empty clause.  
 
Natural D eduction Using Rules  
 
Testing whether a proposition is a tautology by testing every possible truth assignment is 
expensive —there are exponentially many. We need a  deductive system , which will allow us to 
construct proofs of tautologies in a step -by-step fa shion.  
The system we will use is known as  natural deduction . The system consists of a set of  rules of 
inference  for deriving consequences from premises. One builds a proof tree whose root is the 
proposition to be proved and whose leaves are the initial ass umptions or axioms (for proof trees, 
we usually draw the root at the bottom and the leaves at the top).  
For example, one rule of our system is known as  modus ponens . Intuitively, this says that if we 
know P is true, and we know that P implies Q, then we ca n conclude Q.  
P P ⇒ Q 
Q 
 (modus ponens)  
The propositions above the line are called  premises ; the proposition below the line is 
the conclusion . Both the premises and the conclusion may contain metavariables (in this case, P 
and Q) representing arbitrary propositions. When  an inference rule is used as part of a proof, the 
metavariables are replaced in a consistent way with the appropriate kind of object (in this case, 
propositions).  
Most rules come in one of two flavors:  introduction  or elimination  rules. Introduction rules  
introduce the use of a logical operator, and elimination rules eliminate it. Modus ponens is an 
elimination rule for  ⇒. On the right -hand side of a rule, we often write the name of the rule. This 
is helpful when reading proofs. In this case, we have written (modus ponens). We could also 
have written  (⇒-elim)  to indicate that this is the elimination rule for  ⇒. 
Rules for Conjunction  
Conjunction  (∧) has an introduction rule and two elimination rules:  
P Q 
P ∧ Q 
 (∧-intro)  P ∧ Q 
P 
 (∧-elim-left) P ∧ Q 
Q 
 (∧-elim-right)  
Rule for T  
The simplest introduction rule is the one for T. It is called "unit". Because it has no pr emises, this 
rule is an  axiom : something that can start a proof.  
  
T 
 (unit)  
Rules for Implication  
In natural deduction, to prove an implication of the form P ⇒ Q, we assume P, then reason under 
that assumption to try to derive Q. If we are successful, then we can conclude that P ⇒ Q. 
In a proof, we are always allowed to introduce a new assumption P, then reason under that 
assumption. We must give the assumption a name; we have used the name x in the example 
below. Each distinct assumption must have a di fferent name.  
  
[x : P]  
 (assum)  75 
 Because it has no premises, this rule can also start a proof. It can be used as if the proposition P 
were proved. The name of the assumption is also indicated here.  
However, you do not get to make assumptions for free! T o get a complete proof, all assumptions 
must be eventually  discharged . This is done in the implication introduction rule. This rule 
introduces an implication P ⇒ Q by discharging a prior assumption [x : P]. Intuitively, if Q can 
be proved under the assumption P, then the implication P ⇒ Q holds without any assumptions. 
We write x in the rule name to show which assumption is discharged. This rule and modus 
ponens are the introduction and elimination rules for implications.  
[x : P]  
⋮ 
Q 
P ⇒ Q 
 (⇒-intro/x)  P P ⇒ Q 
Q 
 (⇒-elim, modus ponens)  
A proof is valid only if every assumption is eventually discharged. This must happen in the proof 
tree below the assumption. The same assumption can be used more than once.  
Rules for Disjunction  
P 
P ∨ Q 
 (∨-intro -
left) Q 
P ∨ Q 
 (∨-intro -
right)  P ∨ Q P ⇒ R Q ⇒ R 
R 
 (∨-
elim)  
Rules for Negation  
A negation ¬P can be considered an abbreviation for P ⇒ ⊥: 
P ⇒ ⊥ 
¬P 
 (¬-intro)  ¬P 
P ⇒ ⊥ 
 (¬-elim)  
Rules for Falsity  
[x : ¬P]  
⋮ 
⊥ 
P 
 (reductio ad absurdum, RAA/x)  ⊥ 
P 
 (ex falso quodlibet, EFQ)  
Reductio ad absurdum  (RAA) is an interesting rule. It embodies proofs by contradiction. It says 
that if by assuming that P is false we  can derive a contradiction, then P must be true. The 
assumption x is discharged in the application of this rule. This rule is present in classical logic 
but not in  intuitionistic  (constructive) logic. In intuitionistic logic, a proposition is not 
consider ed true simply because its negation is false.  
Excluded Middle  
Another classical tautology that is not intuitionistically valid is the  the law of the excluded 
middle , P ∨ ¬P. We will take it as an axiom in our system. The Latin name for this rule 
is tertium non datur , but we will call it  magic . 
  
P ∨ ¬P 
 (magic)  
Proofs  
A proof of proposition P in natural deduction starts from axioms and assumptions and derives P 
with a ll assumptions discharged. Every step in the proof is an instance of an inference rule with 
metavariables substituted consistently with expressions of the appropriate syntactic class.  
Example  76 
 For example, here is a proof of the proposition  (A ⇒ B ⇒ C) ⇒ (A ∧ B ⇒ C). 
 
The final step in the proof is to derive  (A ⇒ B ⇒ C) ⇒ (A ∧ B ⇒ C) from  (A ∧ B ⇒ C), which is 
done usin g the rule  (⇒-intro), discharging the assumption [x :  A ⇒ B ⇒ C]. To see how this rule 
generates the proof step, substitute for the metavariables P, Q, x in the rule as follows: P =  (A ⇒ 
B ⇒ C), Q =  (A ∧ B ⇒ C), and x = x. The immediately previous step use s the same rule, but with 
a different substitution: P =  A ∧ B, Q = C, x = y.  
The proof tree for this example has the following form, with the proved proposition at the root 
and axioms and assumptions at the leaves.  
 
A proposition that has a complete proof in a deductive system is called a  theorem  of that system.  
Soundness and Completeness  
A measure of a deductive system's  power is whether it is powerful enough to prove all true 
statements. A deductive system is said to be  complete  if all true statements are theorems (have 
proofs in the system). For propositional logic and natural deduction, this means that all 
tautologies must have natural deduction proofs. Conversely, a deductive system is 
called  sound  if all theorems are true. The proof rules we have given above are in fact sound and 
complete for propositional logic: every theorem is a tautology, and every tautology is a theorem.  
Finding a proof for a given tautology can be difficult. But once the proof is found, checking that 
it is indeed a proof is completely mechanical, requiring no intelligence or insight whatsoever. It 
is therefore a very strong argument that the thin g proved is in fact true.  
We can also make writing proofs less tedious by adding more rules that provide reasoning 
shortcuts. These rules are sound if there is a way to convert a proof using them into a proof using 
the original rules. Such added rules are called  admissible . 
 
Procedural versus Declarative Knowledge  
We have discussed various search techniques in previous units. Now we would consider a set of 
rules that represent,  
1. Knowledge about relationships in the world and  
2. Knowledge about how to solve the problem using the content of the rules.  
Procedural vs Declarative Knowledge  
Procedural Knowledge  77 
  A representation in which the control information that is necessary to use the knowledge 
is embedded in the knowledge itself for e.g. computer programs, direct ions, and recipes; 
these   indicate specific use or implementation;  
 The real difference between declarative and procedural views of knowledge lies in where 
control information reside.  
For example, consider the following  
Man (Marcus)  
Man (Caesar)  
Person (Cle opatra)    
∀x: Man(x) →  Person(x)  
Now, try to answer the question.   ?Person(y)   
The knowledge base justifies any of the following answers.  
Y=Marcus   
Y=Caesar  
Y=Cleopatra  
 We get more than one value that satisfies the predicate.  
 If only one value needed, then  the answer to the question will depend on the order in 
which the assertions examined during the search for a response.  
 If the assertions declarative then they do not themselves say anything about how they will 
be examined. In case of procedural representa tion, they say how they will examine.  
Declarative   Knowledge  
 A statement in which knowledge specified, but the use to which that knowledge is to be 
put is not given.  
 For example, laws, people’s name; these are the facts which can stand alone, not 
dependent  on other knowledge;  
 So to use declarative representation, we must have a program that explains what is to do 
with the knowledge and how.  
 For example, a set of logical assertions can combine with a resolution theorem prover to 
give a complete program for s olving problems but in some cases, the logical assertions 
can view as a program rather than data to a program.  
 Hence the implication statements define the legitimate reasoning paths and automatic 
assertions provide the starting points of those paths.  
 These  paths define the execution paths which is similar to the ‘if then else “in traditional 
programming.  
 So logical assertions can view as a procedural representation of knowledge.  
Logic Programming – Representing Knowledge Using Rules  
 Logic programming is a p rogramming paradigm in which logical assertions viewed as 
programs.  
 These are several logic programming systems, PROLOG is one of them.  
 A PROLOG program consists of several logical assertions where each is a horn clause 
i.e. a clause with at most one posit ive literal.  
 Ex :  P,    P V Q, P  → Q  
 The facts are represented on Horn Clause for two reasons.  
1. Because of a uniform representation, a simple and efficient interpreter can write.  
2. The logic of Horn Clause decidable.  78 
  Also, The first two differences are the f act that PROLOG programs are actually sets of 
Horn clause that have been transformed as follows: - 
1. If the Horn Clause contains no negative literal then leave it as it is.  
2. Also, Otherwise rewrite the Horn clauses as an implication, combining all of the 
negat ive literals into the antecedent of the implications and the single positive 
literal into the consequent.  
 Moreover, This procedure causes a clause which originally consisted of a disjunction of 
literals (one of them was positive) to be transformed into a s ingle implication whose 
antecedent is a conjunction universally quantified.  
 But when we apply this transformation, any variables that occurred in negative literals 
and so now occur in the antecedent become existentially quantified, while the variables in 
the consequent are still universally quantified.  
For example the PROLOG clause P(x): – Q(x, y) is equal to logical expression ∀x: ∃y: Q (x, 
y) → P(x).  
 The difference between the logic and PROLOG representation is that the PROLOG 
interpretation has a fixed control strategy. And so, the assertions in the PROLOG 
program define a particular search path to answer any question.  
 But, th e logical assertions define only the set of answers but not about how to choose 
among those answers if there is more than one.  
Consider the following example:  
1. Logical representation  
                        ∀x : pet(x)   ۸  small (x) →  apartmentpet(x)  
                        ∀x : cat(x)  ۸ dog(x) →  pet(x)  
                        ∀x : poodle (x)   → dog (x)  ۸ small (x)   
                         poodle (fluffy)  
2. Prolog representation  
                   apartmentpet (x)   :  pet(x), small (x)  
                   pet (x): cat (x)  
                   pet (x): dog(x)  
                  dog(x): poodle (x)  
                 small (x): poodle(x)  
                poodle (fluffy)  
 
Forward versus Backward Reasoning  
Forward versus Backward Reasoning  
A search procedure must fi nd a path between initial and goal states.  
There are two directions in which a search process could proceed.  
The two types of search are:  
1. Forward search which starts from the start state  
2. Backward search that starts from the goal state  
The production system  views the forward and backward as symmetric processes.  
Consider a game of playing 8 puzzles. The rules defined are  
Square 1 empty and square 2 contains tile n.  → 79 
  Also, Square 2 empty and square 1 contains the tile n.  
Square 1 empty Square 4 contains tile n. → 
 Also, Square 4 empty and Square 1 contains tile n.  
We can solve the problem in 2 ways:  
1. Reason forward from the initial state  
 Step 1. Begin building a tree of move sequences by starting with the initial configuration 
at the root of the tree.  
 Step 2.  Generate the next level of the tree by finding all rules  whose left -hand side 
matches  against the root node. The right -hand side is used to create new configurations.  
 Step 3. Generate the next level by considering the nodes in the previous level and 
apply ing it to all rules whose left -hand side match.  
2. Reasoning backward from the goal states:  
 Step 1. Begin building a tree of move sequences by starting with the goal node 
configuration at the root of the tree.  
 Step 2. Generate the next level of the tree by  finding all rules  whose right -hand side 
matches  against the root node. The left -hand side used to create new configurations.  
 Step 3. Generate the next level by considering the nodes in the previous level and 
applying it to all rules whose right -hand side match.  
 So, The same rules can use in both cases.  
 Also, In forwarding reasoning, the left -hand sides of the rules matched against the current 
state and right sides used to generate the new state.  
 Moreover, In backward reasoning, the right -hand sides of the rules matched against the 
current state and left sides are used to generate the new state.  
There are four factors influencing the type of reasoning. They are,  
1. Are there more possible start or goal state? We move from smaller set of sets to the 
length.  
2. In what direction is the branching factor greater? We proceed in the direction with the 
lower branching factor.  
3. Will the program be asked to justify its reasoning process to a user? If, so then it is 
selected since it is very close to the way in which the user  thinks.  
4. What kind of event is going to trigger a problem -solving episode? If it is the arrival of a 
new factor, the forward reasoning makes sense. If it is a query to which a response is 
desired, backward reasoning is more natural.  
Example 1 of  Forward ve rsus Backward Reasoning  
 It is easier to drive from an unfamiliar place from home, rather than from home to an 
unfamiliar place. Also, If you consider a home as starting place an unfamiliar place as a 
goal then we have to backtrack from unfamiliar place to home.  
Example 2 of  Forward versus Backward Reasoning  
 Consider a problem of symbolic integration. Moreover, The problem space is a set of 
formulas, which contains integral expressions. Here START is equal to the given formula 
with some integrals. GOAL is eq uivalent to the expression of the formula without any 
integral. Here we start from the formula with some integrals and proceed to an integral 
free expression rather than starting from an integral free expression.  
Example 3 of  Forward versus Backward Reason ing 80 
  The third factor is nothing but deciding whether the reasoning process can justify its 
reasoning. If it justifies then it can apply. For example, doctors are usually unwilling to 
accept any advice from diagnostics process because it cannot explain its reasoning.  
Example 4 of  Forward versus Backward Reasoning  
 Prolog is an example of backward chaining rule system. In Prolog rules restricted to Horn 
clauses. This allows for rapid indexing because all the rules for deducing a given fact 
share the same rule head. Rules matched with unification procedure. Unification tries to 
find a set of bindings for variables to equate a sub -goal with the head of some rule. Rules 
in the Prolog program matched in the order in which they appear.  
Combining Forward and Backward  Reasoning  
 Instead of searching either forward or backward, you can search both simultaneously.  
 Also, That is, start forward from a starting state and backward from a goal state 
simultaneously until the paths meet.  
 This strategy called Bi -directional searc h. The following figure shows the reason for a 
Bidirectional search to be ineffective.  
Forward versus Backward Reaso ning 
 Also, The two searches may pass each other resulting in more work.  
 Based on the form of the rules one can decide whether the same rules can apply to both 
forward and backward reasoning.  
 Moreover, If left -hand side and right of the rule contain pure as sertions then the rule can 
reverse.  
 And so the same rule can apply to both types of reasoning.  
 If the right side of the rule contains an arbitrary procedure then the rule cannot reverse.  
 So, In this case, while writing the rule the commitment to a directio n of reasoning must 
make.  
 
Symbolic Reasoning Under Uncertainty  
Symbolic Reasoning  
 The reasoning is the act of deriving a conclusion from certain properties using a given 
methodology.  
 The reasoning is a process of thinking; reasoning is logically arguing; reasoning is 
drawing the inference.  
 When a system is required to do something, that it has not been explicitly told how to do, 
it must reason. It must figure out what it needs to know from what it already knows.  81 
  Many types of Reasoning have been identified  and recognized, but many questions 
regarding their logical and computational properties still remain controversial.  
 The popular methods of Reasoning include abduction, induction, model -based, 
explanation and confirmation. All of them are intimately relate d to problems of belief 
revision and theory development, knowledge absorption, discovery, and learning.  
Logical Reasoning  
 Logic is a language for reasoning. It is a collection of rules called Logic arguments, we 
use when doing logical reasoning.  
 The logic reasoning is the process of drawing conclusions from premises using rules of 
inference.  
 The study of logic divided into formal and informal logic. The formal logic is sometimes 
called symbolic logic.  
 Symbolic logic is the study of symbolic abstractions (co nstruct) that capture the formal 
features of logical inference by a formal system.  
 The formal system consists of two components, a formal language plus a set of inference 
rules.  
 The formal system has axioms. Axiom is a sentence that is always true within t he system.  
 Sentences derived using the system’s axioms and rules of derivation called theorems.  
 The Logical Reasoning is of our concern in AI.  
Approaches to Reasoning  
 There are three different approaches to reasoning under uncertainties.  
1. Symbolic reasoning  
2. Statistical reasoning  
3. Fuzzy logic reasoning  
Symbolic Reasoning  
 The basis for intelligent mathematical software is the integration of the “power of 
symbolic mathematical tools” with the suitable “proof technology”.  
 Mathematical reasoning enjoys a property called monotonicity, that says, “If a conclusion 
follows from given premises A, B, C… then it also follows from any larger set of 
premises, as long as the original premises A, B, C.. included.”  
 Moreover, Human reasoning is not monotonic.  
 People arrive at c onclusions only tentatively; based on partial or incomplete information, 
reserve the right to retract those conclusions while they learn new facts. Such reasoning 
non-monotonic, precisely because the set of accepted conclusions have become smaller 
when the  set of premises expanded.  
Formal Logic  
Moreover, The Formal logic is the study of inference with purely formal content, i.e. where 
content made explicit.  
Examples – Propositional logic and Predicate logic.  
 Here the logical arguments are a set of rules for  manipulating symbols. The rules are of 
two types,  
1. Syntax rules: say how to build meaningful expressions.  
2. Inference rules: say how to obtain true formulas from other true formulas.  
 Moreover, Logic also needs semantics, which says how to assign meaning to e xpressions.  
Uncertainty in Reasoning  82 
  The world is an uncertain place; often the Knowledge is imperfect which causes 
uncertainty.  
 So, Therefore reasoning must be able to operate under uncertainty.  
 Also, AI systems must have the ability to reason under condi tions of uncertainty.  
Monotonic Reasoning  
 A reasoning process that moves in one direction only.  
 Moreover, The number of facts in the knowledge base is always increasing.  
 The conclusions derived are valid deductions and they remain so.  
A monotonic logic can not handle  
1. Reasoning by default: because consequences may derive only because of lack of evidence 
to the contrary.  
2. Abductive reasoning: because consequences only deduced as most likely explanations.  
3. Belief revision: because new knowledge may contradict old  beliefs.  
 
Introduction to Nonmonotonic Reasoning  
Non-monotonic Reasoning  
The definite clause logic is  monotonic  in the sense that anything that could be concluded before 
a clause is added can still be concluded after it is added; adding knowledge does not  reduce the 
set of propositions that can be derived.  
A logic is  non-monotonic  if some conclusions can be invalidated by adding more knowledge. 
The logic of definite clauses with negation as failure is non -monotonic. Non -monotonic 
reasoning is useful for re presenting defaults. A  default  is a rule that can be used unless it 
overridden by an exception.  
For example, to say that  b is normally true if  c is true, a knowledge base designer can write a rule 
of the form  
b ←c ∧ ∼ aba. 
where  aba is an atom that means a bnormal with respect to some aspect  a. Given  c, the agent can 
infer  bunless it is told  aba. Adding  aba to the knowledge base can prevent the conclusion of  b. 
Rules that imply  abacan be used to prevent the default under the conditions of the body of the 
rule. 
Example 5.27:  Suppose the purchasing agent is investigating purchasing holidays. A resort may 
be adjacent to a beach or away from a beach. This is not symmetric; if the resort was adjacent to 
a beach, the knowledge provider would specify this. Thus, it is reasonable to have the clause  
away_from_beach ← ∼ on_beach.  
This clause enables an agent to infer that a resort is away from the beach if the agent is not told it 
is adjacent to a beach.  
A cooperative system  tries to not mislead. If we are told the reso rt is on the beach, we would 
expect that resort users would have access to the beach. If they have access to a beach, we would 
expect them to be able to swim at the beach. Thus, we would expect the following defaults:  
beach_access ←on_beach ∧ ∼ abbeach_acc ess.  
swim_at_beach ←beach_access ∧ ∼ abswim_at_beach . 
A cooperative system would tell us if a resort on the beach has no beach access or if there is no 
swimming. We could also specify that, if there is an enclosed bay and a big city, then there is no 
swim ming, by default:  
abswim_at_beach  ←enclosed_bay ∧big_city ∧ ∼ abno_swimming_near_city . 
We could say that British Columbia is abnormal with respect to swimming near cities:  83 
 abno_swimming_near_city  ←in_BC ∧ ∼ abBC_beaches . 
Given only the preceding rules, an agent infers  away_from_beach . If it is then told  on_beach , it 
can no longer infer  away_from_beach , but it can now infer  beach_access  and swim_at_beach . If 
it is also told  enclosed_bay  and big_city , it can no longer infer  swim_at_beach . However, if it is 
then told  in_BC , it can then infer  swim_at_beach . 
By having defaults of what is normal, a user can interact with the system by telling it what is 
abnormal, which allows for economy in communication. The user does not have to state the 
obvious.  
One way to thi nk about non -monotonic reasoning is in terms of  arguments . The rules can be 
used as components of arguments, in which the negated abnormality gives a way to undermine 
arguments. Note that, in the language presented, only positive arguments exist that can b e 
undermined. In more general theories, there can be positive and negative arguments that attack 
each other.  
 
Implementation Issues  
 
Weak Slot and Filler Structures  
 
Evolution Frames  
 As seen in the previous example, there are certain problems which are dif ficult to solve 
with Semantic Nets.  
 Although there is no clear distinction between a semantic net and frame system, more 
structured the system is, more likely it is to be termed as a frame system.  
 A frame is a collection of attributes (called slots) and as sociated values that describe 
some entities in the world. Sometimes a frame describes an entity in some absolute sense;  
 Sometimes it represents the entity from a particular point of view only.  
 A single frame taken alone is rarely useful; we build frame sys tems out of collections of 
frames that connected to each other by virtue of the fact that the value of an attribute of 
one frame may be another frame.  
Frames as Sets and Instances  
 The set theory is a good basis for understanding frame systems.  
 Each frame r epresents either a class (a set) or an instance (an element of class)  
 Both  isa and instance  relations have inverse attributes, which we call subclasses & all 
instances.  
 As a class represents a set, there are 2 kinds of attributes that can be associated wit h it. 
1. Its own attributes &  
2. Attributes that are to be inherited by each element of the set.  84 
 
 
Frames as Sets and In stances  
 Sometimes, the difference between a set and an individual instance may not be clear.  
 Example: Team India is an instance of the class of Cricket Teams and can also think of as 
the set of players.  
 Now the problem is if we present Team India as a subc lass of Cricket teams, then Indian 
players automatically become part of all the teams, which is not true.  
 So, we can make Team India a subclass of class called Cricket Players.  
 To do this we need to differentiate between regular classes and meta -classes.  
 Regular Classes are those whose elements are individual entities whereas Meta -classes 
are those special classes whose elements are themselves, classes.  
 The most basic meta -class is the class  CLASS . 
 It represents the set of all classes.  
 All classes are insta nces of it, either directly or through one of its subclasses.  
 The class  CLASS  introduces the attribute cardinality, which is to inherited by all instances 
of CLASS. Cardinality stands for the number.  
Other ways of Relating Classes to Each Other  
 We have dis cussed that a class1 can be a subset of class2.  
 If Class2 is a meta -class then Class1 can be an instance of Class2.  
 Another way is the  mutually -disjoint -with relationship, which relates a class to one or 
more other classes that guaranteed to have no elemen ts in common with it.  
 Another one is,  is-covered -by which relates a class to a set of subclasses, the union of 
which is equal to it.  
 If a class is -covered -by a set S of mutually disjoint classes, then S called a partition of the 
class.  
Slots as Full -Fledge d Objects (Frames)  
Till now we have used attributes as slots, but now we will represent attributes explicitly and 
describe their properties.  
Some of the properties we would like to be able to represent and use in reasoning include,  
 The class to which the a ttribute can attach.  
 Constraints on either the type or the value of the attribute.  
 A default value for the attribute. Rules for inheriting values for the attribute.  
 To be able to represent these attributes of attributes, we need to describe attributes (slo ts) 
as frames.  85 
  These frames will organize into an  isa hierarchy, just as any other frames, and that 
hierarchy can then used to support inheritance of values for attributes of slots.  
 Now let us formalize what is a slot. A slot here is a relation.  
 It maps fr om elements of its domain (the classes for which it makes sense) to elements of 
its range (its possible values).  
 A relation is a set of ordered pairs.  
 Thus it makes sense to say that relation R1 is a subset of another relation R2.  
 In that case, R1 is a spe cialization of R2. Since a slot is a set, the set of all slots, which 
we will call SLOT, is a meta -class.  
 Its instances are slots, which may have sub -slots.  
Frame Example  
In this example, the frames Person, Adult -Male, ML -Baseball -Player (corresponding to major 
league baseball players), Pitcher, and ML -Baseball -Team (for major league baseball team) are all 
classes.  
 
 The fra mes Pee -Wee-Reese and Brooklyn -Dodgers are instances.  
 The isa relation that we have been using without a precise definition is, in fact, the subset 
relation. The set of adult males is a subset of the set of people.  
 The set of major league baseball players subset of the set of adult males, and so forth.  
 Our instance relation corresponds to the relation element -of Pee Wee Reese is an element 
of the set of fielders.  
 Thus he is also an element of all of the supersets of fielders, including major league 
baseball  players and people. The transitivity of  isa follows directly from the transitivity 
of the subset relation.  86 
  Both the isa  and instance relations have inverse attributes, which we call subclasses and 
all instances.  
 Because a class represents a set, there are  two kinds of attributes that can associate with 
it. 
 Some attributes are about the set itself, and some attributes are to inherited by each 
element of the set.  
 We indicate the difference between these two by prefixing the latter with an asterisk (*).  
 For e xample, consider the class ML -Baseball -Player, we have shown only two properties 
of it as a set: It a subset of the set of adult males. And it has cardinality 624.  
 We have listed five properties that all major league baseball players have (height, bats, 
batting average, team, and uniform -color), and we have specified default values for the 
first three of them.  
 By providing both kinds of slots, we allow both classes to define a set of objects and to 
describe a prototypical object of the set.  
 Frames are usefu l for representing objects that are typical of stereotypical situations.  
 The situation like the structure of complex physical objects, visual scenes, etc.  
 A commonsense knowledge can represent using default values if no other value exists. 
Commonsense is g enerally used in the absence of specific knowledge.  
Semantic Nets  
 Inheritance property can represent using  isa and instance  
 Monotonic Inheritance can perform substantially more efficiently with such structures 
than with pure logic, and non -monotonic inheri tance is also easily supported.  
 The reason that makes Inheritance easy is that the knowledge in slot and filler systems is 
structured as a set of entities and their attributes.  
These structures turn out to be useful as,  
 It indexes assertions by the entitie s they describe. As a result, retrieving the value for an 
attribute of an entity is fast.  
 Moreover, It makes easy to describe properties of relations. To do this in a purely logical 
system requires higher -order mechanisms.  
 It is a form of object -oriented p rogramming and has the advantages that such systems 
normally include modularity and ease of viewing by people.  
Here we would describe two views of this kind of structure – Semantic Nets & Frames.  
Semantic Nets   
 There are different approaches to knowledge r epresentation include semantic net, frames, 
and script.  
 The semantic net describes both objects and events.  
 In a semantic net, information represented as a set of nodes connected to each other by a 
set of labeled arcs, which represents relationships among the nodes.  
 It is a directed graph consisting of vertices which represent concepts and edges which 
represent semantic relations between the concepts.  
 It is also known as associative net due to the association of one node with other.  
 The main idea is that th e meaning of the concept comes from the ways in which it 
connected to other concepts.  
 We can use inheritance to derive additional relations.  87 
 
 
Figure: A Semantic Network  
Intersection Search  Semantic Nets  
 We try to find relationships among objects by spreading activation out from each of two 
nodes. And seeing where the activation meets.  
 Using this we can answer the questions like, what is the relation between India and Blue.  
 It takes advantage of the entity -based organization of knowledge that slot and filler 
representation provides.  
Representing Non -binary Predicates  Semantic Nets  
 Simple binary predicates like isa(Person, Mam mal) can represent easily by semantic nets 
but other non -binary predicates can also represent by using general -purpose predicates 
such as  isa and instance . 
 Three or even more place predicates can also convert to a binary form by creating one 
new object rep resenting the entire predicate statement and then introducing binary 
predicates to describe a relationship to this new object.  
Conceptual Dependency  
Introduction to  Strong Slot and Filler Structures  
 The main problem with semantic networks and frames is tha t they lack formality; there is 
no specific guideline on how to use the representations.  
 In frame when things change, we need to modify all frames that are relevant – this can be 
time-consuming.  
 Strong slot and filler structures typically represent links b etween objects according to 
more rigid rules, specific notions of what types of object and relations between them are 
provided and represent knowledge about common situations.  
 Moreover, We have types of strong slot and filler structures:  
1. Conceptual Depende ncy (CD)  
2. Scripts  
3. Cyc 
Conceptual Dependency (CD)  
Conceptual Dependency originally developed to represent knowledge acquired from natural 
language input.  
The goals of this theory are:  
 To help in the drawing of the inference from sentences.  
 To be independent of the words used in the original input.  
 That is to say: For any 2 (or more) sentences that are identical in meaning there should be 
only one representation of that meaning.  
Moreover, It has used by many programs that portend to understand English (MARGIE,  SAM, 
PAM).  88 
 Conceptual Dependency (CD) provides:  
 A structure into which nodes representing information can be placed.  
 Also, A specific set of primitives.  
 A given level of granularity.  
Sentences are represented as a series of diagrams depicting actions usin g both abstract and real 
physical situations.  
 The agent and the objects represented.  
 Moreover, The actions are built up from a set of primitive acts which can modify by 
tense.  
CD is based on events and actions. Every event (if applicable) has:  
 an ACTOR o a n ACTION performed by the Actor  
 Also, an OBJECT that the action performs on  
 A DIRECTION in which that action is oriented  
These are represented as slots and fillers. In English sentences, many of these attributes left out.  
A Simple Conceptual Dependency Rep resentation  
For the sentences, “I have a book to the man” CD representation is as follows:  
 
Where the symbols have  the following meaning.  
 Arrows indicate directions of dependency.  
 Moreover, The double arrow indicates the two -way link between actor and action.  
 O — for the object case relation  
 R – for the recipient case relation  
 P – for past tense  
 D – destination  
 Primi tive Acts of Conceptual Dependency Theory  
ATRANS  
 Transfer of an abstract relationship (i.e. give)  
PTRANS  
 Transfer of the physical location of an object (e.g., go)  
PROPEL  
 Also, Application of physical force to an object (e.g. push)  
MOVE  
 Moreover, Movement o f a body part by its owner (e.g. kick)  
GRASP  
 Grasping of an object by an action (e.g. throw)  
INGEST  
 Ingesting of an object by an animal (e.g. eat)  
EXPEL  
 Expulsion of something from the body of an animal (e.g. cry)  
MTRANS  
 Transfer of mental information (e.g . tell)  
MBUILD  
 Building new information out of old (e.g decide)  
SPEAK  89 
  Producing of sounds (e.g. say)  
ATTEND  
 Focusing of a sense organ toward a stimulus (e.g. listen)  
There are four conceptual categories. These are,  
ACT  
 Actions {one of the CD primitives}  
PP 
 Also, Objects {picture producers}  
AA 
 Modifiers of actions {action aiders}  
PA 
 Modifiers of PP’s {picture aiders}  
Advantages of  Conceptual Dependency  
 Using these primitives involves fewer inference rules.  
 So, Many inference rules already represented in CD s tructure.  
 Moreover, The holes in the initial structure help to focus on the points still to established.  
Disadvantages of  Conceptual Dependency  
 Knowledge must decompose into fairly low -level primitives.  
 Impossible or difficult to find the correct set of pr imitives.  
 Also, A lot of inference may still require.  
 Representations can be complex even for relatively simple actions.  
 Consider: Dave bet Frank five pounds that Wales would win the Rugby World Cup.  
 Moreover, Complex representations require a lot of stora ge. 
Scripts  
Scripts Strong Slot  
 A script is a structure that prescribes a set of circumstances which could be expected to 
follow on from one another.  
 It is similar to a thought sequence or a chain of situations which could be anticipated.  
 It could be consi dered to consist of a number of slots or frames but with more specialized 
roles.  
Scripts are beneficial because:  
 Events tend to occur in known runs or patterns.  
 Causal relationships between events exist.  
 Entry conditions exist which allow an event to take place  
 Prerequisites exist for events taking place. E.g. when a student progresses through a 
degree scheme or when a purchaser buys a house.  
Script Components  
Each script contains the following main components.  
 Entry Conditions: Must be satisfied before eve nts in the script can occur.  
 Results: Conditions that will be true after events in script occur.  
 Props: Slots representing objects involved in the events.  
 Roles: Persons involved in the events.  
 Track: the Specific variation on the more general pattern in t he script. Different tracks 
may share many components of the same script but not all.  90 
  Scenes: The sequence of events that occur. Events represented in conceptual dependency 
form.  
 Advantages and Disadvantages of Script  
Advantages  
 Capable of predicting impl icit events  
 Single coherent interpretation may be build up from a collection of observations.  
Disadvantage  
 More specific (inflexible) and less general than frames.  
 Not suitable to represent all kinds of knowledge.  
To deal with inflexibility, smaller module s called memory organization packets (MOP) 
can combine in a way that appropriates for the situation.  
Script Example  
 
 It must activate based on its significance.  
 If the topic important, then the script should open.  
 If a topic just mentioned, then a pointer to that script could hold.  
 For example, given “John enjoyed t he play in theater”, a script “Play in Theater” 
suggested above invoke.  
 All implicit questions can  answer correctly.  
Here the significance of this script is high.  
 Did John go to the theater?  
 Also, Did he buy the ticket?  
 Did he have money?  
If we have a sent ence like “John went to the theater to pick his daughter”, then invoking this 
script will lead to many wrong answers.  
 Here significance of the script theater is less.  91 
 Getting significance from the story is not straightforward. However, some heuristics can apply to 
get the value.  
 
CYC  
What is CYC?  
 An ambitious attempt to form a very large knowledge base aimed at capturing 
commonsense reasoning.  
 Initial goals to capture knowledge from a hundred randomly selected articles in the 
Encyclopedia Britannica.  
 Also, Both Implicit and Explicit knowledge encoded.  
 Moreover, Emphasis on study of underlying information (assumed by the authors but not 
needed to tell to the readers.  
Example: Suppose we read that Wellington learned of Napoleon’s death  Then we (humans) 
can c onclude Napoleon never new that Wellington had died.  
How do we do this?  
So, We require special implicit knowledge or commonsense such as:  
 We only die once.  
 You stay dead.  
 Moreover, You cannot learn anything when dead.  
 Time cannot go backward.  
Why build lar ge knowledge bases:  
1. Brittleness  
 Specialised knowledge bases are brittle. Hard to encode new situations and non -
graceful degradation in performance. Commonsense based knowledge bases 
should have a firmer foundation.  
2. Form and Content  
 Moreover, Knowledge repr esentation may not be suitable for AI. Commonsense 
strategies could point out where difficulties in content may affect the form.  
3. Shared Knowledge  
 Also, Should allow greater communication among systems with common bases 
and assumptions.  
How is CYC coded?  
 By hand.  
 Special CYCL language:  
 LISP -like. 
 Frame -based  
 Multiple inheritances  
 Slots are fully fledged objects.  
 Generalized inheritance — any link not just  isa and instance . 
 
 
 
Module 2  
Game Playing:  92 
 Game Playing  
 Charles Babbage, the nineteenth -century compute r architect thought about programming 
his analytical engine to play chess and later of building a machine to play tic -tac-toe. 
 There are two reasons that games appeared to be a good domain.  
1. They provide a structured task in which it is very easy to measure  success or 
failure.  
2. They are easily solvable by straightforward search from the starting state to a 
winning position.  
 The first is true is for all games bust the second is not true for all, except simplest games.  
 For example, consider chess.  
 The average b ranching factor is around 35. In an average game, each player might make 
50. 
 So in order to examine the complete game tree, we would have to examine 35100 
 Thus it is clear that a simple search is not able to select even its first move during the 
lifetime o f its opponent.  
 It is clear that to improve the effectiveness of a search based problem -solving program 
two things can do.  
1. Improve the generate procedure so that only good moves generated.  
2. Improve the test procedure so that the best move will recognize and  explored first.  
 If we use legal -move generator then the test procedure will have to look at each of them 
because the test procedure must look at so many possibilities, it must be fast.  
 Instead of the legal -move generator, we can use plausible -move generat or in which only 
some small numbers of promising moves generated.  
 As the number of lawyers available moves increases, it becomes increasingly important 
in applying heuristics to select only those moves that seem more promising.  
 The performance of the overa ll system can improve by adding heuristic knowledge into 
both the generator and the tester.  
 In game playing, a goal state is one in which we win but the game like chess. It is not 
possible. Even we have good plausible move generator.  
 The depth of the resul ting tree or graph and its branching factor is too great.  
 It is possible to search tree only ten or twenty moves deep then in order to choose the best 
move. The resulting board positions must compare to discover which is most 
advantageous.  
 This is done usi ng static evolution function, which uses whatever information it has to 
evaluate individual board position by estimating how likely they are to lead eventually to 
a win.  
 Its function is similar to that of the heuristic function h’ in the A* algorithm: in t he 
absence of complete information, choose the most promising position.  
MINIMAX Search Procedure  
 The minimax search is a depth -first and depth limited procedure.  
 The idea is to start at the current position and use the plausible -move generator to 
generate the set of possible successor positions.  93 
  Now we can apply the static evolution function to those positions and simply choose the 
best one.  
 After doing so, we can back that value up to the starting position to represent our 
evolution of it.  
 Here we assume t hat static evolution function returns larger values to indicate good 
situations for us.  
 So our goal is to maximize the value of the static evaluation function of the next board 
position.  
 The opponents’ goal is to minimize the value of the static evaluation  function.  
 The alternation of maximizing and minimizing at alternate ply when evaluations are 
to be pushed back up corresponds to the opposing strategies of the two players is 
called MINIMAX.  
 It is the recursive procedure that depends on two procedures  
 MOV EGEN(position, player) — The plausible -move generator, which returns a 
list of nodes representing the moves that can make by Player in Position.  
 STATIC(position, player) – static evaluation function, which returns a number 
representing the goodness of Positi on from the standpoint of Player.  
 With any recursive program, we need to decide when recursive procedure should stop.  
 There are the variety of factors that may influence the decision they are,  
 Has one side won?  
 How many plies have we already explored? Or h ow much time is left?  
 How stable is the configuration?  
 We use DEEP -ENOUGH which assumed to evaluate all of these factors and to return 
TRUE if the search should be stopped at the current level and FALSE otherwise.  
 It takes two parameters, position, and dep th, it will ignore its position parameter and 
simply return TRUE if its depth parameter exceeds a constant cut off value.  
 One problem that arises in defining MINIMAX as a recursive procedure is that it needs to 
return not one but two results.  
 The backed -up value of the path it chooses.  
 The path itself. We return the entire path even though probably only the first 
element, representing the best move from the current position, actually needed.  
 We assume that MINIMAX returns a structure containing both results  and we have two 
functions, VALUE and PATH that extract the separate components.  
 Initially, It takes three parameters, a board position, the current depth of the search, and 
the player to move,  
 MINIMAX(current,0,player -one) If player –one is to move  
 MINIMA X(current,0,player -two) If player –two is to move  
Adding alpha -beta cutoffs  
 Minimax procedure is a depth -first process. One path is explored as far as time allows, 
the static evolution function is applied to the game positions at the last step of the path.  
 The efficiency of the depth -first search can improve by branch and bound technique in 
which partial solutions that clearly worse than known solutions can abandon early.  
 It is necessary to modify the branch and bound strategy to include two bounds, one for  
each of the players.  
 This modified strategy called alpha -beta pruning.  94 
  It requires maintaining of two threshold values, one representing a lower bound on that a 
maximizing node may ultimately assign (we call this alpha).  
 And another representing an upper bound on the value that a minimizing node may assign 
(this we call beta).  
 Each level must receive both the values, one to use and one to pass down to the next level 
to use.  
 The MINIMAX procedure as it stands does not need to treat maximizing and minimizing  
levels differently. Since it simply negates evaluation each time it changes levels.  
 Instead of referring to alpha and beta, MINIMAX uses two values, USE -THRESH and 
PASSTHRESH.  
 USE -THRESH used to compute cutoffs. PASS -THRESH passed to next level as its 
USE THRESH.  
 USE -THRESH must also pass to the next level, but it will pass as PASS -THRESH so that 
it can be passed to the third level down as USE -THRESH again, and so forth.  
 Just as values had to negate each time they passed across levels.  
 Still, there is no di fference between the code required at maximizing levels and that 
required at minimizing levels.  
 PASS -THRESH should always the maximum of the value it inherits from above and the 
best move found at its level.  
 If PASS -THRESH updated the new value should prop agate both down to lower levels. 
And back up to higher ones so that it always reflects the best move found anywhere in the 
tree. 
 The MINIMAX -A-B requires five arguments, position, depth, player, Use -thresh, and 
passThresh.  
 MINIMAX -A-B(current,0,player -one,maximum value static can compute, minimum 
value static can compute).  
Iterative Deepening Search(IDS) or Iterative Deepening Depth First Search(IDDFS)  
There are two common ways to traverse a graph,  BFS and DFS. Considering a Tree (or Graph) 
of huge height and width, both BFS and DFS are not very efficient due to following reasons.  
1. DFS  first traver ses nodes going through one adjacent of root, then next adjacent. The 
problem with this approach is, if there is a node close to root, but not in first few subtrees 
explored by DFS, then DFS reaches that node very late. Also, DFS may not find shortest 
path to a node (in terms of number of edges).  95 
 
 
2. BFS goes level by level, bu t requires more space. The space required by DFS is O(d) 
where d is depth of tree, but space required by BFS is O(n) where n is number of nodes in 
tree (Why? Note that the last level of tree can have around n/2 nodes and second last 
level n/4 nodes and in BFS we need to have every level one by one in queue).  
IDDFS  combines depth -first search’s space -efficiency and breadth -first search’s fast search (for 
nodes closer to root).  
How does IDDFS work?  
IDDFS calls DFS for different depths starting from an initial  value. In every call, DFS is 
restricted from going beyond given depth. So basically we do DFS in a BFS fashion.  
Algorithm:  
 
 
 
// Returns true if target is reachable from  
// src within max_depth  
bool IDDFS(src, target, max_depth)  
    for limit from  0 to max_depth  
       if DLS(src, target, limit) == true 
           return  true 
    return  false     
 
bool DLS(src, target, limit)  
    if (src == target)  
        return  true; 
 
    // If reached the maximum depth,  
    // stop recursing.  
    if (limit <= 0)  
        return  false ;    
 
    foreach  adjacent i of src  96 
         if DLS(i, target, limit?1)              
            return  true 
 
    return  false  
An important thing to note is, we visit top level nodes multiple times. The last (or max depth) 
level is visited once , second last level is visited twice, and so on. It may seem expensive, but it 
turns out to be not so costly, since in a tree most of the nodes are in the bottom level. So it does 
not matter much if the upper levels are visited multiple times.  
Planning  
Blocks World Problem  
In order to compare the variety of methods of planning, we should find it useful to look at all of 
them in a single domain that is complex enough that the need for each of the mechanisms is 
apparent yet simple enough that easy -to-follow e xamples can be found.  
 There is a flat surface on which blocks can be placed.  
 There are a number of square blocks, all the same size.  
 They can be stacked one upon the other.  
 There is robot arm that can manipulate the blocks.  
Actions of the robot arm  
1. UNSTACK (A, B): Pick up block A from its current position on block B.  
2. STACK(A, B): Place block A on block B.  
3. PICKUP(A): Pick up block A from the table and hold it.  
4. PUTDOWN(A): Put block A down on the table.  
Notice that the robot arm can hold only one block at a ti me. 
Predicates  
 In order to specify both the conditions under which an operation may be performed and 
the results of performing it, we need the following predicates:  
1. ON(A, B): Block A is on Block B.  
2. ONTABLES(A): Block A is on the table.  
3. CLEAR(A): There is n othing on the top of Block A.  
4. HOLDING(A): The arm is holding Block A.  
5. ARMEMPTY: The arm is holding nothing.  
Robot problem -solving systems (STRIPS)  
 List of new predicates that the operator causes to become true is ADD List  
 Moreover, List of old predicates t hat the operator causes to become false is DELETE List  
 PRECONDITIONS list contains those predicates that must be true for the operator to be 
applied.  
STRIPS style operators for BLOCKs World  
STACK(x, y)  
P: CLEAR(y)^HOLDING(x)  
D: CLEAR(y)^HOLDING(x)  
A: ARMEM PTY^ON(x, y)  
UNSTACK(x, y)  
PICKUP(x)  
P: CLEAR(x) ^ ONTABLE(x) ^ARMEMPTY  
D: ONTABLE(x) ^ ARMEMPTY  97 
 A: HOLDING(x)  
PUTDOWN(x)  
Goal Stack Planning  
To start with goal stack is simply:  
 ON(C,A)^ON(B,D)^ONTABLE(A)^ONTABLE(D)  
This problem is separate into four sub -problems, one for each component of the goal.  
Two of the sub -problems ONTABLE(A) and ONTABLE(D) are already true in the initial state.  
 
Alternative 1: Goal Stack:  
 ON(C,A)  
 ON(B,D)  
 ON(C,A)^ON(B,D)^OTAD  
Alternative 2: Goal stack:  
 ON(B,D)  
 ON(C,A)  
 ON(C,A)^ON(B,D)^OTAD  
Exploring Operators  
 Pursuing alternative 1, we check for operators that could cause ON(C, A)  
 Out of the 4 operators, there is only one STACK. So it yields:  
 STACK(C,A)  
 ON(B,D)  
 ON(C,A)^ON(B,D)^OTAD  
 Preconditions for STACK(C, A) should be satisfied, we must establish them as sub -goals:  
 CLEAR(A)  
 HOLDING(C)  
 CLEAR(A)^HOLDING(C)  
 STACK(C,A) o ON(B,D)  
 ON(C,A)^ON(B,D)^O TAD  
 Here we exploit the Heuristic that if HOLDING is one of the several goals to be achieved 
at once, it should be tackled last.  
Goal stack Planning  
 Next, we see if CLEAR(A) is true. It is not. The only operator that could make it true is 
UNSTACK(B, A). Al so, This produces the goal stack:  
 ON(B, A)  
 CLEAR(B)  
 ON(B,A)^CLEAR(B)^ARMEMPTY  
 UNSTACK(B, A)  
 HOLDING(C)  98 
  CLEAR(A)^HOLDING(C)  
 STACK(C,A)  
 ON(B,D)  
 ON(C,A)^ON(B,D)^OTAD  
 We see that we can pop predicates on the stack till we reach HOLDING(C) for which we 
need to find a suitable operator.  
 Moreover, The operators that might make HOLDING(C) true: PICKUP(C) and 
UNSTACK(C, x). Without looking ahead, since we cannot tell which of these operators 
is appropriate. Also, we create two branches of the search tree correspondi ng to the 
following goal stacks:  
 
Complete plan  
1. UNSTACK(C, A)  
2. PUTDOWN(C )  
3. PICKUP(A)  
4. STACK(A, B)  
5. UNSTACK(A, B)  
6. PUT DOWN(A)  
7. PICKUP(B)  
8. STACK(B, C)  
9. PICKUP(A)  
10. STACK(A,B)  
Planning Components  
 Methods which focus on ways of decomposing the original problem into appropriate 
subparts and on ways of recording and handling interactions among the subparts as they 
are detected duri ng the problem -solving process are often called as planning.  
 Planning refers to the process of computing several steps of a problem -solving procedure 
before executing any of them.  
Components of a planning system  
Choose the best rule to apply next, based on  the best available heuristic information.  
 The most widely used technique for selecting appropriate rules to apply is first to isolate 
a set of differences between desired goal state and then to identify those rules that are 
relevant to reduce those differ ences.  
 If there are several rules, a variety of other heuristic information can be exploited to 
choose among them.  
Apply the chosen rule to compute the new problem state that arises from its application.  99 
  In simple systems, applying rules is easy. Each rule  simply specifies the problem state 
that would result from its application.  
 In complex systems, we must be able to deal with rules that specify only a small part of 
the complete problem state.  
 One way is to describe, for each action, each of the changes it  makes to the state 
description.  
Detect when a solution has found.  
 A planning system has succeeded in finding a solution to a problem when it has found a 
sequence of operators that transform the initial problem state into the goal state.  
 How will it know w hen this has done?  
 In simple problem -solving systems, this question is easily answered by a straightforward 
match of the state descriptions.  
 One of the representative systems for planning systems is predicate logic. Suppose that as 
a part of our goal, we h ave the predicate P(x).  
 To see whether P(x) satisfied in some state, we ask whether we can prove P(x) given the 
assertions that describe that state and the axioms that define the world model.  
Detect dead ends so that they can abandon and the system’s effor t directed in more fruitful 
directions.  
 As a planning system is searching for a sequence of operators to solve a particular 
problem, it must be able to detect when it is exploring a path that can never lead to a 
solution.  
 The same reasoning mechanisms that  can use to detect a solution can often use for 
detecting a dead end.  
 If the search process is reasoning forward from the initial state. It can prune any path that 
leads to a state from which the goal state cannot reach.  
 If search process reasoning backwar d from the goal state, it can also terminate a path 
either because it is sure that the initial state cannot reach or because little progress made.  
Detect when an almost correct solution has found and employ special techniques to make it 
totally correct.  
 The kinds of techniques discussed are often useful in solving nearly decomposable 
problems.  
 One good way of solving such problems is to assume that they are completely 
decomposable, proceed to solve the sub -problems separately. And then check that when 
the sub-solutions combined. They do in fact give a solution to the original problem.  
Goal Stack Planning  
 Methods which focus on ways of decomposing the original problem into appropriate 
subparts and on ways of recording. And handling interactions among the subp arts as they 
are detected during the problem -solving process are often called as planning.  
 Planning refers to the process of computing several steps of a problem -solving procedure 
before executing any of them.  
  
 
Goal Stack Planning Method  
 In this method, t he problem solver makes use of a single stack that contains both goals 
and operators. That have proposed to satisfy those goals.  100 
  The problem solver also relies on a database that describes the current situation and a set 
of operators described as PRECONDIT ION, ADD and DELETE lists.  
 The goal stack planning method attacks problems involving conjoined goals by solving 
the goals one at a time, in order.  
 A plan generated by this method contains a sequence of operators for attaining the first 
goal, followed by a complete sequence for the second goal etc.  
 At each succeeding step of the problem -solving process, the top goal on the stack will 
pursue.  
 When a sequence of operators that satisfies it, found, that sequence applied to the state 
description, yielding new de scription.  
 Next, the goal that then at the top of the stack explored. And an attempt made to satisfy it, 
starting from the situation that produced as a result of satisfying the first goal.  
 This process continues until the goal stack is empty.  
 Then as one l ast check, the original goal compared to the final state derived from the 
application of the chosen operators.  
 If any components of the goal not satisfied in that state. Then those unsolved parts of the 
goal reinserted onto the stack and the process resume d. 
Nonlinear Planning using Constraint Posting  
 Difficult problems cause goal interactions.  
 The operators used to solve one sub -problem may interfere with the solution to a previous 
sub-problem.  
 Most problems require an intertwined plan in which multiple su b-problems worked on 
simultaneously.  
 Such a plan is called nonlinear plan because it is not composed of a linear sequence of 
complete sub -plans.  
Constraint Posting  
 The idea of constraint posting is to build up a plan by incrementally hypothesizing 
operator s, partial orderings between operators, and binding of variables within operators.  
 At any given time in the problem -solving process, we may have a set of useful operators 
but perhaps no clear idea of how those operators should order with respect to each ot her. 
 A solution is a partially ordered, partially instantiated set of operators to generate an 
actual plan. And we convert the partial order into any number of total orders.  
Constraint Posting versus State Space search  
State Space Search  
 Moves in the space : Modify world state via operator  
 Model of time: Depth of node in search space  
 Plan stored in Series of state transitions  
Constraint Posting Search  
 Moves in the space: Add operators, Oder Operators, Bind variables Or Otherwise 
constrain plan  
 Model of Time:  Partially ordered set of operators  
 Plan stored in Single node  
Algorithm: Nonlinear Planning (TWEAK)  
1. Initialize S to be the set of propositions in the goal state.  
2. Remove some unachieved proposition P from S.  101 
 3. Moreover, Achieve P by using step addition, prom otion, DE clobbering, simple 
establishment or separation.  
4. Review all the steps in the plan, including any new steps introduced by step addition, to 
see if any of their preconditions unachieved. Add to S the new set of unachieved 
preconditions.  
5. Also, If S i s empty, complete the plan by converting the partial order of steps into a total 
order, instantiate any variables as necessary.  
6. Otherwise, go to step 2.  
Hierarchical Planning  
 In order to solve hard problems, a problem solver may have to generate long plans . 
 It is important to be able to eliminate some of the details of the problem until a solution 
that addresses the main issues is found.  
 Then an attempt can make to fill in the appropriate details.  
 Early attempts to do this involved the use of macro operator s, in which larger operators 
were built from smaller ones.  
 In this approach, no details eliminated from actual descriptions of the operators.  
ABSTRIPS  
A better approach developed in ABSTRIPS systems which actually planned in a hierarchy of 
abstraction spac es, in each of which preconditions at a lower level of abstraction ignored.  
ABSTRIPS approach is as follows:  
 First solve the problem completely, considering only preconditions whose criticality 
value is the highest possible.  
 These values reflect the expect ed difficulty of satisfying the precondition.  
 To do this, do exactly what STRIPS did, but simply ignore the preconditions of lower 
than peak criticality.  
 Once this done, use the constructed plan as the outline of a complete plan and consider 
preconditions at the next -lowest criticality level.  
 Augment the plan with operators that satisfy those preconditions.  
 Because this approach explores entire plans at one level of detail before it looks at the 
lower -level details of any one of them, it has called length -first approach.  
The assignment of appropriate criticality value is crucial to the success of this hierarchical 
planning method.  
Those preconditions that no operator can satisfy are clearly the most critical.  
Example, solving a problem of moving the robot, f or applying an operator, PUSH -THROUGH 
DOOR, the precondition that there exist a door big enough for the robot to get through is of high 
criticality since there is nothing we can do about it if it is not true.  
 
 
 
 
Other Planning Techniques  
Reactive Systems  102 
  The idea of reactive systems is to avoid planning altogether, and instead, use the 
observable situation as a clue to which one can simply react.  
 A reactive system must have access to a knowledge base of some sort that describes what 
actions should be taken  under what circumstances.  
 A reactive system is very different from the other kinds of planning systems we have 
discussed. Because it chooses actions one at a time.  
 It does not anticipate and select an entire action sequence before it does the first thing.  
 The example is a Thermostat. The job of the thermostat is to keep the temperature 
constant inside a room.  
 Reactive systems are capable of surprisingly complex behaviors.  
 The main advantage reactive systems have over traditional planners is that they opera te 
robustly in domains that are difficult to model completely and accurately.  
 Reactive systems dispense with modeling altogether and base their actions directly on 
their perception of the world.  
 Another advantage of reactive systems is that they are extrem ely responsive since they 
avoid the combinatorial explosion involved in deliberative planning.  
 This makes them attractive for real -time tasks such as driving and walking.  
Other Planning Techniques  
Triangle tables  
 Provides a way of recording the goals that each operator expected to satisfy as well as the 
goals that must be true for it to execute correctly.  
Meta -planning  
 A technique for reasoning not just about the problem solved but also about the planning 
process itself.  
Macro -operators  
 Allow a planner to b uild new operators that represent commonly used sequences of 
operators.  
Case -based planning:  
 Re-uses old plans to make new ones.  
UNDERSTANDING  
Understanding is the simplest procedure of all human beings. Understanding means ability to 
determine some new k nowledge from a given knowledge. For each action of a problem, the 
mapping of some new actions is very necessary. Mapping the knowledge means transferring the 
knowledge from one representation to another representation. For example, if you will say “I 
need  to go to New Delhi” for which you will book the tickets. The system will have 
“understood” if it finds the first available plane to New Delhi. But if you will say the same thing 
to you friends, who knows that your family lives in “New Delhi”, he/she will have “understood” 
if he/she realizes that there may be a problem or occasion in your family. For people, 
understanding applies to inputs from all the senses. Computer understanding has so far been 
applied primarily to images, speech and typed languages. It  is important to keep in mind that the 
success or failure of an “understanding” problem can rarely be measured in an absolute sense but 
must instead be measured with respect to a particular task to be performed. There are some 
factors that contribute to th e difficulty of an understanding problem.  
(a) If the target representation is very complex for which you cannot map from the original 
representation.  103 
 (b) There are different types of mapping factors may arise like one -to-one, one -to-many and 
many  to many .  
(c) Some noise or disturbing factors are also there.  
(d) The level of interaction of the source components may be complex one.  
(e) The problem solver might be unknown about some more complex problems.  
(f) The intermediary actions may also be unavaila ble.  
 
Consider an example of an English sentence which is being used for communication with a 
keyword  based data retrieval system. Suppose I want to know all about the temples in India. So I 
would need to be translated into a representation such as  The ab ove sentence is a simple sentence 
for which the corresponding representation may be easy to implement. But what for the complex 
queries?  
 
Consider the following query.  
“Ram told Sita he would not eat apple with her. He has to go to the office”.  
 
This ty pe of complex queries can be modeled with the conceptual dependency representation 
which is more complex than that of simple representation. Constructing these queries is very 
difficult since more informationare to be extracted. Extracting more information  will require 
some more knowledge. Also the type of mapping process is not quite easy to the problem solver. 
Understanding is the process of mapping an input from its original form to a more useful one. 
The simplest kind of mapping is “one -toone”.  
In one -to-one mapping each different problems would lead to only one solution. But there are 
very few inputs which are one -to-one. Other mappings are quite difficult to implement. Many -to-
one mappings are frequent is that free variation is often allowed, either b ecause of the physical 
limitations of that produces the inputs or because such variation simply makes the task of 
generating the inputs.  
Many  to one mapping require that the understanding system know about all the ways that a target 
representation can be expressed in the source language. One -to-many mapping requires a great 
deal of domain knowledge in order to make the correct choice among the available target 
representation.  
The mapping process is simplest if each component can be mapped without concern for the other 
components of the statement. If the number of interactions increases, then the complexity of the 
problem will increase. In many understanding situations the input to which meaning should be 
assigned is not always the input that is presented t o the under stander.  
Because of the complex environment in which understanding usually occurs, other things often 
interfere with the basic input before it reaches the under stander. Hence the understanding will be 
more complex if there will be some sort o f noise on the inputs.  
 
 
 
 
Natural Language Processing  
Introduction to  Natural Language Processing   104 
  Language meant for communicating with the world.  
 Also, By studying language, we can come to understand more about the world.  
 If we can succeed at building c omputational mode of language, we will have a powerful 
tool for communicating with the world.  
 Also, We look at how we can exploit knowledge about the world, in combination with 
linguistic facts, to build computational natural language systems.  
Natural Lang uage Processing (NLP) problem can divide into two tasks:  
1. Processing written text, using lexical, syntactic and semantic knowledge of the language 
as well as the required real -world information.  
2. Processing spoken language, using all the information needed a bove plus additional 
knowledge about phonology as well as enough added information to handle the further 
ambiguities that arise in speech.  
Steps in Natural Language Processing  
Morphological Analysis  
 Individual words analyzed into their components and non -word tokens such as 
punctuation separated from the words.  
Syntactic Analysis  
 Linear sequences of words transformed into structures that show how the words relate to 
each other.  
 Moreover, Some word sequences may reject if they violate the language’s rule for  how 
words may combine.  
Semantic Analysis  
 The structures created by the syntactic analyzer assigned meanings.  
 Also, A mapping made between the syntactic structures and objects in the task domain.  
 Moreover, Structures for which no such mapping possible may reject.  
Discourse integration  
 The meaning of an individual sentence may depend on the sentences that precede it. And 
also, may influence the meanings of the sentences that follow it.  
Pragmatic Analysis  
 Moreover, The structure representing what said reinter preted to determine what was 
actually meant.  
Summary  
 Results of each of the main processes combine to form a natural language system.  
 All of the processes are important in a complete natural language understanding system.  
 Not all programs are written with exactly these components.  
 Sometimes two or more of them collapsed.  
 Doing that usually results in a system that is easier to build for restricted subsets of 
English but one that is harder to extend to wider coverage.  
 
 
Steps Natural Language Processing  
Morp hological Analysis  
 Suppose we have an English interface to an operating system and the following sentence 
typed: I want to print Bill’s .init file.  
 The morphological analysis must do the following things:  105 
  Pull apart the word “Bill’s” into proper noun “Bill ” and the possessive suffix “’s”  
 Recognize the sequence “.init” as a file extension that is functioning as an adjective in the 
sentence.  
 This process will usually assign syntactic categories to all the words in the sentence.  
Syntactic Analysis  
 A syntactic analysis must exploit the results of the morphological analysis to build a 
structural description of the sentence.  
 The goal of this process, called parsing, is to convert the flat list of words that form the 
sentence into a structure that defines the units  that represented by that flat list.  
 The important thing here is that a flat sentence has been converted into a hierarchical 
structure. And that the structure corresponds to meaning units when a semantic analysis 
performed.  
 Reference markers (set of entiti es) shown in the parenthesis in the parse tree.  
 Each one corresponds to some entity that has mentioned in the sentence.  
 These reference markers are useful later since they provide a place in which to 
accumulate information about the entities as we get it.  
 
Semantic Analysis  
 The semantic analysis must do two important things:  
1. It must map individual words into appropriat e objects in the knowledge base or 
database.  
2. It must create the correct structures to correspond to the way the meanings of the 
individual words combine with each other.  
Discourse Integration  
 Specifically, we do not know whom the pronoun “I” or the proper noun “Bill” refers to.  
 To pin down these references requires an appeal to a model of the current discourse 
context, from which we can learn that the current user is USER068 and that the only 
person named “Bill” about whom we could be talking is USER073.  
 Once the correct referent for Bill known, we can also determine exactly which file 
referred to.  
Pragmatic Analysis  
 The final step toward effective understanding is to decide what to do as a result.  106 
  One possible thing to do to record what was said as a fact a nd done with it.  
 For some sentences, a whose intended effect is clearly declarative, that is the precisely 
correct thing to do.  
 But for other sentences, including this one, the intended effect is different.  
 We can discover this intended effect by applying a set of rules that characterize 
cooperative dialogues.  
 The final step in pragmatic processing to translate, from the knowledge -based 
representation to a command to be executed by the system.  
Syntactic Processing  
 Syntactic Processing is the step in which a  flat input sentence converted into a 
hierarchical structure that corresponds to the units of meaning in the sentence. This 
process called parsing.  
 It plays an important role in natural language understanding systems for two reasons:  
1. Semantic processing mu st operate on sentence constituents. If there is no syntactic 
parsing step, then the semantics system must decide on its own constituents. If 
parsing is done, on the other hand, it constrains the number of constituents that 
semantics can consider.  
2. Syntacti c parsing is computationally less expensive than is semantic processing. 
Thus it can play a significant role in reducing overall system complexity.  
 Although it is often possible to extract the meaning of a sentence without using 
grammatical facts, it is no t always possible to do so.  
 Almost all the systems that are actually used have two main components:  
1. A declarative representation, called a grammar, of the syntactic facts about the 
language.  
2. A procedure, called parser that compares the grammar against inpu t sentences to 
produce parsed structures.  
Grammars and Parsers  
 The most common way to represent grammars is a set of production rules.  
 The first rule can read as “A sentence composed of a noun phrase followed by Verb 
Phrase”; the Vertical bar is OR; ε represents the empty string.  
 Symbols that further expanded by rules called non -terminal symbols.  
 Symbols that correspond directly to strings that must found in an input sentence called 
terminal symbols.  
 Grammar formalism such as this one underlies many l inguistic theories, which in turn 
provide the basis for many natural language understanding systems.  
 Pure context -free grammars are not effective for describing natural languages.  
 NLPs have less in common with computer language processing systems such as 
compilers.  
 Parsing process takes the rules of the grammar and compares them against the input 
sentence.  
 The simplest structure to build is a Parse Tree, which simply records the rules and how 
they matched.  
 Every node of the parse tree corresponds either to an input word or to a non -terminal in 
our grammar.  
 Each level in the parse tree corresponds to the application of one grammar rule.  107 
 Example for Syntactic Processing – Augmented Transition 
Network  
Syntactic Processing is the step in which a flat input sente nce is converted into a hierarchical 
structure that corresponds to the units of meaning in the sentence. This process called parsing.  
It plays an important role in natural language understanding systems for two reasons:  
1. Semantic processing must operate on sentence constituents. If there is no syntactic 
parsing step, then the semantics system must decide on its own constituents. If parsing is 
done, on the other hand, it constrains the number of constituents that semantics can 
consider.  
2. Syntactic parsing is c omputationally less expensive than is semantic processing. Thus it 
can play a significant role in reducing overall system complexity.  
Example: A Parse tree for a sentence: Bill Printed the file   
 
The grammar specifies two things about a language:  
1. Its weak generative capacity, by which we mean the set of sentences that contained 
within the language. This set m ade up of precisely those sentences that can completely 
match by a series of rules in the grammar.  
2. Its strong generative capacity, by which we mean the structure to assign to each 
grammatical sentence of the language.  
Augmented Transition Network (ATN)  
 An augmented transition network is a top -down parsing procedure that allows various 
kinds of knowledge to incorporated into the parsing system so it can operate efficiently.  
 ATNs build on the idea of using finite state machines (Markov model) to parse sentenc es. 
 Instead of building an automaton for a particular sentence, a collection of transition 
graphs built.  
 A grammatically correct sentence parsed by reaching a final state in any state graph.  
 Transitions between these graphs simply subroutine calls from one  state to any initial 
state on any graph in the network.  
 A sentence determined to be grammatically correct if a final state reached by the last 
word in the sentence.  
 The ATN is similar to a finite state machine in which the class of labels that can attach to 
the arcs that define the transition between states has augmented.  
Arcs may label with:  
 Specific words such as “in’.  
 Word categories such as noun.  108 
  Procedures that build structures that will form part of the final parse.  
 Procedures that perform arbitrary tests on current input and sentence components that 
have identified.  
Semantic Analysis   
 The structures created by the syntactic analyzer assigned meanings.  
 A mapping made between the syntactic structures and objects in the task domain.  
 Structures for which  no such mapping is possible may rejected.  
 The semantic analysis must do two important things:  
 It must map individual words into appropriate objects in the knowledge base or 
database.  
 It must create the correct structures to correspond to the way the meani ngs of the 
individual words combine with each other.  Semantic Analysis AI  
 Producing a syntactic parse of a sentence is only the first step toward understanding it.  
 We must produce a representation of the meaning of the sentence.  
 Because understanding is a mapping process, we must first define the language into 
which we are trying to map.  
 There is no single definitive language in which all sentence meaning can describe.  
 The choice of a target language for any particular natural language understanding 
program  must depend on what is to  do with the meanings once they constructed.  
 Choice of the target language in Semantic Analysis AI  
 There are two broad families of target languages that used in NL systems, 
depending on the role that the natural language system pl aying in a larger system:  
 When natural language considered as a phenomenon on its own, as for example 
when one builds a program whose goal is to read the text and then answer 
questions about it. A target language can design specifically to support language  
processing.  
 When natural language used as an interface language to another program (such as 
a db query system or an expert system), then the target language must legal input 
to that other program. Thus the design of the target language driven by the 
backe nd program.  
Discourse and Pragmatic Processing  
To understand a single sentence, it is necessary to consider the discourse and pragmatic context 
in which the sentence was uttered.  
There are a number of important relationships that may hold between phrases a nd parts of their 
discourse contexts, including:  
1. Identical entities. Consider the text:  
 Bill had a red balloon. o John wanted it.  
 The word “it” should identify as referring to the red balloon. These types of 
references called anaphora.  
2. Parts of entities. C onsider the text:  
 Sue opened the book she just bought.  
 The title page was torn.  
 The phrase “title page” should be recognized as part of the book that was just 
bought.  109 
 3. Parts of actions. Consider the text:  
 John went on a business trip to New York.  
 He left on  an early morning flight.  
 Taking a flight should recognize as part of going on a trip.  
4. Entities involved in actions. Consider the text:  
 My house was broken into last week.  
 Moreover, They took the TV and the stereo.  
 The pronoun “they” should recognize as re ferring to the burglars who broke into 
the house.  
5. Elements of sets. Consider the text:  
 The decals we have in stock are stars, the moon, item and a flag.  
 I’ll take two moons.  
 Moons mean moon decals.  
6. Names of individuals:  
 Dev went to the movies.  
7. Causal chain s 
 There was a big snow storm yesterday.  
 So, The schools closed today.  
8. Planning sequences:  
 Sally wanted a new car  
 She decided to get a job.  
9. Implicit presuppositions:  
 Did Joe fail CS101?  
The major focus is on using following kinds of knowledge:  
 The current f ocus of the dialogue.  
 Also, A model of each participant’s current beliefs.  
 Moreover, The goal -driven character of dialogue.  
 The rules of conversation shared by all participants.  
 
Statistical Natural Language Processing  
Formerly, many language -processing ta sks typically involved the direct hand coding of 
rules,  which is not in general robust to natural -language variation. The machine -learning 
paradigm calls instead for using  statistical inference  to automatically learn such rules through the 
analysis of large  corpora  of typical real -world examples (a  corpus  (plural, "corpora") is a set of 
documents, possibly with human or computer annotations).  
Many different classes of machine learning algorithms have been applied to natural -language 
processing tasks. These algorithms take as input a large set of "features" that are generated from 
the input d ata. Some of the earliest -used algorithms, such as  decision trees , produced systems of 
hard if -then rules similar to the systems of hand -written rules that were then common. 
Increasingly, however, research has focused on  statistical models , which make 
soft, probabil istic decisions based on attaching  real-valued  weights to each input feature. Such 
models have the advantage that they can express the relative  certainty of many different possible  
answers rather than only one, producing more reliable results when such a model is included as a 
component of a larger system.  
Systems based on machine -learning algorithms have many advantages over hand -produced rules:  110 
  The learning procedures used during machine learning automatically focus on the most 
common cases, whereas when writing rules by hand it is often not at all obvious where 
the effort should be directed.  
 Automatic learning procedures can make use of statistical inference algorithms to 
produce models that are robust to unfamiliar input (e.g. containing words or structures 
that have not been seen before) and to erroneous input (e.g. with misspelled words or 
words accidentally omitted). Generally, handling such input gracefully with hand -written 
rules—or more generally, creating systems of hand -written rules that make soft 
decisions —is extremely difficult, error -prone and time -consuming.  
 Systems based on automatically learning the rules can be made more accurate simply by 
supplying more input data.  However, systems based on hand -written rules can only be 
made more accurate by increasing the complexity of the rules, which is a much more 
difficult task. In particular, there is a limit to the complexity of systems based on hand -
crafted rules, beyond wh ich the systems become more and more unmanageable. 
However, creating more data to input to machine -learning systems simply requires a 
corresponding increase in the number of man -hours worked, generally without significant 
increases in the complexity of the  annotation process.  
Spell Checking  
Spell checking is one of the applications of natural language processing that impacts billions of 
users daily. A good introduction to spell checking can be found on Peter Norvig’s webpage. The 
article introduces a simple  21-line spell checker implementation in Python combining simple 
language and error models to predict the word a user intended to type.  The language model 
estimates how likely a given word `c` is in the language for which the spell checker is 
designed, thi s can be written as `P(C)`. The error model estimates the probability `P(w|c)` 
of typing the misspelled version `w` conditionally to the intention of typing the correctly 
spelled word `c`. The spell checker then returns word `c` corresponding to the highest  value of 
`P(w|c)P(c)` among all possible words in the language.  
 
 
 
 
 
 
 
 Module 3   
LEARNING  
Learning is the improvement of performance with experience over time.  
Learning element is the portion of a learning AI system that decides how to modify the 
perfor mance element and implements those modifications.  
We all learn new knowledge through different methods, depending on the type of material to be 
learned, the amount of relevant knowledge we already possess, and the environment in which the 
learning takes pl ace. There are five methods of learning . They are,  
1. Memorization (rote learning)  
2. Direct instruction (by being told)  
3. Analogy  
4. Induction  111 
 5. Deduction  
Learning by memorizations is the simplest from of le4arning. It requires the least amount of 
inference and is accomplished by simply copying the knowledge in the same form that it will be 
used directly into the knowledge base.  
Example: - Memorizing multiplication tables, formulate , etc.  
Direct instruction is a complex form of learning. This type of le arning requires more inference 
than role learning since the knowledge must be transformed into an operational form before 
learning when a teacher presents a number of facts directly to us in a well organized manner.  
Analogical learning is the process of le arning a new concept or solution through the use of 
similar known concepts or solutions. We use this type of learning when solving problems on an 
exam where previously learned examples serve as a guide or when make frequent use of 
analogical learning. This  form of learning requires still more inferring than either of the previous 
forms. Since difficult transformations must be made between the known and unknown situations.  
Learning by induction is also one that is used frequently by humans . it is a powerful  form of 
learning like analogical learning which also require s more inferring than the first two methods. 
This learning re  quires the use of inductive inference, a form of invalid but useful inference. We 
use inductive learning  ofinstances  of examples  of the concept.  For example  we learn the  
concepts of  color or sweet taste after experiencing the sensations associated with several 
examples of colored objects or sweet foods.  
Deductive learning is accomplished through a sequence of deductive inference steps using 
known facts. From the known facts, new facts or relationships are logically derived. Deductive 
learning usually requires more inference than the other methods.  
Review Questions: - 
1. what is perception ?  
2. How do we overcome the Perceptual Problems?  
3. Explain in detail the constraint satisfaction waltz algorithm?  
4. What is learning ?  
5. What is Learning element ?  
6. List and explain the methods of learning?  
Types of learning :- Classification or taxonomy of learning types serves as a guide in studyin g or 
comparing a differences among them. One can develop learning taxonomies based on the type of 
knowledge representation used (predicate calculus , rules, frames), the type of knowledge 
learned (concepts, game playing, problem solving), or by the area of  application(medical 
diagnosis , scheduling , prediction and so on).  
The classification is intuitively more appealing and is one which has become popular among 
machine learning researchers . it is independent of the knowledge domain and the representation 
scheme is used. It is based on the type of inference strategy employed or the methods used in the 
learning process. The five different learning methods under this taxonomy are:  
Memorization (rote learning)  
Direct instruction(by being told)  
Analogy  
Inductio n 
Deduction  
Learning by memorization is the simplest form of learning. It requires the least5 amount of 
inference and is accomplished by simply copying the knowledge in the same form that it will be 112 
 used directly into the knowledge base. We use this type o f learning when we memorize 
multiplication tables ,  
for example.  
A slightly more complex form of learning is by direct instruction. This type of learning requires 
more understanding and inference than role learning since the knowledge must be transformed 
into an operational form before being integrated into the knowledge base. We use this type of 
learning when a teacher presents a number of facts directly to us in  a well  organized manner.  
The third type listed, analogical learning, is the process of learnin g an ew concept or solution 
through the use of similar known concepts or solutions. We use this type of learning when 
solving problems on an examination where previously learned examples serve as a guide or 
when we learn to drive a truck using our knowledg e of car driving. We make frewuence use of 
analogical learning. This form of learning requires still more inferring than either of the previous 
forms, since difficult transformations must be made between the known and unknown situations. 
This is a kind of application of knowledge in a new situation.  
The fourth type of learning is also one that is used frequency by humans. It is a powerful form of 
learning which, like analogical learning, also requires more inferring than the first two methods. 
This form of learning requires the use of inductive inference, a form of invalid but useful 
inference. We use inductive learning when wed formulate a general concept after seeing a 
number of instance or examples of the concept. For example, we learn the concepts of col or 
sweet taste after experiencing the sensation associated with several examples of colored objects 
or sweet foods.  
The final type of acquisition is deductive learning. It is accomplished through a sequence of 
deductive inference steps using known facts. F rom the known facts, new facts or relationships 
are logically  derived. Deductive learning usually requires more inference than the other methods. 
The inference method used is, of course , a deductive type, which is a valid from of inference.  
In addition to  the above classification, we will sometimes refer to learning methods as wither 
methods or knowledge -rich methods. Weak methods are general purpose methods in which little 
or no initial knowledge is available. These methods are more mechanical than the cl assical AI 
knowledge – rich methods. They often rely on a form of heuristics search in the learning process.  
 
Rote Learning  
Rote learning is the basic learning activity. Rote learning  is a memorization  technique based 
on repetition . It is also called memorization because the  knowledge , without any modification is, 
simply copied into the knowledge base. As computed values are stored, this technique can save a 
significant amount of time.  
Rote learning technique can also be used in complex  learning sy stems  provided sophisticated 
techniques are employed to use the stored values faster and there is a generalization to keep the 
number of stored information down to a manageable level. Checkers -playing  program,  for ex   
The idea is that one will be able to q uickly  recall  the meaning of the material the more one 
repeats it. Some of the alternatives to rote learning include  meaningful learning , associative 
learning , and  active learning . ample, uses this technique to learn the board positions it evaluates 
in its  look-ahead search.  113 
 Learning By Taking Advice .  
 
This is a simple form of learning. Suppose a programmer writes a set of instructions to instruct 
the computer what to do, the programmer is a teacher and the computer is a student. Once 
learned (i.e. program med), the system will be in a position to do new things.  
 
The advice may come from many sources: human experts, internet to name a few. This type of 
learning requires more inference than rote learning. The knowledge must be transformed into an 
operational form before stored in the knowledge base. Moreover the reliability of the source of 
knowledge should be considered.  
The system should ensure that the new knowledge is conflicting with the existing knowledge. 
FOO (First Operational Operationaliser), for exa mple, is a learning system which is used to learn 
the game of Hearts. It converts the advice which is in the form of principles, problems, and 
methods into effective executable (LISP) procedures (or knowledge). Now this knowledge is 
ready to use.  
 
General Learning Model.  
General Learning Model: - AS noted earlier, learning can be accomplished using a number of 
different methods, such as by memorization facts, by being told, or by studying examples like 
problem solution. Learning requires that new knowledge structures be created from some form of 
input stimulus. This new knowledge must then be assimilated into a knowledge base and be 
tested in some way for its utility. Testing means that the knowledge should be used in 
performance of some task from which mean ingful feedback can be obtained, where the feedback 
provides some measure of the accuracy and usefulness of the newly acquired knowledge.  
General Learning Model  
 
general learning model is depicted in figure 4.1 where the environment has been incl uded as a 
part of the overall learner system. The environment may be regarded as either a form of nature 
which produces random stimuli or as a more organized training source such as a teacher which 
provides carefully selected training examples for the lear ner component. The actual form of 
environment used will depend on the particular learning paradigm. In any case, some 
representation language must be assumed for communication between the environment and the 
learner. The language may be the same representa tion scheme as that used in the knowledge base 
(such as a form of predicate calculus). When they are hosen to be the same, we say the single 
representation trick is being used. This usually results in a simpler implementation since it is not 
necessary to t ransform between two or more different representations.  114 
  
For some systems the environment may be a user working at a keyboard . Other systems will use 
program modules to simulate a particular environment. In even more realistic cases the system 
will have r eal physical sensors which interface with some world environment.  
 
Inputs to the learner component may be physical stimuli of some type or descriptive , symbolic 
training examples. The information conveyed to the learner component is used to create and 
modify knowledge structures in the knowledge base. This same knowledge is used by the 
performance component to carry out some tasks, such as solving a problem playing a game, or 
classifying instances of some concept.  
 
 given a task, the performance component produces a response describing its action in 
performing the task. The critic module then evaluates this response relative to an optimal 
response.  
 
Feedback , indicating whether or not the performance was acceptable , is then sent by the critic 
module to th e learner component for its subsequent use in modifying the structures in the 
knowledge base. If proper learning was accomplished, the system’s performance will have 
improved with the changes made to the knowledge base.  
 
The cycle described above may be re peated a number of times until the performance of the 
system has reached some acceptable level, until a known learning goal has been reached, or until 
changes ceases to occur in the knowledge base after some chosen number of training examples 
have been obs erved.  
 
There are several important factors which influence a system’s ability to learn in addition to the 
form of representation used. They include the types of training provided, the form and extent of 
any initial background knowledge , the type of feedb ack provided, and the learning algorithms 
used.   
 
The type of training used in a system can have a strong effect on performance, much the same as 
it does for humans. Training may consist of randomly selected instance or examples that have 
been carefully se lected and ordered for presentation. The instances may be positive examples of 
some concept or task a being learned, they may be negative, or they may be mixture of both 
positive and negative. The instances may be well focused using only relevant informati on, or 
they may contain a variety of facts and details including irrelevant data.  
 
There are Many forms of learning can be characterized as a search through a space of possible 
hypotheses or solutions. To make learning more efficient. It is necessary to co nstrain this search 
process or reduce the search space. One method of achieving this is through the use of 
background knowledge which can be used to constrain the search space or exercise control 
operations which limit the search process.  
 
Feedback is esse ntial to the learner component since otherwise it would never know if the 
knowledge structures in the knowledge base were improving or if they were adequate for the 
performance of the given tasks. The feedback may be a simple yes or no type of evaluation, or it 115 
 may contain more useful information describing why a particular action was good or bad. Also , 
the feedback may be completely reliable, providing an accurate assessment of the performance or 
it may contain noise, that is the feedback may actually be incorrect some of the time. Intuitively , 
the feedback must be accurate more than 50% of the time; otherwise the system carries useful 
information, the learner should also to build up a useful corpus of knowledge quickly. On the 
other hand, if the feedback  is noisy or unreliable, the learning process may be very slow and the 
resultant knowledge incorrect.  
 
Learning Neural Network  
Perceptron   
 The perceptron an invention of (1962) Rosenblatt was one of the earliest neural network 
models.  
 Also, It models a neu ron by taking a weighted sum of its inputs and sending the output 1 
if the sum is greater than some adjustable threshold value (otherwise it sends 0).  
 
Figure: A neuron & a Perceptron  
 
Figure: Perceptron with adjustable threshold  
 In case of zero with two inputs g(x) = w0 + w1x1 + w2x2 = 0  116 
  x2 =  -(w1/w2)x1   – (w0/w2)   → equation for a line  
 the location of the line is determined  by the weight w0 w1 and w2  
 if an input vector lies on one side of the line, the perceptron will output 1  
 if it lies on the other side, the perception will output 0  
 Moreover, Decision surface: a line that correctly separates the training instances 
correspo nds to a perfectly function perceptron.  
Perceptron Learning Algorithm  
Given: A classification problem with n input feature (x 1, x2, …., x n) and two output classes.  
Compute A set of weights (w 0, w1, w2,….,w n) that will cause a perceptron to fire whenever th e 
input falls into the first output class.  
1. Create a perceptron with n+ 1 input and n+ 1 weight, where the x 0 is always set to 1.  
2. Initialize the weights (w 0, w1,…., w n) to random real values.  
3. Iterate through the training set, collecting all examples  misclas sified  by the current set of 
weights.  
4. If all examples are classified correctly, output the weights and quit.  
5. Otherwise, compute the vector sum S of the misclassified input vectors where each vector 
has the form (x 0, x1, …, Xn). In creating the sum, add to S a vector x if x  is an input for 
which the perceptron incorrectly fails to fire, but – x  if x  is an input for which the 
perceptron incorrectly fires. Multiply sum by a scale factor η.  
6. Moreover, Modify the weights (w 0, w1, …, w n) by adding the elements of the vector S to 
them.  
7. Go to step 3.  
 The perceptron learning algorithm is a search algorithm. It begins with a random initial 
state and finds a solution state. The search space is simply all possible assignments of real 
values to the weights of the perce ption, and the search strategy is gradient descent.  
 The perceptron learning rule is guaranteed to converge to a solution in a finite number of 
steps, so long as a solution exists.  
 Moreover, This brings us to an important question. What problems can a perce ptron 
solve? Recall that a single -neuron perceptron is able to divide the input space into two 
regions.  
 Also, The perception can be used to classify input vectors that can be separated by a 
linear boundary. We call such vectors linearly separable.  
 Unfortun ately, many problems are not linearly separable. The classic example is the XOR 
gate. It was the inability of the basic perceptron to solve such simple problems that are 
not linearly separable or non -linear.  
Genetic Learning  
Supervised Learning   
Supervised  learning is the machine learning task of inferring a function from labeled training 
data.  
Moreover, The training data consist of a set of training examples.  
In supervised learning, each example a pair consisting of an input object (typically a vector) and  
the desired output value (also called the supervisory signal).  
Training set   
A training set a set of data used in various areas of information science to discover potentially 
predictive relationships.  117 
 Training sets used in artificial intelligence, machine  learning, genetic programming, intelligent 
systems, and statistics.  
In all these fields, a training set has much the same role and often used in conjunction with a test 
set. 
Testing set   
A test set  is a set of data used in various areas of information sci ence to assess the strength and 
utility of a predictive relationship.  
Moreover, Test sets are used in artificial intelligence, machine learning, genetic programming, 
and statistics. In all these fields, a test set has much the same role.  
Accuracy of classi fier: Supervised learning   
In the fields of science, engineering, industry, and statistics. The accuracy of a measurement 
system is the degree of closeness of measurements of a quantity to that quantity’s actual (true) 
value.  
Sensitivity analysis:  Supervis ed learning   
Similarly, Local Sensitivity as correlation coefficients and partial derivatives can only use, if the 
correlation between input and output is linear.  
Regression: Supervised learning   
In statistics,  regression analysis  is a statistical process for estimating the relationships among 
variables.  
Moreover, It includes many techniques for modeling and analyzing several variables. When the 
focus on the relationship between a dependent variable and one or more independent variables.  
More specifically, regression analysis helps one understand how the typical value of the 
dependent variable (or ‘criterion variable’) changes when any one of the independent variables 
varied. Moreover,   While the other independent variables held fixed.  
 
Expert systems : 
 
Expe rt system  = knowledge  + problem -solving methods . ... A  knowledge  base that captures 
the domain -specific  knowledge  and an inference engine that consists of algorithms for 
manipulating the  knowledge represented  in the  knowledge  base to solve a problem presen ted to 
the system.  
Expert systems (ES) are one of the prominent research domains of AI. It is introduced by the 
researchers at Stanford University, Computer Science Department.  
 
What are Expert Systems?  
 
The expert systems are the computer applications dev eloped to solve complex problems in a 
particular domain, at the level of extra -ordinary human intelligence and expertise.  
 
Characteristics of Expert Systems  
 High performance  
 Understandable  
 Reliable  
 Highly responsive  
 118 
 Capabilities of Expert Systems  
The expe rt systems are capable of −  
 Advising  
 Instructing and assisting human in decision making  
 Demonstrating  
 Deriving a solution  
 Diagnosing  
 Explaining  
 Interpreting input  
 Predicting results  
 Justifying the conclusion  
 Suggesting alternative options to a problem  
They  are incapable of −  
 Substituting human decision makers  
 Possessing human capabilities  
 Producing accurate output for inadequate knowledge base  
 Refining their own knowledge  
Components of Expert Systems  
The components of ES include −  
 Knowledge Base  
 Inference E ngine  
 User Interface  
Let us see them one by one briefly −  
 
Knowledge Base  
It contains domain -specific and high -quality knowledge. Knowl edge is required to exhibit 
intelligence. The success of any ES majorly depends upon the collection of highly accurate and 
precise knowledge.  119 
 What is Knowledge?  
The data is collection of facts. The information is organized as data and facts about the task 
domain.  Data, information,  and past experience  combined together are termed as knowledge.  
Components of Knowledge Base  
The knowledge base of an ES is a store of both, factual and heuristic knowledge.  
 Factual Knowledge  − It is the information widely accepted by the Knowledge Engineers 
and scholars in the task domain.  
 Heuristic Knowledge  − It is about practice, accurate judgement, one’s ability of 
evaluation, and guessing.  
Knowledge representation  
It is the method used to organize and formalize the knowledge in the knowledge base. It is in the 
form of IF -THEN -ELSE rules.  
Knowledge Acquisition  
The success of any expert system majorly depends on the quality, completeness, and accuracy of 
the information sto red in the knowledge base.  
The knowledge base is formed by readings from various experts, scholars, and the  Knowledge 
Engineers. The knowledge engineer is a person with the qualities of empathy, quick learning, 
and case analyzing skills.  
He acquires inform ation from subject expert by recording, interviewing, and observing him at 
work, etc. He then categorizes and organizes the information in a meaningful way, in the form of 
IF-THEN -ELSE rules, to be used by interference machine. The knowledge engineer also 
monitors the development of the ES.  
Inference Engine  
Use of efficient procedures and rules by the Inference Engine is essential in deducting a correct, 
flawless solution.  
In case of knowledge -based ES, the Inference Engine acquires and manipulates the know ledge 
from the knowledge base to arrive at a particular solution.  
In case of rule based ES, it −  
 Applies rules repeatedly to the facts, which are obtained from earlier rule application.  
 Adds new knowledge into the knowledge base if required.  
 Resolves rules  conflict when multiple rules are applicable to a particular case.  
To recommend a solution, the Inference Engine uses the following strategies −  
 Forward Chaining  
 Backward Chaining  
Forward Chaining  
It is a strategy of an expert system to answer the question , “What can happen next?”  
Here, the Inference Engine follows the chain of conditions and derivations and finally deduces 
the outcome. It considers all the facts and rules, and sorts them before concluding to a solution.  
This strategy is followed for workin g on conclusion, result, or effect. For example, prediction of 
share market status as an effect of changes in interest rates.  120 
 
 
Backw ard Chaining  
With this strategy, an expert system finds out the answer to the question,  “Why this happened?”  
On the basis of what has already happened, the Inference Engine tries to find out which 
conditions could have happened in the past for this result.  This strategy is followed for finding 
out cause or reason. For example, diagnosis of blood cancer in humans.  
 
User Interface  
User interface provides interaction between user of the ES and the ES itself. It is generally 
Natural Language Processing so as to be used by the user who is well -versed in the task domain. 
The user of the ES need not be necessarily an expert in Artificial Inte lligence.  
It explains how the ES has arrived at a particular recommendation.  The explanation may appear 
in the following forms −  
 Natural language displayed on screen.  
 Verbal narrations in natural language.  
 Listing of rule numbers displayed on the screen.  
The user interface makes it easy to trace the credibility of the deductions.  
Requirements of Efficient ES User Interface  
 It should help users to accomplish their goals in shortest possible way.  
 It should be designed to work for user’s existing or desired work practices.  
 Its technology should be adaptable to user’s requirements; not  the other way round.  
 It should make efficient use of user input.  
Expert Systems Limitations  
No technology can offer easy and complete solution. Large systems are costly, require 
significant development time, and computer resources. ESs have their limitati ons which include 
− 
 Limitations of the technology  121 
  Difficult knowledge acquisition  
 ES are difficult to maintain  
 High development costs  
Applications of Expert System  
The following table shows where ES can be applied.  
Application  Description  
Design Domain  Camera lens design, automobile design.  
Medical Domain  Diagnosis Systems to deduce cause of disease from observed 
data, conduction medical operations on humans.  
Monitoring Systems  Comparing data continuously with observed system or with 
prescribed behavior such as leakage monitoring in long 
petroleum pipeline.  
Process Control Systems  Controlling a physical process based on monitoring.  
Knowledge Domain  Finding out faults in vehicles, computers.  
Finance/Commerce  Detection of possible fraud, suspicious trans actions, stock 
market trading, Airline scheduling, cargo scheduling.  
Expert System Technology  
There are several levels of ES technologies available. Expert systems technologies include −  
 Expert System Development Environment  − The ES development environme nt includes 
hardware and tools. They are −  
o Workstations, minicomputers, mainframes.  
o High level Symbolic Programming Languages such as  LISt Programming (LISP) 
and PROgrammation en  LOGique (PROLOG).  
o Large databases.  
 Tools  − They reduce the effort and cost in volved in developing an expert system to large 
extent.  
o Powerful editors and debugging tools with multi -windows.  
o They provide rapid prototyping  
o Have Inbuilt definitions of model, knowledge representation, and inference 
design.  
 Shells  − A shell is nothing bu t an expert system without knowledge base. A shell 
provides the developers with knowledge acquisition, inference engine, user interface, and 
explanation facility. For example, few shells are given below −  
o Java Expert System Shell (JESS) that provides fully  developed Java API for 
creating an expert system.  
o Vidwan , a shell developed at the National Centre for Software Technology, 
Mumbai in 1993. It enables knowledge encoding in the form of IF -THEN rules.  
Development of Expert Systems: General Steps  
The proces s of ES development is iterative. Steps in developing the ES include −  
Identify Problem Domain  122 
  The problem must be suitable for an expert system to solve it.  
 Find the experts in task domain for the ES project.  
 Establish cost -effectiveness of the system.  
Design the System  
 Identify the ES Technology  
 Know and establish the degree of integration with the other systems and databases.  
 Realize how the concepts can represent the domain knowledge best.  
Develop the Prototype  
From Knowledge Base: The knowledge enginee r works to −  
 Acquire domain knowledge from the expert.  
 Represent it in the form of If -THEN -ELSE rules.  
Test and Refine the Prototype  
 The knowledge engineer uses sample cases to test the prototype for any deficiencies in 
performance.  
 End users test the prot otypes of the ES.  
Develop and Complete the ES  
 Test and ensure the interaction of the ES with all elements of its environment, including 
end users, databases, and other information systems.  
 Document the ES project well.  
 Train the user to use ES.  
Maintain th e ES 
 Keep the knowledge base up -to-date by regular review and update.  
 Cater for new interfaces with other information systems, as those systems evolve.  
Benefits of Expert Systems  
 Availability  − They are easily available due to mass production of software.  
 Less Production Cost  − Production cost is reasonable. This makes them affordable.  
 Speed  − They offer great speed. They reduce the amount of work an individual puts in.  
 Less Error Rate  − Error rate is low as compared to human errors.  
 Reducing Risk  − They ca n work in the environment dangerous to humans.  
 Steady response  − They work steadily without getting motional, tensed or fatigued.  
 
Expert System.  
 
DEFINITION - An ex pert system is a computer program that simulates the judgement and 
behavior of a human or an organization that has expert knowledge and experience in a particular 
field. Typically, such a system contains a knowledge base containing accumulated experience 
and a set of rules for applying the knowledge base to each particular situation that is described to 
the program. Sophisticated expert systems can be enhanced with additions to the knowledge base 
or to the set of rules.  
 
Among the best -known expert systems have been those that play chess and that assist in medical 
diagnosis.  
 
An expert system  is software  that attempts to provide an answer to a problem, or clarify 
uncertainties where normally  one or more human  experts  would need to be consulted. Expert 
systems are most common in a specific  problem domai n, and is a traditional application and/or 123 
 subfield of  artificial intelligence  (AI). A wide variety of methods can be used to simulate the 
performance of the expert; however, common to most or all are: 1) the creation of a  knowledge 
base which uses some  knowledge representation  structure to capture the knowledge of 
the Subject Matter Expert  (SME); 2) a process of gathering that knowledge from the SME and 
codifying it according to the structure, which is called  knowledge engineering ; and 3) once the 
system is developed, it is placed in the same real world  problem solving  situation as the human 
SME, typically as an aid to human workers or as a supplement to some information system. 
Expert systems may or may not have learning components.  
  
factors  
 
 
The MYCIN rule-based expert system introduced a quasi -probabilistic approach called certainty 
factors, whose rationale is explained below.  
 
A human, when reasoning, does not always make statements with 100% confidence: he might 
venture, "If Fritz is green, then he i s probably a frog" (after all, he might be a chameleon). This 
type of reasoning can be imitated using numeric values called confidences. For example, if it is 
known that Fritz is green, it might be concluded with 0.85 confidence that he is a frog; or, if i t is 
known that he is a frog, it might be concluded with 0.95 confidence that he hops. These certainty 
factor (CF) numbers quantify uncertainty in the degree to which the available evidence supports 
a hypothesis. They represent a degree of confirmation, an d are not probabilities in a Bayesian 
sense. The CF calculus, developed by Shortliffe & Buchanan, increases or decreases the CF 
associated with a hypothesis as each new piece of evidence becomes available. It can be mapped 
to a probability update, although  degrees of confirmation are not expected to obey the laws of 
probability. It is important to note, for example, that evidence for hypothesis H may have nothing 
to contribute to the degree to which Not_h is confirmed or disconfirmed (e.g., although a fever  
lends some support to a diagnosis of infection, fever does not disconfirm alternative hypotheses) 
and that the sum of CFs of many competing hypotheses may be greater than one (i.e., many 
hypotheses may be well confirmed based on available evidence).  
 
The CF approach to a rule -based expert system design does not have a widespread following, in 
part because of the difficulty of meaningfully assigning CFs a priori. (The above example of 
green creatures being likely to be frogs is excessively naive.) Alternati ve approaches to quasi -
probabilistic reasoning in expert systems involve fuzzy logic, which has a firmer mathematical 
foundation. Also, rule -engine shells such as Drools and Jess do not support probability 
manipulation: they use an alternative mechanism ca lled salience, which is used to prioritize the 
order of evaluation of activated rules.  
 
In certain areas, as in the tax -advice scenarios discussed below, probabilistic approaches are not 
acceptable. For instance, a 95% probability of being correct means a 5% probability of being 
wrong. The rules that are defined in such systems have no exceptions: they are only a means of 
achieving software flexibility when external circumstances change frequently. Because rules are 
stored as data, the core software does no t need to be rebuilt each time changes to federal and 
state tax codes are announced.  
 124 
  
Chaining  
 
 
Two methods of reasoning when using inference rules are forward chaining and backward 
chaining.  
 
Forward chaining starts with the data available and uses the inference rules to extract more data 
until a desired goal is reached. An inference engine using forward chaining searches the 
inference rules until it finds one in which the if clause is known to be true. It then concludes the 
then clause and adds this inf ormation to its data. It continues to do this until a goal is reached. 
Because the data available determines which inference rules are used, this method is also 
classified as data driven.  
 
Backward chaining starts with a list of goals and works backwards t o see if there is data which 
will allow it to conclude any of these goals. An inference engine using backward chaining would 
search the inference rules until it finds one which has a then clause that matches a desired goal. If 
the if clause of that inferen ce rule is not known to be true, then it is added to the list of goals.  
 
SW Architecture.  
 
The following general points about expert systems and their architecture have been outlined:  
 
1. The sequence of steps taken to reach a conclusion is dynamically syn thesized with each new 
case. The sequence is not explicitly programmed at the time that the system is built.  
 
2. Expert systems can process multiple values for any problem parameter. This permits more 
than one line of reasoning to be pursued and the result s of incomplete (not fully determined) 
reasoning to be presented.  
 
3. Problem solving is accomplished by applying specific knowledge rather than specific 
technique. This is a key idea in expert systems technology. It reflects the belief that human 
experts do not process their knowledge differently from others, but they do possess different 
knowledge. With this philosophy, when one finds that their expert system does not produce the 
desired results, work begins to expand the knowledge base, not to re -program  the procedures.  
  
End user  
 
There are two styles of user -interface design followed by expert systems. In the original style of 
user interaction, the software takes the end -user through an interactive dialog. In the following 
example, a backward -chaining s ystem seeks to determine a set of restaurants to recommend:  
 
Q. Do you know which restaurant you want to go to?  
 
A. No  
 125 
 Q. Is there any kind of food you would particularly like?  
 
A. No  
 
Q. Do you like spicy food?  
 
A. No  
 
Q. Do you usually drink wine with m eals?  
 
A. Yes  
 
Q. When you drink wine, is it French wine?  
 
A. Yes  
  
  
Participants  
  
There are generally three individuals having an interaction in an expert system. Primary among 
these is the end -user, the individual who uses the system for its problem so lving assistance. In 
the construction and maintenance of the system there are two other roles: the problem domain 
expert who builds the system and supplies the knowledge base, and a knowledge engineer who 
assists the experts in determining the representati on of their knowledge, enters this knowledge 
into an explanation module and who defines the inference technique required to solve the 
problem. Usually the knowledge engineer will represent the problem solving activity in the form 
of rules. When these rules  are created from domain expertise, the knowledge base stores the rules 
of the expert system.  
 
 Inference rule  
 
An understanding of the "inference rule" concept is important to understand expert systems. An 
inference rule is a conditional statement with tw o parts: an if clause and a then clause. This rule 
is what gives expert systems the ability to find solutions to diagnostic and prescriptive problems. 
An example of an inference rule is:  
 
If the restaurant choice includes French and the occasion is romanti c, 
 
Then the restaurant choice is definitely Paul Bocuse.  
  
Procedure node interface  
 
The function of the procedure node interface is to receive information from the procedures 
coordinator and create the appropriate procedure call. The ability to call a pr ocedure and receive 
information from that procedure can be viewed as simply a generalization of input from the 
external world. In some earlier expert systems external information could only be obtained in a 126 
 predetermined manner, which only allowed certain information to be acquired. Through the 
knowledge base, this expert system disclosed in the cross -referenced application can invoke any 
procedure allowed on its host system. This makes the expert system useful in a much wider class 
of knowledge domains tha n if it had no external access or only limited external access.  
 
In the area of machine diagnostics using expert systems, particularly self -diagnostic applications, 
it is not possible to conclude the current state of "health" of a machine without some info rmation. 
The best source of information is the machine itself, for it contains much detailed information 
that could not reasonably be provided by the operator.  
 
The knowledge that is represented in the system appears in the rulebase. In the rulebase 
descri bed in the cross -referenced applications, there are basically four different types of objects, 
with the associated information:  
 
1. Classes: Questions asked to the user.  
 
2. Parameters: Place holders for character strings which may be variables that can be  inserted 
into a class question at the point in the question where the parameter is positioned.  
 
3. Procedures: Definitions of calls to external procedures.  
 
3. Rule nodes: Inferences in the system are made by a tree structure which indicates the 
rules or log ic mimicking human reasoning. The nodes of these trees are called rule nodes. 
There are several different types of rule nodes.  
 
 
Expert Systems /Shells . The E.S  shell  simplifies the process of creating a knowledge base. It is 
the shell  that actually process es the information entered by a user relates it to the concepts 
contained in the knowledge base and provides an assessment or solution for a particular problem.  
 
 
Knowledge Acquisition  
Knowledge acquisition  is the process used to define the rules and ontologies req uired for 
a knowledge -based system . The phrase was first used in conjunction with  expert systems  to 
describe the initial tasks associated with developing an expert system, namely finding and 
interviewing  domain  experts and capturing their knowledge via  rules, objects , and  frame -
based  ontologies . 127 
 Expert systems were one of the first successful applications of  artificial intelligence  technology 
to real world business problems . Researchers at  Stanford  and other AI laboratories worked with 
doctors and other hig hly skilled experts to develop systems that could automate complex tasks 
such as  medical diagnosis . Until this point computers had mostly been used to automate highly 
data intensive tasks but not for complex reasoning. Technologies such as  inference 
engine s allowed developers for the first time to tackle more complex problems.  
As expert systems scaled up from demonstration prototypes to industrial strength applications it 
was soon realized that the acquisition of domain expert knowledge was one of if not th e most 
critical task in the  knowledge engineering  process. This knowledge acquisition process became 
an intense area of research on its own. One of the earlier works  on the topic used Batesonian 
theories of learning to guide the process.  
One approach to kn owledge acquisition investigated was to use  natural language parsing  and 
generation to facilitate knowledge acquisition. Natural language parsing could be performed on 
manuals and other expert documents and an initial first pass at the rules and objects co uld be 
developed automatically. Text generation was also extremely useful in generating explanations 
for system behavior. This greatly facilitated the development and maintenance of expert systems.   
A more recent approach to knowledge acquisition is a re -use based approach. Knowledge can be 
developed in  ontologies  that conform to standards such as the  Web Ontology Language 
(OWL) . In this way knowledge can be standardized and shared across a broad community of 
knowledge workers. One example domain where this  approach has been successful 
is bioinformatics . 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 128 
 Refferences  
 
1. Elaine Rich, Kevin Knight, & Shivashankar B Nair, Artificial Intelligence,  McGraw Hill, 3rd ed.,2009  
References:  
1) Introduction to Artificial Intelligence & Expert S ystems, Dan W Patterson,  PHI.,2010  
2) S Kaushik, Artificial Intelligence, Cengage Learning, 1st ed.2011  
 
 

2 
  
 
 
ARTIFICIAL 
INTELLIGENCE  
 
Curated  with  support  from  Intel®  
3 
 Acknowledgements  
Patrons:  
• Mr. Rahul  Singh , IAS, Chairperson,  Central  Board  of Secondary  Education  
 
Guidance  and Support:  
• Dr. Biswajit  Saha , Director  (Skill  Education  & Training),  Central  Board  of Secondary  Education  
• Ms. Shweta  Khurana , Senior  Director  APJ, Government  Partnerships  and Initiatives, 
International Government Affairs Group, Intel  
 
Education  Value  adder,  Curator  and Coordinator:  
• Sh. Ravinder  Pal Singh , Joint  Secretary,  Department  of Skill Education,  Central  Board  of 
Secondary Education  
• Ms Saloni  Singhal , Program  Manager  APJ, Intel  Digital  Readiness  Programs  
• Ms. Sarita  Manuja , Educational  Consultant  & Program  Director,  NHES  
• Ms. Shatarupa  Dasgupta , National  Program  Manager,  Intel  Digital  Readiness  Program  
 
Content  Curation  Team:  
• Ms. Ambika  Saxena , Intel  AI for Youth  Coach  
• Ms. Prachi  Chandra , Intel  AI for Youth  Coach  
• Ms. Shilpa  Sethi , DAV  Public  School,  Sector -14, Gurugram  
• Ms. Shipra  Panigrahi , Indirapuram  Public  School,  Ghaziabad  
• Ms. Sonu  Lohchab , D.A.V.  Public  School,  Sector -49, Gurugram  
• Ms. Ritu Debnath , Gurukul  Global  School  Sec-13, Chandigarh  
• Ms. Anshu  Banerjee , Uttam  School  for Girls,  Ghaziabad  
• Ms. Yukti , Army  Public  School,  Meerut  
• Ms. A. Sayeesubbulakshmi , Delhi  Public  School,  Bangalore  (South),  Bengaluru  4 
 About the Book  
In the rapidly evolving landscape of the global digital economy, Artificial Intelligence (AI) 
stands as the cornerstone of future innovation and growth. Recognizing this, nations 
worldwide  are strategically  positioning  themselves  to harness  the transformative  potential  of 
AI. India, in particular, views AI not just as a technological advancement but as an 
opportunity to foster inclusive economic growth and social development.  
At the forefront  of this vision  is the Central  Board  of Secondary  Education  (CBSE),  which  is on 
a mission to equip the next generation with the skills and mindset necessary to thrive  in an 
AI-driven  world.  As part of this initiative, CBSE has collaborated with Intel India since 2019, 
to curate a comprehensive Facilitator Handbook and accompanying AI training resources. 
The resources aim to empower educators and students alike, fostering a deeper 
understanding of AI concepts and their practical applications.  
This edition of the  ‘AI Facilitator  Handbook’  is more  than  just a curriculum;  it's a roadmap  for 
students to navigate the complexities of AI with confidence and creativity. Enriched with 
updated AI tech and social concepts, real -life examples, and AI project development guides 
using no -code tools, this book is designed to inspire students to not only understand AI  but 
also to leverage it to drive positive social change.  
 
 
Key features  include:  
• Enhanced Content: Concepts are presented with further elaboration and fresh 
examples to facilitate deeper engagement and comprehension.  
• Real -Life Examples: Additional real -world scenarios are integrated to offer clearer 
explanations, making complex AI concepts accessible to students.  
• AI enabled social impact solutions: Students are encouraged to develop AI solutions  
for social impact in a straightforward manner, fostering understanding and 
empowerment.  
• Use Case Walkthroughs: Practical implementation of AI concepts is demonstrated 
across various domains, enabling students to grasp their real -world applications.  5 
 CBSE Grade  IX AI Curriculum  2024 -25 
Units/ 
Subunits   Sessions  Topics  Hours  
1.1  
 
 
 
AI 
Reflection, 
Project 
Cycle and 
Ethics  Understanding 
AI: Domains  and 
Applications  • Define  Artificial  Intelligence  (AI) 
• The applications of  AI in everyday life 
• The three  domains  of AI and their 
applications  10 
1.2 The AI Project 
Cycle - II • The importance  of the AI project  cycle.  
• To structure  the AI problem  statement 
with the AI project cycle  30 
1.3 AI Ethics - II • The difference  between  ethics  and 
morality.  
• The ethical  scenarios  faced  while 
building AI solutions  
• AI bias and to identify bias in AI 15 
2.1  
 
 
 
 
 
Data 
Literacy  Basics  of Data 
Literacy  • Data  Literacy  and its impact  
• How to become  Data  literate?  
• Data  security  and privacy  
• Best practices for Cyber Security  10 
2.2 Acquiring Data, 
Processing,  and 
Interpreting  
Data • Types  of data 
• Sources  of data 
• Best Practices for acquiring data 
• Features  of data and Data  Preprocessing  
• Importance  of Data  Interpretation  
• Tools  used  for Data  Interpretation  20 
2.3 Project  
Interactive  Data 
Dashboard & 
Presentation  • Data  visualization  & its importance  
• Visualization  of data with a No-Code  tool 
• Create  a simple  and interactive  chart 
with a No -Code tool  20 
3.1  
 
 
Math  for AI 
(Statistics 
& 
Probability)  Importance  of 
Math in AI  • The applications  of Mathematics  in AI 
• The different  mathematical  concepts 
important for understanding AI  5 
3.2 Statistics  • Use of statistics  in different  AI 
applications  10 
3.3 Probability  • Use of probability  in different  AI 
applications  10 
4  Introduction  to 
Generative AI  • Definition and Overview  
• Applications  and Use cases  20 
 
Total  150 hours  6 
 
Unit 1 AI Reflection  
Unit  1.1 – Understanding  AI 
 
 
 
Welcome  to an introduction  to Artificial  Intelligence!  What  do you think  Artificial  Intelligence  is? 
 
 
 
What  do you want  to learn  about  AI? 
 
 
 
How  do you think  we should  go about  it? 
 
 
 
 
What  will you learn?  
 
 
 
 
 
● When a machine possesses the ability to mimic human traits, i.e., make decisions, predict the future, learn and improve 
on its own,  it is said to have  artificial  intelligence.  In other  words,  you can say that a machine  is artificially  intelligent  when 
it can accomplish tasks by itself - collect data, understand it, analyse it, learn from it, and improve it.  
● AI is a form  of intelligence;  a type  of technology  and a field  of study.  
● AI theory  and development  of computer  systems  (both  machines  and software)  are able  to perform  tasks  that normally 
require human intelligence.  
● Artificial  Intelligence  covers  a broad  range  of domains  and applications  and is expected  to impact  every  field  in 
the future.  
 
Overall,  its core  idea  is to build  machines  and algorithms  which  are capable  of performing  computational  tasks  that 
would otherwise require human -like brain functions.  7 
 
 
 
How  to make  machine  intelligent?  
 
How  do you think  Artificial  Intelligence  can help  you as you go about  your  daily  life?  Fill in your  ideas 
below.  
 
 
 
8 
 Activity: Game Time  
In this activity,  you will visit a few online  resources  to play games  and experience  the power  of AI. 
Resources:  
Game  1 (Rock,  Paper  and Scissors):  
Rules  for playing  Game  1: 
✔ Type  the link below  to launch  the tool 
✔ Scroll  down  and check  the box “I Agree”.  Click  on Let’ Go 
✔ You may  turn  off the camera  to select  the moves  directly  from 
the screen  
✔ Start  the game  by selecting  "rock",  "scissors"  or "paper"  
✔ Choose  continuously  until  you create  a pattern  and check  how 
AI tries to win.  
Visit  https://next.rockpaperscissors.ai/  to play the game  online.  
 
Game 2 (Semantris):  
Rules  for playing  Game  2: 
✔ Type the link given and click on launch experiment option 
to start the game.  
✔ Click  on Play Arcade  option  to start  playing  the game.  
✔ Each time AI gives you the highlighted  clue, you are 
supposed to enter the most closely associated word to  get 
more scores.  
✔ Check  how  machine  understands  your  words  
Visit  https://research.google.com/semantris/  to experience  the 
magic online.  
Game 3 (Quick, Draw):  
Rules  for playing  Game  3: 
✔ Type  the link and click  on Let’s  Draw  option  to start  playing 
the game.  
✔ An item  will be named  on the screen  for you to draw  in 20 
seconds after you click on Got it!  
✔ AI will guess  whatever  you draw  on the white  screen.  
✔ Try drawing  6 objects  correctly  in a row to win the game! 
Launch the game at https://quickdraw.withgoogle.com/  
9 
 
It’s time  for you to try them  out! 
 
Games are an integral part of our culture. People across the world 
participate in different  kinds  of games  as a form  of social  interaction, 
competition, and enjoyment.  
The basic  principle  of every  game  is rule-setting  and 
following the rules.  
 
 
Write  down  three  rules  in the given  spaces  you would  set before  playing  any game.  
 
 
 
Purpose:  Expose  you to the 3 domains  of AI (Natural  Language  Processing,  Computer  Vision,  and 
Data for AI).  
Brief:  You will go through  three  AI games  in the form  of a challenge.  Game  Descriptions:  
Rock, Paper & Scissors: A game based  on Data  for AI where  the machine  tries  to predict  the next 
move of the participant. It is a replica of a basic rock, paper and scissors game where the  
machine tries to win ahead by learning from the participant’s previous moves.  
 
Semantris: A game based on Natural Language Processing is a set of word association games 
powered by machine -learned, natural language  understanding  technology.  Each  time  you enter  a 
clue, the AI looks at all the words in play and chooses the ones it thinks are most related.  
 
Quick, Draw: A game based on Computer Vision developed by Google  that challenges  players  to 
draw a picture of an object or idea and  then  uses  a neural  network  artificial  intelligence  to guess 
what the drawings represent.  
 
We are going to get serious now! You are challenged by an eccentric data scientist, to solve 3 
challenges he designed. You have  60 mins  before  he inserts  a virus  into every  electronic  device  in 
the world! We will work in groups of 4 -5 students now. Whether you are ready or not, the 
countdown is going to start now! Grab a seat in front of the computer and start your challenge.  
10 
 
Game 1: The AI Game Challenge  
 
Guess what…… ? 
❖ Here  are some  visuals  that will help  you guess  the games  you are going  to play.  You have  10 
seconds to guess and write the name of the games below:  
 
 
 
 
 11 
 Pair Activity:  
Team  up with  a partner  and let the challenge  begin!  
 
Game  1: Rock,  Paper  and Scissors  
 
(based  on Data  for AI ) 
 
Write  three  things  you learnt  from  the game.  
 
 
 
 
 
 
 
Game  2: Semantris  
(based on Natural Language Processing - NLP) 
Mention  three  things  you understood  about  the game.  
 
 
 
 
 
 
 
 
 
 
 
Game  3: Quick  Draw  
(based  on Computer Vision  – CV) 
Did you face  any difficulty  while  playing  this game?  How 
did you overcome this?  
 
 
 
 
12 
 Depending  on the type  of data,  we can divide  AI into different  domains:  
 
 
Some  AI Applications  
 
Face  Lock  in Smartphones  
Smartphones nowadays come with the feature of face  locks  in 
which the smartphone’s owner can set up his/her face as an 
unlocking mechanism for it. The front camera detects and 
captures the face and saves its features during initiation. Next 
time onwards, whenever the features match, the phone is 
unlocked.  
 
 
 
 
 
Smart assistants  
Smart  assistants  like Apple’s  Siri and Amazon’s  Alexa  recognize  patterns 
in speech, then infer meaning and provide a useful response.  
 
 
Computer Vision, is an AI domain works with videos and images enabling 
machines to interpret and understand visual information.  
CV 
Natural Language Processing (NLP) is an AI domain focused on textual data 
enabling machines to comprehend, generate, and manipulate human language.  
NLP  
Statistical  
Data  
Statistical Data refers to statistical techniques to analyse, interpret and draw insights 
from numerical/tabular data.  
acquired data.  13 
 
Fraud  and Risk Detection  
Finance companies were fed with bad 
debts  and losses  every  year.  However,  they 
had a lot of data which used to get 
collected  during  the initial  paperwork  while 
sanctioning loans. They decided to bring in 
data scientists to rescue them from losses. 
Over the years,  banking  companies  learned 
to divide and conquer data via customer 
profiling, past expenditures, and other 
essential variables to analyse the 
probabilities of risk and default. Moreover, 
it also helped them to push their banking 
products based on customer’s purchasing 
power.  
 
 
Medical Imaging : For the last decades, computer supported medical imaging 
application  that has been  a trustworthy  help  for physicians.  It doesn’t  only  create 
and analyse images, but also becomes an assistant and helps doctors with their 
interpretation. The application is used to read and convert 2D scan images into 
interactive 3D models that enable medical professionals to gain a detailed 
understanding of a patient’s health condition.  
 
Let’s  Discuss  
Why  should  these  three  games  be relevant  for AI awareness?  
Group  Activity:  Reflect  and Analyse  
Take three different colour strands and work them into a braid. See how long your braid can become 
within 30 seconds!! Ready? Go!!!  
Let’s understand : To understand AI, we draw an analogy from the three strands in a braid. One is the 
Statistical Data strand,  the second  is the Natural  Language  Processing  strand  and the third  strand  is the 
Computer Vision. They all together constitute the concept called Artificial Intelligence.  
 
14 
 Revision  Time  
Part  A 
Quiz  Time:  AI Quiz  
1. Which  one of the following  is an application  of AI? 
a. Remote  controlled  Drone  
b. Self-Driving  Car 
c. Self-Service  Kiosk  
d. Self-Watering  Plant  System  
2. This language  is easy  to learn  and is one of the most  popular  languages  for AI today:  
a. C++ 
b. Python  
c. Ruby  
d. Java 
3. This field  is enabling  computers  to identify  and process  images  as humans  do: 
a. Face  Recognition  
b. Model -view -controller  
c. Computer  Vision  
d. Eye-in-Hand  System  
4. What  does  NLP stand  for in AI? 
a. Neutral  Learning  Projection  
b. Neuro -Linguistic  Programming  
c. Natural  Language  Processing  
d. Neural  Logic  Presentation  
5. Which  of the following is  not a domain  of artificial intelligence?  
a. Data  Management  System  
b. Computer  Vision  
c. Natural  Language  Processing  
d. Data  Science  
6. How  excited  are you about  this AI curriculum?  
a. Very  Excited!  
b. A bit excited  
c. Same as always  
d. Not excited  at all 
 
Part  B 
1. How  can AI be used  as a tool to transform  the world  into a better  place?  
2. Can you list down  a few applications  in your  smartphone  that widely  make  use of 
computer vision?  
3. Draw  out the difference  between  the three  domains  of AI with  respect  to the types  of 
data they use.  
4. Identify  the features  and the domain  of AI used  in them:  15 
 
(a) (b) 
 
(c) 
 
 
 
 
 
5. Separate  the following  areas  based  on the kinds  of domains  widely  used  in them:  
a. Crop  productivity  
b. Traffic  regulation  
c. Maps  and navigation  
d. Text  editors  and autocorrect  
e. Identifying  and predicting  disease  
6. After the pandemic, it’s been essential for everyone to wear a mask. However, you see many 
people not wearing masks when in public places. Which domain of AI can be used to build a 
system to detect people not wearing masks?  
7. Search for an online game that recognizes the image drawn by you. Write down the 
observations including the AI domain used by it.  
 
Teamwork:  
Pair yourself up with your classmates to come up with the dialogues.  One out of the two will act like a 
chatbot answering stress -related queries during exams and the other can ask the questions. For 
example, you can ask ways to remain optimistic during exams and your friend acting as the chatbot 
may respond with answers like meditating, strolling through a park, etc.  
16 
 
1.2 AI Project  Cycle  
 
Lesson  Title: AI Project Cycle Approach:  Interactive  Session  
Summary:  Students  will learn  about the  AI Project  Cycle and  get familiar  with it. 
Learning  Objectives: Students will know how they can get started on an AI project.  
Learning  Outcomes: Describe the stages  in the AI project cycle.  
Pre-requisites: Basic computer  literacy  
Key-concepts:  AI project cycle 
 
Let us think!  
● Problem  Scoping  means  
 
 
● Data  Acquisition  means  
 
 
● Data  Exploration  means  
 
 
● Modelling means  
 
 
● Evaluation  means  
 
 17 
 
Ask students  about  possible  solutions  to this problem  before  moving  ahead. 
Invite them to think of non -AI solutions as well.  ● Deployment  means  
 
 
 
Let us understand!  
Let us go through  the AI project  cycle  with  the help  of an example.  
 
Problem:  Pest  infestation  damages  crops  
The cotton industry in India consists of 6 million local farmers. Cotton crops frequently get infected with 
the Pink  Bollworm.  It is difficult  to see these  insects  with  the naked  eye.  Small  farmers  find it very  difficult 
to get rid of these insects. They do not have advanced tools and techniques to protect their plants from 
Pink Bollworm.  
 
Can we solve  this problem  with  AI? How?  
Watch  the video  at this link - https://w ww.youtube.com/watch?v=LP_A4jydmz4  
 
Now that you are aware of AI concepts, plan to use them in accomplishing your task. 
Start  with  listing  down  all the factors  which  you need  to consider  to save  the cotton  crop. 
This system aims to:  
 
 
 18 
 
 
Now, as you interact with the  farmers,  you get to know  different  types  of worms  affecting 
the cotton crop. You will collect the following data  
● Images  of the pest 
● Farmer  names  
● Village  names  
● Farm  size 
● Pesticide  usage  
 
After acquiring the required data, you realise that it is not uniform. Some  images  are small  in size while 
others are big. Some  images  and other  data  are missing  while  you have  multiple  copies  of others.  So, we 
clean the data, try to make it uniform and fill in the missing data to make it more understandable.  
By exploring  the data,  researchers  can identify  patterns  and trends  related  to Pink  Bollworm  infestations, 
pesticide usage, crop yields, and other relevant factors.  
 
After exploring the data, now you know that you need to develop an AI -enabled app using which the 
farmers will click the pictures of the collected pests using the phone camera. The AI app then decides 
whether the image is  valid.  Based  on the number  of pests  recognized  by the system  and rules  laid out by 
entomologists, recommendations are displayed  
 
 
Your pest management system is now complete! You test it by first emptying the trap of pests onto a 
blank sheet of paper and opening the app, then clicking pictures of pests. You notice that the results 
were 70% correct. After evaluating this model, you  work  on other  shortlisted  AI algorithms  and work  on 
them.  
You test the algorithms  to 
 
 
19 
 
You can add ‘Small  farms  that used  the app saw jumps  in profit  margins  of up to 26.5  percent.  
A drop -in pesticide  costs  of up to 38 percent  was also observed’.   
After  proper  testing,  you deploy  your  pest  management  app by getting  it installed  on 
farmer’s mobile phones.  
 
 
Let us look  at the main  features  of CottonAce  app- 
CottonAce app  
▪ CottonAce  is a mobile  application  that can help 
farmers protect their crops from pests.  
▪ CottonAce  uses  AI to warn  the farmers  about  a 
possible pest infestation.  
▪ It aids farmers  in – 
▪ Determining  the correct  amount  of pesticides  
▪ Knowing  the right  time  to spray  pesticides  
▪ Seeking  professional  help  as needed.  
 
How  does  it work?  
▪ A farmer  sets up a trap to capture  pests.  
▪ Take  a picture  of the captured  pests.  
▪ Upload  the picture  on the app.  
▪ The app detects  the insect,  level  of infestation,  and 
the required measures to cure it.  
 
 
 
 
 
 
20 
 
Conclusion:  
“Greater  efficiency  implies  that the solution  can be developed  faster  and in a more  convenient  way.  Due 
to modularity,  the complex  problem  of cotton  diseases  and the process  of making  a solution  for it can be 
broken down into simpler steps”.  What  is AI project  cycle  mapping?  
Mapping  the individual  steps  in an AI project  to the steps  in the AI project  cycle.  
Let us map  the steps  of Pest  Management  project  to the steps  in the AI project  cycle.  
 
 
 
Why  do we need  an AI Project  Cycle?  
 
 
 
 
 
 
 21 
 AI Project  Cycle  – Defined!  
What  you did just now was an example of AI Project Cycle. Starting with Problem Scoping, you set the 
goal for your AI project by stating the problem which you wish to solve with it.  
 
1.2.1 Problem  Scoping  
 
Let us start  with  the first step  of AI Project  cycle  – Problem  Scoping.  
Let us Recap  
What  according  you does  Problem  Scoping  mean?  Write  in your  words  below:  
 
 
 
 
 
 
 
 
 
It is a fact that we are surrounded by problems. They could be small or big, sometimes ignored or 
sometimes  even  critical.  Many  times,  we become  so used  to a problem  that it becomes  a part of our life. 
Identifying such a problem and having a vision to solve it, is what Problem Scoping is about.  
 
Title:  Problem  Scoping  Approach:  Instructor -led Interactive  Session  + 
Activity  
Summary:  Students  will be introduced  to the 4Ws  problem  Canvas  and Problem  Statement 
template.  They  will be able  to set goal  for their  AI projects  to solve  problems  around  them.  
Learning Objectives:  
● Students  will know  how  they  can get started  on an AI project.  
● To problem  scope  with  the help  of template/worksheet.  
Learning Outcomes:  
● Apply  the problem  scoping  framework.  
● Frame  a Goal  for the project.  
Pre-requisites:  Basic  computer  literacy  
Key-concepts:  Problem  scoping  
AI project  cycle  is the cyclical  process  followed  to complete  an AI project.  
AI project  cycle  takes  us through  different  steps  involved  in a project.  
AI project  cycle  helps  us: 
 
 
 
to create  better  AI projects  easily  
to create  AI projects  faster  
to understand  the process  22 
 
Session  Preparation  
Logistics:  For a class  of 40 Students  [Group  activity  – Groups  of 4] 
 
 
Let us now  start  scoping  a problem.  Look  around  you and select  a theme  which  interests  you the 
most. Suggested themes are:  
 
 
You can either  select  any one out of these  or you can think  of one on your  own.  For more  options,  you 
can also refer to the 17 Sustainable Development Goals we discussed in the Purpose module.  
 
Your  selected  theme  is: 
 23 
 
Why  did you select  this theme?  
 
 
 
 
 
 
 
 
As we know,  a theme  is a broad  term  which  covers  all the aspects  of relevance  under  it. 
For example:  
In Agriculture, there are pest issues, yield rates, sowing and harvesting patterns, etc. all being 
very different from each other but still a part of the Agriculture theme. Thus, to effectively 
understand the problem and elaborate it, we need to select one topic under the theme.  
Some  examples  are: 
Theme:  Digital  Literacy  Topics:  Online  learning  platforms,  digital  awareness,  e-books,  etc. 
Theme:  Health  Topics:  Medicinal  Aid, Mobile  Medications,  Spreading  of diseases,  etc. 
Theme:  Entertainment  Topics:  Media,  Virtual  Gaming,  Interactive  AVs,  Promotions  etc. 
Our Sun is here to throw more light on this! Go back to your selected  
Theme , select  various  Topics  related  to your  theme  and fill them  up in the rays of this sun. 
 
 
Choose  one Topic  out of the ones  mentioned  in the rays of the Sun above,  and fill it in below:  
 24 
 
Let us now list down  the problems  which  come  under  our Topic.  You can recall  daily  life scenarios  where 
you may have witnessed problems related to the Topic of your choice. Also, you can go online and 
research around your chosen topic.  
Fill up the problems  that you find under  your  topic  below.  
 
 
Great! We now know that there exist lot of problems to be solved around us!  Thus,  to set up the GOAL 
of your project, select one problem out of the ones listed above which you want to solve  using  your  AI 
knowledge. This Problem now becomes the  target  of your  AI project  and helps  you getting  a clear  vision 
of what is to be achieved.  
Let us now frame the selected problem as a goal. For example, a goal can be stated as How might we 
help farmers determine the best times for seeding and for sowing their crops?  
 
 
It’s your  turn  now!  Write  the Goal  of your  project  below : 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Since  you have  now  determined  the Goal  of your  project,  let’s  start  working  around  it. 25 
 
4Ws  Problem  Canvas  
 
 
 
 
 
 
The 4Ws Problem canvas helps you in identifying the key elements related to the problem. Let us go 
through each of the blocks one by one.  
Who?  
The “Who” block helps  you in analysing  the people  getting  affected  directly  or indirectly  due to it. Under 
this, you find out who the ‘Stakeholders’ to this problem are and what you know about them. 
Stakeholders are the people who face this problem and would be benefited with the solution.  
Let us fill the “Who” canvas!  26 
 
 
 
What?  
Under the “What” block, you need to look into what you have on hand. At this stage, you need to 
determine the nature of the problem. What is the problem and how do you know that it is  a problem? 
Under this block, you also gather evidence to prove that the problem you have selected  actually  exists. 
Newspaper articles, Media, announcements, etc. are some examples.  
Let us  fill the “What”  canvas!  
 
 
 
 
Where?  
Now that you know who is associated with the problem and what the problem actually is; you need to 
focus on the context/situation/location of the problem. This block will help you look into the  situation  in 
which the problem arises, the context of it, and the locations where it is prominent.  
Let us fill the “Where”  canvas!  
27 
 
 
Why?  
You have finally listed down  all the major  elements  that affect  the problem  directly.  Now  it is convenient 
to understand who the people that would be benefitted by the solution are; what is to be solved; and 
where will the solution be deployed. These three canvases now become the base of why you want to 
solve this problem. Thus, in the “Why” canvas, think about the benefits which the stakeholders would 
get from the solution and how would it benefit them as well as the society.  
Let us fill the “Why”  canvas!  
 
28 
 
Problem  Statement  Template  
 
After filling the 4Ws Problem canvas, you now need to summarise  all the cards  into one template.  The 
Problem Statement Template helps us to summarise all  the key points  into one single  Template  so that 
in future, whenever there is a need to look back at the basis  of the problem,  we can take  a look  at the 
Problem Statement Template and understand the key elements of it.  
 
Problem  Statement  Template  with  space  to fill details  according  to your  Goal:  
 
Our [stakeholders]  Who  
has a problem  that [issue,  problem,  need]  What  
when  / while  [context,  situation].  Where  
An ideal  solution  would  [benefit  of solution  for them]  Why  
Now  let us create  a problem  statement  template  for our Pest  management  case  study  
 
4W canvas  for Pest  Management  
 
Our Farmers  Who  
has a problem  that Cotton  Crops  got infected  with  pest  -Pink  Ballworm  What  
when / while  On the crops  in the field  Where  
An ideal solution would  To create  an AI-enabled  app that aids farmers  in – 
▪ Determining  the correct  amount  of pesticides  
▪ Knowing  the right  time  to spray  pesticides  
▪ Increase  in Production  
▪ Increase  in the profit  share  of the farmers.  Why  29 
 Revision  Time  
1. What  are the various  stages  of Al Project  Cycle?  Can you explain  each  with  an example?  
2. How  is an Al project  different  from  an IT project?  
3. Explain  the 4Ws  problem  canvas  in problem  scoping.  
4. Why  is there  a need  to use a Problem  Statement  Template  during  problem  scoping?  
5. What  is Problem  Scoping?  What  are the steps  of Problem  Scoping?  
6. Who  are the stakeholders  in the problem  scoping  stage?  30 
 1.2.2 Data  Acquisition  
 
Lesson  Title:  Data  Acquisition  Approach:  Interactive  Session  + System  Maps  
Summary: Students will learn how to acquire data from reliable and authentic sources and will 
understand how  to analyse  the data  features  which  affect  their  problem  scoped.  Also,  they  will learn 
the concept of System Maps  
Learning Objectives:  
● Students  will learn  various  ways  to acquire  data.  
● Students  will learn  about  data  features.  
● Students  will learn  about  System  Maps.  
Learning Outcomes:  
● Identify  data  required  regarding  a given  problem.  
● Draw  System  Maps.  
Pre-requisites:  Basic  computer  literacy  
Key-concepts:  
● Develop  an understanding  of reliable  and authentic  data  sources.  
● System  Mapping  
In the previous module, we learnt how to  scope  a problem  and set a Goal  for the project.  After 
setting the goal, we listed  down  all the necessary  elements  which  are directly/indirectly  related 
to our problem. This was done using the 4Ws problem canvas. 4Ws were:  
1. Who?  
a. Who  are the stakeholders?  
b. What  do we know  about  them?  
2. What?  
a. What  is the problem?  
b. How  do you that it is a problem?  (is there  an evidence?)  
3. Where?  
a. What  is the context/situation  the stakeholders  experience  this problem?  
b. Where  is the problem  located?  
4. Why?  
a. What  would  hold  value  for the stakeholders?  
b. How  will the solution  improve  their  situation?  
 
To summarise,  we then  go for the problem  statement  template  where  we put in all the details 
together at one place.  
Our [Stakeholders]  has/have a problem that  [issue,  problem, 
need]   when/while    
[context, situation]. An ideal situation would be   [benefit   of 
solution for them]  . 31 
 
What  is Data  Acquisition?  
As we move ahead in the AI Project Cycle, we come across the second element which is: Data 
Acquisition . As the term clearly mentions, this stage is about acquiring data for the project. Let us first 
understand what is data. Data can be a  piece  of information  or facts  and statistics  collected  together  for 
reference or analysis. Whenever we want an AI  project  to be able  to predict  an output,  we need  to train 
it first using data.  
For example, If you want to make an Artificially Intelligent system which can predict the salary of any 
employee based on his previous salaries, you would feed the data of his previous salaries into the 
machine. This is the data with which the  machine  can be trained.  Now,  once  it is ready,  it will predict  his 
next salary efficiently. The previous salary data here is known as Training Data while the next salary 
prediction data set is known as the Testing Data . 
 
For better efficiency of an AI project, the Training data needs to be relevant and authentic. In the 
previous  example,  if the training  data  was not of the previous  salaries  but of his expenses,  the machine 
would  not have  predicted  his next  salary  correctly  since  the whole  training  went  wrong.  Similarly,  if the 
previous salary data was not authentic, that is, it was not correct, then too the prediction could have 
gone wrong. Hence …. 
For any AI project to be efficient, the training data should be authentic and relevant to the problem 
statement scoped.  
Data  Features  
 
Look at your problem statement once again and try to find the data features required to address this 
issue. Data features refer to the type of data you want to collect . In our previous example, data 
features would be salary amount, increment percentage, increment period, bonus, etc.  
 
Acquiring  Data  from  reliable  sources  
 32 
 
After mentioning the Data features, you get to know what sort of data is to be collected. Now, the 
question arises - From where can we get this data? There can be various ways in which you can collect 
data. Some of them are:  
 
 
 
 
Sometimes, you use the internet and try to acquire data for your project from some random websites. 
Such data might not be authentic as its accuracy  cannot  be proved.  Due to this,  it becomes  necessary  to 
find a reliable source of data from where some authentic information can be taken. At the same time, 
we should keep in mind that the data which we collect is open -sourced and not someone’s property. 
Extracting private data can be an offense. One  of the most  reliable  and authentic  sources  of information 
are the open -sourced websites hosted by the government. These government portals have general 
information collected in suitable format which can be downloaded and used wisely.  
Some  of the open -sourced  Govt.  portals  are: data.gov.in,  india.gov.in  
List down  ways  of acquiring  data  for a project  below: 
1. 
 
 
 
 
2. 
 
 
 
 
 
3. 
 
 
 
 33 
 
System  Maps  
Session Preparation  
Logistics:  For a class  of 40 students  [Group  Activity  – Groups  of 4] 
Materials  Required:  
 
ITEM  QUANTITY  
Computers  10 
Chart Paper  10 
Sketch -Pens  40 
Resources:  
Link to make  System  maps  Online  using  an Animated  tool:  https://ncase.me/loopy/  
 
 
Purpose : The purpose of this section is to introduce the concepts System Maps and its elements, 
relationships and feedback loops.  
Say: “Now that we have listed all the Data features, let us look at the concept of System Maps. 
System Maps help us  to find relationships  between  different  elements  of the problem  which  we have 
scoped. It helps us in strategizing the solution for achieving the goal of our project. Here is an 
example of a System very familiar to you – Water Cycle. The major elements of this system are 
mentioned here. Take a look at these elements and try to understand the System Map for this 
system. Also take a look at the  relations  between  all the elements.  After  this,  make  your  own  system 
map for the data features which you have listed. You can also use the online animated tool for 
creating your System Maps.”  
Brief:  
We use system maps to understand complex issues with  multiple  factors  that affect  each  other.  In a 
system, every element is interconnected. In a system map, we try to represent that relationship 
through the use of arrows. Within a system map, we will identify loops. These loops are  important 
because they represent a specific chain of causes and effects. A system typically has several chains 
of causes and effects. You may notice that some arrows are longer than others. A longer arrow 
represents a longer time for a change to happen. We also call this a time delay. To change the 
outcome of a system, as a change  maker,  we have  two options  - change  the elements  in a system  or 
change the relationships between elements. It is usually more effective to change the relationship 
between elements in a system. You may also notice the use of ‘+’ signs and ‘ -’ signs. These are an 
indicator of the nature of the relationship between elements. What we did was a very basic 
introduction to systems thinking, you can use Google to find more detailed information on how to 
make systems maps.  34 
 A system map shows the components and boundaries of a system and the components of the 
environment at a specific point in time. With the help of System Maps, one can easily define a 
relationship amongst different elements which come under a system. Relating this concept to our 
module, the Goal of our project becomes a system whose elements are the data features mentioned 
above. Any change in these elements changes the system outcome too. For example, if a person 
received 200% increment in a month, then this change in his salary would affect the prediction of his 
future salary. The more the increment presently, the more salary in future is what the system would 
predict. Here is a sample System Map:  
The Water  Cycle  
The concept of Water cycle is very simple to understand and is known to all. It explains how water 
completes its cycle transforming from one form to another.  It also adds  other  elements  which  affect  the 
water cycle in some way.  
The elements  which  define  the Water  cycle  system  are: 
 
 
 
 
 
  
 
 
  
 
 
  
 
 
 
Clouds  Snow  Underground 
Soil Rivers  
 
 
 
  
 
 
  
 
 
  
 
 
 
Oceans  Trees  Land  Animals  
35 
 
Let us draw  the System  Map  for the Water  Cycle  now.  
 
 
 
In this System Map, all the elements of the Water cycle are put in circles. The  map  here  shows  cause  & 
effect relationship of elements with each other with the help of arrows. The arrow - head depicts the 
direction  of the effect  and the sign (+ or -) shows  their  relationship.  If the arrow  goes  from  X to Y with  a 
+ sign, it means that both are directly related to each other. That is, If X increases, Y also  increases  and 
vice versa. On the other hand, If the arrow goes from X to Y with a – sign, it means that both the 
elements are inversely related to each other which means if X increases, Y would decrease and vice 
versa.  
Now,  it’s your  turn  to build  your  own  System  Map!  
Considering  the data  features  for your  problem,  draw  a system  map  in the box provided.  
(Hint:  You can also use this animated  tool for drawing  and understanding  system  maps: 
https://ncase.me/loopy/ ) 36 
 Revision  Time  
1. How  will you differentiate  between  Training  Data  and Testing  Data?  Elaborate  with  examples.  
2. Name  various  methods  for collecting  data.  For each  method,  can you name  at least  one project  in 
which you may use that method of data collection?  
3. What  must  you keep  in mind  while  collecting  data  so it is useful?  
4. Imagine  you are responsible  to enable  farmers  from  a village  to take  their  produce  to the market  for 
sale. Can you draw a system map that encompasses all the steps and factors involved?  
5. Name  a few government  websites  from  where  you can get open -source  data.  37 
 1.2.3 Data  Exploration  
 
Title: Data Exploration  Approach: Activity  
Summary:  Students  will explore  different  types  of graphs  used in data visualization  and will 
be able to find trends and patterns out of it.  
Learning Objectives:  
● Students  will explore  various  types  of graphical  representations.  
● Students  will learn how to  visualize the data they have.  
Learning Outcomes:  
● Recognize  different  types  of graphs  used in data visualization.  
● Exploring  various  patterns  and trends  out of the data explored.  
Pre-requisites: Basic computer  literacy  
Key-concepts: Data Visualization  
Let us Recap!  
Quiz  Time!  
1. Which  one of the following  is the second  stage  of AI project  cycle?  
a. Data  Exploration  
b. Data  Acquisition  
c. Modelling  
d. Problem  Scoping  
2. Which  of the following  comes  under  Problem  Scoping?  
a. System  Mapping  
b. 4Ws  Canvas  
c. Data  Features  
d. Web  scraping  
3. Which  of the following  is not valid  for Data  Acquisition?  
a. Web  scraping  
b. Surveys  
c. Sensors  
d. Announcements  
4. If an arrow  goes  from  X to Y with  a – (minus)  sign,  it means that 
a. If X increases,  Y decreases  
b. The direction  of relation  is opposite  
c. If X increases,  Y increases  
d. It is a bi-directional  relationship  38 
 
5. Which  of the following  is not a part of the 4Ws  Problem  Canvas?  
a. Who?  
b. Why?  
c. What?  
d. Which?  
 
 
Let us explore:  
 
Session  Preparation  
Logistics:  For a class  of 40 Students.  [Group  Activity  – Groups  of 4] 
Materials  Required:  
 
ITEM  QUANTITY  
Computers  10 
 
Resources:  
Link to visualisation  website:  https://datavizcatalogue.com/  
Purpose: To understand why we do data exploration before jumping straight into training an AI 
Model.  
Say: “Why do you think we need to explore and visualize data before jumping into the AI model? 
When we pick up a library book, we tend to look at the book cover, read the back cover and skim 
through the content of the book prior to choosing it as it helps us understand if this book is 
appropriate for our needs and interests. Similarly, when we get a  set of data  in our hands,  spending 
time to  explore  it will help  get a sense  of the trends,  relationships  and patterns  present  in the data.  It 
will also help us better decide on which model/models to use in the subsequent AI Project Cycle 
stage. We use visualization as a method because it is much easier to comprehend information 
quickly and communicate the story to others.”  
Brief:  
In this session, we will be exploring various types of Graphs  using  an online  open - sourced  website. 
Students will learn about various new ways to visualise the data.  
When  to intervene?  
Ask the students to figure out which types of graphs would be suitable for the data features that 
they have listed for their problem. Let them take their time in going through each graph and its 
description and decide which one suits their needs the best.  39 
 
In the previous  modules,  you have  set the goal  of your  project  and have  also found  ways  to acquire  data. 
While acquiring data, you must have noticed that the data  is a complex  entity  – it is full of numbers  and 
if anyone wants to make some  sense  out of it, they  have  to work  some  patterns  out of it. For example,  if 
you go  to the library  and pick up a random  book,  you first try to go through  its content  quickly  by turning 
pages and by reading the description before borrowing it for yourself, because it helps you in 
understanding if the book is appropriate to your needs and interests or not.  
Thus,  to analyse  the data,  you need  to visualise  it in some  user -friendly  format  so that you can: 
• Quickly  get a sense  of the trends,  relationships  and patterns  contained  within  the data.  
• Define  strategy  for which  model  to use at a later  stage.  
• Communicate  the same  to others  effectively.  To visualise  data,  we can use various  types  of visual    
representations.  
Are you aware  of visual  representations  of data?  Fill them  below:  
 
 
 
As of now,  we have  a limited  knowledge  of data  visualisation  techniques.  To explore  various 
data visualisation techniques, visit this link: https://datavizcatalogue.com/  
On this website,  you will find various  types  of graphical  representations,  flowcharts,  hierarchies, 
process descriptors, etc. Go through the page and look at various new ways and identify the 
ones which interest you the most.  40 
 
 
 
Identify  the icons  of different  graphs:  
 
 
 
 
 
 
 
 
41 
 List down  5 new  data  visualisation  techniques  which  you learnt  from  https://datavizcatalogue.com  
 
 
Data  Visualisation  Technique  1 
Name of the 
Representation   
One-line 
Description   
 
 
 
 
 
How  to draw  it  
Suitable for 
which data 
type?   
 
Data  Visualisation  Technique  2 
Name of the 
Representation   
One-line 
Description   
 
 
 
 
 
 
How  to draw  it  
Suitable for 
which data 
type?   42 
 Data  Visualisation  Technique  3 
Name of the 
Representation   
One-line 
Description   
 
 
 
 
How  to draw  it  
Suitable for 
which data 
type?   
 
Data  Visualisation  Technique  4 
Name of the 
Representation   
One-line 
Description   
 
 
 
 
How  to draw  it  
Suitable  for 
which data 
type?   43 
 
Data  Visualisation  Technique  5 
Name of the 
Representation   
One-line 
Description   
 
 
 
 
How  to draw  it  
Suitable for 
which data 
type?   
 
Sketchy  Graphs  
Session  Preparation  
Logistics:  For a class  of 40 Students.  [Group  Activity  – Groups  of 4] 
Materials  Required:  
 
ITEM  QUANTITY  
Chart Paper  10 
Sketch -pens  10 
Ruler  10 
Basic Stationary  10 Sets 
 44 
 Let us now look at the scoped  Problem  statement  and the data  features  identified  for achieving  the goal 
of your project. Try looking for the data required for  your  project  from  reliable  and authentic  resources. 
If you are not able to find  data  online,  try using  other  methods  of acquiring  the data  (as discussed  in the 
Data Acquisition stage).  
Once you have acquired the data, you need to visualise it. Under the sketchy graphs activity, you will 
visualise your collected data in a graphical format for better understanding.  
For this, select  one of the representations  from  the link or choose  the ones  which  you already  know.  The 
basis of your selection should be the data feature which you want you to visualise in that particular 
representation. Do this for all the data features you have for the problem you have scoped. Let us 
answer the following questions for a better understanding:  
1. Which  data  feature  are you going  to represent?  
 
 
 
 
 
 
 
 
2. Which  representation  are you going  to use for this data  feature?  Why?  
 
 
 
 
 
 
 
 
 
 
 
Now, let’s start drawing visual representations for all the Data features extracted, and try to find a 
pattern or a trend from it.  
For example, if the problem statement is: How can we predict whether a song makes it  to the billboard 
top 10?  
We would require data features like: Current trends of music, genre of music, tempo of music,  duration 
of song, popularity of a singer, etc.  
Now to analyse a pattern, we can say that the popularity of the singer would directly have  an effect  on 
the output of the system.  Thus,  we would  plot a graph  showing  the popularity  of various  singers  and the 
one who is most popular has the maximum chance of getting to  the billboard.  In this way,  the graphical 
representation helps us understand the trends and patterns out of the data collected and to design a 
strategy around them for achieving the goal of the project.  45 
 Do it yourself:  
Take a chart paper  and start  representing  your  data  features  in various  types  of graphs.  After  completing 
this exercise, present your work to your friends and explain to them the trends and patterns you have 
observed in it.  
List down  the trends  you might  have  observed  in your  representations  below:  
1.    
 
 
 
2.    
 
 
 
 
3.    
 
 
 
 
4.    
 
 
 
 
5.    
 
 
 
 
6.    
 
 46 
 Revision  Time  
 
1. What  is the significance  of Data  Exploration  after  you have  acquired  the data  for the problem 
scoped? Explain with examples.  
2. What  do you think  is the relevance  of Data  Visualization  in Al? 
3. List any five graphs  used  for data  visualization.  
4. How  is Data  Exploration  different  from  Data  Acquisition?  
5. Use an example  to explain  at least  one Data  Visualization  technique.  47 
 
Purpose: To differentiate between Artificial Intelligence (AI), Machine Learning (ML) and Deep 
Learning (DL).  
Say: “As we enter the world of modelling, it is a good time to clarify something many of you  may  be 
having doubts about. You may have heard  the terms  AI, ML and DL when  research  content  online  and 
during this course. They are of course related, but how?  
Artificial Intelligence, or AI for short, refers to any technique  that  enables  computers  to mimic  human 
intelligence. An artificially intelligent machine works on algorithms and data fed to it and gives the 
desired output.  
Machine Learning, or ML for short, enables machines to improve at tasks with experience. The 
machine here learns from the new data fed  to it while  testing  and uses  it for the next  iteration.  It also 
takes into account the times when it went wrong and considers the exceptions too.  1.2.4 Modelling  
 
Title: Modelling  Approach:  Session  + Activity  
Summary:  In this module, students’ progress from data exploration to AI modeling, learning 
about key distinctions between Artificial Intelligence (AI), Machine Learning (ML), and Deep 
Learning (DL). The module introduces two approaches to AI modeling: Rule -Based and 
Learning -Based.  
Learning Objectives:  
● Understand and differentiate between AI, ML, and DL.  
● Explain the differences between Rule -Based and Learning -Based AI approaches.  
● Develop a basic understanding of how AI models are trained and tested.  
Learning Outcomes:  
● Define AI, ML, and DL and explain their relationships.  
● Identify the key differences between Rule -Based and Learning -Based AI models.  
Pre-requisites:  Basic understanding of AI concepts from previous modules.  
Key-concepts:  
● AI, ML and DL  
● Rule -Based Approach  
● Learning -Based Approach  
● AI Modeling  
In the previous module of Data Exploration, you explored the data you had acquired at the Data 
Acquisition stage for the problem you scoped in the Problem Scoping stage. Now, you have visualised 
some trends and patterns out of the data which would help you develop a strategy for your project. To 
build an AI based project, we need to work  around  Artificially  Intelligent  models  or algorithms.  This could 
be done either by designing your own model or by using the pre -existing AI models.  Before  jumping  into 
modelling let us clarify the definitions of Artificial Intelligence (AI), Machine Learning (ML) and Deep 
Learning (DL).  
AI, ML & DL 48 
  
As you have been progressing towards building AI readiness, you must have come across a very common 
dilemma  between  AI and ML. Many  of the times,  these  terms  are used  interchangeably  but are they  the same? 
Is there no difference between Machine Learning and Artificial Intelligence? Is Deep Learning also Artificial 
Intelligence? What exactly is Deep Learning? Let us see … 
 
 
As you can see  in the Venn  Diagram,  Artificial 
Intelligence is the umbrella terminology 
which covers machine and deep learning 
under it and Deep Learning comes under 
Machine Learning. It is a funnel type 
approach where there are a lot of 
applications of AI out of which few  are those 
which come under ML out of which very  few 
go into DL.  
 
 
 
 
Defining  the terms:  
1. Artificial Intelligence , or AI, refers to any technique that enables computers to mimic human 
intelligence. The AI -enabled machines think algorithmically and execute what they have been 
asked for intelligently.  
2. Machine Learning , or ML, enables machines to improve at tasks with experience. The machine 
learns from its mistakes and takes them into consideration in the next execution. It improvises 
itself using its own experiences.  
3. Deep Learning , or DL, enables software to train itself to perform tasks  with  vast amounts  of data. 
In deep learning, the machine is trained with huge amounts of data which helps it into training 
itself  around  the data.  Such  machines  are intelligent  enough  to develop  algorithms  for themselves.  
Deep Learning is the most advanced form of Artificial Intelligence out of these three. Then comes 
Machine Learning which is intermediately intelligent and Artificial Intelligence covers all the concepts 
and algorithms which, in some way or the other mimic human intelligence.  
 
Modelling  
Purpose:  Classification  of Models  into Rule -based  approach  and Learning  approach.  
Say:  “In general,  there  are two approaches  taken  by researchers  when  building  AI models.  They  either  
Deep Learning, or DL for short, enables software to train itself to perform tasks with vast amounts of 
data. Since the system has got huge set of data, it is able to train itself with the help of multiple 
machine learning algorithms working altogether to perform a specific task.  
Artificial Intelligence is the umbrella term which holds both Deep Learning as well as Machine 
Learning. Deep Learning,  on the other  hand,  is the very  specific  learning  approach  which  is a subset  of 
Machine Learning as it comprises of multiple Machine Learning algorithms.”  
49 
 take a rule -based approach or learning approach. A Rule based  approach  is generally  based  on the data 
and rules fed to the machine, where the  machine  reacts  accordingly  to deliver  the desired  output.  Under 
learning approach, the machine is  fed with  data  and the desired  output  to which  the machine  designs  its 
own algorithm (or set of rules) to match the data to the desired output fed into the machine”  
 
 
AI Modelling  refers  to developing  algorithms,  also called  models  which  can be trained  to get intelligent 
outputs. That is, writing codes to make a machine artificially intelligent.  
Let us ponder  
Use your  knowledge  and thinking  ability  and answer  the following  questions:  
1. What  makes  a machine  intelligent?  
 
 
 
 
 
 
 
2. How  can a  machine be  Artificially Intelligent?  
 
 
 
 
 
 
 
3. Can Artificial  Intelligence  be a threat  to Human  Intelligence?  How?  
 
 
 
 
 
 
 50 
 In the previous module of Data exploration, we have seen various types of graphical representations 
which can be used for representing different parameters of data. The graphical representation makes 
the data understandable  for humans  as we can discover  trends  and patterns  out of it. But when  it comes 
to machine accessing and analysing data, it needs the data in the most  basic  form  of numbers  (which  is 
binary – 0s and 1s) and when it comes to discovering patterns and  trends  in data,  the machine  goes  for 
mathematical representations of the same. The ability to mathematically describe the relationship 
between parameters is the heart of every AI model. Thus, whenever we talk about developing AI 
models, it is the mathematical approach towards analysing data which we refer to.  
 
Generally,  AI models  can be classified  as follows:  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Rule  Based  Approach  
Refers to the Al modelling where the rules are defined by the developer. The machine  follows  the rules 
or instructions mentioned by the developer and performs its task accordingly. For example, we have a 
dataset which tells us about the conditions on the basis of which we can decide if child can go out to 
play golf or not. The parameters are: Outlook, Temperature, Humidity  and Wind.  Now,  let's take  various 
possibilities of these parameters  and see in which  case  the children  may  play golf and in which  case  they 
cannot. After looking through all the cases, we feed this data into the machine along with the rules 
which tell the machine all the possibilities. The machine trains on this data and now is ready to be 
tested. While testing the machine, we tell the machine that Outlook Overcast; Temperature = Normal; 
Humidity = Normal and Wind = Weak. On the  basis  of this testing  dataset,  now  the machine  will be able 
to tell if the child can go out to play golf or not and will display the prediction to us. This is  known  as a 
rule-based approach because we fed the data along with rules to the machine and the machine after 
getting trained on them is now able to predict answers for the same. A drawback/feature for this 
approach is that the learning is static. The machine once trained, does not take into consideration any 
changes made in the original training dataset.  
51 
 
Rule  Based  AI Model  
 52 
 
 
Learning  Based  Approach  
Refers to the Al modelling where the machine learns by itself. Under the Learning Based approach,  the 
Al model gets trained on the data fed to it and then is able to design a model which is adaptive to the 
change  in data.  That  is, if the model  is trained  with  X type  of data  and the machine  designs  the algorithm 
around it, the model  would  modify  itself  according  to the changes  which  occur  in the data  so that all the 
exceptions are handled in this case. For example, suppose you have  a dataset  comprising  of 100 images 
of apples and bananas each.  These  images  depict  apples  and bananas  in various  shapes  and sizes.  These 
images  are then  labelled as either apple or banana so that all apple images are labelled 'apple' and all 
the banana images have 'banana' as their label. Now, the Al model is trained with this dataset and the 
model is programmed in such a  way that it can distinguish  between  an apple  image  and a banana  image 
according to their features and can predict the label of any image which is fed to it as an apple or a 
banana. After training, the machine is now fed with testing data. Now, the testing data might not have 
similar  images as the ones on which the model has been trained. So, the model adapts to the features 
on which it has been trained and accordingly predicts if the image is of an apple or banana.  
 
Learning  Based  AI Model  
Revision  Time  
1. What  are the various  stages  of the Al Project  Cycle?  Explain  each  with  examples.  
2. What  is Artificial  Intelligence?  Give  an example  where  Al is used  in day-to-day life. 
3. How  is Machine  Learning  related  to Artificial  Intelligence?  
4. Compare  and contrast  Rule -based  and Learning -based  approach  in Al modeling  indicating  clearly 
when each of these may be used.  53 
 
1.2.5 Evaluation  
In Stage  5, we have  Evaluation,  the main  objective  of this stage  is to test different  models  and choose  the 
best model . 
 
Lesson Title:  Evaluation  Approach:  Interactive  Session  + Activity  
Summary : In this module youth will be learn concept of evaluation in the AI project cycle. They 
will also learn  that evaluation  is essential  for assessing  the success  of AI projects,  identifying  areas 
for improvement, and making data -driven decisions.  
Learning Objectives  
● Students  will be able  to understand  the importance  of evaluation  in the 
AI project cycle.  
● Students  will be able  to apply  evaluation  techniques  to assess  the effectiveness 
of AI projects.  
● Students  will be able  to identify  areas  for improvement  in AI projects  through  evaluation . 
Learning  Outcomes  
● By the end of this lesson,  students  should  be able  to apply  evaluation  techniques  in their 
own AI projects.  
● Pre-requisites:  Basic  knowledge  of Artificial  Intelligence  and problem  solving  
Key-concepts  
Importance  of Evaluation  techniques.  
 
What  is evaluation?  
 
Evaluation is the process of understanding the reliability of any AI model, based on outputs by 
feeding test dataset into the model and comparing with actual answers. There can be different 
Evaluation techniques, depending of the type and purpose of the model. Remember that It’s not 
recommended  to use the data  we used  to build  the model  to evaluate  it. This is because  our model 
will simply  remember  the whole  training  set, and will therefore  always  predict  the correct  label  for 
any point in the training set. This is known as overfitting.  
 
Once  a model  has been  made  and trained,  it needs  to go through  proper  testing  so that one can 
calculate  the efficiency  and performance  of the model.  Hence,  the model  is tested  with  the help  
of Testing  Data  (which  was separated  out of the acquired  dataset  at Data  Acquisition  stage)  and the 
efficiency of the model is calculated on the basis of the parameters mentioned below:  
 
 
 54 
 
Note:  You will learn  more  about  these  techniques  in grade  X. 
 
▪ We test our models  to check  their  performance 
and improve  our models  for best  performance.  
▪ The model  is tested  with  collected  data.  
▪ We also check  if the model  is solving  the 
identified AI problem properly.  
 
Model  Evaluation  Terminologies  
There  are various  new  terminologies  which  come  into the picture  when  we work  on evaluating  our 
model. Let’s explore them with an example of the Forest fire scenario.  
 
The Scenario  
Imagine that you have come up with an AI based prediction model which has been deployed in a 
forest which is prone to forest fires. Now, the objective of the model is to predict whether a forest 
fire has broken  out in the forest  or not. Now,  to understand  the efficiency  of this model,  we need  to 
check  if the predictions  which  it makes  are correct  or not. Thus,  there  exist  two conditions  which  we 
need to ponder upon: Prediction and Reality. The prediction is the output which is given by the 
machine and the reality is the real scenario in the forest when the prediction has been made. Now 
let us look at various combinations that we can have with these two conditions.  
 
Case  1: Is there  a forest  fire?  Here,  we can see in the picture  that  a forest  fire has broken  out in the 
forest.  
 
 
 
55 
 
Here, we can see in the picture that a forest fire has broken out in the forest. The model predicts a 
Yes which  means  there  is a forest  fire. The Prediction  matches  with  the Reality.  Hence,  this condition 
is termed as True Positive.  
 
Case  2: Is there  a forest  fire?  
 
 
 
Case  3: Is there  a forest  fire?  
 
 
 56 
 
Here  the reality  is that there  is no forest  fire. But the machine  has incorrectly  predicted  that there  is 
a forest fire. This case is termed as False Positive.  
Case  4: Is there  a forest  fire?  
 
 
 
 
 
Here,  a forest  fire has broken  out in the forest  because  of which  the Reality  is Yes but the machine 
has incorrectly  predicted  it as a No which  means  the machine  predicts  that there  is no Forest  Fire. 
Therefore, this case becomes False Negative  
 
 57 
 
 
At this particular  stage,  we may  need  to evaluate  the model  to find out which  algorithm  makes  
the best  prediction.  
 
The figure  shows  the accuracy  of 5 different  algorithms  as discussed  in the Modeling  stage.  
 
ROC  is a metric  used  to find out the accuracy  of a model.  
Evaluation   
 
 
 
 
 
 
 
 
 
 
Note: The graph above compares the accuracy of five different algorithms —BLS (Broad Learning System ), MLP  
(Multi -Layer Perceptron ), CNN  (Convolutional Neural Network ), Wavelet MLP  (Wavelet Multi -Layer Perceptron ), 
and SVM  (Support Vector Machine )—demonstrating how an AI developer can choose the most suitable 
algorithm for a specific use case. While these algorithms are advanced topics within the curriculum, facilitators 
are encouraged to prompt learners to explore them further through online resources.  
 
Chapter  Review  
Q1. What  is Evaluation?  
Q2. What are various Model evaluation techniques? 
Q3. Why  is model  evaluation  important  in AI projects?  
Q4. What  do you understand  by the terms  True  Positive  and False  Positive?  58 
 
1.2.6 Deployment  
In Stage  6, we have  Deployment,  the main  objective  of this stage  is to make  our solution  ready  to be 
used.  
Lesson Title:  Deployment  Approach:  Interactive  Session  + Activity  
● Summary : In this module  youth  will learn  about  the term  "deployment"  in the context 
of AI projects and why it is an important step.  
● They  will Connect  the concept  of deployment  to real-world  examples  such  as 
deploying a chatbot on a website or a predictive model in a mobile app.  
Learning Objectives  
● Students  will be able  to understand  the concept  of deployment  in the AI project  cycle 
and demonstrate their knowledge through hands -on activities.  
Learning  Outcomes  
● By the end of this lesson,  students  should  be able  to emphasize  the importance  of 
deployment in the AI project cycle.  
● Challenge  students  to think  about  how  they  can apply  their  knowledge  of deployment  
in future  AI projects  and encourage  them  to continue  exploring  different  deployment 
methods.  
● Pre-requisites:  Basic  knowledge  of Artificial  Intelligence  and problem  solving  
Key-concepts  
● Importance  of Deployment  in Ai project  cycle  
What  is deployment?  
Deployment  as the final  stage  in the AI project  cycle  where  the AI model  or solution  is implemented  in a 
real-world scenario.  
Key Steps  in Deployment  Process  
the key steps  involved  in the deployment  process:  a. Testing  and validation  of the AI model  b. Integration 
of the model with existing systems c. Monitoring and maintenance of the deployed model.  
Some  examples  of successful  AI projects  that have  been  deployed  in various  industries,  such  as 
self-driving cars, medical diagnosis systems, and chatbots.  
 
▪ AI can be used  on Mobile  Apps,  Website  Apps,  etc. 59 
 
Revision  Time  
Choose  the correct  answer!  
1. Does  modeling  mean  creating  an AI model?  
a. YES b. NO 
2. Can we use  AI on  mobile phones?  
a. YES b. NO 
3. What  is deployment  in the context  of an AI project  cycle?  
4. Why  is deployment  an important  phase  in the AI project  cycle?  
5. What  are some  common  challenges  in deploying  AI models?  
 
Case  Study:  Preventable  Blindness  
Problem:  Prevent  loss of vision,  and delay  in report  generation  
●  Approximately  537 million  adults  (20-79 years)  are living 
with diabetes.  
●  Diabetes  can lead  to Diabetic  Retinopathy  It damages  the 
blood vessels of the retina and can lead to blurred vision 
and blindness.  
●  Lack  of qualified  doctors  and delay  in reports  increase  the 
risk of Diabetic Retinopathy  
 
One of the early  symptoms  of the defect  is ‘Blurred  vision’  as 
shown below:  
     Normal Vision  Blurred  Vision  
 
 
 
How  can we solve  this problem  with  AI? 
Solution : Using  AI to detect  Diabetic  Retinopathy  in 
pictures of eyes  
 
AI solution  at Aravind  Eye Hospital,  India  
 
●  An AI eye screening  solution  is developed  in 
partnership with Google.  
60 
 
●  AI models  have  achieved  an accuracy  of 98.6%  in 
detecting diabetic retinopathy, on par with the 
performance of specialist eye doctors.  
● Seventy -one vision  centers  in rural  Tamil  Nadu,  India 
are using this solution.  
● Trained  technicians  take  pictures  of patients’  eyes  with 
cameras.  
● The digital  images  are analyzed  by AI for the presence 
of Diabetic Retinopathy.  
● AI has made  the detection  of Diabetic  Retinopathy 
quicker.  
● Any technician  can use this machine,  even  without  a 
skilled doctor.  
More  and more  parents  can be treated  at an early  stage.  
Hence,  early  detection  using  AI can significantly  benefit  rural  populations 
Let us map this problem to AI project cycle  
How  would  you scope  the problem?  
 
 
 
AI Project  Cycle  Mapping  Template  
Problem 
Scoping  Data 
Acquisition  Data 
Exploration  Modeling  Evaluation  Deployment  
Blindness due 
to diabetic 
Retinopathy 
that can be 
prevented  Collecting  
data  from  
patients  
from  many  
clinics using  
retinal  
cameras.  Validating  all 
the data  to 
make  sense  
out of it and 
come  up 
with a 
model.  Creating  an AI 
model to 
correctly  
diagnose  
Diabetic  
Retinopathy  
when  given  a 
retinal  image  
as input.  Test  the model  
for accuracy  and 
then fine tune  
the model  
further  to get 
the desired  out 
-put. Using  the model  in 
tools  that can be 
used  in clinics  in 
even the remote  
and rural  parts  of 
the country.  
 
61 
 Activity Time!  
Purpose:  Implementation  of AI project  cycle  to develop  an AI Model  for Personalized  Education.  
Activity Introduction:  
▪ In this activity,  students  use the AI project  cycle  to conceptualize  a solution  for the given  problem.  
▪ AI project  cycle  is a 6-step  process  which  aids in problem  solving  using  Artificial  Intelligence  
Description:  
▪ All individuals  have  different  cognitive  levels  and personalities.  
▪ Different  people  need  attention  towards  different  parts  of their  learning.  
▪ A generalized education system often falls short in addressing individual learning needs, whereas 
personalized education allows students to learn at their own pace, catering to their unique  
strengths and challenges.  
 
Activity Guidelines:  
▪ Understand  the problem.  
▪ Learn  the various  aspects  and developments  in the field.  
▪ Fill the AI Project  Cycle  mapping  template  for the problem.  
▪ The solution  to the problem  of personalized  education  is an AI algorithm  that trains  over  the 
behavior  and choices  of a student.  Thus,  all the requirements  specific  to a student  could  be 
recognized and addressed to.  
 
AI Project  Cycle  mapping  template  for Preventable  blindness:  
 
Fill the AI Project  Cycle  mapping  template  for the discussed  problem  of personalized  education. 
[Hint: Take the reference of the above AI Project cycle mapping template]  
 
 
AI Project  Cycle  Mapping  Template  
Problem 
Solving  Data 
Acquisition  Data 
Exploration  Modelling  Evaluation  Deployment  
      
Personalised Education: For students, personalized education customizes learning experiences to 
match their individual needs, abilities, and interests. This approach enables students to progress at 
their own pace, concentrate on areas needing more attention, and ultimately improve engagement 
and academic performance.  62 
 
Revision  Time:  
1. Rearrange  the steps  of AI project  cycle  in correct  order:  
a. Data  Acquisition  
b. Problem  Scoping  
c. Modelling  
d. Data  Exploration  
e. Deployment  
f. Evaluation  
 
2. The process  of breaking  down  the big problem  into a series  of simple  steps  is known  as: 
a. Efficiency  
b. Modularity  
c. Both a) and b) 
d. None  of the above  
3. The primary  purpose  of data  exploration  in AI project  cycle  is   
a. To make  data  more  complicated  
b. To simplify  complex  data  
c. To discover  patterns  and insights  in data  
d. To visualize  data  
4. Deployment  is the final  stage  in the AI project  cycle  where  the AI model  or solution  is implemented 
in a real -world scenario. (True/False)  
 
5. Identify  A, B and C in the following  diagram  (Hint:  How  AI, ML &DL  related  to each  other)  
 63 
 Ask:  “learners  to imagine  themselves  in the scenario  before  moving  on to discussion  questions.”  Unit  1.3 
Ethics  and Morality  
Title:  AI Ethical  Issues  Approach:  Interactive  Session  + Activity  
Summary:  Students  will learn  about  Morals  and Ethics,  ethical  values  related  to personal  data 
and ethical steps for a safer AI.  
Objectives:  
● Understanding  the concept  of Ethics  and Morals.  
● Students  will learn  to differentiate  between  Morality  and Ethics.  
● Students  will explore  various  Ethics  with  Personal  Data,  Issues  around  AI Ethics,  AI Ethics 
Principles.  
Pre-requisites:  
● Basic  knowledge  of AI Project  Cycle  and its steps.  
● Basic  understanding  of ethics  and ethics  in AI. 
Key- Concepts:  
● Familiarizing  with  AI project  cycle,  need  for using  it and how  to map  it with  different  projects.  
● Familiarizing  with  AI ethics  and issues  around  AI ethics.  
● Ethical  principles  for safer  AI 
Let’s  take  a look  at the given  ethical  scenarios. 
Ethical Scenario – I 
Imagine a situation where you are a high school teacher. You have to 
check  a lot of essay  submissions,  which  will take  a lot of time.  You find 
an AI tool that can correct the essays submissions and assign them 
grades.  
 
 
 
 
 
 
 
Let’s  Discuss : 
1. Would  you use the tool to grade  the essays?  
 
2. Why  would  you do that?  
 
3. What  will be the advantages  and disadvantages  of using  the AI tool?  
 
4. Can you think  of any challenges  which  the AI tool might  face?  
 
64 
 Say: Watch  another  interesting  reference  video  on ethical  scenarios 
https://w ww.youtube.com/watch?v=nyTmeb4vFqE  
Ask learners  to imagine  themselves  in the scenario  before  moving  on to discussion  questions.  
Ask below  questions  one by one.  Wait  for the response  from  the learners.  Let the learners  know  that 
these questions do not necessarily have a right answer.   
 
 
 
Watch  another  interesting  reference  video  on ethical  scenarios 
https://www.youtube.com/watch?v=nyTmeb4vFqE  
 
Ethical  Scenario  – II 
Burger  
▪ Imagine  a situation  where  you oversee  burgers  at a 
fast-food restaurant  
▪ It is a busy  day with  a lot of orders  coming  in fast. 
 
▪ While  cooking,  you drop  a burger  on the dirty  floor!  
 
▪ Your  boss  passes  by and says,  “Just  pick it up and serve  it!” 
 
▪ What  would  you do? 
 
 
 
Ethical  Questions:  
 
Examples  of Ethical  questions  
• If a shopkeeper  gives  me back  more  money  than  what  is due,  is it better  to return  it? Or should  I 
keep it with me?  
• Is taking  pens  from  a library  considered  stealing?  
• Is taking  extra  paper  napkins  from  a restaurant  considered  theft?  
• You order  a new  dress  from  Amazon  and after  wearing  it on your  friends  birthday  party,  you 
returned it stating the reason inappropriate fitting . Wait  for the learners  to respond.  
Ask them  why they  choose  to respond  in a certain  way.  
Point  out different  responses  from  different  learners  in the same  situation.  
65 
 Say “Different societies or religions can consider different things right or wrong. What might be 
considered  very  good  by one person,  society  or religion  might  not be considered  as good  by another.”  Moral  Questions  
 
Examples  of moral  questions  
• Is it OK to lie? If so, under  what  circumstances?  
• If a family  is hungry  and has no other  way to get food,  is it OK 
to steal food from a rich store owner? Why or why not?  
• Is a collective  decision  made  by people,  always,  right?  Or can it 
be wrong?  
 
Let’s  Discuss:  
1. What  is ethics  according  to you?  
 
2. What  are morals  according  to you?  
 
3. Did you notice  any differences  or similarities  between  ethical 
and moral questions?  
 
 
 
Ethic s vs Morals  
 
Morals  Ethics  
The beliefs  dictated  by our society.  The guiding  principles  to decide  what  is good  or bad.  
Morals  are not fixed  and can be different 
for different societies.  These  are values  that  a person  themselves  chooses 
for their life.  
Examples:  
Always  speak  the truth 
Always be loyal  
Always be generous  Examples:  
Is it good to speak the truth in all situations? 
Is it good  to be loyal  under  all circumstances? 
Is it necessary to always be generous?  
 
 
66 
 
Fun activity:  
Purpose:  Use Moral  Machine  Platform  to exercise  the morality  of persons.  
Moral  Machine  is a platform  for gathering  a human  perspective  on moral  decisions  made  by artificial 
intelligence, such as self -driving cars.  
At the end,  you will be able  to see how  their  responses  compare  with  other  people.  
Activity Guidelines:  
To perform  the activity:  
Go to this https://www.moralmachine.net  
 
 
● Click  on ‘Start Judging’ and you will see a screen as shown.  
 67 
 
● Answer  the questions  till the end.  
 
 
 
 
Let’s  summarise:  
● The results  will tell you which  characters  you preferred  over  the others.  
● Saving more lives matters to you. When given a choice, you would prefer to save as many people as 
you can.  
● It does  not matter  to you much  if a person  obeys  the law or not when  it comes  to saving  people.  
● You will also get to know  what  beliefs  you value  with  the choices  you make  in the game.  
● You prefer  protecting  passengers,  instead  of pedestrians  more.  
● When an equal number of people are getting hurt, you prefer to not be a part of the  consequences, 
and you do not intervene.  68 
 
Ethics  and Personal  Data  
There  is a student  named  Jack 
▪ Jack spends  a lot of time  on the internet  every  day. 
 
▪ He does  his research  assignments,  connects  with  his 
friends,  uses  social  media,  plays  his favorite  games,  and 
shops on the internet.  
▪ This means  that a lot of his personal  information  is on the 
internet.  
 
 
 
 
 
 
 
 
Ethics  with  Personal  Data  
▪ There  are around  5.34  billion  smartphone  users  in the world  as of 
July 2022, with their information available on the internet.  
▪ AI can help  us find out data  related  to a particular  person,  from  all 
the available data.  
▪ Such  AI solutions  are used  by organizations  to give us customized 
recommendations for products, songs, videos, etc.  
▪ In this way,  AI can influence  our decision -making  at times  
 
▪ This calls  for a need  for ethical  principles  that govern  AI and people  who  are creating  AI. 
69 
 Ask: “what  the learners  did if they  received  lesser  marks  than  they  had expected.”   
 
Let’s  discuss:  
1. Can you think  of what  kind  of personal  data  might  be stored  on the internet?  
 
2. What  are some  other  ways  this personal  data  could  be used  to influence  individuals?  
 
3. Would  it be ethical  if governments  had access  to all the personal  data  of the citizens?  
 
 
Major  Issues  around  AI Ethics  
Let’s  learn  some  more  about  Jack:  
 
▪ He is an average  middle  school  student.  
 
▪ His school  recently  started  using  an AI-based  essay  grading  system.  
 
▪ The system  takes  in an essay  and assigns  grades  after  evaluation.  
 
▪ Jack is worried  that he scored  a bad grade,  even  though  he wrote  a really  good  essay.  
Let’s  discuss  
 
▪ What  do you think  happened  here?  
 
 
▪ Why  did the AI evaluate  Jack’s  essay  incorrectly?  
 
 
 Say “Try  to identify  if the learners  can relate  to Jack and what  he uses  the internet  for. Ask the 
learners if they also use the voice assistant, phone camera, and internet search just like Jack.  70 
 
 
 
 
 
 
 
 
 
 
 
What  are the principles  of AI Ethics? 
AI Ethics Principles  
Identifying  the principles  
● To make  AI better,  we need  to identify  the factors 
responsible for it.  
● The following  principles  in AI Ethics  affect  the quality  of AI 
solutions  
▪ Human Rights  
▪ Bias 
▪ Privacy  
▪ Inclusion  
Let’s  look  at the AI Ethics  principles  in detail:  
Human  Rights  
● When  building  AI solutions,  we need  to ensure  that they  follow  human 
rights.  
● Here  are a few things  that you should  take  care  of 
▪ Does  your  AI take  away  Freedom?  
 
▪ Does  your  AI discriminate  against  People?  
 
▪ Does  your  AI deprive  people  of jobs?  
 
▪ What  are some  other  human  rights  which  need  to be protected  when  it comes  to AI? 
71 
 . Ask the learners  to recall  the discussion  on bias from  level  0Are  there  any biases  that  they  have?  
Ask learners  about  their  understanding  of privacy.  Are there  things  that  would  want  to keep  private  and 
not share with others?   
Bias 
● Bias (partiality  or preference  for one over  the other)  often  comes  from  the collected  data.  The 
bias in training data also appears in the results.  
● Here  are a few things  that you should  take  care  of 
▪ Does  your  data  equally  represent  all the sections 
of the included populations?  
▪ Will your  AI learn  to discriminate  against  certain 
groups of people?  
▪ Does  your  AI exclude  some  people?  
 
▪ What  are some  other  biases  that might  appear  in 
an AI?  
 
 
Privacy  
● We need  to have  rules  which  keep  our individual  and private  data  safe 
● Here  are a few things  that you should  take  care  of 
▪ Does  your  AI collect  personal  data  from  people?  
 
▪ What  does it  do with  the data?  
 
▪ Does  your  AI let people  know  about  the data  that it is collecting  for its use?  
 
▪ Will your  AI ensure  a person’s  safety?  Or will it compromise  it? 
 
▪ What  are some  other  ways  in which  AI can breach  someone’s  privacy?  
 
 Brief  learners  on basic  human  rights.  Ask them  some  rights  that they  enjoy  and what  are the other 
rights that they think they should have?  
72 
 
Ask learners,  “if they  have  felt excluded  from  any group.  How  does  it feel?  Why  does  exclusion  
happen  in the first place?”  
Let’s  discuss:  
1.Do you follow  some  ethics  in your  life? 
2.How  does AI  Ethics impact  us in  daily life? 
3.Can you think  of some  examples  for each  of the 4 AI Ethics 
principles – Human Rights, Bias, Privacy, Inclusion?  
Inclusion  
● AI MUST  NOT  discriminate  against  a particular  group  of population,  causing  them  any kind  of 
disadvantage.  
● Here  are a few things  you should  take  care  of 
▪ Does  your  AI leave  out any person  or a group?  
 
▪ Is a rich person  and a poor  person  benefitted  equally  from 
your AI?  
▪ How  easy  is it to use your  AI? 
 
▪ Who  does  your  AI help?  
 
▪ How  can we make  AI more  inclusive?  
 
 
 
 
 
 
73 
 Revision  Time  
1. The guiding principles to decide what is good or bad is known as  . 
2. When  building  AI solutions,  we need  to ensure  that they  follow   . 
3. Praneet  has taken  extra  packets  of mouth  freshener  after  dinner  from  a restaurant.  Is it considered  as 
theft?” Is it -Moral or Ethical concern?  
4. Rakshit  and Aman  are talking  about  purchasing  a new  mobile.  They  discuss  various  features  which 
they want in their mobile. Aman finds that, he started getting notification of various models of 
Mobiles that meets his requirement? Write which ethical concern the above example depicts.  
5. “Preference  for one over  the other”  is known  as  . 
6. Artificial  Intelligence  and machine  learning  systems  can display  unfair  behaviour  if not trained 
properly. (True/False)  
7. Search  for images  of personal  secretary  on Google,  displaying  predominantly  the images  of Women  is 
an example of  . 
8. An Ethical  AI framework  makes  sure  that transparency,  fairness  and accountability  is develop  into the 
systems to provide unbiased results. (True/False)  
 
 
Answer  the following:  
1. Differentiate  between  Ethics  and Moral  with  suitable  examples.  
2. Define  principles  of AI. 
3. Explain  Data  privacy.  
4. Craft  a description  of how  considerations  for inclusivity  are addressed  during  the development  of 
AI models.  
5. Write  Major  Issues  around  AI Ethics.  
6. A company had been working on a secret AI recruiting tool. The machine -learning specialists 
uncovered a big problem: their new recruiting engine did not like women chefs. The system 
taught itself that male candidates are preferable. It penalised resumes that included the word 
“women chef". This led to the failure of the tool.  
a. What  aspect  of AI ethics  is illustrated  in the given  scenario?  
b. What  could  be the possible  reasons  for the ethical  concern  identified?  
7. As Artificially Intelligent machines become more and more powerful, their ability to accomplish 
tedious tasks is becoming better. Hence, it is now that AI machines have started replacing 
humans in factories. While people see it in a negative way and say AI has the power to bring 
mass unemployment and one day, machines would enslave humans, on the other hand, other 
people say that machines are meant to ease our lives. If machines over take monotonous and 
tedious tasks, humans should upgrade their skills to remain their masters always.  
What  according  to you is a better  approach  towards  this ethical  concern?  Justify  your  answer.  74 
 
Unit  2 - Data  Literacy  
Unit  2.1 - Basics  of Data  Literacy  
Lesson  Title:  Basics  of Data  Literacy  Approach:  Session  + Activity  
Summary: In this module, students are familiarized with the concept of Data Literacy. Further, they 
would be able to recognize  the different  categories  of data  and will be introduced  to the culture  of data 
literacy.  
Learning Objectives  
● Define  data  literacy  and explain  its importance  with  a real-world  example  
● Relate  to the impact  created  by data  literacy  in everyday  life 
● Develop  awareness  about  personal  data,  data  privacy,  and data  security  
Learning  Outcomes  
● Define  data  literacy  and recognize  its importance  
● Understand  how  data  literacy  enables  informed  decision -making  and critical  thinking  
● Apply  the Data  Literacy  Process  Framework  to analyze  and interpret  data  effectively  
● Differentiate  between  data  privacy  and security  
● Identify  potential  risks  associated  with  data  breaches  and unauthorized  access.  
● Learn  measures  to protect  data  privacy  and enhance  data  security  
Pre-requisites:  Basic  knowledge  of AI and data  
 
Key-concepts  
● Understanding  of data  literacy  
● Identify  the difference  between  Quantitative  (Numerical)  and Qualitative  (Categorical)  Data  
● Impact  of data  literacy  with  the help  of case  studies  and scenarios  
● Best  practices  for Cyber  Security   
 
 
2.1.1 Introduction  to Data  Literacy  
Data literacy means knowing how to understand, work with, and talks about data. It's about  being  able 
to collect, analyze, and show data in ways that make sense.  
Reference  Video:  https://www.youtube.com/watch?v=yhO_t -c3yJY  
 
 75 
 
Data  Pyramid  is made  of different  stages  of working  with  data  
 
 
 
 
 
 
 
 
 
 
 
 
Let us understand  different  parts  of Data  pyramid 
Moving up from the bottom  
● Data  is available  in a raw form.  Data  in this form  is not very  useful.  
● Data  is processed  to give us information  about  the world.  
● Information  about  the world  leads  to knowledge  of how  things  are happening.  
● Wisdom  allows  us to understand  why  things  are happening  in a particular  way.  
 
 
Let’s  understand  Data  Pyramid  with  a simple  Traffic  Light  example:  
 
Rahul  rated  the 3 films  he watched  consecutively  as bad,  best  and average  respectively" 
Can you filter the data from this statement? Are they of the same type?  
 
 
 
 
76 
 
2.1.2 Impact  of Data  Literacy  
Activity:  Impact  of News  Articles  (Select  any trending  news)  
Session  Preparation  Logistics:  For a class  of 40 Students  [Pair  Activity]  
Materials  Required:  
ITEM  QUANTITY  
Online  Data  Sources  Clues  NA 
Computers  20 
 
 
Author of the Source  Weblink  to the Source  How  was the situation 
described by the Source  Key figures in the 
source  
    
    
    
    
 
You have  to rank  the sources  of the news  articles  from  most  accurate  to least,  state reasons for your 
choice.  
 
Rank  Data  Source  Remarks  
   
   
   
   
 
So, we can conclude  that every  data  tells a story,  but we must  be careful  before  believing  the story  
Data  literacy  is essential  because  it enables  individuals  to make  informed  decisions,  think  critically,  solve 
problems, and innovate.  77 
 
2.1.3 How  to become  Data  Literate?  
Every  data  tells a story,  but we must  be careful  before  believing  the story.  Data  Literate  is a person  who 
can interact with data to understand the world around them.  
Let’s  understand  it with  following  example:  
Scenario:  Buying  a Video  game  online  
Data  literacy  helps  people  research  about  products  while  shopping  over  the 
internet  
How  do you decide  the following  things  when  we are shopping  online?  
● Which  is the cheapest  product  available?  
● Which  product  is liked  by the users  the most?  
● Does  a particular  product  meet  all the requirements?  
 
A data  literate  person  can – 
● Filter  the category  as per the requirement  – If the budget  is low,  select  the price  filter  as low to high 
● Check  the user  ratings  of the products  
● Check  for specific  requirements  in the product  
 
Data  Literacy  Process  Framework  
The data  literacy  framework  provides  guidance  on using  data  efficiently  and with  all levels  of awareness. 
Data literacy framework is an iterative process.  
 
78 
 
2.1.4 What  are Data  Security  and Privacy?  How  are they  related  to 
AI? 
Data  Privacy  and Data  Security  are often  used  interchangeably  but they  are different  from  each  other.  
 
 
What  is Data  Privacy?  
Data privacy referred to as information privacy is concerned with the proper handling of sensitive data 
including personal data and other confidential data, such as certain financial data and intellectual 
property data, to meet regulatory requirements as well as protecting the confidentiality and 
immutability of the data.  
Here  are examples  of two things  which  may  compromise  our data  privacy  
 
 
 
Why  is it important?  
 
 
79 
 The following  best  practices  can help  you ensure  data  privacy:  
● Understanding  what  data,  you have  collected,  how  it is handled,  and where  it is stored.  
● Necessary  data  required  for a project  should  only  be collected.  
● User  consent  while  data  collection  must  be of utmost  importance.  
 
What  is Data  Security?  
Data security  is the practice  of protecting  digital  information  from 
unauthorized access, corruption, or theft throughout its entire lifecycle.  
 
Why  is it important?  
 
Due to the rising  amount  of data  in the cloud  there  is an increased  risk of 
cyber  threats.  The most  appropriate  step  for such  an amount  of traffic  being  generated  is how  we 
control and protect the transfer of sensitive or personal information at every known place.  
The most  possible  reasons  why data  security  is more  important  now  are: 
• Cyber -attacks  affect  all the people  
• The fast-technological  changes  will boom  cyber  attacks  
 
2.1.5 Best  Practices  for Cyber  Security  
Cyber  security  involves  protecting  computers,  servers,  mobile  devices, 
electronic systems, networks, and data from harmful attacks.  
Reference  Links:  
Video: https://www.youtube.com/watch?v=aO858HyFbKI  
 
CBSE Manual on Cyber Security: 
https://www.cbse.gov.in/cbsenew/documents/Cyber%20Safety.pdf  
 
 Do’s  
• Use strong,  unique  passwords  with  a mix of characters  for each  account.  
• Activate  Two -Factor  Authentication  (2FA)  for added  security.  
• Download  software  from  trusted  sources  and scan  files before  opening.  
• Prioritize  websites  with  "https://"  for secure  logins.  
• Keep  your  browser,  OS, and antivirus  updated  regularly.  
• Adjust  social  media  privacy  settings  for limited  visibility  to close  contacts.  
• Always  lock your  screen  when  away.  
• Connect  only  with  trusted  individuals  online.  
• Use secure  Wi-Fi networks.  
• Report  online  bullying  to a trusted  adult  immediately.  
80 
 Don’t ‘s 
 
• Avoid  sharing  personal  info like real name  or phone  number.  
• Don't  send  pictures  to strangers  or post  them  on social  media.  
• Don't  open  emails  or attachments  from  unknown  sources.  
• Ignore  suspicious  requests  for personal  info like bank  account  details.  
• Keep  passwords  and security  questions  private.  
• Don't  copy  copyrighted  software  without  permission.  
• Avoid  cyberbullying  or using  offensive  language  online.  
 
Revision  Time:  
1. Cultivating  Data  Literacy  means:  
a) Utilize  vocabulary  and analytical  skills  
b) Acquire,  develop,  and improve  data  literacy  skills  
c) Develop  skills  in statistical  methodologies  
d) Develop  skills  in Math  
 
2. Data  Privacy  and Data  Security  are often  used  interchangeably  but they  are different  from  each  other  
a) True  
b) False  
 
3. The provides  guidance  on using  data  efficiently  and with  all levels  of awareness.  
a) data  security  framework  
b) data  literacy  framework  
c) data  privacy  framework  
d) data  acquisition  framework  
 
4.  allows  us to understand  why  things  are happening  in a particular  way 
a) data  
b) information  
c) knowledge  
d) wisdom  
 
5.   is the practice of protecting digital information from unauthorized  access,  corruption,  or theft 
throughout its entire lifecycle.  
a) data  security  
b) data  literacy  
c) data  privacy  
d) data  acquisition  
81 
 
Purpose:  
The purpose  of this activity  is to engage  participants  in acquiring  data  from  online  sources.  The ability 
to locate and access relevant data sources is crucial for AI Projects.  
Brief: [Pair Activity] Participants will be locating an online dataset suitable for training an AI model. 
They  will conduct  a search  for weather  forecast  related  datasets  on various  online  platforms  and then 
paste images or screenshots of the datasets found.  2.2 Acquiring  Data,  Processing,  and Interpreting  Data  
 
Lesson  Title:  Acquiring  Data,  Processing,  and Interpreting  Data  Approach:  Session  + Activity  
Summary:  You will get an understanding  of data  processing,  data  interpretation  and keywords  related 
to data.  
Learning Objectives  
● Familiarizing  youth  with  different  data  terminologies  like data  acquisition,  processing, analysis, 
presentation, and interpretation  
● Discussing  different  methods  of data  interpretation  like qualitative  and quantitative.  
● Understanding  the methods  and different  collection  techniques  
● Critically  think  about  their  advantages  and disadvantages  
● Identifying  various  data  presentation  methods  with  examples  and interpreting  them  
● Gain  awareness  about  the advantages  and impact  of Data  interpretation  on business  growth  
Learning  Outcomes  
● Determine  the best  methods  to acquire  data.  
● Classify  different  types  of data  and enlist  different  methodologies  to acquire  it. 
● Define  and describe  data  interpretation.  
● Enlist  and explain  the different  methods  of data  interpretation.  
● Recognize  the types  of data  interpretation.  
● Realize  the importance  of data  interpretation  
● Pre-requisites:  Acquaintance  with  data  and its different  types.  
Key-concepts  
● Familiarizing  with  different  data  terminologies  like data  processing,  analysis,  presentation,  and 
interpretation  
● Quantitative  and Qualitative  Data  Interpretation  
● Types  of Data  Interpretation  – Textual,  Tabular  and Graphical  with  examples.  
Activity  
Session  Preparation  Logistics:  For a class  of 40 Students  [Pair  Activity]  
Materials  Required:  
ITEM  QUANTITY  
Online  Data  Sources  Clues  NA 
Computers  20 
 82 
 
 
 
2.2.1.  Types  of data  
Artificial  Intelligence  is crucial,  with  data  serving  as its foundation.  We come  across  different types of 
information every day. Some common types of data include:  
 
 
Textual  Data  (Qualitative  Data)  Numeric  Data  (Quantitative  Data)  
 
● It is made  up of words  and phrases  
● It is used  for Natural  Language  Processing  (NLP)  
● Search  queries  on the internet  are an example 
of textual data  
● Example:  “Which  is a good  park  nearby?”   
● It is made up of numbers  
● It is used  for Statistical  Data  
● Any measurements,  readings,  or values 
would count as numeric data  
● Example:  Cricket  Score,  Restaurant  Bill 
 
Numeric  Data  is further  classified  as: 
● Continuous  data  is numeric  data  that is continuous.  E.g.,  height,  weight,  temperature,  voltage  
● Discrete  data  is numeric  data  that contains  only  whole  numbers  and cannot  be fractional  
E.g. the number  of students  in the class  – it can only  be a whole  number,  not in decimals  
83 
 
Types  of Data  used  in three  domains  of AI: 
 
 
 
2.2.2 Data  Acquisition/Acquiring  Data  
Data Acquisition, also known as acquiring data, refers to the procedure of gathering data. This involves 
searching for datasets suitable for training AI models. The process typically comprises three key steps:  
 
 84 
 
Acquiring  Data  – Sample  Data  Discovery  
Let’s  say we want  to collect  data  for making  a CV model 
for a self -driving car  
● We will require  pictures  of roads  and the objects  on 
roads  
● We can search  and download  this data  from  the 
internet  
● This process  is called  data  discovery  
 
Acquiring  Data  – Sample  Data  Augmentation  
● Data  augmentation  means  increasing  the amount  of data 
by adding copies of existing data with small changes  
● The image given here does not change, but we get data 
on the image  by changing  different  parameters  like color 
and brightness  
● New  data  is added  by slightly  changing  the existing  data  
 
Acquiring  Data  – Sample  Data  Generation  
● Data  generation  refers  to generating  or 
recording data using sensors  
● Recording  temperature  readings  of a building 
is an example of data generation  
● Recorded  data  is stored  in a computer  in a 
suitable form  
 
Sources  of Data  
Various  Sources  for Acquiring  Data:  
● Primary  Data  Sources  — Some  of the sources  for primary  data  include  surveys,  interviews, 
experiments, etc. The data generated from the experiment is an example of primary data.  
Here  is an excel  sheet  showing  the data  collected  for students  of a class.  
 
85 
 
● Secondary  Data  Sources —Secondary  data  collection  obtains  information  from  external  sources, 
rather than generating it personally. Some sources for secondary data collection include:  
 
 
2.2.3 Best Practices for Acquiring Data 
Checklist  of factors  that  make  data  good  or bad 
 
Data  acquisition  from  websites  
 
 86 
 
Ethical  concerns  in data  acquisition  
While  gathering  data  and choosing  datasets,  certain  ethical  issues  can be addressed  before  they  occur  
 
 
2.2.4 Features  of Data  and Data  Preprocessing  
Usability  of Data  
There  are three  primary  factors  determining  the usability  of data:  
1. Structure - Defines  how  data  is stored.  
 
2. Cleanliness - Clean data is free from duplicates, missing values, outliers, and other anomalies that 
may  affect  its reliability  and usefulness  for analysis.  In this particular  example,  duplicate  values  are 
removed after cleaning the data.  
 
87 
 
3. Accuracy - Accuracy indicates how well the data matches real-world values, ensuring reliability. 
Accurate  data  closely  reflects  actual  values  without  errors,  enhancing  the quality  and trustworthiness 
of the dataset.  
In this particular  example,  we are comparing  data  gathered  from  measuring  the length  of a small  box 
in centimeters.  
 
 
Kaggle  assigns  a usability  score  to the data  sets that are present  on the website  based  on scores 
given by the users of that data.  
 
What  kind  of data  is more  usable,  according  to you?  
 
 
 
 
If we have  a lot of data  which  is not clean,  is it good  for AI? 
 
 
 
 
 
Features  of Data  
Data features are the characteristics or properties of the data. They describe each piece of information 
in a dataset. For example, in a table of student records, features could include things like the student's 
name, age, or grade. In a photo dataset, features might be the colors present in each image. These 
features help us understand and analyze the data.  
 
In AI models,  we need  two types  of features:  independent  and dependent.  88 
 
 
 
Independent  features  are the input  to the model —they're  the information  we provide  to make 
predictions.  
 
Dependent  features , on the other  hand,  are the outputs  or results  of the model —they're what we're 
trying to predict.  
 
89 
 
2.2.5 Data  Processing  and Data  Interpretation  
Data  processing  and interpretation  have  become  very  important  in today’s  world  
Can you answer  this?  
▪ Niki has 7 candies, and  Ruchi has 4 candies  
▪ How  many  candies  do Niki and Ruchi  have  in total?  
▪ We can answer  this question  using  data  processing  
▪ Who  should  get more  candies  so that both  Niki and Ruchi 
have an equal number of candies?  
▪ How  many  candies  should  they  get? 
▪ We can answer  this question  using  data  interpretation  
 
 
Data  Processing  
▪ Data  processing  helps  computers  understand  raw data.  
▪ Use of computers  to perform  different  operations  on data  is 
included under data processing.  
Data  Interpretation  
▪ It is the process  of making  sense  out of data  that has been 
processed.  
▪ The interpretation  of data  helps  us answer  critical  questions 
using data.  
90 
 
Understanding  some  keywords  related  to Data  
Acquire  Data - Acquiring  data  is to collect  data  from  various  data 
sources.  
 
Data  Processing - After  raw data  is collected,  data  is processed  to derive  meaningful 
information from it.  
 
Data  Analysis  – Data  analysis  is to examine  each  component  of the data 
in order to draw conclusions.  
 
Data  Interpretation  – It is to be able  to explain  what  these 
findings/conclusions mean in a given context.  
 
Data  Presentation - In this step,  you select,  organize,  and group  ideas  and 
evidence in a logical way.  
 
 
Methods  of Data  Interpretation  
How  to interpret  Data?  
 
Based  on the two types  of data,  there  are two ways  to interpret  data - 
● Quantitative  Data  Interpretation  
● Qualitative  Data  Interpretation  
 
91 
 
Purpose : 
This activity  will engage  youth  with  longitudinal  studies  – a study  conducted  over  a considerable 
amount of time to identify trends and patterns  
The ability to identify trends and patterns in datasets allows us to make informed decisions about 
future outcomes, predict potential challenges, and develop effective strategies for addressing issues 
based on evidence and historical data.  Qualitative  Data  Interpretation  
● Qualitative  data  tells us about  the emotions 
and feelings of people  
● Qualitative  data  interpretation  is focused  on 
insights and motivations of people  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Data  Collection  Methods  – Qualitative  Data  Interpretation  
Record  keeping:  This method  uses  existing  reliable  documents  and other  similar  sources  of information 
as the data source. It is similar to going to a library.  
Observation:  In this method,  the participant  – their  behavior  and emotions  – are observed  carefully 
Case Studies: In this method, data is collected from case studies.  
Focus  groups:  In this method,  data  is collected  from  a group  discussion  on relevant  topic.  
Longitudinal  Studies:  This data  collection  method  is performed  on the same  data  source  repeatedly  over 
an extended period.  
One-to-One Interviews:  In this method,  data  is collected  using  a one-to-one interview.  
 
Activity  – Trend  Analysis  
 
92 
 
Activity Guidelines  
Let’s  do a small  activity  based  on Identifying  trends.  
● Visit  the link:  https://trends.google.com/trends/?geo=IN  (Google  Trends)  
● Explore  the website  
● Check  what  is trending  in the year  2022  – Global  
▪ Make  a list of trending  sports  (top 5) 
▪ Make  a list of trending  movies  (top 5) 
● Check  what  is trending  globally  in the year  2022 
 
 
5 Steps  to Qualitative  Data  Analysis  
1. Collect  Data  
2. Organize  
3. Set a code  to the Data  Collected  
4. Analyze  your  data  
5. Reporting  
Quantitative  Data  Interpretation  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
▪ Quantitative  data  interpretation  is made  on numerical  data  
▪ It helps  us answer  questions  like “when,”  “how  many,”  and “how  often”  
▪ For example  – (how  many)  numbers  of likes  on the Instagram  post  
93 
 
Data  Collection  Methods  -Quantitative  Data  Interpretation  
Interviews:  Quantitative  interviews  play a key role in collecting  information.  
Polls:  A poll is a type  of survey  that asks  simple  questions  to respondents.  Polls  are usually  limited  to one 
question.  
Observations:  Quantitative  data  can be collected  through  observations  in a particular  time  period  
Longitudinal  Studies:  A type  of study  conducted  over  a long  time  
Survey:  Surveys  can be conducted  for a large  number  of people  to collect  quantitative  data.  
 
4 Steps  to Quantitative  Data  Analysis  
1. Relate  measurement  scales  with  variables  
2. Connect  descriptive  statistics  with  data  
3. Decide  a measurement  scale  
4. Represent  data  in an appropriate  format  
Let’s  summarize  Qualitative  and Quantitative  data  interpretation  
Qualitative  & Quantitative  Data  Interpretation  
 
Qualitative  Data  Interpretation  Quantitative  Data  Interpretation  
Categorical  Numerical  
Provides  insights  into feelings  and emotions  Provides  insights  into quantity  
Answers  how  and why Answers  when,  how  many  or how  often  
Methods  – Interviews,  Focus  Groups  Methods  – Assessment,  Tests,  Polls,  Surveys  
Example  question  – Why  do students  like 
attending online classes?  Example  question  – How  many  students 
like attending online classes?  
 
Types  of Data  Interpretation  
There  are three  ways  in which  data  can be presented:  
 
 
Textual  DI 
▪ The data  is mentioned  in the text form,  usually  in a paragraph.  
▪ Used  when  the data  is not large  and can be easily  comprehended  by reading.  
▪ Textual  presentation  is not suitable  for large  data.  
▪ Example:  94 
 
Tabular  DI 
▪ Data  is represented  systematically  in the form  of rows  and columns.  
▪ Title  of the Table  (Item  of Expenditure)  contains  the description  of the table  content.  
▪ Column  Headings  (Year;  Salary;  Fuel  and Transport;  Bonus;  Interest  on Loans;  Taxes)  contains  the 
description of information contained in columns.  
Graphical  DI 
Bar Graphs  
In a Bar Graph,  data  is represented  using  vertical  and horizontal  bars.  
 
 
Pie Charts  
▪ Pie Charts  have  the shape  of a pie and each  slice  of the pie represents  the portion  of the entire 
pie allocated to each category  
▪ It is a circular  chart  divided  into various  sections  (think  of a cake  cut into slices)  
▪ Each  section  of the pie chart  is proportional  to the corresponding  value  
 95 
 
Duration:  40 Minutes  
Purpose  
 
 
This activity  will engage  youth  with  data  visualization  and interpretation  
visualization  makes  it easier  for us to extract  useful  information  contained  in the dataset  
Line  Graphs  
▪ A line graph  is created  by connecting  various  data  points.  
▪ It shows  the change  in quantity  over  time.  
 
Activity:  Visualize  and Interpret  Data  
 
Activity  Guidelines  
● The table  shows  the details  of a class  consisting  of 50 students  and their  scores  ranging  in the listed 
categories for 5 subjects: Math, Physics, Chemistry, Social Science, and Biology  
 
Student  Performance  
Marks  Range  Math  Physics  Chemistry  Social Science  Biology  
Less than 20 6 3 1 0 0 
Between  20-29 14 11 9 15 8 
Between  30-40 17 20 21 22 19 
Between  41-44 8 10 14 10 16 
45 and Above  5 6 5 3 7 
Total  Students  50 50 50 50 50 
 
● Copy  the table  in an Excel  sheet  and create  the following  visualizations  for the given  data:  
▪ Make  a bar graph  showing  the marks  distribution  for all 5 subjects  
▪ Make  a pie chart  showing  the marks  distribution  for Physics  
▪ Make  a line chart  displaying  the marks  distribution  for Chemistry  96 
 
Brief:  
The following are questions for the quiz. You can either go for a Pen/Paper Quiz or you can visit any 
open -sourced,  free,  online  portal;  one of which  is Kahoot,  and create  your  quiz there.  For Kahoot:  Go to 
https://kahoot.com/  and create  your  login  ID on it. Then,  add your  own  kahoot  in it simply  by adding  all 
the given questions into it. Once created, you can initiate the quiz from your ID and students can 
participate in it by putting in the Game pin.  Importance  of Data  Interpretation  
 
 
 
 
Quiz Time: AI Quiz 
Session  Preparation  
Logistics:  For a class  of 40 Students  [Pair  Activity] 
Materials Required:  
ITEMS  QUANTITY  
COMPUTERS  20 
 
Quiz Questions  
1. What  are the basic  building  blocks  of qualitative  data?  
a. Individuals  
b. Units  
c. Categories  
d. Measurements  97 
 2. Which  among  these is  not a type  of data interpretation?  
a. Textual  
b. Tabular  
c. Graphical  
d. Raw  data  
3. Quantitative  data  is numerical  in nature.  
a. True  
b. False  
4. A Bar Graph  is an example  of? 
a. Textual  
b. Tabular  
c. Graphical  
d. None  of the above  
5.   relates to the manipulation of data to produce meaningful insights.  
a. Data  Processing  
b. Data  Interpretation  
c. Data  Analysis  
d. Data  Presentation  
2.3 Project  Interactive  Data  Dashboard  & Presentation  
 
Lesson  Title:  Project  Interactive  Data  Dashboard  and 
Presentation  Approach:  Session  + Activity  
Summary: In this module,  you will reflect  on your  learnings  from  the previous  units  till learnt. 
You will further engage in an activity  on data  collection  and data  visualization  using  the visual 
data analytics platform, Tableau.  
Learning Objectives  
● Demonstrating  comprehension  and retention  of learnings  from  previous  units  
● Apply  acquired  knowledge  to select  and employ  appropriate  data  visualization  methods  
Learning  Outcomes  
● Summarize  the topics  learned  previously  
● Recognize  the importance  of data  visualization  
● Discover  different  methods  of data  visualization  
Pre-requisites:  
● Meet  the learning  outcomes  of units  till learnt  
● Basic  computer  skills.  
Key-concepts  
● Mapping  AI Project  Cycle.  
● Data  Literacy.  
● Sources  of data.  
● Data  acquisition.  
● Usability of data.  
● Data  processing  and interpretation.  
● Data  visualization  using  Tableau.  98 
 
Purpose:  
 
To initiate  the concept  of data  collection  
Material  required: 
Paper,  Pen, A partner!  
Icebreaker  Activity  
Tic-Tac-Toe 
 
 
Instructions  
 
▪ Partner  with  a person  to play the game.  
▪ There  will be three  rounds  of tic-tac-toe. Take  a piece  of paper  and draw  three  tic-tac-toe tables.  
▪ Play three  rounds  of tic-tac-toe. 
▪ After  3 rounds,  answer  the questions  given  on the next  slide.  
Now  answer  the following  questions  
 
▪ Who  won  round  one?  
▪ Who  won  round  two?  
▪ Who  won  round  three?  
▪ How  many  X’s were  used  in each  round?  
▪ How  many  O’s were  used  in each  round?  
If you answered  any of the above  questions,  you collected  data! 
Activity  
Data  Visualization  Using  Tableau  
Your  favorite  songs  
● Think  about  songs!  Which  songs  do you listen  to? Which  songs  do you sing? 
● Do you have a favorite  song,  artist,  album,  or playlist?  
● Let's start thinking  about  the different  aspects  of a song,  like instruments  and lyrics.  
● Do your favorite  songs  have anything  in common?  99 
 
▪ Maybe  your  favorite  music  falls within  the same  genre.  
▪ A genre  refers  to the different  styles  of music.  
▪ Common  genres  include  hip-hop,  pop,  alternative,  and rock.  
▪ Classifying  songs  by genre,  and other  traits  allows  us to see trends  in our favorite  music.  
▪ All of this information  is valuable  data  that we can count,  summarize,  and present!  
Instructions  
● Draw  a grid with  6 columns  as shown.  
● Title  the first column  Song  Name,  then  write  down  the names  of 5-10 of your  favorite  songs  
● For this activity,  we're  going  to collect  data  about  the Album,  Artist,  Genre,  Year,  and Song  Length.  
● Add those  headings  to your  table.  
● Fill out the table  by looking  up each song on Google,  Spotify,  or Apple  Music.  
 
 
Let’s  visualize  
● Count  the number  of songs  that fall into each  genre.  
● Make a bar chart to visualize  the number  of songs  within  each  genre  using  your  counting.  Color  each 
bar a different color.  
● You will get a graph  as shown  in the image.  
● Looking  at the data  visualization,  can you tell which  genre  has the most  songs?  
 
 
 
Let’s  see how  Tableau  makes  it faster  and easier  for us to present  data  
Instructions  
▪ Download  Tableau  public  with  the help  of an adult  using  this link - 
https://public.tableau.com/en -us/s/download  100 
 
▪ Install  the package  via the install  wizard.  
 
 
 
 
▪ Once  installed,  double  click  the program  to open  the Tableau  Public  Desktop  application.  
 
 
▪ Once  open,  this is what  you should see. 
 
 
▪ Now  we are ready  to pull in our data! 
▪ If you haven't  already,  make  sure  to enter  all of your  song  data  into the "Song  Data"  Excel  template 
provided.  
 101 
 
 
▪ To pull in the data,  click  on Microsoft  Excel  in the top left corner.  
 
▪ Now  drag  the sheet  with  your  data  to Drag  tables  here  section.  
 
▪ First,  let's recreate  the bar chart  we made  to visualize  the number  of songs  per genre!  
▪ Click  Sheet1  in the bottom  left corner  of the screen  
 
102 
 
▪ Hover  over  the word  “Genre”.  You will notice  a blue  oval appear  behind  it. 
 
▪ Click  and drag  “Genre”  up and to the right,  releasing  it next  to the word  Columns  when  a little 
orange arrow appears.  
 
 
▪ Hover  over  the word  “Genre”.  You will notice  a blue  oval appear  behind  it. 
 
▪ Click  and drag  “Genre”  up and to the right,  releasing  it next  to the word  Columns  when  a little  orange 
arrow appears.  
 
 
▪ Now  drag  “Sample  (Count)”  to Rows,  following  the same  steps  as above.  
 
▪ “Sample  (Count)”  represents  the total  number  of songs  in your  table.  
 103 
 
▪ Tableau  made  us a bar graph!  
 
 
▪ What  if you want  to make  each  bar a different  color?  
 
▪ Simply  click  and drag  “Genre”  out to where  it says  Color.  
 
 
▪ Tableau  colored  our genres  for us! 
 104 
 
Let’s  explore  another  way of visualization  
▪ First,  we'll  start  by duplicating  our current  bar chart  sheet.  This will create  an exact  copy  in a new 
sheet.  
▪ You'll  do this by right  clicking  "Sheet  1" and selecting  "Duplicate" . 
 
 
▪ In the upper  right  corner,  click  "Show  Me”. 
▪ will see all of the different  types  of visualizations  that Tableau  can create  using  Genre  and Sheet 
Count 1. Select “ Packed Bubbles ”. 
 
▪ Tableau  quickly  transformed  our bar chart  to a chart  of bubbles.  
▪ Pop genre  is the most  popular  because  it is the biggest  circle.  
 
▪ We can make  the text a little  more  fun and easier  to read. 
105 
 
▪ To do that,  click  the label  square.  
 
 
 
▪ This opens  up a box that allows  us to change  the font and text size. 
▪ Let's  change  the font size to 12 and the font to "Chalkboard".  
 106 
 
▪ We have our complete  bubble  chart  now!  
 
Useful  Videos  to watch  
▪ https://www.youtube.com/watch?v=NLCzpPRCc7U  
▪ https://www.youtube.com/watch?v=_M8BnosAD78  
Note:  You may  also use Ms Excel  or Datawrapper  (https://www.datawrapper.de/ ) for the data 
visualization instead of Tableau.  
Revision  Time:  
1. At which  stage  of the AI project  cycle  does  Tableau  software  prove  useful?  
2. Name  any five graphs  that can be made  using  Tableau  software  
3. In the below  excel  sheet - 
 
▪ Is the Year  qualitative  or quantitative?  
▪ Is Song  Length  discrete  or continuous?  
▪ Is the Genre  discrete  or continuous?  
4. What  is the importance  of data  visualization?  
107 
 Activity 1: 
Purpose : observing  and analyzing  the numbers  & Find  the pattern.  
▪ Find  the missing  number  in the following  series: 
2, 4, 6, 8, 10, 12, ?  
4, 10, 16, 22, 28, ? 
34, 31, 28, 25, 22, ? 
▪ If Year  1 Profit  was INR 1000;  Year  2 Profit  was INR 1500;  Year  3 Profit  was INR 2000;  Year  4 
Profit was INR 2500, can you predict the profit for Year 5?  
Ask the learners  
● “How  did you solve  these  puzzles?”  
● “Was  there  any pattern  that you recognized  which  could  help  you solve  the puzzles?”  Unit  3: Math  for AI (Statistics  & Probability)  
3.1 Importance  of Math  for AI 
 
Title:  Math  for AI Approach:  Interactive  Session  + Activity  
Learning objectives:  
▪ Discuss  the applications  of Mathematics  in AI. 
▪ To know  the different  mathematical  concepts  important  for understanding  AI? 
▪ How  are statistics  and probability  used  in different  AI applications?  
Summary:  In this chapter, Students are introduced to the mathematics required for designing an AI 
project. They will know about the essential mathematical concepts required to understand an AI 
project from the basics. They will be introduced to mathematical concepts of linear algebra, calculus, 
statistics, and probability  through  easy  activities  and examples.  Learners  will also be able  to identify  the 
use of statistics and probability in everyday life.  
Learning Outcomes:  
▪ Students  will be able  to understand  the importance  of mathematics  in the field  of AI. 
▪ Students  will be able  to identify  the essential  mathematical  concepts  required  for the 
understanding of A  
▪ Students  will be able  to define  statistics  and probability  and describe  their  applications  in AI 
Pre-requisites : 
▪ Basic  mathematical  knowledge  and analytical  ability  
▪ Basic  familiarity  with  AI 
Key- Concepts:  
▪ Important  mathematical  concepts  in AI 
▪ Introduction  to statistics  and probability  
 
 
108 
 
How  are Math  and AI related?  
Math  is the study  of patterns  
▪ To solve  the puzzles,  you identify  an order/arrangement  in the list of numbers  or the images.  
▪ This arrangement  is called  a pattern.  
▪ These  patterns  exist  all around  us. 
▪ We have  patterns  in numbers,  images,  and language.  
 
Ask learners  if they  can identify  any patterns  around  themselves.  
 
AI is a way to recognize  patterns  
● AI can learn  to recognize  patterns,  like human  beings.  
● AI can see patterns  in different  types  of data  - numbers,  images,  and speech  and text.  
● These  patterns  help  AI to solve  puzzles  – like identifying  dogs  and muffins,  or predicting 
hurricanes!  109 
 
Activity 3: 
Purpose:  To find connections  between  sets of images  and using  that to solve  problems,  think  smartly, 
and grasp tricky ideas.  
 
Complete  the sequence  in the left column  by identifying  the 
correct missing piece in the right column out of A or B.   
Hence,  
▪ Math  is the study  of patterns  
▪ AI is a way to recognize  patterns  in order  to take  decisions  
▪ AI needs  Math  to study  and recognize  patterns  in order  to take  decisions  
Can you identify  any pattern  in the image  given  below?  
 
 
 
Understanding  math  will help  us to better  understand  AI and its way of working,  but what  kind  of math 
is needed for AI?  
Let us take  a look!  Say “Just  like we can recognize  patterns  in numbers,  words,  pictures,  etc.,  AI can also recognize 
similar patterns.”  110 
 
Essential  Mathematics  for AI 
 
Let’s  think  and answer  the following  questions:  
▪ 11, 22, 33, 44, 55 – Can you find  out the middle  value  from  the given numbers?  
 
 
▪ In the given  figure,  which  of the two lines  is more  slanted?  Line 1 or Line 2? 
 
 
 
 
▪ A has 2 plants,  B has 3 plants,  C has 1 plant,  D has 7 plants.  How  many  plants  are there  in total?  
 
 
 
▪ If the coin  shown  in the figure  below  is used  for a toss,  what  can be the possible  result?  
 
Just like us, AI can also solve  4 type  of problems  using  Math.  111 
 Ask learners  to answer  some  or all of these  questions  as an assignment.  Meanwhile,  take  dummy 
numbers and walk the learners through the questions.  
● Can you find out the total  weight  of your  family  members?  
● Can you find out the total  number  of students  in your  school?  
● Can you find out the maximum  temperature  in your  city during  the last month?  
Activity  4: 
Purpose:  Uses  of Statistics  in real life. 
Write  any two applications  of Statistics  in real life. AI uses  Math  for: 
▪ Statistics  (Exploring  data ): Example  – What  is the middle  value  of the data?  Which  is the most 
common value in the data?  
▪ Calculus  (training  and improving  AI model ): Example  – which  line is more  slanted?  Which  figure 
covers more area?  
▪ Linear  Algebra  (finding  out unknown  or missing  values ): Example  – How  many  plants  are there 
in total? How many cars are there in a city?  
▪ Probability  (predicting  different  events ): Example  – what  will be the possible  results  of a coin 
toss? Will it rain tomorrow?  
3.2 Statistics  
 
Definition  of Statistics :” Statistics  is used  for collecting,  exploring,  and analyzing  the data.  It also helps 
in drawing conclusions from data.”  
▪ Data  is collected  from  various  sources.  
▪ Data  is explored  and cleaned  to be used.  
▪ Analysis  of data  is done  to understand  it better.  
▪ Conclusions  and decisions  can be  made  from  the data.  
Applications  of Statistics:  
▪ Predict  the performance  of sports  teams  
▪ It can be used  to find out specific  things  such  as: 
o the reading  level  of students  
o the opinions  of voters  
o the average  weight  of a city’s  resident  
 
Some  more  applications  of Statistics 
Disaster Management  
▪ Authorities  use statistics  to alert  the citizens  residing  in places  that might  be affected  by a natural 
disaster in near future.  112 
 
▪ The disaster  management  teams  use statistics  to know  about  the population,  and about  the 
services and infrastructure present in the affected area.  
 
Ask students  to think  about  more  ways  in which  statistics  can be used  for disaster  management . 
Sports  
▪ The Tokyo  2020  Olympics  were  postponed  due to the developing  global  situation  in light  of the 
Covid -19 pandemic.  
▪ Statistics  revealed  that COVID  cases  sharply  increased  in Japan  during  the planned  period  of 
Olympics.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Ask learners  to think  of more  ways  in which  statistics  can be used  in sports.  
 
Disease  prediction  
▪ US government  uses  statistics  to understand  which  disease  is affecting  the population  the most.  
113 
 
▪ This helps  them  in curing  these  diseases  more effectively.  
▪ Example  - government  can analyze  the areas  where  COVID  cases  are increasing,  or where  the 
vaccination drive needs to be improved.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Weather  forecast  
▪ Computers  use statistics  to forecast  weather.  
 
▪ They  compare  the weather  conditions  with  the information  about  past  seasons  and conditions.  
 114 
 
Activity  5: Car Spotting  and Tabulating  
Purpose:  To implement  the concept  of data  collection,  analysis  and interpretation. 
Activity Introduction:  
In this activity,  youth  will engage  in data  collection  and tabulation.  
Data  collection  plays  a key role in Artificial  Intelligence  as it forms  the basis  of statistics  and 
interpretation by AI.  
This activity  will also require  youth  to answer  a set of questions  based  on the recorded  data.  
Few  more  facts  
• Kids watch  around  1.5-3 hours  of TV per day while  being  in childcare.  
• 72% of teens often (or sometimes) check for messages or notifications as  soon  as they  wake  up, 
while roughly four -in-ten feel anxious when they do not have their cellphone with them.  
• 77%  of children  don’t  get enough  physical  exercise.  
• Almost  a quarter  (23%)  of children  aged  five to 16 believe  that playing a computer game with 
friends is a form of exercise.  
• 69%  of all children  experience  one or more  sleep -related  problems  at least  a few nights  a week.  
• Only  54%  of US children  aged  3 to 5 years  attend  full-day preschool  programs.  
• At least  264 million  children  worldwide  (about  12%)  don’t  go to school.  
 
 
Activity Guidelines  
Data  Collection  
● Visit the following link: 
https://www.youtube.com/watch?v=4A5L3x3TVuc&ab_channel=CarvingCanyons  
● Fill the table  while  watching  the video  using  tally.  
 
 
Reference  Tally  115 
 
Data  Analysis  
● How  many  cars are spotted  in total?  
 
 
● Which  colour  has been  spotted  the maximum  amount  of time?  
 
Data  Interpretation  
 
● What  is the most  common  colour  choice  for the residents  of this area?  
 
● Answer  hint:  The colour  observed  the maximum  number  of times.  
 
3.3 Probability  
Purpose:  To understand  the possibility  of occurrence  of an event.  
 
 
Introduction  to probability  
Probability  is a way to tell us how  likely  something  is to happen.  For example  – When  a coin  is tossed, 
there are two possible results or outcomes:  
heads  (H) or tails (T) 
The probability  equation  defines  the likelihood  of the happening  of an event.  It is the ratio  of favorable 
outcomes to the total favorable outcomes. The probability formula can be expressed as,  
 
Probability  of an Event  = 
Number  of Favorable  Outcomes  / Total  Number  of Possible  Outcomes  
We say that the probability of the coin landing H is ½ and the probability of the coin landing T is ½ 
When  we talk about  probability,  we use a few terms  that help  us understand  the chances  for something 
to happen.  116 
 
Probability  can be expressed  in the following  ways:  
▪ Certain  events:  An event  will happen  without  a doubt  
▪ Likely  events:  The probability  of one event  is higher  than  the probability  of another  event  
▪ Unlikely  events:  One event  is less likely  to happen  than  another  event  
▪ Impossible  events:  There's  no chance  of an event  happening  
▪ Equal  Probability  events:  Chances  of each  event  happening  is same  
The probability  of an event  occurring  is somewhere  between  impossible  and certain.  
• If an event  is certain  or sure  to happen,  it will have  a probability  of 1. 
For example,  the probability  that it will rain in the state  of Florida  at least  once  in a specific  year  is 1. 
• If an event  will never  happen  or is impossible,  it will have  a probability  of 0. 
For example,  the probability  that you can pick a red ball from  a bag containing  only  blue  balls  is 0. 
 
Imagine you have a bag full of stars where 7 stars are   and 3 stars  are  
Try to fill in the blanks with – likely, unlikely, certainly, impossible, equal probability  
1. If you pick a star from the bag without looking, it is  that you will pick  . 
 
2. If you pick a star from the bag without looking, it is  that you will pick a  . 
3. If you pick a star from the bag without looking, it is  that you will pick a  . 
4. If you remove  4  from  the bag,  and pick a star without  looking,  there  is an   
that you will pick either   or  . 
117 
 
5. If you pick an object from the bag without looking, you will  pick a star.  
Let’s  try to understand  the concept  of Probability  using  a relatable  example. 
Consider a relatable scenario!  
You want  to go to your  best  friend's  birthday  party  next  Saturday.  Your  parents  decide  to make  a deal 
with you.  
Scenario  1 
 
 
 
Scenario  2 
 
 
 118 
 
Scenario  3 
 
 
 
 
 
 
Scenario  4 
 
Hope  the terms  impossible,  unlikely,  even,  likely  and certain  are clearer  now! 
Moving on, take a look at some applications of Probability in Real Life!  119 
 s 
y Probability  - Applications 
Sports  
▪ Probability  can be used  in estimating  batting  average  in Cricket.  
 
▪ Batting  average  in Cricket  represents  how  many  runs  a batsman 
would score before getting out.  
▪ For instance,  if a batsman  had scored  45 runs  out of 100 from  
only boundaries in the last match. Then, there is a chance that 
he will score  45%  of his runs  in the next  match  from  boundaries.  
 
Weather  Forecasting  
▪ One of the most  common  real-life examples  of using 
probability is weather forecasting.  
▪ It is used  by weather  forecasters  to assess  how  likely  it i 
that there  will be rain,  snow,  clouds,  etc.,  on a given  da 
in a certain area.  
▪ Forecasters  may  say things  like “there  is a 70%  chance  of 
rain today between 4 PM and 6 PM” to indicate a 
medium  to high  likelihood  of rain during  certain  hours.  
 
 
Traffic  Estimation  
▪ Regular  people  often  use probability  when  they  decide  to drive  to 
someplace.  
▪ Based  on the time  of day,  location  in the city, weather  conditions,  
etc. people  tend  to make  probability  predictions  about  how  bad 
traffic will be during a certain time.  
▪ For example,  if you think  there’s  a 90%  probability  that traffic  will 
be heavy  from  6 PM to 7:30  PM in your  vicinity  then  you may 
decide to wait during that time.  
Let’s  discuss  
1. Does  math  play a crucial  role in AI life cycle?  
 
2. What  is statistics?  
 
120 
 Key Takeaway:  
1. Math  is essential  for understanding  AI models  in depth.  
2. Different  math  concepts  used  for AI are Statistics,  Probability,  Linear  Algebra  and Calculus.  
3. Applications  of math  can be found  in everyday  life. 3. What  is probability?  
 
 
 
Reflection  
▪ Why  is math  necessary  for designing  an AI project?  
Revision  Time  
Part  A 
1. Match  the following:  
A B 
I) Probability  a) exploring  data  
ii) Calculus  b) finding out unknown or missing values  
iii) Statistics  c) predicting  different  events  
iv) Linear Algebra  d) training  and improving  AI model.  
2. If you are to throw  an arrow  to this pie chart,  in which  color  is the arrow  more  likely  to fall? 
a) Red 
b) Blue  
c) Yellow  
d) Green  
3. If you select  a balloon  without  looking,  how  likely  is it that you will pick a blue one? 
a) Probable  
b) Certain  
c) Unlikely  
d) Impossible  
4. With  one throw  of a 6-sided  die, what's  the probability  of getting  an even  number?  
a) 1/5 
b) 2/5 
c) 5/6 
d) 1/2 
5. Which  of the following  is an equation?  
a) 2x + 5 
b) x + 2 = 4x 
c) x^2 + 2x 
d) 5 + 5x + 5x^2 
6. What  is the value  of x? 10x-8=6x 
a)  8 
b) 4 
c)  2 
d) 6 
7. Which  two are examples  of descriptive  statistics?  
a) Median and correlation.  
b) Mean  and standard  deviation.  
121 
 c) Mode  and regression  analysis.  
d) Variance  and Hypothesis  testing.  
8. What  is the probability  of getting  head  when  you toss a coin  once?  
a) 0.75  
b) 1 
c) 0 
d) 0.5 
9. Getting  seven  in die throwing  is a possible  event.  (True/False).  
10. The median of  the data:  155, 160, 145,  149, 150,  147, 152,  144, 148 is 
a) 149 
b)150 
c)147 
d)144  
Answer  the following  question:  
1. Explain  the relationship  between  Mathematics  and Artificial  Intelligence,  providing  justification  for 
their interconnection.  
2. Aman  is confused,  how  probability  theory  is utilized  in artificial  intelligence,  help  Aman  by providing 
two examples to illustrate its importance.  
3. Define  Certain  events  and likely  events  with  examples.  
4. Write  any two examples  of Impossible  and equal  probability  events.  
5. Radhika  collected  the data  of the age distribution  of cases  admitted  during  a day in a hospital.  
Age (in years)  10 12 14 15 16 
Cases  admitted  (in a day)  5 7 9 22 11 
Find  the average  number  of cases  admitted  in hospital.  Also,  draw  a line graph  to represent  the data 
graphically.  
6. Identify  the likely,  unlikely,  impossible  and equal  probability  events  from  the following  
a. Tossing  a coin 
b. Rolling  an 8 on a standard  die 
c. Throwing  ten 5’s in a row 
d. Drawing  a card  of any suite  122 
 Unit  4 - Generative  Artificial  Intelligence  
 
Lesson  Title:  Introduction  to Generative  AI Approach:  Interactive  Session  + Activity  
Summary:  
The lesson covers four main topics, including an introduction to Generative AI, how it works, 
how  to use it, and the ethical  considerations  that come  with  its use. By the end of the lesson, 
students will have a basic understanding of Generative AI, how it can be used, and the 
potential ethical implications to consider.  
Learning Objectives  
● To understand  Generative  AI and its types.  
● To know  examples  and benefits  of using  Generative  AI. 
● To identify  popular  Generative  AI tools  and their  applications.  
● To sensitize  the students  about  the ethical  considerations  of using  Generative  AI. 
● To explain  students  about  the potential  negative  impact  of Generative  AI on society.  
Learning Outcomes:  
● Students  will be able  to define  Generative  AI & classify  different  kinds.  
● Students  will be able  to explain  how  Generative  AI works  and recognize  how  it learns.  
● Students  will be able  to apply  Generative  AI tools  to create  content.  
● Students  will understand  the ethical  considerations  of using  Generative  AI. 
Pre-requisites:  
● Knowledge  of AI project  cycle.  
Key-concepts:  
● Generative  AI 
Programs/Applications  Used:  
● MS PowerPoint  
● MS Word  
● Web  browser  (any)  123 
 
Purpose:  
To understand  the difference  between  real and AI-Generated  Images.  
Examine  the images  and determine  whether  either  of the images  is a real image  or an 
AI-generated image. Also, give reasons for your answer.  
Activity:  Guess  the Real  Image  vs. the AI-Generated  Image  
 
 
 
 
 
 
Let's  look  at the concepts behind  the generation  of these images.  124 
 
Supervised  Learning  and Discriminative  Modeling  
 
Image  Source:  Generative  AI, Explained  by Humans.  (n.d.).  https://lingarogroup.com/blog/generative -ai-explained -by-humans  
 
The classification  of data  elements  into categories  or labels  was initially  taught  to the machine  learning 
models by humans.  
 
Unsupervised  Learning  and Generative  Modeling  
 
Image  Source:  Generative  AI, Explained  by Humans.  (n.d.).  https://lingarogroup.com/blog/generative -ai-explained -by-humans  
 
In unsupervised  or self-supervised  learning,  the machine  learning  model  takes  unlabeled  datasets  and 
figures out patterns and inherent structures within them — without human intervention.  
 
What  is Generative  AI? 
▪ Generative artificial intelligence (AI) refers to the algorithms that generate new data that 
resembles  human -generated  content,  such  as audio,  code,  images,  text,  simulations,  and videos.  
▪ This technology  is trained  with  existing  data  and content,  creating  the potential  for applications 
such as natural language processing, computer vision, the metaverse, and speech synthesis.  125 
 
Activity  
Watch  the video:  https://www.youtube.com/watch?v=26fJ_ADteHo  and Share  your  views 
Let us have a look at timeline of Generative AI  
Source:  https:// www.desdevpro.com/blog/talk -rise-of-generative -ai 
 
Generative AI has evolved over several years to reach its current form. Over time, advancements in 
neural networks and deep learning techniques have significantly enhanced its capabilities. From early 
experiments in generative models to breakthroughs in natural language processing and image 
generation, the development of generative AI has been a continuous journey of innovation and 
refinement. Today, generative AI encompasses a wide range of applications, including text generation, 
image synthesis, and creative content creation, showcasing the culmination of years of research and 
development efforts.  
 
What  do you understand  about  generative  AI? 
 
 
Give  a few examples  of generative  AI. 
 
 
What  do you know  about  Deep  Fake?  
 
 
Generative  AI vs Conventional  AI 
In contrast to other forms of AI, Generative AI is specially made to produce new and unique content 
rather  than  merely  processing  or categorizing  already -existing  data.  Here  are some  significant  variations:  126 
 
 
           
 
 
Types  of Generative  AI 
Generative  AI comes  in a variety  of forms,  each  with  unique  advantages  and uses.  Some  of the most 
typical varieties are listed below:  
 
 
 
 
 Goal  
  
 
Generative AI creates new content, whereas 
conventional AI analyzes, processes, and 
classifies data.  
 
 Training  
  
 
Generative AI models use vast libraries of samples 
to train neural networks and other complicated 
structures to produce new content based on those 
patterns. Conventional AI employs fewer complex 
algorithms and training methods.  
 
 
 
 Output  
  
 
Generative AI output is fresh, innovative, and 
often unexpected.  
Conventional AI produces more predictable 
output based on existing data.  
 
Applications  
  
 
Generative AI benefits art, music, literature, 
gaming, and design.  
Conventional AI is used in banking, healthcare, 
image recognition, and language  processing.  127 
 
 
 
 
 
 
Examples  of Generative  AI 
Generative  AI has many  applications,  from  art and music  to language  and natural  language  processing. 
Here are some examples of how generative AI is being used in various fields:  
▪ Art: Generative  AI is being  used  to create  unique  works  of art. 
 
▪ For example,  The Next  Rembrandt  project  used  data  analysis  and 3D printing  to create  a new 
painting in the style of Rembrandt  
(Watch  video:  Video  source:  The Next  Rembrandt.  (2016,  April  5). The Next  Rembrandt  [Video].  YouTube. 
https://www.youtube.com/watch?v=IuygOYZ1Ngo ) 
▪ Music : Generative  AI is being  used  to create  new  music,  either  by composing  original  pieces  or by 
remixing existing ones.  
▪ For example,  AIVA  is an AI composer  that can create  original  pieces  of music  in various  genres.  
(Watch  video:  Video  source:  TED.  (2018,  October  1). How  AI could  compose  a personalized  soundtrack  to 
your life | Pierre Barreau [Video]. YouTube. https://www.youtube.com/watch?v=wYb3Wimn01s ) 
▪ Language:  Generative  AI is being  used  to generate  new  language,  such  as chatbots  that can hold  
conversations  with  users  or natural  language  generation  systems  that can produce  written 
content.  
(Watch  video:  Video  source:  BBC News.  (2023,  January  15). What  is ChatGPT,  the AI software  taking  the 
internet by storm? - BBC News [Video]. YouTube. https://www.youtube.com/watch?v=BWCCPy7Rg -s) 
 
Benefits  of using  Generative  AI 
Overall,  generative  AI offers  a range  of benefits,  including  increased  creativity,  efficiency,  personalization, 
exploration, accessibility, and scalability. By leveraging these benefits, businesses and organizations can 
improve their content creation processes and provide better experiences for their users.  
128 
 
 
 
 
Limitations  of Using  Generative  AI 
 
Hands -on Activity: GAN Paint  
▪ GAN  Paint  directly  activates  and deactivates  neurons  in a deep  network  trained  to create  pictures.  
▪ Each  left button  ("door",  "brick",  etc.)  represents  20 neurons.  The software  shows  that the network 
learns about trees, doorways, and roofs by drawing.  
▪ Switching  neurons  directly  shows  the network's  visual  world  model.  
129 
 ▪ To use GAN  Paint,  you will first need  to select  a base  image  
from  the website's  library.  You can then  use the brush  tool 
to add objects  and textures  to the image.  As you paint,  the 
GAN network will learn to generate more realistic images.  
▪ You are encouraged  to experiment  with  GAN  Paint  and see 
what you can create. Have fun!  
Link:  https://ganpaint -v2.vizhub.ai/  
 
Generative  AI tools  
There  are many  generative  AI tools  available  today  that enable  users  to create  and experiment  with 
generative models. Here are some popular tools:  
▪ Artbreeder : Artbreeder  is a web -based  tool that 
enables users to generate new images by 
combining different GAN models. Users can 
select  and combine  different  GAN  models  to 
create new and unique images.  
Hands -on Activity  
Generate  Images  with  Text  Prompt  
1. Go to artbreeder.com  
2. Select  Create  from  menu  bar and click  on New 
Image under Prompter category.  
3. Give cool text prompt and see how AI 
generates  a picture  from  those  prompts.  
▪ Runway  ML: Runway  ML is a platform  for 
creating,  training,  and deploying  generative  models.  It provides  a user -friendly  interface  for 
building and training various types of generative models, including GANs, VAEs, and image 
classifiers.  
(Watch  video:  Video  source:  https://www.youtube.com/watch?v=trXPfpV5iRQ ) 
Explore  AI Magic  Tools  Of Runway  ML 
1. Go to https://runwayml.com/  
2. Explore  the AI Magic  Tools  
3. Take  any tool of your  choice  and generate  new  content  with  it. 
 
 ChatGPT  
Link: https://chat.openai.com/  
I asked  ChatGPT  to introduce  itself.  And here  is the response  
130 
 
 
 Gemini  
Link: https://gemini.google.com/  
I asked  Gemini  to introduce  itself.  And here  is the response!  
 
 
Image  source:  Khare,  Y. (2023,  April  10). Google  VS Microsoft:  The Battle  of AI Innovation.  Analytics  Vidhya.  
https://w ww.analyticsvidh ya.com/blog/2023/04/ google -vs-microsoft -the-battle -of-ai-innovation/  
Hands -on Activity  
Chit-Chat  with  ChatGPT  & Gemini  
▪ Sign up & Login  into both  ChatGPT  and Gemini.  
131 
 
▪ Chat  with  the ChatGPT  and ask it to write  a paragraph  on How  it Works?  - ChatGPT  
▪ Similarly,  Chat  with  Bard  and ask it to write  a paragraph  on How  it Works?  - Gemini  
 
Here  are 6 prompts  that can be tried  on ChatGPT  and Gemini:  
1. Write  a summary  of the history  of the internet.  
2. Explain  how  to code  a simple  website.  
3. Write  a blog  post  about  the latest  trends  in artificial  intelligence.  
4. Create  a presentation  about  the benefits  of cloud  computing.  
5. Write  a research  paper  about  the future  of technology.  
6. Design  an app that solves  a real-world  problem.  
Document  the findings  from  above  activity  on ChatGPT  vs Gemini  vs Copilot  based  on the parameters 
below:  
▪ Parameter  1: Human -Like Response.  
▪ Parameter  2: Training  Dataset  and Underlying  Technology.  
▪ Parameter  3: Authenticity  of Response.  
▪ Parameter  4: Access  to the Internet.  
▪ Parameter  5: User  Friendliness  and Interface.  
▪ Parameter 6: Text Processing: Summarization, 
Paragraph Writing, Etc.  
▪ Parameter  7: Charges  and Price.  
 
How  to Use Generative  AI Tools  in Real -world  Scenarios  
The table  shows  popular  Generative  AI tools  that can be used  in various  fields.  
 
132 
 
Some  more  tools  
 
 
Ethical  considerations  of using  Generative  AI 
While  Generative  AI offers  many  benefits,  there  are also several  ethical  considerations  that should  be 
considered when using this technology.  
 
 
 
The Potential  Negative  Impact  on Society  
● Generative  AI can be used  to create  fake  news  or deep  fakes  that can spread  misinformation  and 
manipulate public opinion.  
 
 
 
 Bias  
  
 
Generative AI can replicate and amplify existing biases 
present in the data used to train the model.  
This can lead to harmful or discriminatory outcomes, 
especially if the generated content is used in high -stakes 
applications such as hiring, loan approvals, or criminal justice.  
Misinformation  
 
 
Generative AI can be used to create fake news or deepfakes, 
which can be used to spread misinformation and manipulate 
public opinion.  
This can have serious consequences for democracy and trust in 
institutions.  
 
 Privacy  
  
 
Generative AI can potentially be used to generate sensitive 
personal information, such as credit card numbers, social 
security numbers, or medical records.  
This could be used for malicious purposes.  133 
 ● Lead  to job displacement  for humans  who  previously  performed  these  tasks.  
● Generative  AI has the potential  to generate  sensitive  personal  information,  such  as social  security 
numbers or medical records, which could be used for malicious purposes.  
Responsible  Use of Generative  AI 
● Ensuring  that the training  data  used  are diverse  and representative.  
● The outputs  are scrutinized  for bias and misinformation.  
● Prioritizing  user  privacy  and consent,  
● Having  clear  guidelines  around  ownership  and attribution  of generative  content.  
● Engaging  in public  discussions  around  the social  and ethical  implications  of this technology  to ensure 
that it is developed and used in ways that are beneficial to society.  
In short,  responsible  use of Generative  AI is essential  for ensuring  that this technology  is developed  and 
used in ways that benefit society!  
By emphasizing  ethics,  creating  trust,  limiting  negative  repercussions,  defining  legislation,  and 
encouraging innovation, we may maximize Generative AI’s potential to improve society!  
 
Revision  Time  
● What  do you understand  about  Generative  Artificial  Intelligence?  Give  any two examples.  
● Write  any two AI tools  each  for the following - 
▪ Generative  AI image  generation  tools  
▪ Generative  AI text generation  tools  
▪ Generative  AI audio  generation  tools  
● Give  full forms  of the following - 
▪ GANs  
▪ VAEs  
▪ RNNs  
● How  Generative  AI can be helpful  in following  fields - 
▪ Architecture  
▪ Coding  
▪ Music  
▪ Content  Creation  
● Sakshi has been assigned a homework essay on the topic, “The Impact of Climate Change on Coral 
Reefs.”  The essay  requires  Sakshi  to research  and explain  various  aspects  of climate  change,  such  as 
ocean acidification and rising sea temperatures, and their effects on coral reef ecosystems. His 
friend  suggested  using  some  text generation  tool.  List some  guidelines  for Sakshi  to prevent  misuse 
of Generative AI and use it constructively.  
● How  do you think  generative  AI can revolutionize  the creative  industry,  such  as art and fashion,  by 
enabling the generation of unique and innovative designs?  
● Considering the ethical challenges associated with generative AI, what are your thoughts on 
establishing  guidelines  or regulations  to ensure  responsible  use of these  technologies?  How  can we 
balance the potential benefits and risks?  134 
 Answers  to MCQ 
Unit 1  
Subunit 1.1 
1. b, 
2. b 
3. c 
4. c 
5. a 
6. a 
Subunit 1.2.3  
1. b 
2. b 
3. d 
4. a 
5. d 
Subunit 1.2.5  
1. b,a,d,c,f,e  
2. b 
3. c 
4. True  
5. A-AI, B -ML, C -DL 
Subunit 1.2.6  
1. a 
2. a 
Subunit 1.3 
1. Ethics  
2. AI principles  
3. No, it is not considered  theft.  It is an ethical  concern.  
4. Data  Privacy  
5. Bias 
6. True  
7. Bias 
8. True  
 
Unit  2: 
Part  A 
1. i. c 
ii. d 
iii. a 135 
 iv. b 
2. b 
3. b 
4. d 
5, d 
6. b 
7. c 
8. b 
9. d 
10. a 
 
Part  B 
1. c 
2. c 
UNIT  3 
Subunit 3.1.5  
1. b 
2. a 
3. b 
4. c 
5. a 
 
Subunit 3.2 
1. c 
2. d 
3. a 
4. c 
5. b 

A new era of 
generative AI 
for everyone
The technology underpinning 
ChatGPT will transform work 
and reinvent businessTable of 
ContentsWelcome to AI’s new inflection point
How did we get here? | Milestones in the journey to generative AI
Consume or customize: Generative AI for everyone
A look ahead at the fast-paced evolution of technology, regulation and business
Embrace the generative AI era: Six adoption essentials
The future of AI is accelerating
Glossary and References
Authors03
04
05
08
12
19
21
22
2
A new era of generative AI for everyone  |Introduction
Welcome to AI’s new inflection point 
ChatGPT has woken up the world to 
the transformative potential of artificial 
intelligence (AI), capturing global attention 
and sparking a wave of creativity rarely seen 
before. Its ability to mimic human dialogue 
and decision-making has given us AI’s first 
true inflection point in public adoption. 
Finally, everyone, everywhere can see the 
technology’s true disruptive potential for 
themselves. 
ChatGPT reached 100 million monthly 
active users just two months after launch, 
making it the fastest-growing consumer 
application in history.1A foundation model is a generic term for 
large models with billions of parameters. With 
recent advances, companies can now build 
specialized image- and language-generating 
models on top of these foundation models. 
Large language models (LLMs) are both 
a type of generative AI and a type of 
foundation model. 
The LLMs behind ChatGPT mark a significant 
turning point and milestone in artificial 
intelligence. Two things make LLMs game 
changing. First, they’ve cracked the code on 
language complexity. Now, for the first time, 
machines can learn language, context and 
intent and be independently generative and 
creative. Second, after being pre-trained 
on vast quantities of data (text, images or 
audio), these models can be adapted or fine-
tuned for a wide range of tasks. This allows 
them to be reused or repurposed in many 
different ways.Business leaders recognize the significance 
of this moment. They can see how LLMs 
and generative AI will fundamentally 
transform everything from business, to 
science, to society itself—unlocking new 
performance frontiers. The positive impact 
on human creativity and productivity will be 
massive. Consider that, across all industries, 
Accenture found 40% of all working hours 
can be impacted by LLMs like GPT-4. This 
is because language tasks account for 62% 
of the total time employees work, and 65% 
of that time can be transformed into more 
productive activity through augmentation 
and automation (see Figure 3). 
3
A new era of generative AI for everyone  |How did we 
get here? 
Milestones in the journey 
to generative AIMachine learning: Analysis and prediction phase
The first decade of the 2000s marked the rapid advance 
of various machine learning techniques that could analyze 
massive amounts of online data to draw conclusions –  
or “learn” – from the results. Since then, companies have viewed machine learning as an incredibly powerful field 
of AI for analyzing data, finding patterns, generating 
insights, making predictions and automating tasks at a 
pace and on a scale that was previously impossible.
Deep learning: Vision and speech phase
The 2010s produced advances in AI’s 
perception capabilities in the field of machine 
learning called deep learning. Breakthroughs 
in deep learning enable the computer vision that search engines and self-driving cars use 
to classify and detect objects, as well as the 
voice recognition that allows popular AI speech 
assistants to respond to users in a natural way. 
Generative AI: Enter the language-mastery phase
Building on exponential increases in the size and 
capabilities of deep learning models, the 2020s will be 
about language mastery. The GPT-4 language model, 
developed by OpenAI, marks the beginning of a new phase in the abilities of language-based AI applications. Models 
such as this will have far-reaching consequences for business, 
since language permeates everything an organization does day to 
day—its institutional knowledge, communication and processes.2
4
A new era of generative AI for everyone  |Consume or 
customize: 
Generative AI 
for everyone
5
A new era of generative AI for everyone  |Consume or customize: Generative AI for everyone
Consume or customize: Generative AI for everyone
Easy-to-consume generative AI applications like 
ChatGPT, DALL-E, Stable Diffusion and others are 
rapidly democratizing the technology in business 
and society. The effect on organizations will be 
profound. The ability of LLMs to process massive 
data sets allows them to potentially “know” 
everything an organization has ever known—the 
entire history, context, nuance and intent of a 
business, and its products, markets and customers. 
Anything conveyed through language (applications, 
systems, documents, emails, chats, video and audio 
recordings) can be harnessed to drive next-level 
innovation, optimization and reinvention.
97% of global executives agree AI 
foundation models will enable connections 
across data types, revolutionizing where 
and how AI is used.3We’re at a phase in the adoption cycle when 
most organizations are starting to experiment 
by consuming foundation models “off the shelf.” 
However, the biggest value for many will come 
when they customize or fine tune models using 
their own data to address their unique needs:
Consume  
Generative AI and LLM applications are ready to 
consume and easy to access. Companies can 
consume them through APIs and tailor them, to 
a small degree, for their own use cases through 
prompt engineering techniques such as prompt 
tuning and prefix learning.
Customize  
But most companies will need to customize  
models, by fine-tuning them with their own data, 
to make them widely usable and valuable. This will 
allow the models to support specific downstream 
tasks all the way across the business. The effect 
will be to increase a company’s efficacy in using 
AI to unlock new performance frontiers—elevating 
employee capabilities, delighting customers, 
introducing new business models and boosting 
responsiveness to signals of change. 
6
A new era of generative AI for everyone  |Consume or customize: Generative AI for everyone
Companies will use these models to reinvent the 
way work is done. Every role in every enterprise 
has the potential to be reinvented, as humans 
working with AI co-pilots becomes the norm, 
dramatically amplifying what people can achieve. In 
any given job, some tasks will be automated, some 
will be assisted, and some will be unaffected by the 
technology. There will also be a large number of 
new tasks for humans to perform, such as ensuring 
the accurate and responsible use of new  
AI-powered systems. 
Consider the impact in these key functions:
Advising. AI models will become an ever-present 
co-pilot for every worker, boosting productivity 
by putting new kinds of hyper-personalized 
intelligence into human hands. Examples include 
customer support, sales enablement, human 
resources, medical and scientific research, 
corporate strategy and competitive intelligence. 
Large language models could be useful in 
tackling the roughly 70% of customer service 
communication that is not straightforward and 
can benefit from a conversational, powerful and 
intelligent bot, understanding a customer’s intent, 
formulate answers on its own and improve the 
accuracy and quality of answers.4Creating. Generative AI will become an essential 
creative partner for people, revealing new ways 
to reach and appeal to audiences and bringing 
unprecedented speed and innovation in areas like 
production design, design research, visual identity, 
naming, copy generation and testing, and real-
time personalization. Companies are turning to 
state-of-the-art artificial intelligence systems like 
DALL·E, Midjourney and Stable Diffusion for their 
social media visual content generation outreach. 
DALL·E, for example,  creates realistic images and 
art based on text descriptions and can process up 
to 12 billion parameters when transforming words 
into pictures. Images created can then be shared 
on Instagram and Twitter.5
Coding.  Software coders will use generative AI to 
significantly boost productivity — rapidly converting 
one programming language to another, mastering 
programming tools and methods, automating code 
writing, predicting and pre-empting problems, 
and managing system documentation. Accenture 
is piloting the use of OpenAI LLMs to enhance 
developer productivity by automatically generating 
documentation – for example, SAP configuration 
rationale and functional or technical specs. The 
solution enables users to submit requests through 
a Microsoft Teams chat as they work. Correctly 
packaged documents are then returned at speed — 
a great example of how specific tasks, rather than 
entire jobs, will be augmented and automated.Automating.  Generative AI’s sophisticated 
understanding of historical context, next 
best actions, summarization capabilities, and 
predictive intelligence will catalyze a new era 
of hyper-efficiency and hyper-personalization 
in both the back and front office—taking 
business process automation to a transformative 
new level. One multinational bank is using 
generative AI and LLMs to transform how it 
manages volumes of post-trade processing 
emails—automatically drafting messages with 
recommended actions and routing them to the 
recipient. The result is less manual effort and 
smoother interactions with customers.
Protecting. In time, generative AI will support 
enterprise governance and information security, 
protecting against fraud, improving regulatory 
compliance, and proactively identifying 
risk by drawing cross-domain connections 
and inferences both within and outside the 
organization. In strategic cyber defense, LLMs 
could offer useful capabilities, such as explaining 
malware and quickly classifying websites.6 
In the short term, however, organizations can 
expect criminals to capitalize on generative AI’s 
capabilities to generate malicious code or write 
the perfect phishing email.7
7
A new era of generative AI for everyone  |A look ahead at the  
fast-paced evolution  
of technology,  
regulation and  
business
8
A new era of generative AI for everyone  |A look ahead at the fast-paced evolution of technology, regulation and business 
A look ahead at the fast-paced evolution of technology, regulation and business 
Moments like this don’t come around often. 
The coming years will see outsized investment 
in generative AI, LLMs and foundation models. 
What’s unique about this evolution is that the 
technology, regulation, and business adoption 
are all accelerating exponentially at the same 
time. In previous innovation curves, the 
technology typically outpaced both adoption 
and regulation. 
The technology stack
The complex technology underpinning 
generative AI is expected to evolve rapidly 
at each layer. This has broad business 
implications. Consider that the amount of 
compute needed to train the largest AI models 
has grown exponentially – now doubling 
between every 3.4 to 10 months, according to 
various reports.8 Cost and carbon emissions 
are therefore central considerations in 
adopting energy-intensive generative AI. Figure 1: Each layer of the generative AI tech stack will rapidly evolve
Applications: Generative AI and LLMs will be increasingly 
accessible to users in the cloud via APIs and by being embedded 
directly into other applications. Companies will consume them 
as they are or will customize and fine-tune them with proprietary 
data. 
Fine-tuning: The importance of model fine-tuning will create 
demand for a multidisciplinary set of skills spanning software 
engineering, psychology, linguistics, art history, literature and 
library science.
Foundation models: The market will rapidly mature and diversify 
as more pre-trained models emerge. New model designs will 
offer more choices for balancing size, transparency, versatility and 
performance. 
Data: Improving the maturity of the enterprise data lifecycle 
will become a prerequisite for success – requiring mastery of 
new data, new data types and immense volumes. Generative AI 
features within modern data platforms will emerge, enhancing 
adoption at scale.
Infrastructure: Cloud infrastructure will be essential for deploying 
generative AI while managing costs and carbon emissions. Data 
centers will need retrofitting. New chipset architectures, hardware 
innovations, and efficient algorithms will also play a critical role.“The hottest new programming 
platform is the napkin.”  
 
Paul Daugherty, Accenture Group Chief Executive 
& Chief Technology Officer  
 
Referring to the use of OpenAI to generate a working website 
from a napkin drawing
9
A new era of generative AI for everyone  |A look ahead at the fast-paced evolution of technology, regulation and business 
The risk and regulatory environment 
Companies will have thousands of ways to 
apply generative AI and foundation models 
to maximize efficiency and drive competitive 
advantage. Understandably, they’ll want to get 
started as soon as possible. But an enterprise-
wide strategy needs to account for all the 
variants of AI and associated technologies they 
intend to use, not only generative AI and large 
language models. 
ChatGPT raises important questions about the 
responsible use of AI. The speed of technology 
evolution and adoption requires companies 
to pay close attention to any legal, ethical and 
reputational risks they may be incurring. 
It’s critical that generative AI technologies, 
including ChatGPT, are responsible and 
compliant by design, and that models and 
applications do not create unacceptable risk 
for the business. Accenture was a pioneer in 
the responsible use of technology including 
the responsible use of AI in its Code of 
Business Ethics from 2017. Responsible AI is the 
practice of designing, building and deploying 
AI in accordance with clear principles to 
empower businesses, respect people, and 
benefit society — allowing companies to 
engender trust in AI and to scale AI with 
confidence.AI systems need to be “raised” with a diverse 
and inclusive set of inputs so that they reflect 
the broader business and societal norms of 
responsibility, fairness and transparency. When 
AI is designed and put into practice within an 
ethical framework, it accelerates the potential 
for responsible collaborative intelligence, 
where human ingenuity converges with 
intelligent technology. 
This creates a foundation for trust with 
consumers, the workforce, and society, and 
can boost business performance and unlock 
new sources of growth. Figure 2: Key risk and regulatory questions for generative AI
Intellectual property: How will the business protect its own 
IP? And how will it prevent the inadvertent breach of third-party 
copyright in using pre-trained foundation models?
Data privacy and security: How will upcoming laws like 
the EU AI Act be incorporated in the way data is handled, 
processed, protected, secured and used?
Discrimination: Is the company using or creating tools 
that need to factor in anti-discrimination or anti-bias 
considerations?
Product liability: What health and safety mechanisms need 
to be put in place before a generative AI-based product is 
taken to market?
Trust: What level of transparency should be provided to 
consumers and employees? How can the business ensure the 
accuracy of generative AI outputs and maintain user confidence?
Identity: When establishing proof-of-personhood depends on voice 
or facial recognition, how will verification methods be enhanced and 
improved? What will be the consequences of its misuse?
10
A new era of generative AI for everyone  |A look ahead at the fast-paced evolution of technology, regulation and business 
The scale of adoption in business
Companies must reinvent work to find 
a path to generative AI value. Business 
leaders must lead the change, starting 
now, in job redesign, task redesign and 
reskilling people. Ultimately, every role 
in an enterprise has the potential to 
be reinvented, once today’s jobs are 
decomposed into tasks that can be 
automated or assisted and reimagined for 
a new future of human + machine work.
Generative AI will disrupt work as 
we know it today, introducing a new 
dimension of human and AI collaboration 
in which most workers will have a “co-
pilot,” radically changing how work is 
done and what work is done. Nearly 
every job will be impacted – some will 
be eliminated, most will be transformed, 
and many new jobs will be created. 
Organizations that take steps now to 
decompose jobs into tasks, and invest 
in training people to work differently, 
alongside machines, will define new 
performance frontiers and have a big leg 
up on less imaginative competitors.
Nearly 6 in 10 organizations 
plan to use ChatGPT for learning 
purposes and over half are 
planning pilot cases in 2023. 
Over 4 in 10 want to make a 
large investment.954%
48%
36%
40%
43%
33%
34%
31%
28%
30%
26%
30%
26%
28%
27%
25%
26%
24%
24%
20%12%
14%
21%
14%
9%
13%
7%
9%
11%
9%
13%
6%
8%
6%
6%
8%
6%
6%
5%
5%24%
26%
28%
29%
14%
21%
12%
22%
33%
35%
20%
13%
16%
15%
15%
17%
14%
13%
14%
11%10%
12%
15%
18%
34%
33%
46%
38%
27%
26%
41%
50%
50%
50%
52%
50%
54%
57%
56%
64%
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Banking
Insurance
Software & Platforms
Capital markets
Energy
Com munications & Media
Retail
Industry Average
Health
Public Service
Aerospace & Defense
Automotive
High Tech
Travel
Utilities
Life Sciences
Industrial
Consumer Goods & Services
Chemicals
Natural ResourcesFigure 3: Generative AI will transform work across industries
Work time distribution by industry 
and potential AI impact
Based on their employment levels in the US in 2021
40% of working hours across 
industries can be impacted by 
Large Language Models (LLMs)
Why is this the case? Language tasks account for 62% of total worked time 
in the US. Of the overall share of language tasks, 65% have high potential 
to be automated or augmented by LLMs. 
Source: Accenture Research based on analysis of Occupational 
Information Network (O*NET), US Dept. of Labor; US Bureau of 
Labor Statistics.
Notes: We manually identified 200 tasks related to language (out 
of 332 included in BLS), which were linked to industries using their 
share in each occupation and the occupations’ employment level 
in each industry. Tasks with higher potential for automation can 
be transformed by LLMs with reduced involvement from a human 
worker. Tasks with higher potential for augmentation are those in 
which LLMs would need more involvement from human workers.Higher potential for 
automationHigher potential for 
augmentationLower potential for 
augmentation or 
automationNon-language
tasks
11
A new era of generative AI for everyone  |Embrace the 
generative AI era:  
Six adoption 
essentials
12
A new era of generative AI for everyone  |Embrace the generative AI era: Six adoption essentials
Dive in, with a 
business-driven 
mindsetTake a people-
first approach Get your 
proprietary  
data ready Invest in a 
sustainable tech 
foundationAccelerate 
ecosystem 
innovation Level-up your 
responsible AI
6
5
4
3
2
1
13
A new era of generative AI for everyone  |Embrace the generative AI era: Six adoption essentials1Dive in, with a business-driven mindset
Even when new innovations have obvious advantages, 
diffusing them across an organization can be challenging, 
especially if the innovation is disruptive to current ways of 
working. By experimenting with generative AI capabilities, 
companies will develop the early successes, change agents 
and opinion leaders needed to boost acceptance and spread 
the innovation further, kick-starting the transformation and 
reskilling agenda. 
Organizations must take a dual approach to experimentation. 
One, focused on low-hanging fruit opportunities using 
consumable models and applications to realize quick returns. 
The other, focused on reinvention of business, customer 
engagement and products and services using models that 
are customized with the organization’s data. A business-
driven mindset is key to define, and successfully deliver on, 
the business case.
As they experiment and explore reinvention opportunities, 
they’ll reap tangible value while learning more about which 
types of AI are most suited to different use cases, since the 
level of investment and sophistication required will differ 
based on the use case. They’ll also be able to test and 
improve their approaches to data privacy, model accuracy, 
bias and fairness with care, and learn when “human in the 
loop” safeguards are necessary.
98% of global executives agree AI foundation 
models will play an important role in their 
organizations’ strategies in the next 3 to 5 years.10A bank uses enhanced search to equip 
employees with the right information
As part of its three-year innovation plan, 
a large European banking group saw an 
opportunity to transform its knowledge 
base, empower its people with access to 
the right information, and advance its goal 
of becoming a data-driven bank. Using 
Microsoft’s Azure platform and a GPT-
3 LLM to search electronic documents, 
users can get quick answers to their 
questions — saving time while improving 
accuracy and compliance. The project, 
which included employee upskilling, is 
the first of four that will apply generative 
AI to the areas of contract management, 
conversational reporting and ticket 
classification.
14
A new era of generative AI for everyone  |Embrace the generative AI era: Six adoption essentials
2Take a people-first approach 
Success with generative  
AI requires an equal attention on 
people and training as it does on 
technology. Companies should 
therefore dramatically ramp up 
investment in talent to address 
two distinct challenges: creating 
AI and using AI. This means 
both building talent in technical 
competencies like AI engineering 
and enterprise architecture 
and training people across the 
organization to work effectively 
with AI-infused processes. In our 
analysis across 22 job categories, 
for example, we found that 
LLMs will impact every category, 
ranging from 9% of a workday at 
the low end to 63% at the high 
end. More than half of working 
hours in 5 of the 22 occupations 
can be transformed by LLMs. Figure 4: Generative AI will transform work across every job category
57%
49%
28%
45%
25%
27%
21%
33%
31%
30%
29%
22%
29%
27%
29%
23%
25%
23%
15%
16%
8%
14%
9%6%
13%
32%
14%
26%
20%
24%
9%
9%
9%
8%
15%
7%
8%
6%
8%
5%
4%
4%
1%
8%
2%
0%14%
14%
23%
35%
26%
25%
25%
58%
22%
44%
31%
40%
59%
31%
23%
50%
9%
7%
7%
9%
17%
8%
7%23%
24%
17%
6%
22%
28%
30%
0%
38%
17%
32%
22%
6%
34%
43%
19%
61%
66%
75%
75%
66%
76%
84%
0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%Office and Administrative Support
Sales and Related
Computer and Mathematical
Business and Financial Operations
Arts, Design, Entertainment, Sports, and Media
Life, Physical, and Social Science
Architecture and Engineer ing
Legal
Occcup ation Average
Mana gement
Personal Care and Service
Healthcare Practitioners and Technical
Commu nity and Social Service
Healthcare Support
Protective Service
Educational Instruction and Library
Food Preparation and Serving Related
Transportation and Material Moving
Construction and Extraction
Installation, Maintenance, and Repair
Farming, Fishing , and Forestry
Production
Building and Grounds Cleaning and MaintenanceWork time distribution by major 
occupation and potential AI impact
Based on their employment levels in the US in 2021
In 5 out of 22 occupation 
groups, Generative AI can 
affect more than half of all 
hours worked
Source: Accenture Research based on analysis of Occupational 
Information Network (O*NET), US Dept. of Labor; US Bureau of Labor 
Statistics.
Notes: We manually identified 200 tasks related to language (out 
of 332 included in BLS), which were linked to industries using their 
share in each occupation and the occupations’ employment level 
in each job category. Tasks with higher potential for automation can 
be transformed by LLMs with reduced involvement from a human 
worker. Tasks with higher potential for augmentation are those in 
which LLMs would need more involvement from human workers.Higher potential for 
automationHigher potential for 
augmentationLower potential for 
augmentation or 
automationNon-language
tasks
15
A new era of generative AI for everyone  |Embrace the generative AI era: Six adoption essentials
2In fact, independent economic research indicates that 
companies are significantly underinvesting in helping 
workers keep up with advances in AI, which require 
more cognitively complex and judgment-based tasks.11 
Even domain experts who understand how to apply 
data in the real world (a doctor interpreting health data, 
for example) will need enough technical knowledge of 
how these models work to have confidence in using 
them as a “workmate.” 
There will also be entirely new roles to recruit, including 
linguistics experts, AI quality controllers, AI editors, 
and prompt engineers. In areas where generative 
AI shows most promise, companies should start by 
decomposing existing jobs into underlying bundles of 
tasks. Then assess the extent to which generative AI 
might affect each task — fully automated, augmented, 
or unaffected.Figure 5: Reinventing a customer service job, task by task
To assess how specific jobs will be reinvented with AI, an Accenture analysis decomposed 
one customer service job into 13 component tasks. We found: 
4tasks would continue to be performed 
primarily by humans, with low potential 
for automation or augmentation.
4tasks could be fully automated — 
such as gathering, classifying, and 
summarizing information on why a 
customer is contacting the company. 
5tasks could be augmented to help 
humans work more effectively — such 
as using an AI summary to provide a 
rapid solution with a human touch.
Importantly, new job tasks might also be needed to ensure the safe, accurate and responsible use of 
AI in customer service settings, such as providing unbiased information on products and pricing. 
16
A new era of generative AI for everyone  |Embrace the generative AI era: Six adoption essentials3Get your proprietary data ready  
Customizing foundation models will require 
access to domain-specific organizational data, 
semantics, knowledge, and methodologies. In the 
pre-generative AI era, companies could still get 
value from AI without having modernized their 
data architecture and estate by taking a use-case 
centric approach to AI. That’s no longer the case. 
Foundation models need vast amounts of curated 
data to learn and that makes solving the data 
challenge an urgent priority for every business. 
Companies need a strategic and disciplined 
approach to acquiring, growing, refining, 
safeguarding and deploying data. Specifically, they 
need a modern enterprise data platform built on 
cloud with a trusted, reusable set of data products. 
Because these platforms are cross-functional, with 
enterprise-grade analytics and data housed in cloud-
based warehouses or data lakes, data is able to break 
free from organizational silos and democratized for 
use across an organization. All business data can 
then be analyzed together in one place or through a 
distributed computing strategy, such as a data mesh. 
Read more on the practices data-mature 
companies are using to maximize enterprise 
data value: A new dawn for dormant data: 
Unleash the intrinsic value of enterprise 
data with a strong digital core on cloud.4Invest in a sustainable tech foundation
Companies need to consider whether they have the 
right technical infrastructure, architecture, operating 
model and governance structure to meet the high 
compute demands of LLMs and generative AI, while 
keeping a close eye on cost and sustainable energy 
consumption. They’ll need ways to assess the cost 
and benefit of using these technologies versus other 
AI or analytical approaches that might be better 
suited to particular use cases, while also being 
several times less expensive.
As the use of AI increases, so will the carbon 
emissions produced by the underlying infrastructure. 
Companies need a robust green software 
development framework that considers energy 
efficiency and material emissions at all stages of the 
software development lifecycle. AI can also play a 
broader role in making business more sustainable 
and achieving ESG goals. Of the companies we 
surveyed that successfully reduced emissions in 
production and operations, 70% used AI to do it.12 
17
A new era of generative AI for everyone  |Embrace the generative AI era: Six adoption essentials5Accelerate ecosystem innovation 
Creating a foundation model can be a complex, 
compute-intensive and costly exercise. And for 
all but the very largest global companies, doing it 
entirely on their own will be beyond their means 
and capabilities. The good news is that there is a 
burgeoning ecosystem to call on, with substantial 
investments by cloud hyperscalers, big tech players, 
and start-ups. Global investment in AI startups 
and scale-ups is estimated to exceed $50 billion in 
2023 alone.13 These partners bring best practices 
honed over many years, and can provide valuable 
insights into using foundation models efficiently 
and effectively in specific use cases. Having the 
right network of partners—including technology 
companies, professional services firms and academic 
institutions—will be key to navigating rapid change.6Level-up your responsible AI 
The rapid adoption of generative AI brings fresh urgency 
to the need for every organization to have a robust 
responsible AI compliance regime in place. This includes 
controls for assessing the potential risk of generative AI 
use cases at the design stage and a means to embed 
responsible AI approaches throughout the business. 
Accenture’s research suggests most companies still 
have a long way to go. Our 2022 survey of 850 senior 
executives globally revealed widespread recognition 
of the importance of responsible AI and AI regulation. 
But only 6 percent of organisations felt they had a fully 
robust responsible AI foundation in place.
An organization’s responsible AI principles should be 
defined and led from the top and translated into an 
effective governance structure for risk management 
and compliance, both with organizational principles 
and policies and applicable laws and regulations. 
Responsible AI must be CEO-led, beginning with a focus 
on training and awareness and then expanding to focus 
on execution and compliance. Accenture was one of the 
first to take this approach to Responsible AI years ago, 
with a CEO-led agenda, and now a formal compliance 
program. Our own experience shows that a principles-
driven compliance approach provides guardrails while 
being flexible enough to evolve with the fast pace 
of changing technology, ensuring companies aren’t 
constantly playing “catch up.” 
To be responsible by design, organizations need to move 
from a reactive compliance strategy to the proactive 
development of mature Responsible AI capabilities 
through a framework that includes principles and 
governance; risk, policy and control; technology and 
enablers and culture and training. 
18
A new era of generative AI for everyone  |The future of AI  
is accelerating
19
A new era of generative AI for everyone  |The future of AI is accelerating
This is a pivotal moment. For several years, 
generative AI and foundation models have been 
quietly revolutionizing the way we think about 
machine intelligence. Now, thanks to ChatGPT, 
the whole world has woken up to the possibilities 
this creates. 
While artificial general intelligence (AGI) remains 
a distant prospect, the speed of development 
continues to be breathtaking. We’re at the start of 
an incredibly exciting era that will fundamentally 
transform the way information is accessed, 
content is created, customer needs are served, 
and businesses are run. 
Embedded into the enterprise digital core, 
generative AI, LLMs, and foundation models will 
optimize tasks, augment human capabilities, and 
open up new avenues for growth. In the process, 
these technologies will create an entirely new 
language for enterprise reinvention. Businesses are right to be optimistic about the 
potential of generative AI to radically change how 
work get done and what services and products 
they can create. They also need to be realistic 
about the challenges that come with profoundly 
rethinking how the organization works, with 
implications for IT, organization, culture, and 
responsibility by design.
Companies need to invest as much in evolving 
operations and training people as they do in 
technology. Radically rethinking how work gets 
done, and helping people keep up with technology-
driven change, will be two of the most important 
factors in realizing the full potential of this step-
change in AI technology. 
Now’s the time for companies to use 
breakthrough advances in AI to set new 
performance frontiers—redefining themselves 
and the industries in which they operate.
20
A new era of generative AI for everyone  |Glossary
ChatGPT is a generative AI chatbot interface built on top of OpenAI’s GPT-3.5 
large language model (see below). ChatGPT (and ChatGPT plus, which uses 
GPT-4) allows users to interact with the underlying AI in a way that seems 
remarkably accurate and feels surprisingly human. You can ask it to explain 
a subject, write an essay, run a calculation, generate some Python code, or 
simply have a conversation. 
Generative AI is the umbrella term for the ground-breaking form of creative 
artificial intelligence that can produce original content on demand. Rather 
than simply analyzing or classifying existing data, generative AI is able to 
create something entirely new, whether text, images, audio, synthetic data, or 
more. 
Foundation models are complex machine learning systems trained on vast 
quantities of data (text, images, audio, or a mix of data types) on a massive 
scale. The power of these systems lies not only in their size but also in the fact 
they can quickly be adapted or fine-tuned for a wide range of downstream 
tasks. Examples of foundation models include BERT, DALL-E, and GPT-4.
Large Language Models (LLMs) represent a subset of foundation models 
that are trained specifically on text sources. GPT-3, for instance, was trained 
on almost 500 billion words from millions of websites.14  
Its successor, GPT-4, can take image as well as text as inputs.
Fine-tuning is the process by which foundation models are adapted for 
specific downstream tasks using a particular dataset. That can include 
everything from the hyper-specific (training a model to compose emails 
based on your personal writing style) to the enterprise level (training an LLM 
on enterprise data to transform a company’s ability to access and analyze its 
core intelligence).
Data is the fundamental bedrock of generative AI. Not only in training 
foundation models themselves, but also in fine-tuning those models to 
perform specific tasks. In an enterprise context, examples might include 
everything from legacy code to real-time operational data to customer 
insights.References
1. ChatGPT sets record for fastest-growing user base - analyst note, Reuters, February 2023  
https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/
2. The Next Big Breakthrough in AI Will Be Around Language, Harvard Business Review, September, 2020  
https://hbr.org/2020/09/the-next-big-breakthrough-in-ai-will-be-around-language
3. Accenture Tech Vision 2023 
4. ChatGPT Is Coming to a Customer Service Chatbot Near You, Forbes, January 2023  
https://www.forbes.com/sites/rashishrivastava/2023/01/09/chatgpt-is-coming-to-a-customer-service-chatbot-near-
you/?sh=730eeab97eca
5. How AI Transforms Social Media, Forbes, March 2023  
https://www.forbes.com/sites/forbestechcouncil/2023/03/16/how-ai-transforms-social-media/?sh=739221ca1f30
6. Large AI Models have Real Security Benefits, Dark Reading, August, 2022  
https://www.darkreading.com/dr-tech/large-language-ai-models-have-real-security-benefits
7. OPWNAI: Cybercriminals starting to use ChatGPT, Checkpoint Research, January, 2023  
https://research.checkpoint.com/2023/opwnai-cybercriminals-starting-to-use-chatgpt/
8. Accenture Technology Vision 2023
9. CXO Pulse Survey, conducted by Accenture Research, February 2023
10. Accenture Technology Vision 2023
11. The Productivity J-Curve: How Intangibles Complement General Purpose Technologies - American Economic Association (aeaweb.org)
12. Uniting technology and sustainability, Accenture, May, 2022  
Technology Sustainability Key to ESG Goals | Accenture
13. Pace Of Artificial Intelligence Investments Slows, But AI Is Still Hotter Than Ever, Forbes, October, 2022  
https://www.forbes.com//sites/joemckendrick/2022/10/15/pace-of-artificial-intelligence-investments-slows-but-ai-is-still-hotter-than-
ever/?sh=853d8124c76c
14. OpenAI’s GPT-3 Language Model: A Technical Overview, Lambda, June, 2020  
https://lambdalabs.com/blog/demystifying-gpt-3
21
A new era of generative AI for everyone  |Authors
Paul Daugherty
Group Chief Executive &  
Chief Technology OfficerBhaskar Ghosh
Chief Strategy Officer,  
Accenture Karthik Narain
Lead – Accenture Cloud First
Lan Guan
Lead – Cloud First, Data & AIJim Wilson
Global Managing Director – Thought 
Leadership & Technology ResearchThe authors would like to acknowledge 
Tomas Castagnino, Elise Cornille,  
Ray Eitel-Porter, Linda King,  
Amy Sagues, Ezequiel Tacsir and 
Denise Zheng for their contributions.
About Accenture
Accenture is a leading global professional services company 
that helps the world’s leading businesses, governments and 
other organizations build their digital core, optimize their 
operations, accelerate revenue growth and enhance citizen 
services—creating tangible value at speed and scale. We are a 
talent and innovation led company with 738,000 people serving 
clients in more than 120 countries. Technology is at the core of 
change today, and we are one of the world’s leaders in helping 
drive that change, with strong ecosystem relationships. We 
combine our strength in technology with unmatched industry 
experience, functional expertise and global delivery capability. 
We are uniquely able to deliver tangible outcomes because 
of our broad range of services, solutions and assets across 
Strategy & Consulting, Technology, Operations, Industry X and 
Accenture Song. These capabilities, together with our culture 
of shared success and commitment to creating 360° value, 
enable us to help our clients succeed and build trusted, lasting 
relationships. We measure our success by the 360° value we 
create for our clients, each other, our shareholders, partners and 
communities. Visit us at www.accenture.com.Contact us
For more information, contact the Accenture Generative AI/
Large Language Model Center of Excellence at:  
LLMCoE@accenture.com.
Copyright © 2023 Accenture. All rights reserved.  
Accenture and its logo are registered trademarks of Accenture
Disclaimer: This content is provided for general information purposes and is not intended to be used in place of consultation with our professional advisors. This document refers to marks owned by third 
parties. All such third-party marks are the property of their respective owners. No sponsorship, endorsement or approval of this content by the owners of such marks is intended, expressed or implied.

 
 
Unit - I 
Introduction, History, Intelligent Systems, Foundations of AI, Sub areas of AI, 
Applications.  Problem Solving – State -Space Search and Control Strategies: 
Introduction, General Problem  Solving, Characteristics of Problem, Exhaustive 
Searches, Heuristic Search Techniques,  Iterative -Deepening A*, Constraint 
Satisfaction  
Topic: AI Introduction:  
In today's world, technology is growing very fast, and we are getting in touch 
with different new technologies day by day.  Here, one of the booming 
technol ogies of computer science is Artificial Intelligence which is ready to 
+create a new revolution in the world by making intelligent machines.The 
Artificial Intelligence is now all around us. It is currently working with a 
variety of subfields, ranging from general to specific, such as self -driving cars, 
playing chess, proving theorems, playing music, Painting, etc.  
AI is one of the fascinating and universal fields of Computer science which has 
a great scope in future. AI holds a tendency to cause a machine t o work as a 
human.   
Artificial Intelligence is composed of two words  Artificial  and Intelligence , 
where Artificial defines  "man -made,"  and intelligence defines  "thinking 
power" , hence AI means  "a man -made thinking power."  
So, we can define AI as:  
 "It is a branch of computer science by which we can create intelligent 
machines which can behave like a human, think like humans, and able to make 
decisions."   
Artificial Intelligence exists when a machine can have human based skills such 
as learning, reasoning, and solving problems  
With Artificial Intelligence you do not need to preprogram a machine to do 
some work, despite that you can create a machine with programmed 
algorithms which can work with own intelligence, and that is the 
awesomeness of AI.  
It is belie ved that AI is not a new technology, and some people says that as per 
Greek myth, there were Mechanical men in early days which can work and 
behave like humans.  
Why Artificial Intelligence?  
Before Learning about Artificial Intelligence, we should know that  what is the 
importance of AI and why should we learn it. Following are some main 
reasons to learn about AI:  
o With the help of AI, you can create such software or devices which can 
solve real -world problems very easily and with accuracy such as health 
issues, marketing, traffic issues, etc.  
o With the help of AI, you can create your personal virtual Assistant, such 
as Cortana, Google Assistant, Siri, etc.  
o With the help of AI, you can build such Robots which can work in an 
environment where survival of hum ans can be at risk.  
o AI opens a path for other new technologies, new devices, and new 
Opportunities.  
Goals of Artificial Intelligence : 
Following are the main goals of Artificial Intelligence:  
1. Replicate human intelligence  
2. Solve Knowledge -intensive tasks  
3. An i ntelligent connection of perception and action  4. Building a machine which can perform tasks that requires human 
intelligence such as:  
o Proving a theorem  
o Playing chess  
o Plan some surgical operation  
o Driving a car in traffic  
5. Creating some system which can exhibit  intelligent behavior, learn new 
things by itself, demonstrate, explain, and can advise to its user.  
What Comprises to Artificial Intelligence?  
Artificial Intelligence is not just a part of computer science even it's so vast 
and requires lots of other fact ors which can contribute to it. To create the AI 
first we should know that how intelligence is composed, so the Intelligence is 
an intangible part of our brain which is a combination of  Reasoning, 
learning, problem -solving perception, language understandin g, etc . 
To achieve the above factors for a machine or software Artificial Intelligence 
requires the following discipline:  
o Mathematics  
o Biology  
o Psychology  
o Sociology  
o Computer Science  
o Neurons Study  
o Statistics   
Advantages of Artificial Intelligence : 
Following are some main advantages of Artificial Intelligence:  
o High Accuracy with less errors:  AI machines or systems are prone to 
less errors and high accuracy as it takes decisions  as per pre -experience 
or information.  
o High -Speed:  AI systems can be of very high -speed and fast-decision 
making , because of that AI systems can beat a chess champion in the 
Chess game.  
o High reliability:  AI machines are highly reliable and can perform the 
same action multiple times with high accuracy.  
o Useful for risky areas:  AI machines c an be helpful in situations such as 
defusing a bomb, exploring the ocean floor, where to employ a human 
can be risky.  
o Digital Assistant:  AI can be very useful to provide digital assistant to 
the users such as AI technology is currently used by various E-
commerce websites to show the products as per customer requirement.  
o Useful as a public utility:  AI can be very useful for public utilities such 
as a self-driving car which can make our journey safer and hassle -free, 
facial recognition for security purpose, N atural language processing to 
communicate with the human in human -language, etc.  
Disadvantages of Artificial Intelligence : 
Every technology has some disadvantages, and thesame goes for Artificial 
intelligence. Being so advantageous technology still, it has  some disadvantages 
which we need to keep in our mind while creating an AI system. Following are 
the disadvantages of AI:  
o High Cost:  The hardware and software requirement of AI is very costly 
as it requires lots of maintenance to meet current world requirements.  
o Can't think out of the box:  Even we are making smarter machines with 
AI, but still they cannot work out of the box, as the robot will only do 
that work for which they are trained, or programmed.  
o No feelings and emotions : AI machines can be an  outstanding 
performer, but still it does not have the feeling so it cannot make any 
kind of emotional attachment with human, and may sometime be 
harmful for users if the proper care is not taken.  
o Increase dependency on machines:  With the increment of tech nology, 
people are getting more dependent on devices and hence they are losing 
their mental capabilities.  o No Original Creativity : As humans are so creative and can imagine 
some new ideas but still AI machines cannot beat this power of human 
intelligence an d can not be creative and imaginative  
 
Topic:History of Artificial Intelligence:  
Artificial Intelligence is not a new word and not a new technology for 
researchers. This technology is much older than you would imagine.  
 
 
 
Maturation of Artificial Intelligence (1943 -1952)  
o Year 1943:  The first work which is now recognized as AI was done by 
Warren McCulloch and Walter pits in 1943. They proposed a model 
of artificial neurons . 
o Year 1949:  Donald Hebb demonstrated an updating rule for modifying the 
conne ction strength between neurons. His rule is now called  Hebbian 
learning . 
o Year 1950:  The Alan Turing who was an English mathematician and 
pioneered Machine learning in 1950. Alan Turing publishes  "Computing 
Machinery and Intelligence"  in which he proposed a  test. The test can 
check the machine's ability to exhibit intelligent behavior equivalent to 
human intelligence, called a  Turing test . 
The birth of Artificial Intelligence (1952 -1956)  
o Year 1955:  An Allen Newell and Herbert A. Simon created the "first 
artificial intelligence program"Which was named as  "Logic Theorist" . This 
program had proved 38 of 52 Mathematics theorems, and find new and more 
elegant proofs for some theorems.  
o Year 1956:  The word "Artificial Intelligence" first adopted by American 
Computer  scientist John McCarthy at the Dartmouth Conference. For the 
first time, AI coined as an academic field.  
At that time high -level computer languages such as FORTRAN, LISP, or COBOL 
were invented. And the enthusiasm for AI was very high at that time.  The go lden years -Early enthusiasm (1956 -1974)  
o Year 1966:  The researchers emphasized developing algorithms which can 
solve mathematical problems. Joseph Weizenbaum created the first chatbot 
in 1966, which was named as ELIZA.  
o Year 1972:  The first intelligent humanoid robot was built in Japan which 
was named as WABOT -1. 
The first AI winter (1974 -1980)  
o The duration between years 1974 to 1980 was the first AI winter duration. 
AI winter refers to the time period where computer scientist dealt with a 
severe shortag e of funding from government for AI researches.  
o During AI winters, an interest of publicity on artificial intelligence was 
decreased.  
A boom of AI (1980 -1987)  
o Year 1980:  After AI winter duration, AI came back with "Expert System". 
Expert systems were programmed that emulate the decision -making ability 
of a human expert.  
o In the Year 1980, the first national conference of the American Association 
of Artificial Intelligence  was held at Stanford University . 
The second AI winter (1987 -1993)  
o The duration bet ween the years 1987 to 1993 was the second AI Winter 
duration.  o Again Investors and government stopped in funding for AI research as due 
to high cost but not efficient result. The expert system such as XCON was 
very cost effective.  
The emergence of intellig ent agents (1993 -2011)  
o Year 1997:  In the year 1997, IBM Deep Blue beats world chess champion, 
Gary Kasparov, and became the first computer to beat a world chess 
champion.  
o Year 2002:  for the first time, AI entered the home in the form of Roomba, a 
vacuum cleaner.  
o Year 2006:  AI came in the Business world till the year 2006. Companies 
like Facebook, Twitter, and Netflix also started using AI.  
 
 
Deep learning, big data and artificial general intelligence (2011 -present)  
o Year 2011:  In the year 2011, IBM's Watso n won jeopardy, a quiz show, 
where it had to solve the complex questions as well as riddles. Watson had 
proved that it could understand natural language and can solve tricky 
questions quickly.  
o Year 2012:  Google has launched an Android app feature "Google n ow", 
which was able to provide information to the user as a prediction.  
o Year 2014:  In the year 2014, Chatbot "Eugene Goostman" won a 
competition in the infamous "Turing test."  o Year 2018:  The "Project Debater" from IBM debated on complex topics 
with two mas ter debaters and also performed extremely well.  
o Google has demonstrated an AI program "Duplex" which was a virtual 
assistant and which had taken hairdresser appointment on call, and lady on 
other side didn't notice that she was talking with the machine.  
Topic:Intelligent system:  
 intelligent systems can also include sophisticated  AI-based  software systems, such 
as chatbots , expert systems  and other types of software.  
Essentially, an  intelligent  device  is anything that contains a functional, although 
not usually  general -purpose , computer with Internet connectivity.   An embedded 
system  may be powerful and capable of complex processing and data analysis, but 
it is usually specialized for tasks relevant to the host machine.  
Intelligent systems exist all around us in point -of-sale ( POS) terminals,  digital 
televisions , traffic lights,  smart meters , automobiles,  digital signage  and a irplane 
controls, among a great number of other possibilities. Built -in intelligence is an 
integral component of the developing  internet of things ( IoT), in whi ch almost 
everything imaginable can be  provided with  unique identifiers  and the ability to 
automatically transfer data over a network without requiring human -to-human or 
human -to-computer interaction.    
Topic:Foundations of AI and sub areas of AI:  
AI into five distinct research areas  
  Machine Learning  
 Expert Systems  
 Computer Vision  
 Natural Language Processing  
 Robotics  
1.Machine Learning:  
Machine learning (ML) is the science of  empowering machines to make decisions 
without human intervention . This sub -discipline forms the backbone of AI, 
enabling computers to learn and interpret patterns in images, sounds, and structured 
data using multidimensional arrays. ML is fu rther subdivided into four types of 
learning:  
 Supervised learning : Given an array of features (i.e., week of the year, price, 
etc.) and a labeled output variable (e.g., sales), predict the best possible estimate 
of the label variable given some new input a rray. 
 Unsupervised learning : Given an array of features (e.g., demographic 
information, ZIP code, etc.), expose and visualize hidden relationships and 
anomalies within the array.   Semi -supervised : Given an array of features and a limited quantity of some 
labeled output variable, predict the best possible estimates for the missing label 
variables.  
 Reinforcement learning:  Given some objective, train an artificial agent to 
maximize its utility according to some user -defined utility function.  
 
Machine learning is to thank for many high -profile innovations over the 
past few years, but it’s more difficult to do correctly than commonly 
believed.  
 
 
2.Expert Systems:  
An expert system (ES) is an  artificial agent which  leverages pre -programmed 
knowled ge to offer advice or make decisions . An expert system can  take 
advantage of human insights  discovered through trial and error  
 Expert systems are  more predictable  and are  less likely to make extreme 
errors  when faced with previously -unseen inputs  
 Expert sy stems  have historically been faster and easier to implement , though 
ML has become much more accessible in recent years  
 
Google’s Nest thermometer  
Google’s Nest home automation technology is a prime example of an ES at work. 
Users program their preferences  into Nest over time, enabling the system to 
automatically adjust housing temperature to desired levels and reduce heating 
expenses. Using ML to predict desired temperatures could lead to wild swings in 
settings and energy costs, so user -defined logic is c ritical to stabilizing these 
predictions. (Another examples :MYCIN,DENDRAL,PXDES,CADET)  
 
3.Computer Vision : 
Computer vision (CV) is the  automatic extraction, analysis, and interpretation 
of images or videos . CV converts photos and videos into numerical arra ys, 
enabling ML algorithms to draw inferences, make predictions, and even generate 
new images based on user -defined inputs.  
 
 
Example of an im age being converted to an array:  
Potential uses for CV have been studied for decades, but CV has only recently 
become possible at scale thanks to three innovations:  
 More efficient algorithms:  Deep learning and convolutional neural 
networks.specifically  significantly reduces the memory footprint and 
computational runtime of CV tasks.  
 Better computing resources : GPU improvements, distributed architectures 
(e.g., Spark), and the availability of inexpensive cloud computing resources 
have made it cheaper than ever to run memory -hungry CV algorithms.  
 Availability of images to train on : The proliferation of social media pl atforms, 
community forums, and digital / mobile cameras have drastically increased the 
number of publicly -available images that can be used to train CV algorithms.  
 
 
These three innovations have opened the floodgates for new CV use cases, 
including self-driving cars and automated retailers (e.g., Amazon Go). As cameras, 
LIDAR (light detection and ranging) , and other spatial sensors become less 
expensive, we’ll soon find ways to alleviate many of our most inefficient processes 
using CV.  
 
4.Natural Language Processing : 
Natural language processing (NLP) is the  automatic extraction, analysis, and 
generation of human language . NLP algorithms parse sentences in various ways 
(e.g., splitting by word, splitting by letter, reading both left -to-right and rig ht-to-left, 
etc.) to automatically draw inferences about the writer’s meaning and intent. NLP’s 
various use cases include:  
 Named entity recognition and conference resolution  
 Part-of-speech tagging  
 Reading comprehension & question answering  
 Machine translat ion 
 Text summarization & topic modeling  
 Spellcheck & autocomplete  
Like CV, NLP has come a long way over the past decade thanks to innovations in 
deep learning that have made it faster and easier to train ML models on human 
language. In the past, engineers would spend hours examining, filtering, and 
transforming text to avoid computational bottlenecks. Today, out -of-the-box 
solutions like  fast.ai ’s NLP library  can crush reading comprehension accuracy 
records without need for time -intensive preprocessing.  
 
Siri and Alexa are great examples of NLP in action: by listening for “wake words”, 
these tools allow you to play music, search the Web, create to-do lists, and control 
popular smart -home products — all while your smartphone stays in your pocket. 
These virtual assistants will continue to improve over time as they gather data from 
existing users, unlocking new use cases and integrating with the mod ern enterprise.  
5.Robotics : 
Robotics is the  science of designing, constructing, operating, and applying 
robots  to solve human problems. Robots come in thousands of shapes and sizes, 
making it difficult to nail down the precise meaning of the term.  
 
Boston Dynamic’s Atlas  
The field of robotics research has evolved at breakneck speed over the past decade . 
Most robots rely on expert systems to accomplish their objective, but the robots of 
tomorrow will become exponentially more useful by incorporating m achine 
learning, computer vision, and natural language processing into their operating 
systems.  
Topic:AI Applications:  
1. AI in E -Commerce:  
a)Personalized Shopping  
Artificial Intelligence technology is used to create recommendation engines 
through which you can engage better with your customers. These 
recommendations are made in accordance with their browsing history, preference, 
and interests. It helps in improving your relationship with your customers and their 
loyalty towards your brand.  
b)AI-powered A ssistants  
Virtual shopping assistants and chatbots help improve the user experience while 
shopping online. Natural Language Processing is used to make the conversation 
sound as human and personal as possible. Moreover, these assistants can have real -
time e ngagement with your customers. Did you know that on amazon.com, soon, 
customer service could be handled by chatbots?  
c)Fraud Prevention  
Credit card frauds and fake reviews are two of the most significant issues that E -
Commerce companies deal with. By considering the usage patterns, AI can help 
reduce the possibility of credit card frauds taking place. Many customers prefer to 
buy a product or service based on customer reviews. AI can help identify and 
handle fake reviews.   2. AI in Navigation : 
Based on  research from  MIT, GPS technology can provide users with accurate, 
timely, and detailed information to improve safety. The technology uses a 
combination of Con volutional Neural Network and Graph Neural Network, which 
makes lives easier for users by automatically detecting the number of lanes and 
road types behind obstructions on the roads. AI is heavily used by Uber and many 
logistics companies to improve operat ional efficiency, analyze road traffic, and 
optimize routes.  
3. AI in Robotics:  
Robotics is another field where artificial intelligence applications are commonly 
used. Robots powered by AI use real -time updates to sense obstacles in its path and 
pre-plan i ts journey instantly.   
It can be used for - 
 Carrying goods in hospitals, factories, and warehouses  
 Cleaning offices and large equipment  
 Inventory management   
 
4. AI in Human Resource  
Did you know that companies use intelligent software to ease the hiring process?  
Artificial Intelligence helps with blind hiring. Using machine learning software, 
you can examine applications based on specific parameters. AI drive systems can 
scan job candidates' profiles, and resumes to provide recruiters an understanding of 
the talent pool they must choose from.    
5. AI in Healthcare : 
Artificial Intelligence finds diverse applications in the healthcare sector. AI is used 
in healthcare to build sophisticated machines that can detect diseases and identify 
cancer cells. AI can h elp analyze chronic conditions with lab and other medical 
data to ensure early diagnosis. AI uses the combination of historical data and 
medical intelligence for the discovery of new drugs.  
6. AI in Agriculture:  
Artificial Intelligence is used to identify defects and nutrient deficiencies in the 
soil. This is done using computer vision, robotics, and machine learning, AI can 
analyze where weeds are growing. AI bots can help to harvest crops at a higher 
volume and faster pace than human laborers.  7. AI in Ga ming:  
Another sector where Artificial Intelligence applications have found prominence is 
the gaming sector. AI can be used to create smart, human -like NPCs to interact 
with the players.  
It can also be used to predict human behavior using which game design and testing 
can be improved. The Alien Isolation games released in 2014 uses AI to stalk the 
player throughout the game. The game uses two Artificial Intelligence systems - 
‘Director AI’ that frequently knows your location and the ‘Alien AI,’ driven by 
sensors and behaviors that continuously hunt the player.  
8. AI in Automobiles:  
Artificial Intelligence is used to build self -driving vehicles. AI can be used along 
with the vehicle’s camera, radar, cloud services, GPS, and control signals to 
operate the vehic le. AI can improve the in -vehicle experience and provide 
additional systems like emergency braking, blind -spot monitoring, and driver -
assist steering.  
9. AI in Social Media:  
Instagram  
On Instagram, AI considers your likes and the accounts you follow to det ermine 
what posts you are shown on your explore tab.   Facebook  
Artificial Intelligence is also used along with a tool called DeepText. With this 
tool, Facebook can understand conversations better. It can be used to translate 
posts from different languages automatically.  
Twitter  
AI is used by Twitter for fraud detection, removing propaganda, and hateful 
content. Twitter also uses AI to recommend tweets that users might enjoy, based 
on what type of tweets they engage with.  
10. AI in Marketing:  
Artificial intelligence applications are popular in the marketing domain as well.  
 Using AI, marketers can deliver highly targeted and personalized ads with the 
help of behavioral analysis, pattern recognition, etc. It also helps with 
retargeting audiences at the righ t time to ensure better results and reduced 
feelings of distrust and annoyance.  
 AI can help with content marketing in a way that matches the brand's style and 
voice. It can be used to handle routine tasks like performance, campaign reports, 
and much more.    
 Chatbots powered by AI, Natural Language Processing, Natural Language 
Generation, and Natural Language Understanding can analyze the user's 
language and respond in the ways humans do.    AI can provide users with real -time personalizations based on their b ehavior and 
can be used to edit and optimize marketing campaigns to fit a local market's 
needs.   
Topic:  Problem Solving – State -Space Search and C ontrol 
Strategies: Introduction  
The reflex  agents  are known as the simplest agents because they directly map 
states into actions. Unfortunately, these agents fail to operate in an environment 
where the mapping is too large to store and learn.  Goal -based  agent,  on the other 
hand, considers future actions and the desired outcomes.  
Here, we will discuss one type of goal-based agent known as a  problem -solving  
agent , which uses atomic representation with no internal states visible to 
the problem -solving  algorithms . 
Problem -solving agent  
The problem -solving agent perfoms precisely by defining problems and its several 
solutions.  
According to psychology, “ a problem -solving  refers  to a state  where  we wish to 
reach  to a definite  goal from  a present  state  or condition.”  
According to computer science,  a problem -solving  is a part of artificial  
intelligence  which  encompasses  a number  of techniques  such as algorithms,  
heuristics  to solve  a problem.  
Therefore, a problem -solving agent is a  goal-driven  agent  and focuses on 
satisfying the goal.  
Steps  performed  by Problem -solving  agent  
 Goal  Formulation:  It is the first and simplest step in problem -solving. It organizes 
the steps/sequence required to formulate one goal out of multiple goals as well as actions to achieve that goal. Goal formulation is based on the current situation and 
the agent’s performa nce measure (discussed below).  
 Problem  Formulation:  It is the most important step of problem -solving which 
decides what actions should be taken to achieve the formulated goal. There are 
following five components involved in problem formulation:  
 Initial  State: It is the starting state or initial step of the agent towards its goal.  
 Actions:  It is the description of the possible actions available to the agent.  
 Transition  Model:  It describes what each action does.  
 Goal  Test:  It determines if the given state is a goal state.  
 Path  cost:  It assigns a numeric cost to each path that follows the goal. The 
problem -solving agent selects a cost function, which reflects its performance 
measure. Remember,  an optimal  solution  has the lowest  path  cost among  all the 
solutions . 
Note:  Initial  state,  actions , and  transition  model  together define the  state -
space  of the problem implicitly. State -space of a problem is a set of all states 
which can be reached from the initial state followed by any sequence of actions. 
The state -space  forms a directed map or graph where nodes are the states, links 
between the nodes are actions, and the path is a sequence of states connected by 
the sequence of actions.  
 Search:  It identifies all the best possible sequence of actions to reach the goal sta te 
from the current state. It takes a problem as an input and returns solution as its 
output.  
 Solution:  It finds the best algorithm out of various algorithms, which may be 
proven as the best optimal solution.  
 Execution:  It executes the best optimal solutio n from the searching algorithms to 
reach the goal state from the current state.  
Example  Problems  Basically, there are two types of problem approaches:  
 Toy Problem:  It is a concise and exact description of the problem which is used 
by the researchers to compare the performance of algorithms.  
 Real -world  Problem:  It is real -world based problems which require solutions. 
Unlike a toy problem, it does not depend on descriptions, but we can have a 
general formulation of the problem.  
Some  Toy Problems  
 8 Puzzle  Problem:  Here, we have a 3×3 matrix with movable tiles numbered from 
1 to 8 with a blank space. The tile adjacent to the blank space can slide into that 
space. The objective is to reach a specified goal state similar to the goal state, as 
shown in the below  figure.  
 In the figure, our task is to convert the current state into goal state by sliding digits 
into the blank space.  
 
In the above figure, our task is to convert the current(Start) state into goal state by 
sliding digits into the blank space.  
The prob lem formulation  is as follows:  
 States:  It describes the location of each numbered tiles and the blank tile.  
 Initial  State:  We can start from any state as the initial state.  
 Actions:  Here, actions of the blank space is defined, i.e., either  left, right,  up or 
down  
 Transition  Model:  It returns the resulting state as per the given state and actions.  
 Goal  test: It identifies whether we have reached the correct goal -state.  
 Path  cost:  The path cost is the number of steps in the path where the cost of each 
step is  1. 
Note:  The 8 -puzzle problem is a type of  sliding -block  problem  which is used for 
testing new search algorithms in  artificial  intelligence . 
 8-queens  problem:  The aim of this problem is to place eight queens on a 
chessboard in an order where no queen may attack another. A queen can attack 
other queens either  diagonally  or in same  row and column.  
From the following figure, we can understand the problem as well as its correct 
solution.  
 
It is noticed from the above figure that each queen is set into the chessboard in a 
position where no other queen is placed diagonally, in same row or column. 
Ther efore, it is one right approach to the 8 -queens problem.  
For this problem,  there  are two main  kinds  of formulation:  
 Incremental  formulation:  It starts from an empty state where the operator 
augments a queen at each step.  
Following  steps  are involved  in this formulation:  
 States:  Arrangement of any 0 to 8 queens on the chessboard.  
 Initial  State:  An empty chessboard  
 Actions:  Add a queen to any empty box.  
 Transition  model:  Returns the chessboard with the queen added in a box.  
 Goal  test: Checks whether 8 -quee ns are placed on the chessboard without any 
attack.  
 Path  cost:  There is no need for path cost because only final states are counted.  
In this formulation, there is approximately  1.8 x 1014 possible sequence to 
investigate.  
 Complete -state  formulation:  It starts with all the 8 -queens on the chessboard and 
moves them around, saving from the attacks.  
Following  steps  are involved  in this formulation  
 States:  Arrangement of all the 8 queens one per column with no queen attacking 
the other queen.  
 Actions:  Move th e queen at the location where it is safe from the attacks.  
This formulation is better than the incremental formulation as it reduces the state 
space from  1.8 x 1014 to 2057 , and it is easy to find the solutions.  
Some  Real -world  problems   Traveling  salespers on problem(TSP):  It is a  touring  problem  where the 
salesman can visit each city only once. The objective is to find the shortest tour 
and sell -out the stuff in each city.  
 VLSI  Layout  problem:  In this problem, millions of components and connections 
are posi tioned on a chip in order to minimize the area, circuit -delays, stray -
capacitances, and maximizing the manufacturing yield.  
The layout  problem  is split into two parts:  
 Cell layout:  Here, the primitive components of the circuit are grouped into cells, 
each performing its specific function. Each cell has a fixed shape and size. The 
task is to place the cells on the chip without overlapping each other.  
 Channel  routing:  It finds a specific route for each wire through the gaps between 
the cells.  
 Protein  Design:  The objective is to find a sequence of amino acids which will fold 
into 3D protein having a property to cure some disease.  
Searching  for solutions  
We have seen many problems. Now, there is a need to search for solutions to solve 
them.  
In this section, we will understand how searching can be used by the agent to solve 
a problem.  
For solving different kinds of problem, an agent makes use of different strategies 
to reach the goal by searching the best possible algorithms. This process of 
searching is known as  search  strategy.  
Measuring problem -solving performance  
Before discussing different search strategies, the  performance  measure  of an 
algorithm should be measured. Consequently, There are four ways to measure the 
performance of an algorithm:  Completeness:  It measures if the algorithm guarantees to find a solution (if any 
solution exist).  
Optimality:  It measures if the strategy searches for an optimal solution.  
Time  Complexity:  The time taken by the algorithm to find a solution.  
Space  Complexity:  Amount of memory required to perform a search.  
The complexity of an algorithm depends on  branching  factor  or maximum  
number  of successors , depth  of the shallowest  goal node  (i.e., number of steps 
from root to the path) and  the maximum  length  of any path  in a state  space.  
Search  Strategies  
There are two types of strategies that describe a solution for a given problem:  
Uninformed Search (Blind Search)  
This type of search strategy does not have any additional information about the 
states except the information provided in the problem definition. They can only 
generate the successors and distinguish a goal state from a non -goal state. These 
type of search does not maintain any internal state, that’s why it is also known 
as Blind  search.  
There  are following  types  of uninfo rmed  searches:  
 Breadth -first search  
 Uniform cost search  
 Depth -first search  
 Depth -limited search  
 Iterative deepening search  
 Bidirectional search  
Informed Search (Heuristic Search)  
This type of search strategy contains some additional information about the s tates 
beyond the problem definition. This search uses problem -specific knowledge to find more efficient solutions. This search maintains some sort of internal states via 
heuristic functions (which provides hints), so it is also called  heuristic  search . 
There  are following  types  of informed  searches:  
 Best first search (Greedy search)  
 A* search  
TOPIC:General Problem Solving : 
It was the first useful computer program that came into existence in the AI world. 
The goal was to make it work as a universal problem -solving machine .GPS was 
the first program that was intended to solve any general problem  
To program the GPS, the authors created a new language 
called  Information  Processin g Language  (IPL). The basic premise is to express 
any problem with a set of well -formed formulas. In a graph, the source refers to 
the starting node and the sink refers to the ending node.  
Even though GPS was intended to be a general purpose, it could onl y solve well -
defined problems, such as proving mathematical theorems in geometry and logic. 
It could also solve word puzzles and play chess.  
Solving a problem with GPS : 
Let's see how to structure a given problem to solve it using GPS:  
1. The first step is to define the goals . Let's say our goal is to get some milk 
from the grocery store.  2. The next step is to define the preconditions . These preconditions are in 
reference to the goals. To get milk from the grocery store, we need to have 
a mode of transportation  and the grocery store should have milk available.  
3. After this, we need to define the operators . If my mode of transportation is 
a car and if the car is low on fuel, then we need to ensure that we can pay 
the fueling station. We need to ensure that you can pay for the milk at the 
store.  
An operator takes care of the conditions and everything that affects them. It 
consists of actions, preconditions, and the changes resulting from taking actions. 
In this case, the action is giving money to the grocery store. O f course, this is 
contingent upon you having the money in the first place, which is the 
precondition. By giving them the money, you are changing your money 
condition, which will result in you getting the milk.  
There are many applications of machine learnin g that exist today. It is used in 
image recognition, robotics, speech recognition, predicting stock market 
behavior, and so on.  
Topic:Characterstic of problems:  
In order to choose the most appropriate problem solving method, it is necessary to 
analyze the  problem along various key dimensions.  
 Is the problem decomposable into a set of independent smaller or easier 
sub-problems?  
A very large and composite problem can be easily solved if it can be broken into 
smaller problems and recursion could be used.  For example, we want to solve : - ∫ 𝑥2 + 3𝑥 + 𝑠𝑖𝑛2𝑥 𝑐𝑜𝑠2𝑥 𝑑𝑥 
This can be done by breaking it into three smaller problems and solving each by 
applying specific rules. Adding the results we can find the complete 
solution.But  there are certain problems  which cannot be decomposed into sub -
problems.  
For example Blocks world problem in which, start and goal state are given as,  
 
Here, solution can be achieved by moving blocks in a sequence such that goal state 
can be derived.  
Can solution steps be ignored or at least undone if they prove unwise?  
Problem fall under three classes, (i) ignorable, (ii) recoverable and (iii) 
irrecoverable.  
This classification is with reference to the steps of the solution to a 
problem.Consider theorem proving. We may later find that it is of no use. We can 
still proceed further, since nothing is lost by this redundant step. This is an example 
of ignorable solutions steps.  
Now consider the  8 puzzle problem  tray and arranged in specific order.  
While moving from the start state towards goal state, we may make some stupid 
move but we can backtrack and undo the unwanted move. This only involves 
additional steps and the solution steps are recoverable.  
Lastly con sider the  game of chess . If a wrong move is made, it can neither be 
ignored nor be recovered. The thing to do is to make the best use of current 
situation and proceed. This is  an example of an irrecoverable solution steps.  
 Is the problem’s universe predictable?  
Problems can be classified into those with certain outcome ( eight puzzle  ) and those 
with unc ertain outcome (playing cards).  
In certain — outcome problems, planning can be done to generate a sequence of 
operators that guarantees to lead to solution.  
Planning helps to avoid unwanted solution steps.  
For uncertain outcome problems, planning can at be st generate a sequence of 
operators that has a good probability of leading to a solution.  
 What is the role of knowledge  
Though one could have unlimited computing power, the size of the knowledge base 
available for solving the problem does matter in arriving at a good solution.  Take for example the  game of playing chess , just the rules for determining legal 
moves and some simple control mechanism is sufficient to arrive a t a solution.  
But additional knowledge about good strategy and tactics could help to constrain 
the search and speed up the execution of the program. The solution would then be 
realistic.  
Exhaustive Searches : 
Many important problems require finding an element with a special property in a 
domain that grows exponentially (or faster) with an instance size. Typically, such 
problems arise in situations that involve —explicitly or implicitly —combinatorial 
objects such as permutations, combinations, and subsets  of a given set. Many such 
problems are optimization problems: they ask to find an element that maximizes or 
minimizes some desired characteristic such as a path length or an assignment cost.  
  
We illustrate exhausti ve search by applying it to two  importan t problems: the 
traveling salesman problem, and the knapsack prob lem. 
1.Traveling Salesman Problem : 
The traveling salesman problem (TSP)  has been intriguing researchers for the last 
150 years by its seemingly simple formulation, important applications, and  
interesting connections to other combinatorial problems. In layman’s terms, the 
problem asks to find the shortest tour through a given set of  n cities that visits each 
city exactly once before returning to the city where it started. The problem can be 
conveniently modeled by a weighted graph, with the graph’s vertices representing 
the cities and the edge weights specifying the distances. Then the problem can be 
stated as the problem of finding the shortest  Hamiltonian circuit  of the graph. (A Hamiltonia n circuit is defined as a cycle that passes through all the vertices of the 
graph exactly once. It is named after the Irish mathematician Sir William Rowan 
Hamilton (1805 –1865), who became interested in such cycles as an application of 
his algebraic discov eries.)  
  
It is easy to see that a Hamiltonian circuit can also be defined as a sequence 
of n + 1 adjacent vertices  vi0 , vi1, . . . , v in−1, vi0 , where the first vertex of the 
sequence  is the same as the last one and all the other  n − 1 vertices are dist inct. 
Further, we can assume, with no loss of generality, that all circuits start and end at 
one particular vertex (they are cycles after all, are they not?). Thus, we can get all 
the tours by generating all the permutations of  n − 1 intermediate cities, c ompute 
the tour lengths, and find the shortest among them.   
An inspection of Figure 3.7 reveals three pairs of tours that differ only by their 
direction. Hence, we could cut the number of vertex permutations by half. We 
could, for example, choose any two i ntermediate vertices, say,  b and c, and then 
consider only permutations in which  b precedes  c. (This trick implicitly defines a 
tour’s direction.)  
2.Knapsack Problem : 
Here is another well -known problem in algorithmics. Given  n items of known 
weights  w1, w 2, . . . , w n and values  v1, v2, . . . , v n and a knapsack of capacity  W , 
find the most valuable subset of the items that fit into the knapsack. If you do not 
like the idea of putting yourself in the shoes of a thief who wants to steal the most .  
Heuristic Search:  
A Heuristic  is a technique  to solve  a problem  faster  than classic  methods,  or to find 
an approximate  solution  when  classic  methods  cannot.  This is a kind of a shortcut  
as we often  trade  one of optimality,  completeness,  accuracy,  or precis ion for speed.  
A Heuristic  (or a heuristic  function)  takes  a look at search  algorithms.  At each 
branching  step, it evaluates  the available  information  and makes  a decision  on 
which  branch  to follow.  
It does so by ranking  alternatives.  The Heuristic  is any device  that is often  effective  
but will not guarantee  work  in every  case.  
So why do we need  heuristics ? One reason  is to produce,  in a reasonable  amount  of 
time,  a solution  that is good  enough  for the problem  in question.  It doesn’t  have  to 
be the best- an approximate  solution  will do since  this is fast enough.  
Most  problems  are exponential.  Heuristic  Search  let us reduce  this to a rather  
polynomial  number.  We use this in AI because  we can put it to use in situations  
where  we can’t  find known  algorithm.  

THE BIG BOOK OF GENERATIVE AICONTENTSIntroduction  ............................................................................................................................................................................................................ 3
The Path to Deploying Production-Quality GenAI Applications  ............................................................................................. 5 
Stage 0: Foundation Models  ................................................................................................................................................................................................................................................................ 5 
 Use Case: Introducing DBRX: A New State-of-the-Art Open LLM  ...................................................................................................................................................................... 5 
Stage 1: Prompt Engineering  .............................................................................................................................................................................................................................................................. 19 
 Use Case: Automated Analysis of Product Reviews Using Large Language Models  ......................................................................................................................... 20 
Stage 2: Retrieval Augmented Generation  ............................................................................................................................................................................................................................ 25 
 Use Case: Improve Your RAG Application Response Quality With Real-Time Structured Data  .................................................................................................. 27 
Stage 3: Fine-Tuning a Foundation Model  .............................................................................................................................................................................................................................. 33 
 Use Case: Creating a Bespoke LLM for AI-Generated Documentation  ........................................................................................................................................................ 34 
 Use Case: Efficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models  ..................................................... 43 
Stage 4: Pretraining  ................................................................................................................................................................................................................................................................................ 60 
 Use Case: Training Stable Diffusion From Scratch for <$50K With MosaicML  ........................................................................................................................................ 62 
 Use Case: Deep Dive: How We Trained Stable Diffusion for Less Than $50K  .......................................................................................................................................... 68 
Stage 5: LLM Evaluation  ......................................................................................................................................................................................................................................................................... 81 
 Use Case : Best Practices for LLM Evaluation of RAG Application  .................................................................................................................................................................... 82 
 Use Case: Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks  ............................................................................................. 98
Summary  ................................................................................................................................................................................................................ 117 
GenAI Training  ............................................................................................................................................................................................................................................................................................. 117 
Additional Resources  ............................................................................................................................................................................................................................................................................. 1172THE BIG BOOK OF GENERATIVE AIAchieving Production-Quality GenAI Requires New Tools and Skills
Generative AI has opened new worlds of possibilities for businesses and is being emphatically embraced 
across organizations. According to a recent MIT Tech Review  report, all 600 CIOs surveyed stated they are 
increasing their investment in AI, and 71% are planning to build their own custom large language models (LLMs) 
or other GenAI models. However, many organizations have found it challenging to deploy these applications at 
production quality. To meet the standard of quality required for customer-facing applications, AI output must 
be accurate, governed and safe. 
Data Infrastructure Must Evolve to Support  
GenAI-Powered Applications
Making the leap to generative AI is not just about deploying a chatbot; it requires a reshaping of the foundational 
aspects of data management. Central to this transformation is the emergence of data lakehouses  as the new 
“modern data stack.” These advanced data architectures are essential to harnessing the full potential of GenAI, 
driving faster, more cost-effective and wider democratization of data and AI technologies. As businesses 
increasingly rely on GenAI-powered tools and applications for competitive advantage, the underlying data 
infrastructure must evolve to support these advanced technologies effectively and securely.
No Matter Where You Are on Your Path to Deploying GenAI Applications, 
the Quality of Your Data Matters
Businesses need to achieve production quality with their GenAI applications. Developers need rich tools for 
understanding the quality of their data and model outputs, along with an underlying platform that lets them 
combine and optimize all aspects of the GenAI process. GenAI has many components such as data preparation, 
retrieval models, language models (either SaaS or open source), ranking and post-processing pipelines, prompt 
engineering, and training models on custom enterprise data.
To help you overcome common enterprise challenges with building GenAI, we’ve compiled a collection of 
technical content and code samples. We’ll start each section with a brief overview and then provide use cases 
and example code for reference. Introduction3THE BIG BOOK OF GENERATIVE AIIn this eBook, you’ll learn: 
 ■How to plan a path from basic to advanced GenAI applications, leveraging your organization’s data
 ■How to use retrieval augmented generation (RAG) to make an off-the-shelf AI system smarter
 ■How to evaluate LLMs and where you want to invest in more powerful AI tools and systems that drive 
more significant operational gain
 ■How to build a custom LLM that may be better, faster and cheaper for your organization
 ■When it might be worth it to pretrain your own model — and more
Use cases for GenAI covered:
 ■How to use LLMs to gain actionable insights from product reviews
 ■How to use RAG for a chatbot to improve the quality of output
 ■How to train your own generative AI model in a cost-effective manner
 ■How to monitor and evaluate your deployed LLMs and GenAI applications
4THE BIG BOOK OF GENERATIVE AIThe Path to Deploying 
Production-Quality 
GenAI ApplicationsStage 0: Foundation Models
Before setting off to create production-quality GenAI applications, we need to cover the base language models 
that serve as the foundation for layers of increasingly complex techniques. Foundation models commonly refer 
to large language models that have been trained over extensive datasets to be generally good at some task 
(chat, instruction following, code generation, etc.).
We won’t cover many models, as it is a constantly shifting landscape, but it is important to note that while 
underlying architectures may differ drastically, foundation models generally fall under two categories: 
proprietary (such as GPT-3.5 and Gemini) and open source (such as Llama2-70B and DBRX). The main difference 
between the two is that while proprietary models historically have an edge on outright performance, users have 
to send their data out to a third party and don’t have control over the underlying model as they’re often being 
updated and changed. 
Open source models, on the other hand, offer users full control over the model and the ability to run it on their 
own terms with their own governance and data privacy. Here’s a current list of many open source GenAI models  
across different domains that are all free for commercial use. Databricks has also created their own state-of-
the-art open source foundation model so users can build the highest-quality production GenAI applications.
Foundation Model Use Case
INTRODUCING DBRX: A NEW STATE-OF-THE-ART OPEN LLM
We are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of 
standard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides 
the open community and enterprises building their own LLMs with capabilities that were previously limited 
to closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with 
Gemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on 
programming, in addition to its strength as a general-purpose LLM.5THE BIG BOOK OF GENERATIVE AIThis state-of-the-art quality comes with marked improvements in training and inference performance. DBRX 
advances the state-of-the-art in efficiency among open models thanks to its fine-grained mixture-of-experts 
(MoE) architecture. Inference is up to 2x faster than LLaMA2-70B, and DBRX is about 40% of the size of Grok-1 in 
terms of both total and active parameter-counts. When hosted on Mosaic AI Model Serving, DBRX can generate 
text at up to 150 tok/s/user. Our customers will find that training MoEs is also about 2x more FLOP-efficient 
than training dense models for the same final model quality. End-to-end, our overall recipe for DBRX (including 
the pretraining data, model architecture, and optimization strategy) can match the quality of our previous-
generation MPT models with nearly 4x less compute.
Figure 1: DBRX outperforms established open source models on language understanding (MMLU), 
Programming (HumanEval), and Math (GSM8K).
6THE BIG BOOK OF GENERATIVE AIThe weights of the base model ( DBRX Base ) and the fine-tuned model ( DBRX Instruct ) are available on Hugging 
Face under an open license. Starting today, DBRX is available for Databricks customers to use via APIs, and 
Databricks customers can pretrain their own DBRX-class models from scratch or continue training on top of  
one of our checkpoints using the same tools and science we used to build it. DBRX is already being integrated 
into our GenAI-powered products, where — in applications like SQL — early rollouts have surpassed GPT-3.5 
Turbo and are challenging GPT-4 Turbo. It is also a leading model among open models and GPT-3.5 Turbo on 
RAG tasks.
Training mixture-of-experts models is hard. We had to overcome a variety of scientific and performance 
challenges to build a pipeline robust enough to repeatedly train DBRX-class models in an efficient manner. Now 
that we have done so, we have a one-of-a-kind training stack that allows any enterprise to train world-class MoE 
foundation models from scratch. We look forward to sharing that capability with our customers and sharing our 
lessons learned with the community.
Download DBRX today from Hugging Face ( DBRX Base , DBRX Instruct ), or try out DBRX Instruct in our HF Space , 
or see our model repository on github: databricks/dbrx .
What Is DBRX?
DBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token 
prediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which 
36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. Compared 
to other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of 
smaller experts. DBRX has 16 experts and chooses 4, while Mixtral and Grok-1 have 8 experts and choose 2. This 
provides 65x more possible combinations of experts and we found that this improves model quality. DBRX uses 
rotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA). It uses the GPT-
4 tokenizer as provided in the tiktoken  repository. We made these choices based on exhaustive evaluation and 
scaling experiments.7THE BIG BOOK OF GENERATIVE AIDBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We 
estimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of 
models. This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and 
Databricks notebooks for data processing, Unity Catalog  for data management and governance, and MLflow for 
experiment tracking. We used curriculum learning for pretraining, changing the data mix during training in ways 
we found to substantially improve model quality.
Quality on Benchmarks vs. Leading Open Models
Table 1 shows the quality of DBRX Instruct and leading established, open models. DBRX Instruct is the leading 
model on composite benchmarks, programming and mathematics benchmarks, and MMLU. It surpasses all chat 
or instruction fine-tuned models on standard benchmarks.
Composite benchmarks. We evaluated DBRX Instruct and peers on two composite benchmarks: the Hugging 
Face Open LLM Leaderboard  (the average of ARC-Challenge, HellaSwag, MMLU, TruthfulQA, WinoGrande,  
and GSM8k) and the Databricks Model Gauntlet  (a suite of over 30 tasks spanning six categories: world 
knowledge, commonsense reasoning, language understanding, reading comprehension, symbolic problem 
solving, and programming).
Among the models we evaluated, DBRX Instruct scores the highest on two composite benchmarks: the Hugging 
Face Open LLM Leaderboard (74.5% vs. 72.7% for the next highest model, Mixtral Instruct) and the Databricks 
Gauntlet (66.8% vs. 60.7% for the next highest model, Mixtral Instruct).
Programming and mathematics. DBRX Instruct is especially strong at programming and mathematics. It scores 
higher than the other open models we evaluated on HumanEval (70.1% vs. 63.2% for Grok-1, 54.8% for Mixtral 
Instruct, and 32.2% for the best-performing LLaMA2-70B variant) and GSM8k (66.9% vs. 62.9% for Grok-1, 61.1% 
for Mixtral Instruct, and 54.1% for the best-performing LLaMA2-70B variant). DBRX outperforms Grok-1, the next 
best model on these benchmarks, despite the fact that Grok-1 has 2.4x as many parameters. On HumanEval, 
DBRX Instruct even surpasses CodeLLaMA-70B Instruct, a model built explicitly for programming, despite the 
fact that DBRX Instruct is designed for general-purpose use (70.1% vs. 67.8% on HumanEval as reported by Meta 
in the CodeLLaMA blog ).
MMLU. DBRX Instruct scores higher than all other models we consider on MMLU, reaching 73.7%.8THE BIG BOOK OF GENERATIVE AIMODELDBRX  
INSTRUCTMIXTRAL 
INSTRUCTMIXTRAL 
BASELLAMA2-70  
B CHATLLAMA2-70  
B BASEGROK-11
Open LLM Leaderboard2  
(Avg of next 6 rows)74.5% 72.7% 68.4% 62.4% 67.9% —
ARC-challenge 25-shot 68.9% 70.1% 66.4% 64.6% 67.3% —
HellaSwag 10-shot 89.0% 87.6% 86.5% 85.9% 87.3% —
MMLU 5-shot 73.7% 71.4% 71.9% 63.9% 69.8% 73.0%
Truthful QA 0-shot 66.9% 65.0% 46.8% 52.8% 44.9% —
WinoGrande 5-shot 81.8% 81.1% 81.7% 80.5% 83.7% —
GSM8k CoT 5-shot 
maj@1366.9% 61.1% 57.6% 26.7% 54.1%62.9%  
(8-shot)
Gauntlet v0.34  
(Avg of 30+ diverse tasks)66.8% 60.7% 56.8% 52.8% 56.4% —
HumanEval5  
0-Shot, pass@1  
(Programming)70.1% 54.8% 40.2% 32.2% 31.0% 63.2%LLaMA2-70B Base
Table 1: Quality of DBRX Instruct and leading open models. See footnotes for details on how numbers were collected. 
Bolded and underlined is the highest score.9THE BIG BOOK OF GENERATIVE AIQuality on Benchmarks vs. Leading Closed Models
Table 2 shows the quality of DBRX Instruct and leading closed models. According to the scores reported by 
each model creator, DBRX Instruct surpasses GPT-3.5 (as described in the GPT-4 paper), and it is competitive 
with Gemini 1.0 Pro and Mistral Medium.
Across nearly all benchmarks we considered, DBRX Instruct surpasses or - at worst - matches GPT-3.5. DBRX 
Instruct outperforms GPT-3.5 on general knowledge as measured by MMLU (73.7% vs. 70.0%) and commonsense 
reasoning as measured by HellaSwag (89.0% vs. 85.5%) and WinoGrande (81.8% vs. 81.6%). DBRX Instruct 
especially shines on programming and mathematical reasoning as measured by HumanEval (70.1% vs. 48.1%) and 
GSM8k (72.8% vs. 57.1%).
DBRX Instruct is competitive with Gemini 1.0 Pro and Mistral Medium. Scores for DBRX Instruct are higher than 
Gemini 1.0 Pro on Inflection Corrected MTBench, MMLU, HellaSwag, and HumanEval, while Gemini 1.0 Pro is 
stronger on GSM8k. Scores for DBRX Instruct and Mistral Medium are similar for HellaSwag, while Mistral Medium 
is stronger on Winogrande and MMLU and DBRX Instruct is stronger on HumanEval, GSM8k, and Inflection 
Corrected MTBench.10MODELDBRX  
INSTRUCTGPT-3.57 GPT-48CLAUDE  
3 HAIKUCLAUDE 3 
SONNETCLAUDE 3 
OPUSGEMINI  
1.0 PROGEMINI  
1.5 PROMISTRAL  
MEDIUMMISTRAL 
LARGE
MT Bench  
(Inflection corrected , n=5)8.39 ± 0.08 — —8.41 ± 
0.04 8.54 ± 
0.099.03 ± 
0.068.23 ± 0.08 — 8.05 ± 0.128.90 ± 
0.06
MMLU 5-shot 73.7% 70.0% 86.4% 75.2% 79.0% 86.8% 71.8% 81.9% 75.3% 81.2%
HellaSwag 10-shot 89.0% 85.5% 95.3% 85.9% 89.0% 95.4% 84.7% 92.5% 88.0% 89.2%
HumanEval 0-Shot  
pass@1  
(Programming)70.1%  
temp=0, 
N=148.1% 67.0% 75.9% 73.0% 84.9% 67.7% 71.9% 38.4% 45.1%
GSM8k CoT maj@172.8%  
(5-shot)57.1%  
(5-shot)92.0%  
(5-shot)88.9% 92.3% 95.0%86.5%  
(maj1@32)91.7%  
(11-shot)66.7%  
(5-shot)81.0%  
(5-shot)
WinoGrande 5-shot 81.8% 81.6% 87.5% — — — — — 88.0% 86.7%
Table 2: Quality of DBRX Instruct and leading closed models. Other than Inflection Corrected MTBench (which we measured ourselves on model 
endpoints), numbers were as reported by the creators of these models in their respective whitepapers. See footnotes for additional details.11
THE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AIQuality on Long-Context Tasks and RAG
DBRX Instruct was trained with up to a 32K token context window. Table 3 compares its performance to that of 
Mixtral Instruct and the latest versions of the GPT-3.5 Turbo and GPT-4 Turbo APIs on a suite of long-context 
benchmarks (KV-Pairs from the Lost in the Middle  paper and HotpotQAXL, a modified version of HotPotQA that 
extends the task to longer sequence lengths). GPT-4 Turbo is generally the best model at these tasks. However, 
with one exception, DBRX Instruct performs better than GPT-3.5 Turbo at all context lengths and all parts of the 
sequence. Overall performance for DBRX Instruct and Mixtral Instruct are similar.
MODELDBRX  
INSTRUCTMIXTRAL  
INSTRUCTGPT-3.5 TURBO 
(API)GPT-4 TURBO 
(API)
Answer in Beginning Third of Context 45.1% 41.3% 37.3%* 49.3%
Answer in Middle Third of Context 45.3% 42.7% 37.3%* 49.0%
Answer in Last Third of Context 48.0% 44.4% 37.0%* 50.9%
2K Context 59.1% 64.6% 36.3% 69.3%
4K Context 65.1% 59.9% 35.9% 63.5%
8K Context 59.5% 55.3% 45.0% 61.5%
16K Context 27.0% 20.1% 31.7% 26.0%
32K Context 19.9% 14.0% — 28.5%
Table 3: The average performance of models on the KV-Pairs and HotpotQAXL benchmarks. Bold is the highest score. Underlined is the highest score 
other than GPT-4 Turbo. GPT-3.5 Turbo supports a maximum context length of 16K, so we could not evaluate it at 32K. *Averages for the beginning, 
middle, and end of the sequence for GPT-3.5 Turbo include only contexts up to 16K.12THE BIG BOOK OF GENERATIVE AIOne of the most popular ways to leverage a model’s context is retrieval augmented generation (RAG).  
In RAG, content relevant to a prompt is retrieved from a database and presented alongside the prompt to 
give the model more information than it would otherwise have. Table 4 shows the quality of DBRX on two RAG 
benchmarks — Natural Questions and HotPotQA — when the model is also provided with the top 10 passages 
retrieved from a corpus of Wikipedia articles using the embedding model bge-large-en-v1.5. DBRX Instruct  
is competitive with open models like Mixtral Instruct and LLaMA2-70B Chat and the current version  
of GPT-3.5 Turbo.
MODELDBRX  
INSTRUCTMIXTRAL  
INSTRUCTLLAMA2-70B 
CHATGPT 3.5 TUR -
BO (API)GPT 4 TURBO 
(API)
Natural Questions 60.0% 59.1% 56.5% 57.7% 63.9%
HotPotQA 55.0% 54.2% 54.7% 53.0% 62.9%
Table 4: The performance of the models measured when each model is given the top 10 passages retrieved from a Wikipedia corpus 
using bge-large-en-v1.5. Accuracy is measured by matching within the model’s answer. Bold is the highest score. Underlined is the 
highest score other than GPT-4 Turbo. 
Training Efficiency
Model quality must be placed in the context of how efficient the model is to train and use. This is especially 
so at Databricks, where we build models like DBRX to establish a process for our customers to train their own 
foundation models.
We found training mixture-of-experts models to provide substantial improvements in compute-efficiency for 
training (Table 5). For example, training a smaller member of the DBRX family called DBRX MoE-B (23.5B total 
parameters, 6.6B active parameters) required 1.7x fewer FLOPs to reach a score of 45.5% on the Databricks LLM 
Gauntlet than LLaMA2-13B required to reach 43.8%. DBRX MoE-B also contains half as many active parameters 
as LLaMA2-13B.13THE BIG BOOK OF GENERATIVE AILooking holistically, our end-to-end LLM pretraining pipeline has become nearly 4x more compute-efficient 
in the past ten months. On May 5, 2023, we released MPT-7B , a 7B parameter model trained on 1T tokens that 
reached a Databricks LLM Gauntlet score of 30.9%. A member of the DBRX family called DBRX MoE-A (7.7B total 
parameters, 2.2B active parameters) reached a Databricks Gauntlet score of 30.5% with 3.7x fewer FLOPs. This 
efficiency is the result of a number of improvements, including using an MoE architecture, other architecture 
changes to the network, better optimization strategies, better tokenization, and - very importantly - better 
pretraining data.
In isolation, better pretraining data made a substantial impact on model quality. We trained a 7B model on 1T 
tokens (called DBRX Dense-A) using the DBRX pretraining data. It reached 39.0% on the Databricks Gauntlet 
compared to 30.9% for MPT-7B. We estimate that our new pretraining data is at least 2x better token-for-token 
than the data used to train MPT-7B. In other words, we estimate that half as many tokens are necessary to reach 
the same model quality. We determined this by training DBRX Dense-A on 500B tokens; it outperformed MPT-7B 
on the Databricks Gauntlet, reaching 32.1%. In addition to better data quality, another important contributor to 
this token-efficiency may be the GPT-4 tokenizer, which has a large vocabulary and is believed to be especially 
token-efficient. These lessons about improving data quality translate directly into practices and tools that our 
customers use to train foundation models on their own data.
MODEL TOTAL PARAMS ACTIVE PARAMS GAUNTLET SCORE RELATIVE FLOPS
DBRX MoE-A 7.7B 2.2B 30.5% 1x
MPT-7B (1T tokens) — 6.7B 30.9% 3.7x
DBRX Dense-A (1T tokens) — 6.7B 39.0% 3.7x
DBRX Dense-A (500B tokens) — 6.7B 32.1% 1.85x
DBRX MoE-B 23.5B 6.6B 45.5% 1x
LLaMA2-13B — 13.0B 43.8% 1.7x
Table 5:  Details of several test articles that we used to validate the training efficiency of the DBRX MoE architecture and end-to-end training pipeline.14THE BIG BOOK OF GENERATIVE AIInference Efficiency
Figure 2 shows the end-to-end inference efficiency of serving DBRX and similar models using NVIDIA 
TensorRT-LLM with our optimized serving infrastructure and 16-bit precision. We aim for this benchmark 
to reflect real-world usage as closely as possible, including multiple users simultaneously hitting the same 
inference server. We spawn one new user per second, each user request contains an approximately 2000 
token prompt, and each response comprises 256 tokens.
In general, MoE models are faster at inference than their total parameter-counts would suggest. This is due 
to the fact that they use relatively few parameters for each input. We find that DBRX is no exception in this 
respect. DBRX inference throughput is 2-3x higher than a 132B non-MoE model.
Inference efficiency and model quality are typically in tension: bigger models typically reach higher quality, but 
smaller models are more efficient for inference. Using an MoE architecture makes it possible to attain better 
tradeoffs between model quality and inference efficiency than dense models typically achieve. For example, 
DBRX is both higher quality than LLaMA2-70B and - thanks to having about half as many active parameters - 
DBRX inference throughput is up to 2x faster (Figure 2). Mixtral is another point on the improved pareto frontier 
attained by MoE models: it is smaller than DBRX, and it is correspondingly lower in terms of quality but reaches 
higher inference throughput. Users of the Databricks Foundation Model APIs can expect to see up to 150 
tokens per second for DBRX on our optimized model serving platform with 8-bit quantization.15Figure 2:  Inference throughput for various model configurations on our optimized serving infrastructure using NVIDIA TensorRT-LLM at 16-bit 
precision with the best optimization flags we could find. Models are run in tensor-parallel across the entire node. The input prompt contains 
approximately 2000 prompt tokens and we generate 256 output tokens. One new user spawns every second.
16
THE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AIHow We Built DBRX
DBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband. The main process of building DBRX 
- including pretraining, post-training, evaluation, red-teaming, and refining - took place over the course of 
three months. It was the continuation of months of science, dataset research, and scaling experiments, not to 
mention years of LLM development at Databricks that includes the MPT  and Dolly  projects and the thousands 
of models we have built and brought to production with our customers.
To build DBRX, we leveraged the same suite of Databricks tools that are available to our customers. We 
managed and governed our training data using Unity Catalog. We explored this data using newly acquired  
Lilac AI . We processed and cleaned this data using Apache Spark™ and Databricks notebooks. We trained 
DBRX using optimized versions of our open-source training libraries: MegaBlocks , LLM Foundry , Composer , 
and Streaming . We managed large scale model training and finetuning across thousands of GPUs using our 
Mosaic AI Training service. We logged our results using MLflow . We collected human feedback for quality and 
safety improvements through Mosaic AI Model Serving and Inference Tables. We manually experimented with 
the model using the Databricks Playground. We found the Databricks tools to be best-in-class for each of their 
purposes, and we benefited from the fact that they were all part of a unified product experience.
Get Started With DBRX on Databricks
If you’re looking to start working with DBRX right away, it’s easy to do so with the Databricks Mosaic AI 
Foundation Model APIs . You can quickly get started with our pay-as-you-go pricing and query the model from 
our AI Playground  chat interface. For production applications, we offer a provisioned throughput option to 
provide performance guarantees, support for finetuned models, and additional security and compliance. To 
privately host DBRX, you can download the model from the Databricks Marketplace  and deploy the model on 
Model Serving .17THE BIG BOOK OF GENERATIVE AIConclusions
At Databricks, we believe that every enterprise should have the ability to control its data and its destiny in the 
emerging world of GenAI. DBRX is a central pillar of our next generation of GenAI products, and we look forward 
to the exciting journey that awaits our customers as they leverage the capabilities of DBRX and the tools we 
used to build it. In the past year, we have trained thousands of LLMs with our customers. DBRX is only one 
example of the powerful and efficient models being built at Databricks for a wide range of applications, from 
internal features to ambitious use-cases for our customers.
As with any new model, the journey with DBRX is just the beginning, and the best work will be done by those 
who build on it: enterprises and the open community. This is also just the beginning of our work on DBRX, and 
you should expect much more to come.
Contributions
The development of DBRX was led by the Mosaic  team that previously built the MPT model family, in 
collaboration with dozens of engineers, lawyers, procurement and finance specialists, program managers, 
marketers, designers, and other contributors from across Databricks. We are grateful to our colleagues, friends, 
family, and the community for their patience and support over the past months.
In creating DBRX, we stand on the shoulders of giants in the open and academic community. By making DBRX 
available openly, we intend to invest back in the community in hopes that we will build even greater technology 
together in the future. With that in mind, we gratefully acknowledge the work and collaboration of Trevor Gale  
and his MegaBlocks  project (Trevor’s PhD adviser is Databricks CTO Matei Zaharia), the PyTorch  team and 
the FSDP  project, NVIDIA  and the TensorRT-LLM  project, the vLLM  team and project, EleutherAI  and their 
LLM evaluation  project, Daniel Smilkov and Nikhil Thorat at Lilac AI , and our friends at the Allen Institute for 
Artificial Intelligence (AI2) .18THE BIG BOOK OF GENERATIVE AIStage 1: Prompt Engineering
Many companies still remain in the foundational stages of adopting generative AI technology. They have  
no overarching AI strategy in place, no clear use cases to pursue and no access to a team of data scientists and 
other professionals who can help guide the company’s AI adoption journey.
If this is like your business, a good starting point is an off-the-shelf LLM. While these LLMs lack the domain-
specific expertise of custom AI models, experimentation can help you plot your next steps. Your employees can 
craft specialized prompts and workflows  to guide their usage. Your leaders can get a better understanding of 
the strengths and weaknesses of these tools as well as a clearer vision of what early success in AI might look 
like. Your organization can use things like the Databricks AI Playground  to figure out where to invest in more 
powerful AI tools and systems that drive more significant operational gain and even use LLMs as a judge  to help 
evaluate responses.
PRACTICAL APPLICATIONS OF GENAI TECHNOLOGY
Let’s delve into a compelling use case that illustrates the power of prompt engineering with off-the-shelf  
LLMs. Consider the challenge many businesses face: sifting through vast amounts of product reviews  
to glean actionable insights. Without a dedicated team of data scientists or a clear AI strategy, this task  
might seem daunting. However, leveraging the flexibility of LLMs through prompt engineering offers a 
straightforward solution.19THE BIG BOOK OF GENERATIVE AIPrompt Engineering Use Case
Automated Analysis of Product Reviews Using Large Language Models
Keep track of customer feedback at scale
Check out our  LLM Solution Accelerators for Retail  for more details and to download the notebooks.
While conversational AI has garnered a lot of media attention in recent months, the capabilities of large  
language models (LLMs) extend well beyond conversational interactions. It's in these less prominent  
capabilities such as query response, summarization, classification and search that many organizations  
are finding immediate opportunities to supercharge their workforce and up-level customer experiences.
The potential of these applications is staggering. By one estimate , LLMs (and other generative AI  
 technologies) could, in the near future, address tasks that today occupy 60%–70% of employees’ time.  
Through augmentation, numerous studies  have shown that the time to complete various tasks performed  
by knowledge workers such as background research, data analysis and document writing can be cut in half.  
And still other studies  have shown that the use of these technologies can dramatically reduce the time for  
new workers to achieve full productivity.
But before these benefits can be fully realized, organizations must first rethink  the management of the 
unstructured information assets on which these models depend and find ways to mitigate the issues of bias 
and accuracy that affect their output. This is why so many organizations are currently focusing their efforts 
on focused, internal applications where a limited scope provides opportunities for better information access 
and human oversight can serve as a check to errant results. These applications, aligned with core capabilities 
already residing within the organization, have the potential to deliver real and immediate value, while LLMs and 
their supporting technologies continue to evolve and mature.20THE BIG BOOK OF GENERATIVE AIPRODUCT REVIEW SUMMARIZATION COULD USE A BOOST
To illustrate the potential of a more focused approach to LLM adoption, we consider a fairly simple and common 
task performed within many online retail organizations: product review summarization. Today, most organizations 
employ a modestly-sized team of workers to read and digest user feedback for insights that may help improve a 
product's performance or otherwise identify issues related to customer satisfaction.
The work is important but anything but sexy. A worker reads a review, takes notes, and moves on to the next. 
Individual reviews that require a response are flagged and a summary of the feedback from across multiple 
reviews are compiled for review by product or category managers.
This is a type of work that's ripe for automation. The volume of reviews that pour into a site mean the more 
detailed portions of this work are often performed on a limited subset of products across variable windows 
depending on a products importance. In more sophisticated organizations, rules detecting course or 
inappropriate language and models estimating user sentiment or otherwise classifying reviews for positive, 
negative or neutral experiences may be applied to help identify problematic content and draw a reviewer's 
attention to it. But either way, a lot is missed simply because we can't throw enough bodies at the problem to 
keep up and those bodies tend to become bored or fatigued with the monotony of the work.
LARGE LANGUAGE MODELS CAN AUTOMATE PRODUCT REVIEW ANALYSIS
By using an LLM, issues of scale and consistency can be easily addressed. All we need to do is bring the product 
reviews to the model and ask:
 ■What are the top three points of negative feedback found across these reviews?
 ■What features do our customers like best about this product?
 ■Do customers feel they are receiving sufficient value from the product relative to what they are being 
asked to pay?
 ■Are there any reviews that are especially negative or are using inappropriate language?21THE BIG BOOK OF GENERATIVE AIWithin seconds we can have a tidy response, allowing our product managers to focus on responding to issues 
instead of simply detecting them.
But what about the problem of accuracy and bias? Standards for identifying inaccuracies and bias in LLM 
output are evolving as are techniques for better ensuring that outputs align with an organization's expectations, 
and the fine-tuning of models using approved content can go a long way to ensure models have a preference to 
generate content that's at least aligned with how an organization prefers to communicate.
This is a long-winded way of saying there is no ideal solution to the problem as of yet. But when compared  
to where we are with human-driven processes and more simplistic models or rules-based approaches,  
the results are expected to be better or at a minimum no worse than what we currently experience.  
And given that these review summaries are for internal consumption, the impact of an errant model can  
be easily managed.
YOU CAN BUILD A SOLUTION FOR THIS TODAY
To demonstrate exactly how this work could be performed, we have built a Solution Accelerator  for summarizing 
product reviews. This is based heavily on a previously published blog  from Sean Owen that addressed some of 
the core technical challenges of tuning an LLM on the Databricks platform. For the accelerator, we are using the 
Amazon Product Reviews Dataset , which contains 51 million user-generated reviews across 2 million distinct 
books as this provides access to a wide range of reviewer content and presents a scaling challenge many 
organizations will recognize.
We imagine a scenario in which a team of product managers receives customer feedback through online 
reviews. These reviews are important for identifying issues that may need to be addressed regarding a 
particular item and for steering future books to be offered by the site. Without the use of technology, this team 
struggles to read all the feedback and summarize into a workable set notes. As a result, they limit their attention 
to just the most critical items and are able to only process the feedback on a sporadic basis.22THE BIG BOOK OF GENERATIVE AIBut using Databricks, they are able to set up a pipeline to collect feedback from a wider range of products 
and summarize these on a regular basis. Recognizing that positively rated products are likely to highlight the 
strengths of these books while lower rated products are likely to focus on their weaknesses, they separate  
these reviews based on user-provided ratings and task an LLM to extract different sets of information from  
each high-level category of reviews.
Summary metrics are provided to allow product managers an overview of the feedback received and are 
backed by more detailed summaries generated by the LLM (Figure 1).
Figure 1: Summary metrics and bullet-point details extracted from user reviews extracted using an LLM
23THE BIG BOOK OF GENERATIVE AIDATABRICKS BRINGS TOGETHER ALL THE COMPONENTS OF A SOLUTION
The scenario demonstrated above depends on the use of an LLM. In months prior, the use of such an LLM 
required access to specialized computational infrastructures, but with advances in the open source community 
and investments in the Databricks platform, we are now able to run the LLM in our local Databricks environment.
In this particular scenario, the sensitivity of the data was not a motivating factor for this choice. Instead, we 
found that the volume of reviews to be processed tipped the cost scales toward the use of Databricks, allowing 
us to trim about one-third of the cost of implementing a similar solution using a third-party service.
In addition, we found that by implementing our own infrastructure, we were able to scale the environment up 
for faster processing, tackling as many as 760,000 reviews per hour in one test without having to be concerned 
with constraints imposed by an external service. While most organizations will not have the need to scale quite 
to that level, it's nice to know it is there should it be.
But this solution is more than just an LLM. To bring together the whole solution we needed to develop a data 
processing workflow to receive incoming reviews, prepare them for submission to the model and to capture 
model output for further analysis. As a unified data platform, Databricks provides us the means to address  
 both data engineering and data science requirements without data replication. And when we are done 
processing the reviews, our analysts can use their tools of choice to query the output and make business 
decisions. Through Databricks, we have access to the full array of capabilities for us to build a solution aligned 
with our business’ needs.24THE BIG BOOK OF GENERATIVE AIStage 2: Retrieval Augmented Generation
Retrieval augmented generation (RAG) lets you bring in supplemental knowledge resources to make an  
off-the-shelf AI system smarter. RAG won’t change the underlying behavior of the model, but it will improve  
the quality  and accuracy of the responses.
However, at this point, your business should not be uploading its “mission-critical” data. Instead, the RAG 
process typically involves smaller amounts of nonsensitive information.
For example, plugging in an employee handbook can allow your workers to start asking the underlying model 
questions about the organization’s vacation policy. Uploading instruction manuals can help power a service 
chatbot. With the ability to query support tickets using AI, support agents can get answers quicker; however, 
inputting confidential financial data so employees can inquire about the company’s performance is likely a step 
too far.
To get started, your team should first consolidate and cleanse the data you intend to use. With RAG, it’s vital 
that your company stores the data in sizes that will be appropriate for the downstream models. Often, that 
requires users to splice it into smaller segments.
Then, you should seek out a tool like Databricks Vector Search , which enables users to quickly set up their own 
vector database. And because it’s governed by Unity Catalog, granular controls can be put in place to ensure 
employees are only accessing the datasets for which they have credentials.
Finally, you can then plug that endpoint into a LLM. A tool like Databricks MLflow helps to centralize the 
management of those APIs.25THE BIG BOOK OF GENERATIVE AIAmong the benefits of RAG  are reduced hallucinations, more up-to-date and accurate responses,  
and better domain-specific intelligence. RAG-assisted models are also a more cost-effective approach  
for most organizations.
While RAG will help improve the results from commercial models, there are still many limitations to the use 
of RAG. If your business is unable to get the results it wants, it’s time to move on to heavier-weight solutions, 
but moving beyond RAG-supported models often requires a much deeper commitment. The additional 
customization costs more and requires a lot more data.
That’s why it’s key that organizations first build a core understanding of how to use LLMs. By reaching the 
performance limitations of off-the-shelf models before moving on, you and your leadership can further hone  
in on where to allocate resources.
Enhance the Performance of Off-the-Shelf AI Models With RAG
Let’s explore a practical use case that demonstrates how real-time structured data can significantly improve 
the response quality of your RAG applications. This example will showcase how integrating dynamic information 
can transform the effectiveness and applicability of AI in your business operations.
26THE BIG BOOK OF GENERATIVE AIRAG Use Case
Improve Your RAG Application Response Quality With Real-Time Structured Data
by Mani Parkhe , Aakrati Talati , Sue Ann Hong , Craig Wiley , Chenen Liang  and Mingyang Ge
Retrieval augmented generation (RAG)  is an efficient mechanism to provide relevant data as context in 
GenAI applications. Most RAG applications typically use vector indexes to search for relevant context from 
unstructured data such as documentation, wikis, and support tickets. Yesterday, we announced Databricks 
Vector Search Public Preview that helps with exactly that. However, GenAI response quality can be enhanced by 
augmenting these text-based contexts with relevant and personalized structured data. Imagine a GenAI tool on 
a retail website where customers inquire, "Where’s my recent order?" This AI must understand that the query 
is about a specific purchase, then gather up-to-date shipment information for line items, before using LLMs to 
generate a response. Developing these scalable applications demands substantial work, integrating technologies 
for handling both structured and unstructured data with GenAI capabilities.
We are excited to announce the public preview of Databricks Feature & Function Serving, a low latency real-
time service designed to serve structured data from the Databricks Data Intelligence Platform. You can instantly 
access pre-computed ML features as well as perform real-time data transformations by serving any Python 
function from Unity Catalog. The retrieved data can then be used in real-time rule engines, classical ML, and 
GenAI applications.
Using Feature and Function Serving ( AWS )(Azure) for structured data in coordination with Databricks Vector 
Search ( AWS )(Azure) for unstructured data significantly simplifies productionalization of GenAI applications. 
Users can build and deploy these applications directly in Databricks and rely on existing data pipelines, 
governance, and other enterprise features. Databricks customers across various industries are using these 
technologies along with open source frameworks to build powerful GenAI applications such as the ones 
described in the table below.27INDUSTRY USE CASE
Retail  ■Product Recommendations / Search Ranking using user preferences, search history, location . . . etc.
 ■Image and metadata based product search
 ■Inventory management and forecasting using sales data, seasonal trends and market/competitive analysis
Education  ■Personalized learning plans based on past mistakes, historical trends and  cohorts
 ■Automated grading, feedback, follow-ups and progress reporting
 ■Content filtering for issued devices
Financial Services  ■Natural language apps for analysts and investors to correlate earning calls and reports with market intelligence and historical trends
 ■Fraud and risk analysis
 ■Personalized wealth management, retirement planning, what-if analysis and next-best actions 
Travel and Hospitality  ■Chatbots for personalized customer interactions and tailored travel recommendations
 ■Dynamic route planning using weather, live traffic patterns, and historical data
 ■Dynamic price optimization using competitive analysis and demand-based pricing
Healthcare and Life Sciences  ■Patient/member engagement and health summaries
 ■Support apps for personalized care, clinical decisions and care coordination
 ■R&D report summarization, clinical trial analysis, drug repurposing
Insurance  ■Risk assessment for mortgage underwriting using text and structured data about properties and neighborhoods
 ■User chatbots for questions about policies, risk and what-if analysis
 ■Claim processing automation
Technology and Manufacturing  ■Prescriptive maintenance and diagnostics for equipment using guided instruction
 ■Anomaly detection on live data stream against historical statistics
 ■Automated analysis for daily production / shift analysis and future planning
Media and Entertainment  ■In-app content discovery and recommendations, personalized email and digital marketing
 ■Content localization
 ■Personalized gaming experiences and game review28
THE BIG BOOK OF GENERATIVE AITHE BIG BOOK OF GENERATIVE AISERVING STRUCTURED DATA TO RAG APPLICATIONS
To demonstrate how structured data can help enhance the quality of a GenAI application, we use the following 
example for a travel planning chatbot. The example shows how user preferences (example: “ocean view” or 
“family friendly”) can be paired with unstructured information sourced about hotels to search for hotel matches. 
Typically hotel prices dynamically change based on demand and seasonality. A price calculator built into the 
GenAI application ensures that the recommendations are within the user's budget. The GenAI application that 
powers the bot uses Databricks Vector Search and Databricks Feature and Function Serving as building blocks 
to serve the necessary personalized user preferences and budget and hotel information using LangChain’s 
agents API.
You can find the complete notebook  for this RAG Chain application as depicted above. This application can be 
run locally within the notebook or deployed as an endpoint accessible by a chatbot user interface.29THE BIG BOOK OF GENERATIVE AIACCESS YOUR DATA AND FUNCTIONS AS REAL-TIME ENDPOINTS
With Feature Engineering in Unity Catalog you can already use any table with a primary key to serve features 
for training and serving. Databricks Model Serving supports using Python functions to compute features on-
demand . Built using the same technology available under the hood for Databricks Model Serving, feature and 
function endpoints can be used to access any pre-computed feature or compute them on-demand. With a 
simple syntax you can define a feature spec function in Unity Catalog that can encode the directed acyclic 
graph to compute and serve features as a REST endpoint.
 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28from databricks.feature_engineering import (
  FeatureFunction,
  FeatureLookup,
  FeatureEngineeringClient,
)
features = [
  # Lookup columns `latitude` and `longitude` from `restaurants` table in UC using the input `restaurant_id` as key
  FeatureLookup(
    table_name="main.default.restaurants",
    lookup_key="restaurant_id",
    features=["latitude”, “longitude"]
  ),
  # Calculate a new feature called `distance` using the restaurant and user's current location
  FeatureFunction(
    udf_name="main.default.distance",
    output_name="distance",
    # bind the function parameter with input from other features or from request.
    input_bindings={"user_latitude": "user_latitude", "user_longitude": "user_longitude",
                    "restaurant_latitude": "latitude", "restaurant_longitude": "longitude"},
  ),
]
fe = FeatureEngineeringClient()
# Create a feature spec with the features listed above.
# The FeatureSpec can be accessed in UC as a Function.
fe.create_feature_spec(
  name="main.default.restaurant_features",
  features=features,
)30THE BIG BOOK OF GENERATIVE AI 
1
2
3
4
 
5
6
8
9
10
11
12
13
14
15from databricks.feature_engineering.entities.feature_serving_endpoint import (
  ServedEntity,
  EndpointCoreConfig,
)
fe.create_feature_serving_endpoint(
  name="restaurant-features",
    config=EndpointCoreConfig(
    served_entities=ServedEntity(
      feature_spec_name="main.default.restaurant_features",
      workload_size="Small",
      scale_to_zero_enabled=True
    )
  )
)This feature spec function can be served in real-time as a REST endpoint. All endpoints are accessible in 
the Serving left navigation tab including features, function, custom trained models, and foundation models. 
Provision the endpoint using this API.
The endpoint can also be created using a UI workflow as shown in the following graphic
31THE BIG BOOK OF GENERATIVE AINow features can be accessed in real time by querying the endpoint:
To serve structured data to real-time AI applications, precomputed data needs to be deployed to operational 
databases. Users can already use external online stores as a source of precomputed features — for example 
DynamoDB  and Cosmos DB  are commonly used to serve features in Databricks Model Serving. Databricks 
Online Tables ( AWS )(Azure) adds new functionality that simplifies synchronization of precomputed features to a 
data format optimized for low latency data lookups. You can sync any table with a primary key as an online table 
and the system will set up an automatic pipeline to ensure data freshness. 
1
2
3
4
5
 
6curl \
  -u token:$DATABRICKS_TOKEN \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"dataframe_records": [{"user_latitude": 37.9711, "user_longitude": -122.3940, "restaurant_id": 5}]}' \  
  https://<databricks-instance>/serving-endpoints/restaurant-features/invocations
32THE BIG BOOK OF GENERATIVE AIStage 3: Fine-Tuning a Foundation Model
Moving beyond RAG to model fine-tuning lets you start building models that are much more deeply 
personalized to the business. If you have already been experimenting with commercial models across your 
operations, you are likely ready to advance to this stage. There’s a clear understanding at the executive level of 
the value of generative AI, as well as an understanding of the limitations of publicly available LLMs. Specific use 
cases have been established. And now, you and your enterprise are ready to go deeper.
With fine-tuning, you can take a general-purpose model and train it on your own specific data. For example, 
data management provider Stardog relies on the Mosaic AI tools from Databricks  to fine-tune the off-the-shelf 
LLMs they use as a foundation for their Knowledge Graph Platform. This enables Stardog’s customers to query 
their own data across the different silos simply by using natural language.
It’s imperative that organizations at this stage have an underlying architecture in place that will help ensure the 
data supporting the models is secure and accurate. Fine-tuning an AI system requires an immense amount of 
proprietary information and, as your business advances on the AI maturity curve, the number of models running 
will only grow, increasing the demand for data access.
That’s why you need to have the right mechanisms in place to track data from the moment it’s generated to 
when it’s eventually used, and why Unity Catalog  is such a popular feature among Databricks customers. With 
Unity Catalog’s data lineage capabilities, businesses always know where data is moving and who is accessing it.Any Unity Catalog table with primary keys can be used to serve features in GenAI applications using Databricks 
Online Tables.33THE BIG BOOK OF GENERATIVE AIFine-Tuning Use Cases
Creating a Bespoke LLM for AI-Generated Documentation  
It’s easier than you think: 2 engineers, 1 month and less than $1,000
by Matthew Hayes , Hongyi Zhang , Tao Feng , Jan van der Vegt , Zaheera Valani  and Reynold Xin
In this example, we share our experience from prototyping a hackathon project using off-the-shelf SaaS-based 
LLMs to creating a bespoke LLM that is better, faster, and cheaper. The new model took 2 engineers, 1 month 
and less than $1,000 in compute cost to develop. We hope you will find the learnings useful, as we believe 
they apply to a wide class of GenAI use cases. More importantly, it has allowed us to take advantage of rapid 
advances being made in open-source LLMs.
WHAT IS AI-GENERATED DOCUMENTATION?
At the center of each data platform lies a (potentially enormous) collection of datasets (often in the form of 
tables). In virtually every organization we have worked with, the vast majority of tables are not documented. The 
absence of documentation provides a number of challenges, including making it difficult for humans to discover 
the data needed for answering a business question, or more recently, for AI agents to automatically find 
datasets to use in response to questions (a key capability in our platform that we’re calling Data Intelligence ).34THE BIG BOOK OF GENERATIVE AIRather than relying on humans to document these datasets, we prototyped as part of our quarterly hackathon 
a new workflow using an off-the-shelf SaaS-based LLM to automatically generate documentation for tables 
and their columns based on their schema. This new workflow would automatically suggest descriptions for the 
tables and columns and allow users to either individually accept, bulk accept, or modify the suggestions for 
higher fidelity, as shown below. When we showed this prototype to some users, their immediate question was 
universally, “When can I have it?!”
35THE BIG BOOK OF GENERATIVE AICHALLENGES WITH LLMS
As we moved toward launching this feature to all our customers, we ran into three challenges with the model:
1. Quality:  The ultimate success of this feature depends on the quality of the generated documentation. 
Although we could measure the quality (in terms of how often they are accepted), we had limited knobs 
at our disposal to improve it, aside from basic prompting. During the private preview period, we also 
sometimes noticed the quality of the suggestions degrading, without any change to our codebase. Our 
speculation is that the SaaS LLM controller rolled out updates to the model that sometimes affected 
performance on specific tasks.
2. Performance (throughput):  We had limited API quota provisioned with the SaaS LLM provider. We 
work with tens of thousands of organizations, and it is not uncommon that a single organization would 
have millions of tables. It would take too long to generate documentation for all the tables based on the 
throughput quota.
3. Cost:  Related to the above, it was not cost-effective unless we started charging customers for using this 
specific feature.
We have heard similar concerns from a variety of customers as they try to move their LLM-based applications 
from a proof-of-concept to production and saw this as an excellent opportunity for us to explore alternatives 
for an organization like ours.
We experimented with different versions of the SaaS LLMs, but they all had the same challenges. This is not 
surprising in hindsight. The SaaS LLMs are an engineering marvel, but they are very general models that need to 
address all the use cases from table generation to conversing about the meaning of life. The generality means 
it needs to have an extremely large number of parameters, which limits how fast and how cheap it can return 
answers. As it continues to evolve to optimize for different use cases, it might also regress the narrower use case 
we have.36THE BIG BOOK OF GENERATIVE AIBUILDING A BESPOKE MODEL
To address the aforementioned challenges, we started building a bespoke model. It took a team of two 
engineers one month to build a customized, smaller LLM that was better, faster, and cheaper:
 ■Quality: Based on our evaluation (see the following section), the model is significantly better than the 
cheaper version of the SaaS model, and roughly equivalent to the more expensive version.
 ■Performance (throughput):  Because the bespoke model is a lot smaller, it can fit in A10 GPUs, and we 
can increase the inference throughput with horizontal scaling. The smaller GPUs are also more available, 
which enables us to generate the descriptions for all tables faster.
 ■Cost: Each fine-tuning run of the model only costs a few dollars, and in aggregate, it cost less than $1000 
to develop because we did a lot of experiments. It also resulted in a 10 fold reduction in inference cost.
The first step was to treat this as an applied machine learning problem. “Applied machine learning” sounds 
daunting and complicated, but all it meant was that we needed to:
 ■Find training datasets so we can bootstrap an initial model
 ■Identify an evaluation mechanism so we can measure the quality, before rolling it out to production
 ■Train and select models
 ■Collect real-world usage metrics, so we can monitor how well a monitor does in production
 ■Iterate and roll out new models to continuously improve the three dimensions: quality, performance, cost37THE BIG BOOK OF GENERATIVE AITRAINING DATA
We created the initial training dataset for this fine-tuning task, using two different sources of data:
1. North American Industry Classification System (NAICS) codes. This is a public dataset used by Federal 
statistical agencies in classifying business establishments for the purpose of collecting, analyzing, and 
publishing statistical data related to the U.S. business economy.
2. Databricks’ internal use case taxonomy curation datasets. This is a series of internal datasets created by 
our solution architects to show customers best practice architectures.
Then we synthesized CREATE TABLE statements using the above use cases to yield a diverse set of tables and 
generated sample responses including table descriptions and column comments using another LLM. In total,  
we generated ~3600 training examples. 
Notably, we didn’t use any customer data for training this powerful feature that all of our customers can  
benefit from. 
BOOTSTRAPPING MODEL EVALUATION
After the feature launch, we could measure a model’s quality through production metrics such as the rate of 
users accepting the suggestions. But before we made it to the launch, we needed a way to evaluate the model’s 
quality against that of the SaaS LLM.
To do that in an unbiased fashion, we set up a simple double-blind evaluation framework in which we asked 
4 employees to rate table descriptions generated from the two models we wanted to compare using a set of 
62 unseen tables. Our framework then generated a sheet where each row showed the input and showed both 
outputs in a randomized order. The evaluator would vote on the better sample (or give a tie). The framework then 
processed the votes from different evaluators to generate a report; it also summarizes the degree to which each 
of the evaluators agreed.
Based on our experiences so far, having an evaluation dataset of tens to hundreds of data points is a sufficient 
initial milestone and can be generalized to other use cases as well.38THE BIG BOOK OF GENERATIVE AIMODEL SELECTION AND FINE-TUNING
We considered the following criteria for model selection:
 ■Whether the license supports commercial use
 ■Performance (quality) of the model for text generation
 ■Speed of the model
Based on these criteria, MPT-7B and Llama2-7B were the leading candidates, as shown in our LLM guide . We 
considered larger models such as MPT-30B and Llama-2-13B. In the end we chose MPT-7B, as it has the best 
combination of quality and inference performance:
 ■There was no discernable difference in the quality between the MPT-7B and Llama-2-7B fine-tuned 
models for this task.
 ■The smaller 7B models, after fine-tuning, were already meeting the quality bar. It was significantly better 
than the cheaper version of the SaaS model, and roughly equivalent to the more expensive version.
 ■We did not yet observe a measurable benefit of using larger models for this task that would justify the 
increased serving costs.
 ■The latency for the smaller models was significantly better than the larger models while offering 
comparable quality so we could deliver a much snappier product experience.
 ■The smaller model could fit comfortably and be served using A10 GPUs, which were more readily 
available. Their abundance would mean higher inference throughput for the task.
The total time it took to fine-tune the model on the ~3600 examples was only around 15 minutes!
While we chose MPT-7B for our model, we believe the LLM landscape is changing rapidly and the best model 
today won’t be the best model tomorrow. That’s why we consider this to be an iterative and continuous process 
and are focused on using tools that make our evaluation efficient and fast.39THE BIG BOOK OF GENERATIVE AIKEY ARCHITECTURAL COMPONENTS OF OUR PRODUCTION PIPELINE
We were able to build this quickly by relying on the following key components of the Databricks Data 
Intelligence Platform:
 ■Databricks LLM fine-tuning:  It provides a very simple infrastructure for fine-tuning the models for our 
task. We prepared the training data in JSON format, and with a one-line CLI command, we were able to 
fine-tune the LLMs.
 ■Unity Catalog: The models that we use in production are registered in Unity Catalog (UC), providing the 
governance we need to not just for the data, but also the models. With its end-to-end lineage feature, UC 
also gives us traceability from the models back to the datasets they are trained on.
 ■Delta Sharing:  We used Delta Sharing to distribute the model to all production regions we have around 
the world for faster serving.
 ■Databricks optimized LLM serving : Once the models are registered in UC, they can be served using the 
new optimized LLM serving, which provides significant performance improvement in terms of throughput 
and latency improvement compared to traditional serving for LLM serving.40THE BIG BOOK OF GENERATIVE AICOST
The fine-tuning compute cost for the whole project was less than $1000 (each fine-tuning run cost only a few 
dollars). And the final result is a more than 10-fold reduction in cost. Why is the cost-saving so significant? It is 
not surprising if we consider the following:
 ■As mentioned earlier, the SaaS LLMs need to address all the use cases, including acting as a general 
chatbot. The generality requires an extremely large number of parameters, which incurs significant 
compute costs in inference.
 ■When we fine-tune for a more specific task, we can use a much smaller prompt. Larger, general-purpose 
models require longer prompts that include detailed instructions on what the input is and what form the 
output should take. Fine-tuned models can bake instructions and expected structure into the model 
itself. We found we were able to reduce the number of input tokens with no impact on performance by 
more than half.
 ■Inference costs scale with the number of input and output tokens, and costs scale linearly for SaaS 
services that are charged per token. With Databricks’ LLM Serving offering, we offer provisioned 
throughput charged per hour, which provides consistent latencies, uptime SLAs, and autoscaling. Because 
smaller LLMs can fit in smaller GPUs that are much cheaper and more available and because we offer a 
highly optimized runtime, we can aggressively drive down costs. Also, smaller LLMs scale up and down 
faster, meaning we can quickly scale up to meet peaks of demand and aggressively scale down when 
usage is lighter, creating substantial cost efficiency in production.41THE BIG BOOK OF GENERATIVE AICONCLUSION
Having well-documented data is critical to all data users, and growing more important day-by-day to power 
AI-based data platforms (what we’re calling Data Intelligence ). We started with SaaS LLMs for prototyping this 
new GenAI feature but ran into challenges with quality, performance, and cost. We built a bespoke model to do 
the same task at better quality, and yet resulting in higher throughput with scale-out and 10x cost reduction. To 
recap what it took:
 ■2 engineers
 ■1 month
 ■Less than $1,000 in compute for training and experimentation
 ■MPT-7B fine-tuned on 3600 synthetically generated examples, in under 15 minutes
 ■4 human evaluators, with 62 initial evaluation examples
This experience demonstrates how easy it is to develop and deploy bespoke LLMs for specific tasks. This model 
is now live on Databricks in Amazon Web Services and Google Cloud and is being used to power most data 
annotations on the platform.42THE BIG BOOK OF GENERATIVE AIEfficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models
by Avinash Sooriyarachchi
With the rapid advancement of neural network-based techniques and large language model (LLM) research, 
businesses are increasingly interested in AI applications for value generation. They employ various machine 
learning approaches, both generative and non-generative, to address text-related challenges such as 
classification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can 
opt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent 
results, enabling cost-effective and independent solutions deployable across different environments in  
a secure manner.
Ensuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-
tuning. This blog explores arguably the most popular and effective variant of such parameter efficient 
methods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of 
LoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious 
product descriptions when prompted with a product name and a category. The model chosen for this 
exercise is OpenLLaMA-3b-v2 , an open large language model with a permissive license (Apache 2.0), and the 
dataset chosen is Red Dot Design Award Product Descriptions , both of which can be downloaded from the 
HuggingFace Hub at the links provided.
FINE-TUNING, LORA AND QLORA
In the realm of language models, fine-tuning an existing language model to perform a specific task on specific 
data is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of 
the neural network through backpropagation during the training process. It is important to note the distinction 
between this fine-tuning process and training from scratch. In the latter scenario, the model's weights are 
randomly initialized, while in fine-tuning, the weights are already optimized to a certain extent during the 
pretraining phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends 
on the chosen technique.
Full fine-tuning involves optimizing or training all layers of the neural network. While this approach typically 
yields the best results, it is also the most resource-intensive and time-consuming.43THE BIG BOOK OF GENERATIVE AIFortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. 
Although most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked 
this trend by even outperforming full fine-tuning in some cases, as a consequence of avoiding catastrophic 
forgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the  
fine-tuning process).
LoRA is an improved fine-tuning method where instead of fine-tuning all the weights that constitute the  
weight matrix of the pretrained large language model, two smaller matrices that approximate this larger matrix 
are fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the 
pretrained model and used for inference.
QLoRA is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU  
memory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar 
effectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the 
best combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time 
will be the focus here.
LoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease  
of use and QLoRA can be leveraged by using bitsandbytes  and PEFT together. HuggingFace Transformer 
Reinforcement Learning (TRL)  library offers a convenient trainer for supervised fine-tuning with seamless 
integration for LoRA. These three libraries will provide the necessary tools to fine-tune the chosen pretrained 
model to generate coherent and convincing product descriptions once prompted with an instruction  
indicating the desired attributes.44THE BIG BOOK OF GENERATIVE AIPREPPING THE DATA FOR SUPERVISED FINE-TUNING
To probe the effectiveness of QLoRA for fine-tuning a model for instruction following, it is essential to transform 
the data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a 
pretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is fine-
tuned on a dataset that has prompt-response pairs formatted in a consistent manner.
An example observation from our chosen dataset from the Hugging Face hub looks as follows:
As useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following 
in the manner described.
The following code snippet loads the dataset from the Hugging Face hub into memory, transforms the 
necessary fields into a consistently formatted string representing the prompt, and inserts the response (i.e., 
the description), immediately afterward. This format is known as the ‘Alpaca format’ in large language model 
research circles as it was the format used to fine-tune the original LlaMA model from Meta to result in the 
Alpaca model, one of the first widely distributed instruction-following large language models (although not 
licensed for commercial use).PRODUCT CATEGORY DESCRIPTION TEXT
“Biamp Rack Products” “Digital Audio Processors" “High recognition value, uniform 
aesthetics and practical 
scalability — this has been 
impressively achieved with the 
Biamp brand language . . . ““Product Name: Biamp Rack Products; 
Product Category: Digital Audio 
Processors; Product Description: High 
recognition value, uniform aesthetics 
and practical scalability — this has been 
impressively achieved with the Biamp 
brand language . . . “45THE BIG BOOK OF GENERATIVE AI 
1
2
3
4 
5
6
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26import pandas as pd
from datasets import load_dataset
from datasets import Dataset
#Load the dataset from the HuggingFace Hub
rd_ds = load_dataset("xiyuez/red-dot-design-award-product-description")
#Convert to pandas dataframe for convenient processing
rd_df = pd.DataFrame(rd_ds['train'])
#Combine the two attributes into an instruction string
rd_df['instruction'] = 'Create a detailed description for the following product: '+ rd_df['product']+', belonging to 
category: '+ rd_df['category']
rd_df = rd_df[['instruction', 'description']]
#Get a 5000 sample subset for fine-tuning purposes
rd_df_sample = rd_df.sample(n=5000, random_state=42)
#Define template and format data into the template for supervised fine-tuning
template = """Below is an instruction that describes a task. Write a response that appropriately completes the 
request.
### Instruction:
{}
### Response:\n"""
rd_df_sample['prompt'] = rd_df_sample["instruction"].apply(lambda x: template.format(x))
rd_df_sample.rename(columns={'description': 'response'}, inplace=True)
rd_df_sample['response'] = rd_df_sample['response'] + "\n### End"
rd_df_sample = rd_df_sample[['prompt', 'response']]
rd_df['text'] = rd_df["prompt"] + rd_df["response"]
rd_df.drop(columns=['prompt', 'response'], inplace=True)
The resulting prompts are then loaded into a hugging face dataset for supervised fine-tuning. Each such prompt 
has the following format.46THE BIG BOOK OF GENERATIVE AI 
1
2
3
4 
5
6
8
9
10
11
12
13```
Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
Create a detailed description for the following product: Beseye Pro, belonging to category: Cloud-Based Home Security 
Camera
### Response:
Beseye Pro combines intelligent home monitoring with decorative art. The camera, whose form is reminiscent of a water 
drop, is secured in the mounting with a neodymium magnet and can be rotated by 360 degrees. This allows it to be 
easily positioned in the desired direction. The camera also houses modern technologies, such as infrared LEDs, cloud-
based intelligent video analyses and SSL encryption.
### End
```
To facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset  
of this data.
TESTING MODEL PERFORMANCE BEFORE FINE-TUNING
Before any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a 
baseline for pretrained model performance.
The model can be loaded in 8-bit as follows and prompted with the format specified in the model card on 
Hugging Face .47THE BIG BOOK OF GENERATIVE AI 
1
2
3
4 
5
6
7
8
9
10
11
12
13
14
15import torch
from transformers import LlamaTokenizer, LlamaForCausalLM
model_path = 'openlm-research/open_llama_3b_v2'
tokenizer = LlamaTokenizer.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(
model_path, load_in_8bit=True, device_map='auto',
)
#Pass in a prompt and infer with the model
prompt = 'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: 
Optical Mouse\nA:'
input_ids = tokenizer(prompt, return_tensors="pt").input_ids
generation_output = model.generate(
input_ids=input_ids, max_new_tokens=128
)
print(tokenizer.decode(generation_output[0]))
 
1
2
3
4 
5
6Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical 
Mouse A: The Corelogic Smooth Mouse is a wireless optical mouse that has a 1000 dpi resolution. It has a 2.4 GHz 
wireless connection and a 12-month warranty. Q: What is the price of the Corelogic Smooth Mouse? A: The Corelogic 
Smooth Mouse is priced at $29.99. Q: What is the weight of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse 
weighs 0.1 pounds. Q: What is the dimensions of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse has a 
dimensionThe output obtained is not quite what we want.
The first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.48THE BIG BOOK OF GENERATIVE AI 
1
2
3
4 
5
6
7
8
9prompt= """Below is an instruction that describes a task. Write a response that appropriately completes the request.
### Instruction:
Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse
### Response:"""
input_ids = tokenizer(prompt, return_tensors="pt").input_ids
generation_output = model.generate(
input_ids=input_ids, max_new_tokens=128
)
print(tokenizer.decode(generation_output[0]))
 
1
2
3
4 
5
6Corelogic Smooth Mouse is a mouse that is designed to be used by people with disabilities. It is a wireless mouse 
that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people 
with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless 
mouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by 
people with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a 
wireless mouse that is designed to be used bySimilarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is 
expected to be just as suboptimal:
And sure enough, it is:
The model performs what it was trained to do, predicts the next most probable token. The point of supervised 
fine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the 
subsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the  
inference process to examine output quality is done once the model has been loaded in 8-bit as shown  
above for consistency.49THE BIG BOOK OF GENERATIVE AITHE TURNABLE KNOBS
When using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference 
between the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), 
the hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:
Two of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality 
significantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the 
values indicated above for simplicity.
r represents the rank of the low rank matrices learned during the fine-tuning process. As this value is increased, 
the number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower 
r may lead to a quicker, less computationally intensive training process, but may affect the quality of the model 
thus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of 
model output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly. 
1
2
3
4 
5
6
7
8
9
10
11
12
13
14from peft import LoraConfig
...
...
#If only targeting attention blocks of the model
target_modules = ["q_proj", "v_proj"]
#If targeting all linear layers
target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']
lora_config = LoraConfig(
r=16,
target_modules = target_modules,
lora_alpha=8,
lora_dropout=0.05,
bias="none",
task_type="CAUSAL_LM",}50THE BIG BOOK OF GENERATIVE AIWhen fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation 
process will target these modules and apply the update matrices to them. Similar to the situation with "r," 
targeting more modules during LoRA adaptation results in increased training time and greater demand for 
compute resources. Thus, it is a common practice to only target the attention blocks of the transformer. 
However, recent work as shown in the QLoRA paper  by Dettmers et al. suggests that targeting all linear layers 
results in better adaptation quality. This will be explored here as well.
Names of the linear layers of the model can be conveniently appended to a list with the following code snippet:
TUNING THE FINE-TUNING WITH LORA
The developer experience of fine-tuning large language models in general have improved dramatically over the 
past year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To 
perform QLoRA, all that is needed is the following:
1. Load the model to GPU memory in 4-bit (bitsandbytes enables this process)
2. Define the LoRA configuration as discussed previously
3. Define the train and test splits of the prepped instruction following data into Hugging Face  
Dataset objects
4. Define training arguments: These include the number of epochs, batch size and other training 
hyperparameters which will be kept constant during this exercise
5. Pass these arguments into an instance of SFTTrainer
These steps are clearly indicated in the source file in the repository  associated with this blog. 
1
2
3
4 
5
6
7
8
9import re
model_modules = str(model.modules)
pattern = r'\((\w+)\): Linear'
linear_layer_names = re.findall(pattern, model_modules)
names = []
# Print the names of the Linear layers
for name in linear_layer_names:
    names.append(name)
target_modules = list(set(names))51THE BIG BOOK OF GENERATIVE AIThe actual training logic is abstracted away nicely as follows:
If MLflow autologging is enabled in the Databricks workspace, which is highly recommended, all the training 
parameters and metrics are automatically tracked and logged with the MLflow tracking server. This functionality 
is invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed 
using a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks 
Machine Runtime with GPU support. 
1
2
3
4 
5
6
7
8
9
10
11trainer = SFTTrainer(
model,
train_dataset=dataset['train'],
eval_dataset = dataset['test'],
dataset_text_field="text",
max_seq_length=256,
args=training_args,
)
# Initiate the training process
with mlflow.start_run(run_name= ‘run_name_of_choice’):
trainer.train()
52THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”
The first combination of QLoRA hyperparameters attempted is r=8 and targets only the attention blocks, namely 
“q_proj” and “v_proj” for adaptation.
The following code snippets gives the number of trainable parameters:
These choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a 
total of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire 
fine-tuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The 
GPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations. 
1
2model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
53THE BIG BOOK OF GENERATIVE AIAt the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the 
pretrained model as follows:
This model can now be used for inference as any other model.
Qualitative Evaluation  
A couple of example prompt-response pairs are listed below
Prompt (passed to the model in the Alpaca format, not shown for conciseness here):  
Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  
Optical Mouse
Response:
Prompt:  
Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless 
Vacuum Cleaner
Response:
The model has clearly been adapted for generating more consistent descriptions. However the response to the 
first prompt about the optical mouse is quite short and the following phrase “ The vacuum cleaner is equipped 
with a dust container that can be emptied via a dust container ” is logically flawed. 
1peft_model = PeftModel.from_pretrained(model, adapter_location)
 
1
2The Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI 
sensor and a 1000 Hz polling rate. The mouse is available in black and white.
 
1
2
3
4The Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is 
charged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a turbo brush. The brush is suitable 
for cleaning carpets and hard floors. The turbo brush is suitable for cleaning carpets and hard floors. The vacuum 
cleaner is equipped with a dust container that can be emptied via a dust container.54THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #2: QLoRA with r=16 and targeting all linear layers
Surely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during 
adaptation to 16, i.e., double the value of r to 16 and keep all else  the same. This doubles the number of trainable 
parameters to 5,324,800 (~5.3 million)
Qualitative Evaluation  
The quality of output, however, remains unchanged for the same exact prompts.
Prompt:  
Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  
Optical Mouse
Response:
 
1
2The Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 
DPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.
 
1
2
3
4
5The Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is 
charged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a motorized brush. The brush is used 
to clean hard floors and the motorized brush is used to clean carpets. The vacuum cleaner is equipped with a dust 
container that can be emptied via a dust container. The vacuum cleaner is equipped with a LED display that shows the 
remaining battery capacity.Prompt:  
Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless 
Vacuum Cleaner
Response:
The same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is 
used for product description generation in a real-world scenario, this is not acceptable output.55THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #3: QLoRA with r=8 and targeting all linear layers
Given that doubling r does not seemingly result in any perceivable increase in output quality, it is worth 
changing the other important knob. i.e., targeting all linear layers instead of just the attention blocks. Here, the 
LoRA hyperparameters are r=8 and target_layers are  'q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_
proj' and 'lm_head'. This increases the number of parameters updated to 12,994,560 and increases the training 
time to roughly 15.5 minutes.
Qualitative Evaluation  
Prompting the model with the same prompts yield the following:
Prompt:  
Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  
Optical Mouse
Response:
Prompt:  
Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless 
Vacuum Cleaner
Response:
Now it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are 
no logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but 
relevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the 
model’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner. 
1
2
3The Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI 
sensor and a 1000 Hz polling rate. The mouse is equipped with a 2.4 GHz wireless connection and a USB receiver. The 
mouse is available in black and white.
 
1
2
3
4The Hoover Lightspeed cordless vacuum cleaner is a high-performance device that is easy to use. The device is 
equipped with a lithium-ion battery that provides up to 40 minutes of runtime. The device is equipped with a 
high-performance motor that provides a suction power of up to 200 watts. The device is equipped with a 2-in-1 dusting 
brush and a crevice tool. The device is equipped with a LED display that shows the remaining battery charge.56THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #4: LoRA with  r=8 and targeting all linear transformer layers
It is also worth exploring whether the quality of output from the model improves if the pretrained model is 
frozen in 8-bit instead of 4-bit. In other words, replicating the exact fine-tuning process using LoRA instead of 
QLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, 
i.e., r=8 and targeting all linear transformer layers during the adaptation process.
Qualitative Evaluation  
The results for the two prompts used throughout the article are as given below:
Prompt:  
Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  
Optical Mouse
Response:
Prompt:  
Create a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless 
Vacuum Cleaner
Response:
Again, there isn’t much of an improvement in the quality of the output text.  
1
2
3The Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 
DPI sensor and a 1000 Hz polling rate. The mouse is available in three colours: black, white and red. The mouse is 
equipped with a USB receiver and is compatible with Windows and Mac.
 
1
2
3 
4The Hoover Lightspeed cordless vacuum cleaner is a compact and lightweight device that is easy to use. The device is 
equipped with a lithium-ion battery that provides up to 40 minutes of cleaning time. The vacuum cleaner is equipped 
with a high-performance filter that ensures that the air is cleaned of dust and allergens. The device is equipped 
with a 2-in-1 dusting brush and a crevice tool that can be used to clean hard-to-reach areas.57THE BIG BOOK OF GENERATIVE AIKEY OBSERVATIONS
Based on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, 
it can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve 
adaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers 
in the adaptation process, as opposed to just the attention blocks, as commonly documented in technical 
literature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that 
QLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.
FURTHER CONSIDERATIONS FOR USING LORA ADAPTERS IN DEPLOYMENT
It's important to optimize the usage of adapters and understand the limitations of the technique. The size of the 
LoRA adapter obtained through fine-tuning is typically just a few megabytes, while the pretrained base model 
can be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM 
need to be loaded, so the memory requirement remains similar.
Furthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase 
in inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can 
be done with a single line of code as shown here:
The figure below outlines the process from fine-tuning an adapter to model deployment. 
1merged_model = peft_model.merge_and_unload()
58THE BIG BOOK OF GENERATIVE AIWhile the adapter pattern offers significant benefits, merging adapters is not a universal solution. One 
advantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific 
adapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different 
tasks. However, merging weights makes this approach impossible. The decision to merge weights depends on 
the specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly 
effective method for parameter efficient fine-tuning and is widely used.
CONCLUSION
Low Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right 
configuration. Choosing the correct value of rank and the layers of the neural network architecture to target 
during adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further 
memory savings while preserving the adaptation quality. Even when the fine-tuning is performed,  there are 
several important engineering considerations to ensure the adapted model is deployed in the correct manner.
In summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality 
output and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 
observations on a single A100 is shown below.
Try this on Databricks! Clone the GitHub repository  associated with the blog into a Databricks Repo  to get 
started. More thoroughly documented examples to fine-tune models on Databricks are available here .R TARGET_MODULESBASE MODEL 
WEIGHTSQUALITY OF OUTPUTNUMBER OF PARAMETERS UPDATED  
(IN MILLIONS)
8 Attention blocks 4 low 2.662
16 Attention blocks 4 low 5.324
8 All linear layers 4 high 12.995
8 All linear layers 8 high 12.99559THE BIG BOOK OF GENERATIVE AIStage 4: Pretraining
Pretraining a model from scratch refers to the process of training a language model on a large corpus of data 
(e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-
tuning, where an already pretrained model is further adapted to a specific task or dataset. The output of full 
pretraining is a base model that can be directly used or further fine-tuned for downstream tasks.
WHEN TO USE PRETRAINING
Choosing to pretrain an LLM from scratch is a significant commitment, both in terms of data and computational 
resources. Here are some scenarios where it makes sense:
1. Unique data sources:  If you possess a unique and extensive corpus of data that is distinct from what 
available pretrained LLMs have seen, it might be worth pretraining a model to capture this uniqueness
2. Domain specificity:  Organizations might want a base model tailored to their specific domain (e.g., 
medical, legal, code) to ensure even the foundational knowledge of the model is domain-specific
3. Full control over training data: Pretraining from scratch offers transparency and control over the data 
the model is trained on. This may be essential for ensuring data security, privacy and custom tailoring of 
the model’s foundational knowledge.
4. Avoiding third-party biases:  Pretraining ensures that your LLM application does not inherit biases or 
limitations from third-party pretrained models.60THE BIG BOOK OF GENERATIVE AIPRETRAINING IN PRACTICE
Given the resource-intensive nature of pretraining, careful planning and sophisticated tooling are required. 
Libraries like PyTorch FSDP  and Deepspeed , mentioned in the fine-tuning section , are similarly required for 
their distributed training capabilities when pretraining an LLM from scratch. The following only scratches the 
surface on some of the considerations one must take into account when pretraining an LLM: 
 ■Large-scale data preprocessing:  A pretrained model is only as good as the data it is trained on. Thus, 
it becomes vitally important to ensure robust data preprocessing is conducted prior to model training. 
Given the scale of the training data involved, this preprocessing typically requires distributed frameworks 
like Apache Spark ™. Consideration must be given to factors such as dataset mix and deduplication 
techniques to ensure the model is exposed to a wide variety of unique data points.
 ■Hyperparameter selection and tuning: Before executing full-scale training of an LLM, determining 
the set of optimal hyperparameters is crucial. Given the high computational cost associated with LLM 
training, extensive hyperparameter sweeps are not always feasible. Instead, informed decisions based 
on smaller-scale searches or prior research are employed. Once a promising set is identified, these 
hyperparameters are used for the full training run. Tooling like MLflow  is essential to manage and track 
these experiments.
 ■Maximizing resource utilization: Given the high costs associated with long-running distributed GPU 
training jobs, it is hugely important to maximize resource utilization. MosaicML’s composer  is an example 
of a library that uses PyTorch FSDP  with additional optimizations to maximize Model FLOPs Utilization 
(MFU) and Hardware FLOPs Utilization (HFU)  during training.
 ■Handling GPU failures:  Training large models can run for days or even weeks. During such large-scale 
training for this length of time, hardware failures, especially GPU failures, can (and typically do) occur.  
It is essential to have mechanisms in place to handle such failures gracefully. 
 ■Monitoring and evaluation:  Close monitoring of the training process is essential. Saving model 
checkpoints regularly and evaluating validation sets not only act as safeguards but also provide insights 
into model performance and convergence trends.
In cases where pretraining an LLM from scratch is required, Mosaic AI Training  provides a platform to conduct 
training of multibillion-parameter models in a highly optimized and automated manner. Automatically handling 
GPU failures and resuming training without human intervention and leveraging Mosaic AI Streaming  for efficient 
streaming of data into the training process are just some of the capabilities provided out of the box.61THE BIG BOOK OF GENERATIVE AIThe Value of Training Models From Scratch on Databricks  
After diving into the details of starting a model’s training from scratch, why you might do it and the advanced 
tools needed, let’s look at a real-world example to show that training top-notch language models isn’t as 
complex or expensive as it might seem. This shift highlights that even organizations watching their budget can 
start training their own models, with Databricks providing the necessary support and infrastructure. Databricks 
stands out as uniquely capable to help customers train their own models from scratch, enabling them to fully 
own their AI assets.
Pretraining Use Cases
Training Stable Diffusion From Scratch for <$50K With MosaicML
by Mihir Patel , Cory Stephenson , Landan Seguin , Austin Jacobson  and Erica Ji Yuen
We’ve replicated Stable Diffusion 2 for less than $50K, and we’ve open sourced the training code so you can 
too! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, 
making training large-scale diffusion models from scratch more accessible than ever before.
Today, we are excited to show the results of our own training run: under $50K to train Stable Diffusion 2 base1 
from scratch in 7.45 days using the MosaicML platform .62THE BIG BOOK OF GENERATIVE AI
Figure 1: Imagining mycelium couture. Integrating image generation into the design process pushes creative boundaries. All images in this mood board 
were created with our internal diffusion model trained from scratch on the MosaicML Platform.
Training your own image generation model on your own data is now easy and accessible. By training your own 
diffusion models, you can:
 ■Use your proprietary data
 ■Tune the representations for certain art or photography styles
 ■Avoid violating intellectual property laws so your models can be used commercially
We’ve open sourced our code and methods to train a diffusion model from scratch so that you can train your 
own; check it out here ! If you're interested in training your own models, contact us for a demo , and read on to 
learn more about our engineering setup!63THE BIG BOOK OF GENERATIVE AI
SETUP
Model:  Our diffusion model is a ComposerModel  
composed of a Variational Autoencoder (VAE), a 
CLIP model, a U-Net, and a diffusion noise scheduler, 
all from the HuggingFace's Diffusers library. All of 
the model configurations were based on stabilityai/
stable-diffusion-2-base .Figure 2: Getting creative and embracing serendipity. A variety of subjects, art, and photography styles are generated by our diffusion model.
Figure 3: Simplified diagram of the diffusion model.
64THE BIG BOOK OF GENERATIVE AIData:  We trained on a subset of LAION-5B  that includes samples with English-only captions and an aesthetic 
score of 4.5+. Similar to Stable Diffusion 2 base, we did two phases of training based on the image resolution of 
the training data. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 
million image-caption samples. For the second phase of training, we only used images with resolution >=512x512, 
amounting to 300 million image-caption samples.
Compute: Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k 
iterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 
hours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model 
to reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents 
required an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total. Assuming a cost of $2 / A100 
hour, the total price tag is $47.7k.
Tech Stack:  We used Composer  for our training framework, StreamingDataset  to load our 100TB of data, and 
the MosaicML platform  for overcoming infrastructure challenges when training and evaluating on 128 GPUs.
Figure 4: Loss curve for our training run. Our platform caught two hardware failures and automatically restarted the run with no human intervention. 
The loss discontinuity is because phase 2 increases the resolution from 256x256 to 512x512.65THE BIG BOOK OF GENERATIVE AICHALLENGES AND SOLUTIONS
Whether for diffusion models or large language models, training at scale has significant challenges. We trained 
our diffusion model using the MosaicML platform, which addresses these challenges automatically so you can 
focus on training the best possible model. Below are three main challenges with large-scale training and how our 
platform solves them.
INFRASTRUCTURE
Training large models on large datasets requires significant compute. The MosaicML platform effortlessly 
orchestrates hundreds of GPUs on any cloud provider. For example, our primary training run took place on 
a cluster of 128 A100 GPUs. To ensure evaluating the model didn't slow training, we automatically kicked off 
evaluation runs at every checkpoint on different clusters using different cloud providers, seamlessly scaling up 
to 64 GPUs and back down to 8 GPUs depending on availability.
Even after training is underway, software or hardware failures can halt training, leaving GPUs idle until someone 
notices or requiring someone on-call 24/7 to babysit the run. Thankfully, the Node Doctor and Watchdog 
features of the MosaicML platform automatically detect failed nodes and resume jobs as needed. With auto-
resumption, we recover from failures and continue training with zero human intervention, avoiding expensive 
downtime and human babysitting. Just launch and train!
EFFICIENT SOFTWARE
Software is difficult to configure optimally. Our PyTorch-based Composer library  maximizes training efficiency 
at scale. As shown in our previous blog post , Composer demonstrated excellent throughput scaling as the 
number of GPUs increased. For this update, we added further optimizations (Low Precision GroupNorm  and Low 
Precision LayerNorm , Fully Sharded Data Parallel) to achieve near-perfect strong scaling up to 128 GPUs, bringing 
the cost down to $50k. We also used Composer's native Exponential Moving Average (EMA) algorithm, which 
allowed us to start EMA close to the end of training (iteration 800k of the final phase) to gain all the benefits of 
EMA while saving on memory and compute for the majority of training.66THE BIG BOOK OF GENERATIVE AIMANAGING 100TB OF DATA
We trained with a subset of LAION-5B that contained 790 million samples, amounting to >100TB of data. The 
sheer size of the dataset makes it difficult to manage, especially when working with multiple clusters with 
separate local storage. The MosaicML StreamingDataset library  makes working with massive datasets much 
simpler and faster. There were three key features of the StreamingDataset library that were especially useful for 
this training run:
1. Mixing datasets stored in different locations. We bucketed samples based on image resolution into 
different datasets. At training time, we used the MosaicML StreamingDataset library to train on a mixture 
of resolutions from these datasets.
2. Instant mid-epoch resumption. We were able to instantly resume training in the middle of an epoch. This 
saved hours by avoiding the need to iterate over the entire dataset to get back to where we left off.
3. Elastic determinism. The MosaicML StreamingDataset library deterministically shuffles data, even when 
changing the number of GPUs used for training. This made it possible for us to exactly reproduce training 
runs, dramatically simplifying debugging.
HUMAN EVALUATION RESULTS
Evaluating image generation models is difficult, and there is no substitute for human evaluation. In a blind human 
evaluation, we measured user preferences in image quality and prompt alignment between Stable Diffusion 2 
and our diffusion model. Based on user preferences, we concluded that the two models were comparable in 
quality (see Figure 5) All images were generated based on prompts from the Drawbench benchmark proposed 
in the Imagen paper . For more details, see our follow-up blog post coming soon.67THE BIG BOOK OF GENERATIVE AI
Figure 5: Results from our human evaluation of image quality (left) and prompt alignment (right). Error bars show 95% confidence intervals. In both ex -
periments, the difference in user preference rates between the two models was comparable to the uncertainty in the measurement, so we conclude 
that the two models are of comparable overall quality.
Deep Dive: How We Trained Stable Diffusion for Less Than $50K  
by Mihir Patel , Erica Ji Yuen , Cory Stephenson  and Landan Seguin
In our previous example, we showed how we used the MosaicML platform, Streaming datasets, and the 
Composer library to train a Stable Diffusion model from scratch for less than $50,000. Now, we do a deep dive 
into the technical details behind this speedup, demonstrating how we were able to replicate the Stable Diffusion 
2 base model in just 6.8 days.
Try out our code here !
Many organizations require high-performing large AI models tailored to their specific use cases. However, 
training such models is often prohibitively time-consuming and expensive, requiring vast amounts of 
computation and expertise. This is where MosaicML comes in: we provide a comprehensive solution that 
simplifies and accelerates the process of training these models.68THE BIG BOOK OF GENERATIVE AIIn our previous blog post , we announced that we have trained a diffusion model comparable to Stable Diffusion 
2 from scratch for $47.7K. In this post, we dive into the technical details to highlight how we achieved an 8x 
speedup/cost reduction from the number reported by StabilityAI  and a 3x cost reduction over our own 
baseline . All our code is open source  and easy to modify for custom use cases. If you're interested in learning 
more about our stack, please contact us for a demo .
ACCELERATING TRAINING
We’ve introduced a variety of techniques, from fusions to sharding strategies, that dramatically speed up 
training and lower costs by almost 3x.
Figure 1: Stable Diffusion 2 model architecture. For training, the VAE image encoder, CLIP text encoder and U-Net are used. For inference,  
the CLIP Text Encoder, U-Net, and VAE image decoder are used. Only the U-Net weights are updated during training; CLIP and VAE are fixed.69THE BIG BOOK OF GENERATIVE AIXFORMERS FLASHATTENTION
Figure 2: xFormers accelerates cross attention blocks in the U-Net.70THE BIG BOOK OF GENERATIVE AIThe attention layers in the Stable Diffusion architecture can be slow with a naive implementation, so most 
codebases use faster implementations that rely on fused kernels. In our stack, we leverage xFormers 
FlashAttention .
While this was enabled in our original blog post , we found an issue with the usage that resulted in extra memory 
being consumed on rank 0. After fixing this bug, we were able to increase our device microbatch size1 from 4 to 
8. This yielded a sizable speedup, since A100s are more efficient at larger matrix sizes.
PRECOMPUTING LATENTS
Figure 3: Two phase training with precomputed latents. 
First, all VAE and CLIP latents are precomputed and stored. 
Then, the U-Net diffusion model is trained using these 
precomputed latents.71THE BIG BOOK OF GENERATIVE AIStable Diffusion is a combination of three models: a variational autoencoder (VAE), a text encoder (CLIP), and a 
U-Net. During diffusion training, only the U-Net is trained, and the other two models are used to compute the 
latent encodings of the image and text inputs. Standard training involves computing the VAE and CLIP latents for 
every batch, but this does a lot of duplicate work when training for multiple epochs: latents are re-computed for 
each image every time it is used. Instead, we precompute the latents once before training. Empirically, we have 2 
epochs at 256 resolution and 5 epochs at 512 resolution, so we avoid 6 extra VAE and CLIP calls per image-text 
pair in the dataset.
Additionally, when pre-computing the latents, we can lower the precision of the VAE and CLIP models to 
fp16. This could lead to numerical instability if we were training the VAE and CLIP and used this precision for 
the backward pass. However, since we're only using them for inference, we can safely lower the precision, 
which increases speed. The extra memory savings also let us use far larger batch sizes and improve hardware 
utilization during the latent precomputation.72THE BIG BOOK OF GENERATIVE AILOW PRECISION LAYERNORM AND GROUPNORM
Figure 4: Low Precision LayerNorm and Low Precision GroupNorm. Low precision gives faster training and lower memory usage, enabling larger 
microbatches.73THE BIG BOOK OF GENERATIVE AIDiffusion training is done in automatic mixed precision  by default. This uses half precision (fp16) in most 
layers, but fp32 in a few numerically unstable layers like normalization and softmax. The Stable Diffusion U-Net 
architecture uses several LayerNorm and GroupNorm layers, which by default are run in fp32.
Motivated by our finding that half precision LayerNorms are safe to use in language models , we decided to 
try out half precision LayerNorm and GroupNorm layers. This change resulted in identical loss curves and no 
instability in our experiments.
While we did observe some throughput improvement, the real benefit was decreased memory usage. Now, 
along with removing the VAE and CLIP memory by precomputing latents, we have enough space on our 40GB 
A100 to increase our microbatch size from 8 to 16, 4x larger than what we started with!74THE BIG BOOK OF GENERATIVE AIFULLY SHARDED DATA PARALLELISM
Figure 5: Fully Sharded Data Parallel with SHARD_GRAD_OP speeds up the gradient update step and enables linear scaling.75THE BIG BOOK OF GENERATIVE AIMosaicML Composer , our go-to training library, includes support for PyTorch Fully Sharded Data Parallelism 
(FSDP). We primarily use this to shard large scale models like 10B+ parameter LLMs that don't fit in a single 
device across hundreds of GPUs for incredibly fast training. Stable Diffusion doesn't require sharding since it  
fits in a single GPU. However, some of the distributed features in FSDP are still useful for speeding up training  
on a large number of GPUs.
When batches don’t fit into memory, we do several forward and backward passes on smaller microbatches, 
followed by a single gradient update. If we use a small number of GPUs to train, we have far more forward and 
backward passes per gradient update, so the time spent on the gradient update doesn't matter. However, at 
128+ GPUs with a microbatch size of 16, we're only doing one forward and one backward pass for each gradient 
update. At this scale, the gradient update step starts to become a significant bottleneck.
To tackle this problem, we use FSDP's SHARD_GRAD_OP mode. In normal training, each GPU communicates all 
its gradients to every other GPU, and then each GPU updates its local copy of the model. With this FSDP variant, 
each GPU only gets the gradients and updates the weights for a small part of the model before sending the 
updated weights for that part of the model to all of the other GPUs. By dividing the update step across all the 
GPUs, we can ensure the amount of work per GPU decreases as we increase the number of GPUs, helping us 
achieve linear scaling.76THE BIG BOOK OF GENERATIVE AISCHEDULED EMA
Figure 6: Loss curve of our training run with the scheduled exponential moving average (EMA) period highlighted.77THE BIG BOOK OF GENERATIVE AIStable Diffusion 2 uses Exponential Moving Averaging (EMA) , which maintains an exponential moving average 
of the weights. At every time step, the EMA model is updated by taking 0.9999 times the current EMA model 
plus 0.0001 times the new weights after the latest forward and backward pass. By default, the EMA algorithm is 
applied after every gradient update for the entire training period. However, this can be slow due to the memory 
operations required to read and write all the weights at every step.
To avoid this costly procedure, we start with a key observation: since the old weights are decayed by a factor of 
0.9999 at every batch, the early iterations of training only contribute minimally to the final average. This means 
we only need to take the exponential moving average of the final few steps. Concretely, we train for 1,400,000 
batches and only apply EMA for the final 50,000 steps, which is about 3.5% of the training period. The weights 
from the first 1,350,000 iterations decay away by (0.9999)^50000, so their aggregate contribution would have 
a weight of less than 1% in the final model. Using this technique, we can avoid adding overhead for 96.5% of 
training and still achieve a nearly equivalent EMA model.78THE BIG BOOK OF GENERATIVE AIFINAL TIME AND COST ESTIMATES
Figure 7: Throughput at 512x512 images on 128 GPUs as each speedup optimization is enabled. We achieve a total cumulative speedup of 2.71x over 
the baseline.
We’ve shown how we obtained nearly a 3x reduction in time and cost to train Stable Diffusion compared to our 
original results . With xFormers, precomputed latents, low precision LayerNorm, low precision GroupNorm, FSDP, 
and scheduled EMA, Table 1 shows it's possible to train Stable Diffusion in just 6.79 days using 21,000 A100-
hours for a total cost of less than $42,000. We estimated these times and costs by measuring throughput for 
training 1.1 billion 256x256 images and 1.7 billion 512x512 images with a max tokenized length of 77 at a global 
batch size of 2048, as detailed in the Stable Diffusion 2 base model card . This is slightly cheaper than our 
previously reported run  with a cost of $47.7k as it does not account for any time spent on evaluation or restarts 
due to hardware failures.79THE BIG BOOK OF GENERATIVE AINUMBER  
OF A100STHROUGHPUT  
FOR U-NET 
@ 256X256 
(IMAGES / 
SECOND)THROUGHPUT 
FOR U-NET 
@ 512X512 
(IMAGES / 
SECOND)THROUGHPUT 
FOR U-NET @ 
512X512 WITH 
EMA (IMAGES /  
SECOND)DAYS TO TRAIN 
ON MOSAICML 
CLOUDAPPROX. COST 
ON MOSAICML 
CLOUD
8 1100 290 290 101.04 $38,800
16 2180 585 580 50.29 $38,630
32 4080 1195 1160 25.01 $38,420
64 8530 2340 2220 12.63 $38,800
128 11600 4590 3927 6.79 $41,710
Table 1: Estimated time and cost to train a Stable Diffusion model on 1.1 billion images at 256x256 resolution, followed by 1.7 billion images at 512x512 
resolution. Different rows show different numbers of NVIDIA 40GB A100 GPUs at a global batch size of 2048.
These optimizations show that training image generation models from scratch is within reach for everyone. For 
updates on our latest work, join our Community Slack  or follow us on Twitter . If your organization wants to start 
training diffusion models today, please schedule a demo online  or email us at demo@mosaicml.com .
1 When training large models with big batches that don't fit in memory in a single pass, each batch is divided into smaller microbatches. On each 
device, we can do a forward and backward pass for each microbatch and sum the gradients at the end to compute a gradient update equivalent to 
a single forward and backward pass with the entire batch all at once.80THE BIG BOOK OF GENERATIVE AIStage 5: LLM Evaluation
Constant evaluation and monitoring of deployed large language models (LLMs) and generative AI applications 
are crucial due to the dynamic nature of both the data they interact with and the environments in which they 
operate. These systems learn from vast datasets and can evolve over time, potentially leading to shifts in 
performance, accuracy or even the emergence of biases. Continuous monitoring ensures that any deviation 
from expected behavior can be detected and corrected promptly, maintaining the integrity and reliability 
of the AI application. As user needs and societal norms change, ongoing evaluation  allows these models to 
adapt, ensuring their outputs remain relevant, appropriate and effective. This vigilance not only mitigates risks 
associated with AI deployments, such as ethical concerns and regulatory compliance, but also maximizes the 
value and utility these technologies bring to organizations and end users. 
Evaluating LLMs is a challenging and evolving domain , primarily because LLMs often demonstrate uneven 
capabilities across different tasks. An LLM might excel in one benchmark, but slight variations in the prompt 
or problem can drastically affect its performance. The dynamic nature of LLMs and their vast potential 
applications only amplify the challenge of establishing comprehensive evaluation standards.81THE BIG BOOK OF GENERATIVE AIPresent challenges involved with evaluating LLM-powered applications include the following:
 ■Variable performance:  LLMs can be sensitive to prompt variations , demonstrating high proficiency in 
one task but faltering with slight deviations in prompts.
 ■Lack of ground truth:  Since most LLMs output natural language, it is very difficult to evaluate the outputs 
via traditional NLP metrics ( BLEU , ROUGE, etc.). For example, suppose an LLM were used to summarize 
a news article. Two equally good summaries might have almost completely different words and word 
orders, so even defining a “ground truth” label becomes difficult or impossible.
 ■Domain-specific evaluation:  For domain-specific fine-tuned LLMs, popular generic benchmarks 
may not capture their nuanced capabilities. Such models are tailored for specialized tasks, making 
traditional metrics less relevant. This divergence often necessitates the development of domain-specific 
benchmarks and evaluation criteria. See the example of Replit’s code generation LLM . 
 ■Reliance on human judgment:  It is often the case that LLM performance is being evaluated in domains 
where text is scarce or there is a reliance on subject matter expert knowledge. In such scenarios, 
evaluating LLM output can be costly and time-consuming.
To help give examples of how this can be accomplished, here are two great examples of how you can monitor 
and evaluate your deployed LLMs and generative AI applications using Databricks.
LLM Evaluation Examples
Best Practices for LLM Evaluation of RAG Applications   
A Case Study on the Databricks Documentation Bot
by Quinn Leng , Kasey Uhlenhuth  and Alkis Polyzotis
Chatbots are the most widely adopted use case for leveraging the powerful chat and reasoning capabilities 
of large language models (LLM). The retrieval augmented generation (RAG) architecture is quickly becoming 
the industry standard for developing chatbots because it combines the benefits of a knowledge base (via a 
vector store) and generative models (e.g., GPT-3.5 and GPT-4) to reduce hallucinations, maintain up-to-date 
information, and leverage domain-specific knowledge. However, evaluating the quality of chatbot responses 
remains an unsolved problem today. With no industry standards defined, organizations resort to human grading 
(labeling) –which is time-consuming and hard to scale.82THE BIG BOOK OF GENERATIVE AIWe applied theory to practice to help form best practices for LLM automated evaluation so you can deploy RAG 
applications to production quickly and with confidence. This blog represents the first in a series of investigations 
we’re running at Databricks to provide learnings on LLM evaluation. All research in this post was conducted by 
Quinn Leng, Senior Software Engineer at Databricks and creator of the Databricks Documentation AI Assistant . 
CHALLENGES WITH AUTO-EVALUATION IN PRACTICE
Recently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with 
many using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs. The lmsys group’s research 
paper explores the feasibility and pros/cons of using various LLMs (GPT-4, ClaudeV1, GPT-3.5) as the judge for 
tasks in writing, math, and world knowledge.
Despite all this great research, there are still many unanswered questions about how to apply LLM judges  
in practice:
 ■Alignment With Human Grading: Specifically for a document-Q&A chatbot, how well does an 
LLM judge’s grading reflect the actual human preference in terms of correctness, readability and 
comprehensiveness of the answers? 
 ■Accuracy Through Examples:  What’s the effectiveness of providing a few grading examples to the LLM 
judge and how much does it increase the reliability and reusability of the LLM judge on different metrics?
 ■Appropriate Grade Scales:  What grading scale is recommended because different grading scales are 
used by different frameworks (e.g., AzureML  uses 0 to 100 whereas langchain  uses binary scales)?
 ■Applicability Across Use Cases:  With the same evaluation metric (e.g. correctness), to what extent can 
the evaluation metric be reused across different use cases (e.g. casual chat, content summarization, 
retrieval augmented generation)?83THE BIG BOOK OF GENERATIVE AIAPPLYING EFFECTIVE AUTO-EVALUATION FOR RAG APPLICATIONS
We explored the possible options for the questions outlined above in the context of our own chatbot 
application at Databricks. We believe that our findings generalize and can thus help your team effectively 
evaluate RAG-based chatbots at a lower cost and faster speed:
 ■LLM-as-a-judge agrees with human grading on over 80% of judgments. Using LLMs-as-a-judge for our 
document-based chatbot evaluation was as effective as human judges, matching the exact score in over 
80% of judgments and being within a 1-score distance (using a scale of 0-3) in over 95% of judgments.
 ■Save costs by using GPT-3.5 with examples. GPT-3.5 can be used as an LLM judge if you provide 
examples for each grading score. Because of the context size limit it’s only practical to use a low-
precision grading scale. Using GPT-3.5 with examples instead of GPT-4 drives down the cost of LLM judge 
by 10x and improves the speed by more than 3x.
 ■Use low-precision grading scales for easier interpretation. We found lower-precision grading scores like 0, 
1, 2, 3 or even binary (0, 1) can largely retain precision compared to higher precision scales like 0 to 10.0 or 
0 to 100.0, while making it considerably easier to provide grading rubrics to both human annotators and 
LLM judges. Using a lower precision scale also allows consistency of grading scales among different LLM 
judges (e.g., between GPT-4 and claude2).
 ■RAG applications require their own benchmarks. A model might have good performance on a published 
specialized benchmark (e.g. casual chat, math, or creative writing) but that doesn’t guarantee good 
performance on other tasks (e.g. answering questions from a given context). Benchmarks should only be 
used if the use case matches, i.e., a RAG application should only be evaluated with a RAG benchmark.
Based on our research, we recommend the following procedure when using an LLM judge: 
 ■Use a 1-5 grading scale
 ■Use GPT-4 as an LLM judge with no examples to understand grading rules
 ■Switch your LLM judge to GPT-3.5 with one example per score84THE BIG BOOK OF GENERATIVE AIOUR METHODOLOGY FOR ESTABLISHING BEST PRACTICES
The remainder of this post will walk through the series of experiments we conducted to form these  
best practices. 
EXPERIMENT SETUP
85THE BIG BOOK OF GENERATIVE AI The experiment had three steps: 
1. Generate evaluation dataset: We created a dataset from 100 questions and context from Databricks 
documents. The context represents (chunks of) documents that are relevant to the question. 
2. Generate answer sheets:  Using the evaluation dataset, we prompted different language models to 
generate answers and stored the question-context-answer pairs in a dataset called “answer sheets”. In 
this investigation, we used GPT-4, GPT-3.5, Claude-v1, Llama2-70b-chat, Vicuna-33b, and mpt-30b-chat.
3. Generate grades:  Given the answer sheets, we used various LLMs to generate grades and reasoning 
for the grades. The grades are a composite score of Correctness (weighted: 60%), Comprehensiveness 
(weighted: 20%) and Readability (weighted: 20%). We chose this weighting scheme to reflect our 
preference for Correctness in the generated answers. Other applications may tune these weights 
differently but we expect Correctness to remain a dominant factor.86THE BIG BOOK OF GENERATIVE AIAdditionally, the following techniques were used to avoid positional bias and improve reliability:
 ■Low temperature (temperature 0.1) to ensure reproducibility
 ■Single-answer grading instead of pairwise comparison
 ■Chain of thoughts to let the LLM reason about the grading process before giving the final score
 ■Few-shots generation where the LLM is provided with several examples in the grading rubric for each 
score value on each factor (Correctness, Comprehensiveness, Readability)
EXPERIMENT 1: ALIGNMENT WITH HUMAN GRADING
To confirm the level of agreement between human annotators and LLM judges, we sent answer sheets  
(grading scale 0-3) from gpt-3.5-turbo and vicuna-33b to a labeling company to collect human labels,  
and then compared the result with GPT-4’s grading output. Below are the findings:
Human and GPT-4 judges can reach above 80% agreement on the correctness and readability score.  
And if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can  
reach above 95%. The Comprehensiveness metric has less alignment, which matches what we’ve heard  
from business stakeholders who shared that “comprehensive” seems more subjective than metrics like 
Correctness or Readability.
87THE BIG BOOK OF GENERATIVE AIEXPERIMENT 2: ACCURACY THROUGH EXAMPLES
The lmsys paper uses this prompt  to instruct the LLM judge to evaluate based on the helpfulness, relevance, 
accuracy, depth, creativity, and level of detail of the response. However, the paper doesn’t share specifics on the 
grading rubric. From our research, we found many factors can significantly affect the final score, for example:
 ■The importance of different factors: Helpfulness, Relevance, Accuracy, Depth, Creativity
 ■The interpretation of factors like Helpfulness is ambiguous 
 ■If different factors conflict with each other, where an answer is helpful but is not accurate 
We developed a rubric for instructing an LLM judge for a given grading scale, by trying the following:
1. Original Prompt:  Here is the original prompt used in the lmsys paper:
We adapted the original lmsys paper prompt to emit our metrics about correctness, comprehensiveness and 
readability, and also prompt the judge to provide one line justification before giving each score (to benefit from 
chain-of-thought reasoning). Below are the zero-shot version of the prompt which doesn’t provide any example, 
and the few-shot version of the prompt which provides one example for each score. Then we used the same 
answer sheets as input and compared the graded results from the two prompt types.
2. Zero Shot Learning:  Require the LLM judge to emit our metrics about correctness, comprehensiveness 
and readability, and also prompt the judge to provide one line justification for each score. 
Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question 
displayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and 
level of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After 
providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format
 
Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided 
question based on a provided context. 
  You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your 
reasoning and score for the correctness, comprehensiveness and readability of the answer88THE BIG BOOK OF GENERATIVE AI3. Few Shots Learning:  We adapted the zero shot prompt to provide explicit examples for each score in the 
scale. The new prompt:
 
Please act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided 
question based on a provided context.
  You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your 
reasoning and score for the correctness, comprehensiveness and readability of the answer. 
  
  Below is your grading rubric: 
- Correctness: If the answer correctly answer the question, below are the details for different scores:
  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the 
correct answer.
      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s 
completely irrelevant, or sorry I don’t know the answer.
  - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.
      - Example:
          - Question: How to terminate a databricks cluster
          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run 
distributed data processing tasks efficiently.
          - Or answer:  In the Databricks workspace, navigate to the "Clusters" tab. And then this is a hard question that I 
need to think more about it
  - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.
      - Example:
          - Question: How to terminate a databricks cluster”
          - Answer: “In the Databricks workspace, navigate to the "Clusters" tab.
          Find the cluster you want to terminate from the list of active clusters.
          And then you’ll find a button to terminate all clusters at once”
  - Score 3: the answer correctly answer the question and not missing any major aspect
      - Example:
          - Question: How to terminate a databricks cluster
          - Answer: In the Databricks workspace, navigate to the "Clusters" tab.
          Find the cluster you want to terminate from the list of active clusters.
          Click on the down-arrow next to the cluster name to open the cluster details.
          Click on the "Terminate" button. A confirmation dialog will appear. Click "Terminate" again to confirm the action.”
- Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide 
comprehensive explanation and other necessary information. Below are the details for different scores:
  - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.
  - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for 
comprehensiveness.
      - Example:
          - Question: How to use databricks API to create a cluster?
          - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this 
token through the Databricks UI under the 'User Settings' option. And then (the rest is missing)
  - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about 
details. Or is completely missing details about one minor aspect.  89THE BIG BOOK OF GENERATIVE AI      - Example:
          - Question: How to use databricks API to create a cluster?
          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the 
request URL, then you can make the HTTP Request. Then you can handle the request response.
      - Example:
          - Question: How to use databricks API to create a cluster?
          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the 
request URL, then you can make the HTTP Request. Then you can handle the request response.
  - Score 3: the answer is correct, and covers all the main aspects of the question
- Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the 
readability of the answer.
  - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words 
that it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.
  - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a 
meaningful sentence that cover some aspects of the answer.
      - Example:
          - Question: How to use databricks API to create a cluster?
          - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And 
then then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then 
Then
  - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability 
(mentioning of irrelevant pieces, repeated words)
      - Example:
          - Question: How to terminate a databricks cluster
          - Answer: In the Databricks workspace, navigate to the "Clusters" tab.
          Find the cluster you want to terminate from the list of active clusters.
          Click on the down-arrow next to the cluster name to open the cluster details.
          Click on the "Terminate" button…………………………………..
          A confirmation dialog will appear. Click "Terminate" again to confirm the action.
  - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.
- Then final rating:
    - Ratio: 60% correctness + 20% comprehensiveness + 20% readability
From this experiment, we learned several things:
 ■Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results. 
When we included the detailed grading rubric with examples we didn’t see a noticeable improvement in 
GPT-4’s grading results across different LLM models. Interestingly, it caused a slight variance in the range 
of the scores. 90THE BIG BOOK OF GENERATIVE AI
91THE BIG BOOK OF GENERATIVE AI ■Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores,  
and makes the result usable. Including detailed grading rubric/examples has very obvious improvement 
on the grading result from GPT-3.5. Though the actual average score value is slightly different between 
GPT-4 and GPT-3.5 (score 3.0 vs score 2.6), the ranking and precision remains fairly consistent
 ■On the contrary, using GPT-3.5 without a grading rubric gets very inconsistent results and is  
completely unusable
 ■Note that we are using GPT-3.5-turbo-16k instead of GPT-3.5-turbo since the prompt can be larger than 
4k tokens. 
92THE BIG BOOK OF GENERATIVE AI
EXPERIMENT 3: APPROPRIATE GRADE SCALES
The LLM-as-judge paper uses a non-integer 0~10 scale (i.e., float) for the grading scale; in other words, it uses 
a high precision rubric for the final score. We found these high-precision scales cause issues downstream with 
the following:
 ■Consistency:  Evaluators–both human and LLM–struggled to hold the same standard for the same score 
when grading on high precision. As a result, we found that output scores are less consistent across judges 
if you move from low-precision to high-precision scales. 
 ■Explainability: Additionally, if we want to cross-validate the LLM-judged results with human-judged 
results we must provide instructions on how to grade answers. It is very difficult to provide accurate 
instructions for each “score” in a high-precision grading scale–for example, what’s a good example for an 
answer that’s scored at 5.1 as compared to 5.6? 93THE BIG BOOK OF GENERATIVE AIWe experimented with various low-precision grading scales to provide guidance on the “best” one to use, 
ultimately we recommend an integer scale of 0-3 or 0-4 (if you want to stick to the Likert  scale). We tried  
0-10, 1-5, 0-3, and 0-1 and learned:
 ■Binary grading works for simple metrics like “usability” or “good/bad”.
 ■Scales like 0-10 are difficult to come up with distinguishing criteria between all scores.
94THE BIG BOOK OF GENERATIVE AI
As shown in these plots, both GPT-4 and GPT-3.5 can retain consistent ranking of results using different  
low-precision grading scales, thus using a lower grading scale like 0~3 or 1~5 can balance the precision  
with explainability).
Thus, we recommend 0-3 or 1-5 as a grading scale to make it easier to align with human labels, reason about 
scoring criteria, and provide examples for each score in the range. 95THE BIG BOOK OF GENERATIVE AIEXPERIMENT 4: APPLICABILITY ACROSS USE CASES
The LLM-as-judge  paper shows that both LLM and human judgment ranks the Vicuna-13B model as a close 
competitor to GPT-3.5:
However, when we benchmarked the set of models for our document Q&A use cases, we found that even the 
much larger Vicuna-33B model has a noticeably worse performance than GPT-3.5 when answering questions 
based on context. These findings are also verified by GPT-4, GPT-3.5 and human judges (as mentioned in 
Experiment 1) which all agree that Vicuna-33B is performing worse than GPT-3.5.Figure 4:  Average win rate of nine models under different judges on Chatbot Arena.
96THE BIG BOOK OF GENERATIVE AI
We looked closer at the benchmark dataset proposed by the paper and found that the 3 categories of tasks  
(writing, math, knowledge) don’t directly reflect or contribute to the model’s ability to synthesize an answer 
based on a context. Instead, intuitively, document Q&A use cases need benchmarks on reading comprehension 
and instruction following. Thus evaluation results can’t be transferred between use cases and we need to build 
use-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.97THE BIG BOOK OF GENERATIVE AIUSE MLFLOW TO LEVERAGE OUR BEST PRACTICES
With the experiments above, we explored how different factors can significantly affect the evaluation of a 
chatbot and confirmed that LLM as a judge can largely reflect human preferences for the document Q&A use 
case. At Databricks, we are evolving the MLflow Evaluation API to help your team effectively evaluate your LLM 
applications based on these findings. MLflow 2.4 introduced the Evaluation API for LLMs to compare various 
models’ text output side-by-side, MLflow 2.6 introduced LLM-based metrics for evaluation like toxicity and 
perplexity, and we’re working to support LLM-as-a-judge in the near future!
In the meantime, we compiled the list of resources we referenced in our research below:
 ■Doc_qa repository
 ■The code and data we used to conduct the experiments
 ■LLM-as-Judge Research paper from lmsys group 
 ■The paper is the first research for using LLM as judge for the casual chat use cases, it extensively 
explored the feasibility and pros and cons of using LLM (GPT-4, ClaudeV1, GPT-3.5) as the judge for 
tasks in writing, math, world knowledge
Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks
by Abe Omorogbe , Liang Zhang , Sunish Sheth , Corey Zumar , Maheswaran Venkatachalam , Emil Lysgaard   
and Mathias Christiansen
BACKGROUND
In an era where retrieval augmented generation (RAG) is revolutionizing the way we interact with AI-driven 
applications, ensuring the efficiency and effectiveness of these systems has never been more essential. 
Databricks and MLflow are at the forefront of this innovation, offering streamlined solutions for the critical 
evaluation of GenAI applications. 
This blog post guides you through the simple and effective process of leveraging the Databricks Data 
Intelligence Platform to enhance and evaluate the quality of the three core components of your GenAI 
applications: Prompts, Retrieval System, and Foundation LLM, ensuring that your GenAI applications continue  
to generate accurate results.98THE BIG BOOK OF GENERATIVE AIUSE CASE
We are going to be creating a QA chatbot that will answer questions from the MLflow documentation and then 
evaluate the results.
99THE BIG BOOK OF GENERATIVE AISET UP EXTERNAL MODELS IN DATABRICKS
Databricks Model Serving  feature can be used to manage, govern, and access external models from various 
large language model (LLM) providers, such as Azure OpenAI GPT, Anthropic Claude, or AWS Bedrock, within 
an organization. It offers a high-level interface that simplifies the interaction with these services by providing a 
unified endpoint to handle specific LLM related requests.
Major advantages of using Model Serving :
 ■Query Models Through a Unified Interface:  Simplifies the interface to call multiple LLMs in your 
organization. Query models through a unified OpenAI-compatible API and SDK and manage all models 
through a single UI.
 ■Govern and Manage Models:  Centralizes endpoint management of multiple LLMs in your organization.  
This includes the ability to manage permissions and track usage limits.
 ■Central Key Management: Centralizes API key management in a secure location, which enhances 
organizational security by minimizing key exposure in the system and code, and reduces the burden  
on end-users.100THE BIG BOOK OF GENERATIVE AICREATE A SERVING ENDPOINT WITH AN EXTERNAL MODEL IN DATABRICKS
 
1
2
3 
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
23
25
26import mlflow
import mlflow.deployments
client = mlflow.deployments.get_deploy_client("databricks")
endpoint_name = f"test-endpoint-{uuid.uuid4()}"
client.create_endpoint(
name=endpoint_name,
config={
        "served_entities": [
            {
                "name": "test",
                "external_model": {
                    "name": "gpt-3.5-turbo-instruct",
                    "provider": "openai",
                    "task": "llm/v1/completions",
                    "openai_config": {
                        "openai_api_type": "azure",
                        "openai_api_key": "{{secrets/<your-scope-name>/<your-key-name>}}", ## Use Databricks Secrets. 
                        "openai_api_base": "https://<your-endpoint>.openai.azure.com/",
                        "openai_deployment_name": "<your-deployment-name>",
                        "openai_api_version": "2023-05-15",
                    },
                },
            }
        ],
     },
)101THE BIG BOOK OF GENERATIVE AIEXPLORE PROMPTS WITH THE DATABRICKS AI PLAYGROUND
In this section, we will understand: How well do different prompts perform with the chosen LLM?
We recently introduced the Databricks AI Playground , which provides a best-in-class experience for crafting the 
perfect prompt. With no code required, you can try out multiple LLMs served as Endpoints in Databricks, and 
test different parameters and prompts.
Major advantages of the Databricks AI Playground are:
 ■Quick Testing:  Quickly test deployed models directly in Databricks.
 ■Easy Comparison:  Central location to compare multiple models on different prompts and parameters for 
comparison and selection.
USING DATABRICKS AI PLAYGROUND
We delve into testing relevant prompts with OpenAI GPT 3.5 Turbo, leveraging the Databricks AI Playground . 102THE BIG BOOK OF GENERATIVE AICOMPARING DIFFERENT PROMPTS AND PARAMETERS
In the Playground, you are able to compare the output of multiple prompts to see which gives better results. 
Directly in the Playground, you can try several prompts,  models, and parameters to figure out which 
combination provides the best results. The model and parameters combo can then be added to the GenAI  
app and used for answer generation with the right context.
103THE BIG BOOK OF GENERATIVE AIADDING MODEL AND PARAMETERS TO YOUR GENAI APPLICATION
After playing with a few prompts and parameters, you can use the same settings and model in your  
GenAI application.
104THE BIG BOOK OF GENERATIVE AIExample of how to import the same external model in LangChain. We will cover how we turn this into a GenAI 
POC in the next section.
CREATE GENAI POC WITH LANGCHAIN AND LOG WITH MLFLOW
Now that we have found a good model and prompt parameters for your use case, we are going to create a 
sample GenAI app that is a QA chatbot that will answer questions from the MLflow documentation using a 
vector database, embedding model with the Databricks Foundation Model API  and Azure OpenAI GPT 3.5 as 
the generation model. 
1
2
3
4
5
6
7
8
9from langchain.llms import Databricks
llm = Databricks(
    endpoint_name="<endpoint-name>",
    extra_params={"temperature": 0.1,
                 "top_p": 0.1,
                 "max_tokens": 500,
                 } #parameters used in AI Playground
)105THE BIG BOOK OF GENERATIVE AICREATE A SAMPLE GENAI APP WITH LANGCHAIN USING DOCS FROM THE MLFLOW WEBSITE
 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37import os
import pandas as pd
import mlflow
import chromadb
from langchain.chains import RetrievalQA
from langchain.document_loaders import WebBaseLoader
from langchain.llms import Databricks
from langchain.embeddings.databricks import DatabricksEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
loader = WebBaseLoader(
    [ 
     "https://mlflow.org/docs/latest/index.html",
     "https://mlflow.org/docs/latest/tracking/autolog.html", 
     "https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html",
     "https://mlflow.org/docs/latest/python_api/mlflow.deployments.html" ])
documents = loader.load()
CHUNK_SIZE = 1000
text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
llm = Databricks(
    endpoint_name="<endpoint-name>",
    extra_params={"temperature": 0.1,
                 "top_p": 0.1,
                 "max_tokens": 500,
                 } #parameters used in AI Playground
)
# create the embedding function using Databricks Foundation Model APIs
embedding_function = DatabricksEmbeddings(endpoint="databricks-bge-large-en")
docsearch = Chroma.from_documents(texts, embedding_function)
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=docsearch.as_retriever(fetch_k=3),
    return_source_documents=True,
)106THE BIG BOOK OF GENERATIVE AIFor customers wanting to scale the retriever used in their GenAI application, we advise using Databricks Vector 
Search, a serverless similarity search engine that allows you to store a vector representation of your data, 
including metadata, in a vector database.
EVALUATION OF RETRIEVAL SYSTEM WITH MLFLOW
In this section, we will understand: How well does the retriever work with a given query?
In MLflow 2.9.1 , Evaluation for retrievers was introduced and provides a way for you to assess the efficiency 
of their retriever with the MLflow evaluate API. You can use this API to evaluate the effectiveness of your 
embedding model, the top K threshold choice, or the chunking strategy.
CREATING A GROUND TRUTH DATASET
Curating a ground truth dataset for evaluating your GenAI often involves the meticulous task of manually 
annotating test sets, a process that demands both time and domain expertise. In this blog, we’re taking a 
different route. We’re leveraging the power of an LLM to generate synthetic data for testing , offering a quick-
start approach to get a sense of your GenAI app’s retrieval capability, and a warm-up for all the in-depth 
evaluation work that may follow. To our readers and customers, we emphasize the importance of crafting a 
dataset that mirrors the expected inputs and outputs of your GenAI application. It’s a journey worth taking for 
the incredible insights you’ll gain!
You can explore with the full dataset but let's demo with a subset of the generated data. The question column 
contains all the questions that will be evaluated and the source column is the expected source for the answer 
for the questions as an ordered list of strings.107THE BIG BOOK OF GENERATIVE AI 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16eval_data = pd.DataFrame(
    {
        "question": [
            "What is MLflow?",
            "What is Databricks?",
            "How to serve a model on Databricks?",
            "How to enable MLflow Autologging for my workspace by default?",
        ],
        "source": [
            ["https://mlflow.org/docs/latest/index.html"],
            ["https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html"],
            ["https://mlflow.org/docs/latest/python_api/mlflow.deployments.html"],
            ["https://mlflow.org/docs/latest/tracking/autolog.html"],
        ],
    }
)
EVALUATE THE EMBEDDING MODEL WITH MLFLOW
The quality of your embedding model is pivotal for accurate retrieval. In MLflow 2.9.0, we introduced three built-
in metrics mlflow.metrics.precision_at_k(k) , mlflow.metrics.recall_at_k(k)  and mlflow.metrics.ndcg_at_k(k)  
to help determine how effective your retriever is at predicting the most relevant results for you. For example; 
Suppose the vector database returns 10 results (k=10), and out of these 10 results, 4 are relevant to your query. 
The precision_at_10 would be 4/10 or 40%. 108THE BIG BOOK OF GENERATIVE AI 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26def evaluate_embedding(embedding_function):
    CHUNK_SIZE = 1000
    list_of_documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)
    docs = text_splitter.split_documents(list_of_documents)
    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()
    def retrieve_doc_ids(question: str) -> List[str]:
        docs = retriever.get_relevant_documents(question)
        doc_ids = [doc.metadata["source"] for doc in docs]
        return doc_ids
    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:
        return question_df["question"].apply(retrieve_doc_ids)
    with mlflow.start_run() as run:
        evaluate_results = mlflow.evaluate(
                model=retriever_model_function,
                data=eval_data,
                model_type="retriever",
                targets="source",
                evaluators="default",
            )
    return evaluate_results
result1 = evaluate_embedding(DatabricksEmbeddings(endpoint="databricks-bge-large-en"))result2 = evaluate_embed -
ding(<another-embedding-function>)
eval_results_of_retriever_df_bge = result1.tables["eval_results_table"]
display(eval_results_of_retriever_df_bge)109THE BIG BOOK OF GENERATIVE AIThe evaluation will return a table with the results of your evaluation for each question. i.e., for this test, we can 
see that the retriever seems to performing great for the questions "How to enable MLflow Autologging for my 
workspace by default?” with a Precision @ K score is 1, and is not retrieving any of the right documentation for 
the questions "What is MLflow?” since the precision @ K score is 0. With this insight, we can debug the retriever 
and improve the retriever for questions like “What is MLflow?”
Evaluation results when using databricks-bge-large-en embedding model
EVALUATE RETRIEVER WITH DIFFERENT TOP K VALUES WITH MLFLOW
You can quickly calculate the metrics for different Ks by specifying the extra_metrics argument.
 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19with mlflow.start_run() as run:
        evaluate_results = mlflow.evaluate(
        data=eval_results_of_retriever_df_bge,
        targets="source",
        predictions="outputs",
        evaluators="default",
        extra_metrics=[
            mlflow.metrics.precision_at_k(1),
            mlflow.metrics.precision_at_k(2),
            mlflow.metrics.precision_at_k(3),
            mlflow.metrics.recall_at_k(1),
            mlflow.metrics.recall_at_k(2),
            mlflow.metrics.recall_at_k(3),
            mlflow.metrics.ndcg_at_k(1),
            mlflow.metrics.ndcg_at_k(2),
            mlflow.metrics.ndcg_at_k(3),
        ],
    )
display(evaluate_results.tables["eval_results_table"])110THE BIG BOOK OF GENERATIVE AIThe evaluation will return a table with the results of your evaluation for each question, and you can better 
understand which K value to use when retrieving documents. i.e., for this test we can see changing the top K 
value can positively affect the precision of the retriever for questions like “What is Databricks?”
Evaluation result with all precision at K values
111THE BIG BOOK OF GENERATIVE AIEVALUATE THE CHUNKING STRATEGY WITH MLFLOW
The effectiveness of your chunking strategy is critical. We explore how MLflow can assist in this evaluation, 
focusing on the retrieval model type and its impact on overall performance.
 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25def evaluate_chunk_size(chunk_size):
  list_of_documents = loader.load()
  text_splitter = CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=0)
  docs = text_splitter.split_documents(list_of_documents)
  embedding_function = DatabricksEmbeddings(endpoint="databricks-bge-large-en")
  retriever = Chroma.from_documents(docs, embedding_function).as_retriever()
  
  def retrieve_doc_ids(question: str) -> List[str]:
    docs = retriever.get_relevant_documents(question)
    doc_ids = [doc.metadata["source"] for doc in docs]
    return doc_ids
   
  def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:
    return question_df["question"].apply(retrieve_doc_ids)
  with mlflow.start_run() as run:
      evaluate_results = mlflow.evaluate(
          model=retriever_model_function,
          data=eval_data,
          model_type="retriever",
          targets="source",
          evaluators="default",
      )
  return evaluate_results
result1 = evaluate_chunk_size(500)
result2 = evaluate_chunk_size(2000)
display(result1.tables["eval_results_table"])
display(result2.tables["eval_results_table"])112THE BIG BOOK OF GENERATIVE AIThe evaluation will return 2 tables with the results of your evaluation for each question using 2 different chunk 
sizes, and you can better understand which chunk size to use when retrieving documents (i.e., for this example, 
it seems like changing the chunk size did not affect any metric).
Evaluation result with Chunk size of 1000
Evaluation result with Chunk size of 2000
Check out the in-depth notebook on retrieval evaluation
113THE BIG BOOK OF GENERATIVE AIEVALUATION OF GENAI RESULTS WITH MLFLOW
In this section, we will understand: How good is the response of the GenAI app with a given prompt and context?
Assessing the quality of generated responses is key. We will augment the manual process of evaluating with 
questions and answers by leveraging MLflow's QA metrics, and comparing them against a GPT-4 model as a 
benchmark to understand the effectiveness of the generated answers. 
Using an LLM like GPT-4 as a judge to assist in evaluation  can offer several benefits, here are some key benefits:
 ■Rapid and Scalable Experimentation:  In many situations, we think LLM judges represent a sweet-spot: 
they can evaluate unstructured outputs (like a response from a chat-bot) automatically, rapidly, and  
at low-cost.  
 ■Cost-Effective: By automating some evaluations with LLMs, we consider it a worthy companion to human 
evaluation, which is slower and more expensive but represents the gold standard of model evaluation.
USE MLFLOW EVALUATE AND LLM AS A JUDGE
We take some sample questions and use the LLM as a judge, and inspect the results with MLflow, providing a 
comprehensive analysis of the outcome with built-in metrics. We are going to judge the GenAI app on relevance 
(how relevant is the output with respect to both the input and the context).
Create a simple function that runs each input through the chain
 
1
2def model(input_df):
    return input_df["questions"].map(qa).tolist()
 
1
2
3
4
5
6
7
8
9
10eval_df = pd.DataFrame(
    {
        "questions": [
            "What is MLflow?",
            "What is Databricks?",
            "How to serve a model on Databricks?",
            "How to enable MLflow Autologging for my workspace by default?",
        ],
    }
)114THE BIG BOOK OF GENERATIVE AIUse relevance metric to determine the relevance of the answer and context. There are other metrics  you can 
use too.
 
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21from mlflow.deployments import set_deployments_target
from  mlflow.metrics.genai.metric_definitions import relevance
set_deployments_target("databricks") #To retrieve all endpoint in your Databricks Workspace
relevance_metric = relevance(model=f"endpoints:/{endpoint_name}") #You can also use any model you have hosted on Da -
tabricks, models from the Marketplace or models in the Foundation model API
with mlflow.start_run():
    results =  mlflow.evaluate(
        model,
        eval_df,
        model_type="question-answering",
        evaluators="default",
        predictions="result",
        extra_metrics=[relevance_metric, mlflow.metrics.latency()],
        evaluator_config={
            "col_mapping": {
                "inputs": "questions",
                "context": "source_documents",
            }
        }
    )
    print(results.metrics)115THE BIG BOOK OF GENERATIVE AIIn your Databricks Workspace, you can compare and evaluate all your inputs and outputs, as well as the source 
documents, relevance and any other metrics you added to your evaluation function.
Check out more in depth notebooks on LLM evaluation
116THE BIG BOOK OF GENERATIVE AISummaryWhether you’re looking to disrupt traditional industries, enhance creative endeavors or solve complex problems 
in novel ways, the potential applications of generative AI are limited only by your imagination and willingness to 
experiment. Remember, every significant advancement in this field began with a simple idea and the courage to 
explore it further.
For those seeking more knowledge or simply curious about the latest developments in the realm of generative 
AI, we’ve provided some resources on training, demos and product information. 
GenAI Training
Generative AI Engineer Learning Pathway : Take self-paced, on-demand and instructor-led courses on 
generative AI
Free LLM Course (edX) : In-depth course to learn GenAI and LLMs inside and out
GenAI Webinar : Learn how to take control of your GenAI app performance, privacy and cost, and drive value 
with generative AI
Additional Resources
Big Book of MLOps : A deep dive into the architectures and technologies behind MLOps — including LLMs  
and GenAI 
Mosaic AI : Product page covering the features of Mosaic AI within Databricks117Build Production-Quality GenAI Applications — See How
Create high-quality generative AI applications and ensure your output is accurate, 
governed and safe. See why over 10,000 organizations worldwide rely on Databricks for 
all their workloads from BI to AI — test-drive the full Databricks Platform free for 14 days.
About Databricks
Databricks is the data and AI company. More than 10,000 organizations worldwide — 
including Comcast, Condé Nast, Grammarly and over 50% of the Fortune 500 — rely on 
the Databricks Data Intelligence Platform to unify and democratize data, analytics and 
AI. Databricks is headquartered in San Francisco, with offices around the globe, and was 
founded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow.  
To learn more, follow Databricks on LinkedIn , X and Facebook .Try Databricks free Take Generative AI Fundamentals On-Demand Training
© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation . Privacy Policy  | Terms of Use

1 | P a g e  
  
 
 
 
Generative AI  
 
Interview Q  & A 
 
 
 
 
 
 
 
 
 
I 
 
2 | P a g e  
 Table of C ontents  
 
Generic Questions  ................................ ................................ ................................ ................................ ... 3 
Generative AI  ................................ ................................ ................................ ................................ ..........  9 
Discriminative AI  ................................ ................................ ................................ ................................ ... 10 
General Questions on Generative and Discriminative AI  ................................ ................................ ..... 11 
Transformer Architecture  ................................ ................................ ................................ .....................  12 
Large Language Models (LLMs)  ................................ ................................ ................................ .............  16 
Embedding Models  ................................ ................................ ................................ ...............................  20 
Retrieval -Augmented Generation (RAG)  ................................ ................................ ..............................  22 
LLM Fine Tuning  ................................ ................................ ................................ ................................ .... 24 
Misc Topics in Generative AI  ................................ ................................ ................................ .................  29 
Prompt Engineering  ................................ ................................ ................................ ..............................  30 
One-Shot Prompting  ................................ ................................ ................................ .........................  35 
Few-Shot Prompting  ................................ ................................ ................................ .........................  36 
Zero -Shot Prompting  ................................ ................................ ................................ .........................  37 
Chain of Thought Prompting  ................................ ................................ ................................ .............  37 
Hybrid Prompting  ................................ ................................ ................................ ..............................  38 
ReAct Prompting  ................................ ................................ ................................ ...............................  40 
Advance Topics  ................................ ................................ ................................ ................................ ..... 40 
Graph Retrieval Augmented Generation (GraphRAG)  ................................ ................................ ...... 40 
Agentic AI / AI Agents  ................................ ................................ ................................ .......................  46 
Latest Q & A (Based on question asked in different companies)  ................................ ........................  49 
Latest top LLM Leaderboard  ................................ ................................ ................................ ................  57 
LMSYS Leaderboard  ................................ ................................ ................................ ..........................  57 
Research Papers  ................................ ................................ ................................ ................................ ... 57 
2025  ................................ ................................ ................................ ................................ ..............  57 
2024  ................................ ................................ ................................ ................................ ..............  57 
2023  ................................ ................................ ................................ ................................ ..............  58 
2022  ................................ ................................ ................................ ................................ ..............  59 
2020  ................................ ................................ ................................ ................................ ..............  59 
2017  ................................ ................................ ................................ ................................ ..............  59 
 
 3 | P a g e  
  
 
 
 
 
 
 
 
 
Generic Questions  
Useful Resources  
1. GenAI Essential Terms  
2. GenAI  with Azure  Cloud  
3. GenAI with AWS Cloud  
4. GenAI with VertexAI  
5. Agentic  AI Interview Q&A  
Q: What is Generative AI?   
A: Generative AI refers to a class of artificial intelligence models that can generate new data 
or content based on the data they were trained on. These models can create text, images, 
audio, and more.  
Here  is an overview of its evolution:  
• Early developments (1950s -2000s): Basic text generation, rule -based systems  
• Rise of neural networks (2010s): Deep learning advances, like RNNs and LSTMs  
• Transformer architecture (2017): Revolutionized natural language processing  
• GPT type models (2018 -present): Increasingly powerful language models  including 
GPT, Claude, Gemini, Llama3 etc  
• Diffusion models (2020 -present): Advanced image generation capabilities  
4 | P a g e  
 • Multimodal models (2021 -present): Combining text, image, and other modalities  
 
Q: How does Generative AI relate to deep learning, machine learning, and AI?  
A: Generative AI is a specialized branch of artificial intelligence focused on creating new 
content, like text, images, and audio. It exists within a hierarchy of AI concepts:  
• Artificial Intelligence (AI):  The broad field aiming to develop systems that can 
perform tasks requiring human -like intelligence.  
• Machine Learning (ML):  A subset of AI that uses data -driven algorithms to enable 
systems to learn and improve over time without being explicitly programmed.  
• Deep Learning (DL):  A subset of ML that uses neural networks with multiple layers to 
analy se complex patterns in large datasets.  
• Generative AI:  A cutting -edge application of DL that creates new and original 
content by learning patterns from existing data.  
Generative AI leverages deep learning techniques to extract features and recognize 
patterns, building on machine learning’s data -driven foundation to produce creative 
outputs. It’s a leading example of how AI can mimic and extend human cognitive abilities.  
Q: How does a Generative Adversarial Network (GAN) work?  
A: A GAN consists of two neural networks: a generator and a discriminator. The generator 
creates fake data, while the discriminator tries to distinguish between real and fake data. 
Both networks are trained simultaneously in a game -like scenario where the ge nerator 
improves its ability to create realistic data, and the discriminator gets better at detecting 
fakes.  
 
5 | P a g e  
 GAN (Source - GeeksForGeeks)  
Q: What is a Transformer model?  
A: A Transformer model is a type of deep learning architecture introduced in the paper 
"Attention is All You Need." It uses self -attention mechanisms to process input data and is 
highly effective for tasks like language translation and text generation.  
 
Transformer Architecture (Source – Wikipedia)  
Q: What is the difference between supervised and unsupervised learning in the context of 
Generative AI?  
A: In supervised learning, models are trained on labeled data, meaning each training 
example has an associated output label. In unsupervised learning, models learn patterns 
and structures from unlabeled data. Generative AI often uses unsupervised learning to  
generate new data without explicit output labels.  
Q: Can you explain the concept of 'latent space' in Generative AI?  
A: Latent space refers to a lower -dimensional representation of data in which generative 
models, like GANs or VAEs, encode high -dimensional input data. Navigating this space 
allows the model to generate new, similar data by decoding points in the latent spac e. 
Q: What is the purpose of the self -attention mechanism in the Transformer model?  
A: The self -attention mechanism allows the Transformer model to weigh the importance of 
different words in a sentence relative to each other. This helps the model understand 
6 | P a g e  
 context and relationships within the data, leading to more accurate predictions and 
generation.  
 
Q: What is a Variational Autoencoder (VAE)?  
A: A VAE is a type of generative model that learns to encode input data into a probabilistic 
latent space and then decodes it back to the original data space. Unlike traditional 
autoencoders, VAEs introduce a regularization term that forces the latent space to follow a 
known distribution, typically Gaussian.  
 
 
VAE (Source - Wikipedia)  
Q: How does the Encoder -Decoder architecture work?  
A: The Encoder -Decoder architecture consists of two parts: the encoder, which processes 
the input and compresses it into a fixed -size context vector (latent space), and the decoder, 
which takes this context vector and generates the output. This architecture is commonly 
used in tasks like machine translation.  
Q: What are some common applications of Generative AI?  
A: Common applications include text generation (e.g., chatbots, content creation), image 
generation (e.g., deepfakes, art creation), music composition, data augmentation, drug 
discovery, and improving the quality of medical imaging.  
Q: How does a Recurrent Neural Network (RNN) differ from a Transformer in Generative 
AI tasks?  
A: RNNs process sequential data by maintaining a hidden state that captures information 
from previous steps. Transformers, on the other hand, use self -attention mechanisms to 
7 | P a g e  
 process entire sequences in parallel, which makes them more efficient and better at 
capturing long -range dependencies.  
Q: What is BERT, and how is it used in Generative AI?  
A: BERT (Bidirectional Encoder Representations from Transformers) is a pre -trained 
language model designed for understanding context in text by considering both the left and 
right surroundings. While BERT itself is not generative, its architecture can be ada pted for 
text generation tasks in models like GPT (Generative Pre -trained Transformer).  It’s Encoder 
only Model . 
Q: How does GPT differ from BERT?  
A: GPT (Generative Pre -trained Transformer) is designed primarily for text generation, using 
a unidirectional (left -to-right) approach. BERT, on the other hand, is designed for 
understanding and processing text by using a bidirectional approach. GPT generate s 
coherent text, while BERT excels in tasks like question answering and text classification.  
GPT is Decoder only Model and BERT is Encoder only Model  
Q: What is style transfer in the context of Generative AI?  
A: Style transfer is a technique in Generative AI that applies the style of one image (e.g., a 
painting) to the content of another image (e.g., a photograph). This is achieved by 
separating and recombining the content and style representations in the images using 
neural networks.  
Q: How can Generative AI be used in data augmentation?  
A: Generative AI can create synthetic data that resembles real data, which can be used to 
augment training datasets. This is especially useful in scenarios where collecting real data is 
expensive or impractical, helping to improve the performance of machine learning models 
by increasing data diversity.  
Q: What is transfer learning, and how is it applied in Generative AI?  
A: Transfer learning involves using a pre -trained model on a new, related task. In Generative 
AI, models like GPT -3 are pre -trained on large corpora of text and can be fine -tuned for 
specific tasks (e.g., text summarization, translation) with relatively smal ler datasets.  
Q: What is a Deepfake?  
A: A Deepfake is a synthetic media, often a video or audio, created using Generative AI 
techniques, particularly GANs. It involves altering existing media or creating new media to 
make it appear authentic, often used to create realistic but fake representati ons of people.  
Q: How does an autoencoder work, and what is its purpose in Generative AI?  8 | P a g e  
 A: An autoencoder consists of an encoder that compresses the input data into a lower -
dimensional latent space and a decoder that reconstructs the original data from this latent 
space. It is used for tasks like dimensionality reduction, anomaly detection, and  as a building 
block in generative models.  
Q: What is the role of loss functions in training GANs?  
A: Loss functions in GANs guide the training process of both the generator and the 
discriminator. The generator's loss measures how well it can fool the discriminator, while 
the discriminator's loss measures how well it can distinguish real data from fake da ta. 
Balancing these losses is crucial for stable training.  
Q: What is the importance of the "attention mechanism" in NLP tasks?  
A: The attention mechanism allows models to focus on relevant parts of the input sequence 
when making predictions. This is particularly important in NLP tasks for capturing contextual 
dependencies and improving the quality of generated text.  
Q: What are some ethical concerns associated with Generative AI?  
A: Ethical concerns include the potential for misuse in creating fake content (deepfakes), 
intellectual property issues, bias in generated content, and the impact on privacy. Ensuring 
responsible use and developing techniques for detecting AI -generated conte nt are ongoing 
challenges.  
Q: How can Generative AI improve natural language understanding?  
A: Generative AI models like GPT -4, Gemini, Claude, Llama etc can generate coherent and 
contextually relevant text, which helps in building more advanced chatbots, improving 
machine translation, and creating better tools for text summarization and sentiment 
analysis.  
Q: What is reinforcement learning, and is it used in Generative AI?  
A: Reinforcement learning involves training an agent to make decisions by rewarding 
desired actions and penalizing undesired ones. While not typically used in generative tasks, 
reinforcement learning can be combined with generative models in areas like game 
development and optimizing content generation strategies.  
Q: What is the significance of "zero -shot learning" in Generative AI?  
A: Zero -shot learning allows a model to make predictions for classes it has never seen during 
training by leveraging generalizable features learned from seen classes. This is significant in 
Generative AI for creating content or solving tasks without requirin g extensive labeled data 
for every possible category.  
 9 | P a g e  
 Generative AI  
Q: What is Generative AI and how does it differ from other types of AI?   
A: Generative AI refers to models that generate new data samples similar to the training 
data. Unlike discriminative models, which classify input data into categories, generative 
models can create new instances that resemble the input data. Examples include text 
generation, image synthesis, and music composition.  
Q: Can you explain the difference between a Generative Adversarial Network (GAN) and a 
Variational Autoencoder (VAE)?  
A: GANs consist of two networks, a generator and a discriminator, that compete against 
each other to produce realistic data. The generator creates data samples, while the 
discriminator evaluates their authenticity. VAEs, on the other hand, encode data into a  
latent space and decode it back to reconstruct the data, focusing on generating data by 
learning the underlying distribution.  
 
GAN (Source Geek sForGeeks)  
10 | P a g e  
  
VAE (Source - Wikipedia)  
Q: What are some common applications of Generative AI?  
A: Common applications include image and video synthesis, text generation, music and art 
creation, drug discovery, and data augmentation for training other machine learning 
models.  
Q: How does the training process of a GAN work?   
A: Training a GAN involves a two -step process: the generator creates fake data samples, and 
the discriminator evaluates them against real data samples. The generator aims to produce 
more realistic samples to fool the discriminator, while the discriminator ai ms to improve its 
ability to distinguish real from fake data. This adversarial process continues until the 
generator produces highly realistic samples.  
Q: What challenges are associated with training Generative AI models?  
A: Challenges include mode collapse (where the generator produces limited variations of 
data), ensuring training stability, and the need for large amounts of data and computational 
resources.  
Discriminative AI  
Q: What is Discriminative AI and how does it differ from Generative AI?   
A: Discriminative AI models focus on classifying input data into predefined categories. They 
learn the boundary between classes based on the training data. Unlike generative models, 
discriminative models do not generate new data; they only classify existing data.  
 
 
11 | P a g e  
 Q: Can you give an example of a discriminative model and its application?  
A: An example of a discriminative model is a Support Vector Machine (SVM), which classifies 
data by finding the hyperplane that best separates different classes. Applications include 
image classification, spam detection, and medical diagnosis.  
Q: How does a discriminative model learn from data?  
A: Discriminative models learn by optimizing a loss function that measures the difference 
between predicted and actual labels in the training data. Techniques like gradient descent 
are used to minimize this loss and improve the model's accuracy.  
Q: What are the key differences in the training objectives of generative and discriminative 
models?  
A: The training objective of generative models is to learn the underlying data distribution to 
generate new samples, while discriminative models aim to learn the decision boundary that 
separates different classes.  
Q: What are some advantages of using discriminative models over generative models?  
A: Discriminative models typically require less computational resources, have simpler 
training processes, and often achieve higher accuracy in classification tasks compared to 
generative models.  
General Questions on Generative and Discriminative AI  
Q: How can generative and discriminative models complement each other in a machine 
learning pipeline?  
A: Generative models can be used for data augmentation to create additional training 
samples, which can improve the performance of discriminative models. Additionally, 
generative models can help in understanding the data distribution, which can inform the 
design and training of discriminative models.  
 
 
https://www.researchgate.net/figure/A -simple -illustration -of-how -one-can-use-discriminative -vs-generative -models -The-former_fig1_341478640  12 | P a g e  
 Q: Can you discuss a scenario where both generative and discriminative models are used 
together?  
A: In semi -supervised learning, generative models can be used to generate synthetic data to 
augment a small labeled dataset, and then a discriminative model can be trained on this 
augmented dataset to improve classification performance.  
Q: What are the trade -offs between using generative and discriminative models?  
A: Generative models can provide more insights into the data and generate new data, but 
they are often more complex and computationally intensive. Discriminative models are 
usually simpler, faster to train, and can achieve higher accuracy in classification t asks, but 
they do not generate new data.  
Q: How do generative models handle unsupervised learning tasks differently from 
discriminative models?  
A: Generative models can learn from unlabeled data by modeling the data distribution and 
generating new samples, while discriminative models require labeled data to learn the 
decision boundary between classes.  
Transformer Architecture  
 
 
Transformer Architecture (Source – Wikipedia)  
 
13 | P a g e  
 Q: What is the Transformer architecture and why is it significant in Generative AI?  
A: The Transformer architecture is a deep learning model introduced by Vaswani et al. in 
2017, designed to handle sequential data with a mechanism called self -attention. It is 
significant in Generative AI because it efficiently processes long -range dependenc ies in data, 
enabling high -quality text generation, translation, and other language -related tasks.  
Example:  Imagine you're reading a sentence like "The cat sat on the mat because it was 
tired." To understand this sentence fully, you need to know that "it" refers to "the cat." The 
Transformer architecture works like this by looking at all parts of a sentence simu ltaneously 
and figuring out which words are important for understanding the context.  
For example, if the Transformer sees "The cat sat on the mat because it was tired," it 
understands that "it" refers to "the cat" by looking at the whole sentence at once, not just 
word -by-word. This ability to grasp relationships between words that are far  apart helps it 
generate better text and translate languages more accurately.  
Q: Can you explain the self -attention mechanism in Transformers?  
A: Self-attention allows the model to weigh the importance of different words in a sequence 
when generating an output. For each word, the mechanism computes attention scores 
relative to all other words, helping the model understand contextual relationships. This is 
crucial for capturing the nuances in language data.  
Q: What are the main components of the Transformer architecture?   
A: The main components are the encoder and decoder stacks. Each stack consists of 
multiple layers of self -attention and feed -forward neural networks, along with layer 
normalization and residual connections. The encoder processes input sequences, and the 
deco der generates output sequences.  
Details are as follows : 
• Encoder Stack:  This processes the input data.  
• Self-Attention Mechanism:  This allows each word in the input to consider all other 
words in the sequence when forming its representation, helping capture the context 
and relationships between words.  
• Feed -Forward Neural Networks:  After self -attention, the data passes through a 
feed -forward network to further refine and process the information.  
• Layer Normalization:  This technique normalizes the data within each layer to 
improve training stability and performance.  
• Residual Connections:  These connections help in avoiding the vanishing gradient 
problem by allowing gradients to flow through the network more easily during 
training.  
• Decoder Stack:  This generates the output data based on the encoded input.  14 | P a g e  
 • Masked Self -Attention: Similar to self -attention in the encoder but designed to 
prevent the model from seeing future words in the sequence, which is crucial for 
generating text.  
• Encoder -Decoder Attention:  This layer allows the decoder to focus on different parts 
of the input sequence as it generates each word, helping align the generated text 
with the input.  
• Feed -Forward Neural Networks:  Similar to the encoder, these refine the data in the 
decoder.  
• Layer Normalization and Residual Connections:  Also used here to stabilize and 
improve the training process.  
Q: How does positional encoding work in Transformers?   
A: Positional encoding adds information about the position of each word in the sequence 
because the Transformer architecture does not inherently understand word order. This is 
achieved by adding sine and cosine functions of different frequencies to the input  
embeddings, allowing the model to incorporate positional information.  
Q: What advantages do Transformers have over traditional RNNs and LSTMs?  
A: Transformers address the limitations of RNNs and LSTMs, such as difficulty in capturing 
long -range dependencies and slow training due to sequential data processing. Transformers 
use self -attention mechanisms to process entire sequences in parallel, improv ing efficiency 
and performance.  
Q: Can you describe the role of multi -head attention in the Transformer architecture?   
A: Multi -head attention allows the model to focus on different parts of the input sequence 
simultaneously. By splitting the input into multiple heads, each with its own self -attention 
mechanism, the model can capture various aspects of the relationships betw een words, 
enhancing its ability to understand complex patterns.  
Q: How does the Transformer architecture handle the task of machine translation?  
A: In machine translation, the encoder processes the source language sentence, generating 
context -rich representations. The decoder then uses these representations, along with 
previously generated words, to produce the target language sentence. Attention 
mec hanisms help align source and target sentences.  
Q: What is the significance of the "Attention is All You Need" paper?  
A: The paper "Attention is All You Need" by Vaswani et al. introduced the Transformer 
architecture and demonstrated that self -attention mechanisms could replace recurrent and 
convolutional layers in sequence modelling  tasks. This breakthrough significantly improved 
performance and efficiency in NLP tasks.  
 15 | P a g e  
 Q: How are Transformers used in language models like GPT, Gemini , Claude. Llama etc?  
A: Transformers serve as the backbone for large language models like GPT -3. These models 
use a stack of Transformer decoder layers to generate text by predicting the next word in a 
sequence based on the context provided by previous words.  
Q: What are some challenges associated with training Transformer models?  
A: Challenges include the high computational and memory requirements due to parallel 
processing of large sequences, managing overfitting with large model sizes, and ensuring 
efficient use of data to prevent long training times.  
Q: How can you address the issue of long sequence processing in Transformers?   
A: Techniques like sparse attention mechanisms, memory -augmented networks, and 
Transformer variants like Longformer and Reformer can be used to efficiently process longer 
sequences by reducing computational complexity and memory usage.  
 
Q: Explain how the encoder and decoder work together in the Transformer architecture 
using an example.  
A: In a translation task, the encoder processes the sentence "The cat is on the mat" and 
generates context -rich embeddings. The decoder then takes these embeddings and 
generates the translated sentence "Le chat est sur le tapis," using attention mechanisms t o 
align the words correctly.  
Q: What is the role of residual connections in the Transformer architecture?  
A: Residual connections help mitigate the vanishing gradient problem and enable deeper 
networks by allowing gradients to flow more easily through the network. They also stabilize 
training and improve convergence rates.  
Q: How do Transformers handle the challenge of data parallelism?   
A: Transformers process entire sequences in parallel using self -attention mechanisms, which 
allows for efficient data parallelism. This significantly speeds up training and inference 
compared to sequential models like RNNs.  
Q: Describe an application of Transformers in a non -textual domain.   
A: Transformers can be applied to image processing tasks such as image classification and 
segmentation. Vision Transformers (ViTs) split an image into patches and process them 
similarly to sequences of words, leveraging self -attention to capture spatial rela tionships.  
Q: How do layer normalization and dropout contribute to the performance of 
Transformers?  16 | P a g e  
 A: Layer normalization stabilizes the training process by normalizing the input to each sub -
layer, while dropout helps prevent overfitting by randomly zeroing out a fraction of the 
connections during training, enhancing the model's generalization capabilitie s. 
Q: What are some recent advancements in Transformer architectures?  
A: Recent advancements include BERT (Bidirectional Encoder Representations from 
Transformers), which uses a bidirectional approach to understand context from both 
directions, and GPT (Generative Pre -trained Transformer) models, which are scaled -up 
versions w ith trillions  of parameters for superior language generation.  
Q: How do Transformers manage context for very long texts?  
A: Techniques like segment -level recurrence in Transformer -XL, hierarchical attention in 
models like BigBird, and local -global attention patterns in Longformer help manage long -
context dependencies by breaking down the text into manageable chunks while retai ning 
contextual information.  
 
 
Q: How do you handle the scalability issues in training large Transformer models?  
A: Scalability issues can be addressed by distributed training across multiple GPUs, mixed -
precision training to reduce memory usage, model parallelism, and using efficient 
implementations like NVIDIA's Megatron or DeepSpeed from Microsoft.  
Q: Explain the impact of Transformer models on the field of Natural Language Processing 
(NLP).  
A: Transformer models have revolutionized NLP by setting new benchmarks in various tasks 
such as translation, summarization, and question answering. Their ability to handle context 
and dependencies more effectively than previous models has led to significant  
improvements in performance and opened new research avenues.  
Large Language Models (LLMs)  
Q: What is a Large Language Model (LLM)?  
A: A Large Language Model (LLM) is a type of artificial intelligence model that uses deep 
learning techniques to understand, generate, and manipulate human language. Examples 
include GPT -3, GPT -4, and BERT.  
Q: How do LLMs like GPT -3 and GPT -4 work?  17 | P a g e  
 A: LLMs use transformer architectures with attention mechanisms to process input text and 
generate coherent, contextually relevant output. They are pre -trained on vast datasets and 
can be fine -tuned for specific tasks.  
Q: What are the applications of LLMs?  
A: Applications of LLMs include text generation, translation, summarization, question 
answering, sentiment analysis, and conversational agents.  
Q: What is the transformer architecture?  
A: The transformer architecture is a neural network design that relies on self -attention 
mechanisms to weigh the importance of different parts of the input data, enabling efficient 
parallel processing and improved context understanding.  
Q: What is attention mechanism in transformers?  
A: The attention mechanism allows the model to focus on relevant parts of the input 
sequence when generating output, improving the model's ability to capture dependencies 
and context.  
 
 
Q: What is the difference between GPT and BERT?  
A: GPT is a unidirectional transformer model designed for generative tasks, while BERT is a 
bidirectional transformer model focused on understanding context for tasks like question 
answering and classification.  
Q: How is the training data for LLMs typically collected?  
A: Training data for LLMs is usually collected from large -scale text corpora, including books, 
articles, websites, and other publicly available text sources.  
Q: What are the ethical concerns associated with LLMs?  
A: Ethical concerns include the potential for generating biased or harmful content, misuse 
for disinformation, and issues related to privacy and data security.  
Q: How do LLMs handle out -of-vocabulary words?  
A: LLMs use subword  tokenization methods, such as Byte Pair Encoding (BPE) or WordPiece, 
to break down out -of-vocabulary words into known subwords or characters.  
Q: What is the main difference between a chat -based LLM and a pretrained LLM?  18 | P a g e  
 A: The main difference lies in their primary use case and fine -tuning:  
• Pretrained LLMs are general -purpose models trained on vast amounts of text data to 
understand and generate human -like text. They can be used for various natural 
language processing tasks but may require additional fine -tuning for specific 
applications.  
• Chat -based LLMs are specifically fine -tuned for conversational interactions. They are 
often based on pretrained LLMs but have undergone additional training on dialogue 
data to improve their ability to maintain context, understand user intent, and 
generate more natural, contextually appropriate responses in a conversation.  
Q: What are some challenges in developing chat -based LLMs?  
A: Some key challenges include:  
• Maintaining context over long conversations  
• Ensuring consistency in responses  
• Handling ambiguity and understanding user intent  
• Generating safe and appropriate responses  
• Balancing between staying on -topic and being flexible in conversations  
• Managing biases present in training data  
• Optimizing response time for real -time interactions  
 
 
Q: How do chat -based LLMs handle multi -turn conversations?  
A: Chat -based LLMs handle multi -turn conversations through:  
• Maintaining conversation history:  The model considers previous messages in the 
conversation to maintain context.  
• Context window:  A limited amount of prior conversation is included in each input to 
the model.  
• Special tokens:  Markers to indicate speaker changes or separate different parts of 
the conversation.  
• Fine -tuning on dialogue data:  This helps the model learn patterns specific to 
conversational exchanges.  
• Memory mechanisms:  Some advanced systems may use external memory to store 
and retrieve relevant information from earlier in the conversation.  
Q: What is the "temperature" parameter in LLMs, and how does it affect the output?  
A: The temperature parameter controls the randomness of the model's output. It's typically 
a value between 0 and 1:  19 | P a g e  
 • Lower temperature (closer to 0) : Produces more deterministic, focused, and 
conservative outputs. The model is more likely to choose the most probable next 
token.  
• Higher temperature (closer to 1) : Leads to more diverse, creative, and sometimes 
unpredictable outputs. The model is more likely to choose from a wider range of 
possible next tokens.  
A temperature of 0 will always select the most likely next token, while a temperature 
approaching infinity will select tokens randomly.  
Q: What is the "max tokens" parameter, and why is it important?  
A: The max tokens parameter sets the maximum length of the model's output in tokens. It's 
important because:  
• It helps control the length of the generated text.  
• It can prevent the model from producing excessively long or rambling responses.  
• It's useful for managing computational resources and response time, especially in 
real-time applications.  
• It can be adjusted based on the specific task or user requirements.  
Q: Can you explain the concept of "top -p" (or nucleus) sampling?  
A: Top-p sampling, also known as nucleus sampling, is an alternative to temperature -based 
sampling:  
• It involves choosing from the smallest possible set of tokens whose cumulative 
probability exceeds the probability p.  
• This method can produce more diverse outputs while avoiding low -probability 
tokens that might lead to incoherent text.  
• A typical value might be around 0.9, meaning the model considers only the most 
likely tokens that together comprise 90% of the probability mass.  
Q: What is the "presence penalty" in some LLM APIs, and how does it work?  
A: The presence penalty is a parameter that influences token selection based on whether a 
token has appeared in the text so far:  
• It applies a penalty to tokens that have already appeared in the generated text.  
• This encourages the model to use a more diverse vocabulary and avoid repetition.  
• A higher presence penalty makes the model less likely to repeat tokens it has already 
used.  
Q: What is the "frequency penalty" parameter, and how does it differ from the presence 
penalty?  
A: The frequency penalty is similar to the presence penalty but works slightly differently:  20 | P a g e  
 • It applies a penalty to tokens based on how often they've appeared in the generated 
text so far.  
• Unlike the presence penalty, which only considers whether a token has appeared at 
all, the frequency penalty increases with each occurrence of a token.  
• This can help reduce not just repetition of specific words, but also overuse of certain 
phrases or patterns.  
Q: What is "top -k" sampling, and how does it compare to other sampling methods?  
A: Top-k sampling is another method for controlling the randomness of LLM outputs:  
• It involves selecting the next token only from the k most likely options.  
• This can help prevent the selection of highly improbable tokens while still allowing 
for some randomness.  
• Compared to temperature sampling, top -k provides more direct control over the 
pool of possible next tokens.  
• However, it can be less flexible than top -p sampling, as it uses a fixed number of 
options regardless of the probability distribution.  
Q: How does the "stop sequence" parameter work in LLM APIs?  
A: The stop sequence parameter allows you to specify one or more sequences that will 
cause the model to stop generating further output:  
• When the model generates any of the specified stop sequences, it will terminate the 
response at that point.  
• This is useful for controlling the format of the output, especially in structured 
generation tasks.  
• For example, you might use " \n" as a stop sequence to generate single -line 
responses, or "Q:" to stop after generating an answer in a Q&A format.  
 
Q: What is transfer learning in the context of LLMs?  
A: Transfer learning involves pre -training an LLM on a large dataset and then fine -tuning it 
on a smaller, task -specific dataset to adapt it to specific applications.  
 
Embedding Models  
Google Colab Notebook  21 | P a g e  
  
Source: https://qdrant.tech/articles/what -are-embeddings/  
Q: What are word embeddings?   
A: Word embeddings are dense vector representations of words that capture their semantic 
meaning and relationships, enabling models to process text in a numerical form.  
Q: How are embeddings generated?   
A: Embeddings are generated using techniques like Word2Vec, GloVe, or through 
transformer -based models that learn context -dependent representations during training.  
Q: What is the purpose of embeddings in NLP?   
A: Embeddings enable models to understand and manipulate text by representing words 
and phrases as vectors in a continuous space, improving performance on various NLP tasks.  
Q: What is the difference between static and dynamic embeddings?   
A: Static embeddings, like Word2Vec and GloVe, provide a fixed representation for each 
word, while dynamic embeddings, like those from BERT, adjust based on the context in 
which the word appears.  
Q: What are contextual embeddings?  
A: Contextual embeddings are dynamic embeddings that capture the meaning of words 
based on their surrounding context, improving understanding and accuracy in NLP tasks.  
Q: How do transformer models generate embeddings?  
A: Transformer models generate embeddings through multiple layers of self -attention and 
feed -forward networks, capturing rich, context -dependent representations of text.  
Q: What is the role of position embeddings in transformers?  
A: Position embeddings provide information about the position of each token in the input 
sequence, helping the model understand the order and structure of the data.  
22 | P a g e  
 Q: How are sentence embeddings different from word embeddings?  
A: Sentence embeddings represent entire sentences or phrases as vectors, capturing the 
overall meaning, while word embeddings represent individual words.  
Q: What are the applications of sentence embeddings?  
A: Applications include sentence similarity, paraphrase detection, document retrieval, and 
clustering.  
Q: How can embeddings be used for text classification?  
A: Embeddings transform text into vectors that can be fed into machine learning models, 
such as neural networks, for classification tasks like sentiment analysis or topic 
categorization.  
Retrieval -Augmented Generation (RAG)  
PDF  & Notebook  
Q: What is Retrieval -Augmented Generation (RAG)?  
A: RAG is a framework that combines retrieval -based methods with generative models to 
improve the accuracy and relevance of generated content by incorporating external 
information.  
Q: How does RAG work?  
A: RAG retrieves relevant documents or passages from a knowledge base and uses this 
information to guide the generation process, enhancing the output with contextually 
accurate details.  
Q: What are the components of a RAG model?  
A: A RAG model typically consists of a retriever, which fetches relevant information, and a 
generator, which uses the retrieved information to produce the final output.  
 
Q: What are the benefits of using RAG?  
A: Benefits include improved accuracy, relevance, and informativeness of generated 
content, as the model can access and incorporate external knowledge.  
Q: What are the challenges associated with RAG?  23 | P a g e  
 A: Challenges include the complexity of integrating retrieval and generation components, 
handling large -scale knowledge bases, and ensuring the retrieved information is correctly 
utilized.  
Q: How can RAG be applied to question answering?  
A: In question answering, RAG retrieves relevant documents or passages and uses them to 
generate precise and contextually accurate answers to the given questions.  
Q: What datasets are commonly used for training RAG models?  
A: Common datasets include Natural Questions, TriviaQA, and WebQuestions, which 
provide large collections of question -answer pairs and related documents.  
Q: How does RAG improve over traditional generative models?   
A: RAG improves generative models by providing access to external, up -to-date information, 
which helps generate more accurate and contextually relevant responses.  
Q: What role does the retriever play in a RAG model?   
A: The retriever fetches the most relevant documents or passages from a knowledge base, 
providing the generator with contextually useful information to enhance the output.  
Q: How can you evaluate the performance of a RAG model?  
A: Performance can be evaluated using metrics such as BLEU, ROUGE, and Exact Match for 
generation quality, as well as retrieval -specific metrics like Precisio n and Recall.  
Q: What is iterative refinement in RAG?  
A: A technique where the model repeatedly retrieves and generates, refining its output 
based on intermediate results.  
Q: How can RAG systems be evaluated?  
A: Through metrics like relevance, coherence, factual accuracy, and comparison with 
human -generated responses.  
Q: What is the role of attention mechanisms in RAG?  
A: They help the generator focus on the most relevant parts of retrieved information during 
text generation.  
Q: How can RAG be adapted for domain -specific applications?  
A: By using specialized knowledge bases, fine -tuning on domain data, and customizing 
retrieval strategies.  24 | P a g e  
 Q: What are some challenges in scaling RAG systems?  
A: Managing large knowledge bases, reducing latency, and maintaining accuracy with 
increased information volume.  
Q: How can RAG be used to improve AI model transparency?  
A: By providing sources for generated information, allowing users to verify the origin of the 
model's knowledge.  
LLM Fine Tuning  
Q: What is fine -tuning in the context of LLMs?  
A: Fine -tuning involves adapting a pre -trained LLM to a specific task or dataset by continuing 
its training on task -specific data, improving performance on that particular task.  
Q: Explain the process of fine -tuning a language model and its importance in NLP 
applications.  
A: Fine -tuning is the process of taking a pre -trained language model and adapting it to a 
specific task or dataset by continuing training on task -specific data. It's important in NLP 
applications because it allows us to leverage the general language understa nding of large 
pre-trained models while adapting them to specific domains or tasks. This process typically 
involves:  
1. Selecting a pre -trained model  
2. Preparing task -specific data  
3. Adjusting the model architecture if necessary  
4. Training on the new data with appropriate hyperparameters  
5. Evaluating and iterating to improve performance Fine -tuning is crucial as it often 
results in better performance than training from scratch, requires less data, and 
takes less time and computational resources.  
Q: Why is fine -tuning important for LLMs?  
A: Fine -tuning allows LLMs to specialize and improve performance on specific tasks, making 
them more effective and accurate for targeted applications.  
Q: What is the difference between pre -training and fine -tuning?  
A: Pre-training involves training a model on a large, diverse dataset to learn general 
language representations, while fine -tuning adapts the pre -trained model to specific tasks 
using smaller, task -specific datasets.  
 25 | P a g e  
 Q: How does the size of the fine -tuning dataset affect the model?  
A: Larger fine -tuning datasets can lead to better performance and generalization, but even 
small datasets can significantly improve task -specific accuracy due to the pre -trained 
model's foundational knowledge.  
Q: What are some common challenges in fine -tuning LLMs?  
A: Challenges include overfitting to the fine -tuning dataset, computational resource 
requirements, and ensuring the model maintains generalization capabilities.  
Q: What techniques can be used to prevent overfitting during fine -tuning?   
A: Techniques include using regularization methods, dropout, early stopping, and data 
augmentation to ensure the model does not overfit to the fine -tuning dataset.  
Q: How can you assess the success of fine -tuning?  
A: Success can be assessed using performance metrics relevant to the task, such as 
accuracy, F1 score, BLEU score, and human evaluation for tasks like text generation.  
Q: What is domain adaptation in the context of LLM fine -tuning?  
A: Domain adaptation involves fine -tuning an LLM to perform well in a specific domain or 
field, such as medical text or legal documents, by training on domain -specific data.  
Q: What is catastrophic forgetting in the context of fine -tuning, and what strategies can be 
employed to mitigate it?   
A: Catastrophic forgetting in LLM fine -tuning happens when a pre -trained language model 
forgets what it learned before while trying to learn something new. As the model adjusts its 
settings to do better on a new task, it can overwrite important knowledge that  was helpful 
for other tasks or its overall understanding of language.  
Strategies to mitigate this include:  
1. Gradual unfreezing:  Starting fine -tuning with most layers frozen and gradually 
unfreezing them.  
2. Multi -task learning:  Training on multiple tasks simultaneously to maintain general 
knowledge.  
3. Elastic Weight Consolidation (EWC):  Adding a penalty term to the loss function to 
prevent drastic changes to important parameters.  
4. Rehearsal methods:  Periodically revisiting samples from the original pre -training 
dataset.  
5. Layer -wise learning rate decay:  Using lower learning rates for earlier layers of the 
model. These techniques help balance retaining general knowledge with adapting to 
new tasks.  26 | P a g e  
  
Q: Explain the concept of transfer learning in the context of LLM fine -tuning. What are its 
advantages and potential limitations?  
A: Transfer learning in LLM fine -tuning involves using a model pre -trained on a large, 
general dataset and adapting it to a new, often related task. Advantages include:  
1. Reduced training time and computational resources  
2. Better performance, especially with limited task -specific data  
3. Leveraging general language understanding for specific tasks  
4. Ability to adapt to new domains quickly  
Potential limitations include:  
1. Negative transfer, where pre -trained knowledge interferes with the new task  
2. Difficulty in adapting to vastly different domains or languages  
3. Potential biases inherited from the pre -trained model  
4. Challenges in fine -tuning very large models with limited resources  
Q: Describe the role of regularization techniques in fine -tuning LLMs. How do methods like 
dropout and L2 regularization work, and when should they be applied?  
A: Regularization techniques help prevent overfitting during fine -tuning by adding 
constraints to the learning process. Two common methods are:  
1. Dropout : Randomly "drops out" a proportion of neurons during training, forcing the 
network to learn more robust features. It's particularly useful in fine -tuning to 
prevent the model from relying too heavily on specific patterns in the new data.  
2. L2 regularization : Adds a penalty term to the loss function based on the squared 
magnitude of parameters. This encourages the model to use all of its inputs a little 
rather than some of its inputs a lot, leading to better generalization.  
These techniques should be applied when there's a risk of overfitting, particularly when 
fine-tuning on small datasets or for many epochs.  
Q: What is RLHF (Reinforcement Learning from Human Feedback), and how does it 
enhance the fine -tuning process for language models?   
A: RLHF is a technique that incorporates human feedback into the fine -tuning process. It 
works as follows:  
1. The model generates responses to prompts.  
2. Human evaluators rate these responses based on quality, relevance, safety, etc.  
3. These ratings are used to train a reward model.  
4. The language model is then fine -tuned using reinforcement learning, with the 
reward model providing the reward signal.  27 | P a g e  
 RLHF enhances fine -tuning by:  
• Aligning the model's outputs more closely with human preferences  
• Improving the quality and relevance of generated content  
• Helping to mitigate unsafe or biased outputs  
• Allowing for more nuanced optimization beyond traditional supervised learning  
This technique has been crucial in developing more capable and aligned language models, 
particularly for conversational AI applications.  
Q: Explain the concepts of LoRA and QLoRA. How do these techniques improve the 
efficiency of fine -tuning large language models?  
A: LoRA (Low -Rank Adaptation) and QLoRA (Quantized Low -Rank Adaptation) are 
techniques designed to make fine -tuning of large language models more efficient:  
LoRA:  
• Decomposes weight update matrices into lower -rank matrices  
• Significantly reduces the number of trainable parameters  
• Allows for efficient adaptation of specific parts of the model  
QLoRA:  
• Combines LoRA  with quantization techniques  
• Further reduces memory usage by using lower precision (e.g., 4 -bit) representations  
• Enables fine -tuning of larger models on consumer -grade hardware  
These techniques improve efficiency by:  
1. Reducing memory requirements  
2. Decreasing computational costs  
3. Enabling faster training and inference  
4. Allowing for more efficient storage and deployment of fine -tuned models  
They're particularly valuable when working with very large language models or in resource -
constrained environments.  
Q: What is the importance of evaluation metrics in fine -tuning, and how do you choose 
appropriate metrics for different tasks?   
A: Evaluation metrics are crucial in fine -tuning as they:  
1. Provide quantitative measures of model performance  
2. Guide the optimization process  
3. Help in comparing different models or fine -tuning approaches  
4. Indicate when to stop training to prevent overfitting  28 | P a g e  
 Choosing appropriate metrics depends on the task:  
• For classification:  accuracy, precision, recall, F1 score  
• For generation tasks like summarization:  ROUGE score  
• For translation:  BLEU score  
• For question -answering:  Exact Match and F1 score  
• For language modeling:  perplexity  
It's often beneficial to use multiple metrics to get a comprehensive view of performance.  
Q: Describe the process of hyperparameter tuning in the context of fine -tuning. What 
strategies can be employed to efficiently optimize hyperparameters?   
A: Hyperparameter tuning involves finding the optimal set of hyperparameters (e.g., 
learning rate, batch size, number of epochs) for fine -tuning. The process typically includes:  
1. Defining the hyperparameter search space  
2. Choosing a search strategy  
3. Training models with different hyperparameter configurations  
4. Evaluating performance on a validation set  
5. Selecting the best -performing configuration  
Efficient strategies include:  
• Grid Search: Exhaustive search over specified parameter values  
• Random Search: Randomly sampling from the parameter space  
• Bayesian Optimization: Using probabilistic models to guide the search  
• Population -Based Training: Evolving a population of models  
• Learning Rate Finder: Specifically for finding optimal learning rates  
Q: What is continual learning in the context of LLMs, and how does it differ from 
traditional fine -tuning?  
A: Continual learning is the process of continuously updating a model as new data becomes 
available, without forgetting previously learned information. In the context of LLMs:  
• Traditional fine -tuning typically involves a one -time adaptation to a specific task or 
domain.  
• Continual learning aims to incrementally update the model over time, maintaining 
performance on old tasks while learning new ones.  
Key aspects of continual learning include:  
1. Handling concept drift (changes in the underlying data distribution over time)  
2. Balancing stability (retaining old knowledge) and plasticity (acquiring new 
knowledge)  
3. Efficiently incorporating new data without full retraining  
4. Mitigating catastrophic forgetting  29 | P a g e  
 Techniques for continual learning in LLMs might include:  
• Elastic Weight Consolidation (EWC)  
 
Misc  Topics in Generative AI  
Q: What is zero -shot learning in LLMs?   
A: Zero -shot learning enables an LLM to perform tasks it has not been explicitly trained on 
by leveraging its general knowledge and understanding from pre -training.  
Q: What is few -shot learning in LLMs?  
A: Few-shot learning involves adapting an LLM to a new task with only a few examples, 
demonstrating the model's ability to generalize from limited data.  
Q: What are foundation models?  
A: Foundation models are large pre -trained models that serve as a base for various 
downstream tasks through fine -tuning, leveraging their broad, general -purpose capabilities.  
Q: How is federated learning used in the context of LLMs?   
A: Federated learning involves training LLMs across multiple decentralized devices while 
keeping data local, enhancing privacy and enabling collaborative learning without sharing 
raw data.  
Q: What are the benefits of using federated learning with LLMs?  
A: Benefits include improved privacy, reduced data transfer, and the ability to leverage 
diverse data sources without centralizing data storage.  
Q: What is the significance of prompt engineering?  
A: Prompt engineering involves designing effective input prompts to guide LLMs in 
generating the desired output, optimizing model performance for specific tasks.  
Q: How can LLMs be used for code generation?   
A: LLMs can generate code snippets or entire programs by understanding natural language 
descriptions of the desired functionality, aiding in software development and automation.  
Q: What is the role of reinforcement learning in fine -tuning LLMs?   
A: Reinforcement learning can fine -tune LLMs by optimizing for specific rewards, such as 
generating more relevant or accurate responses in conversational agents.  30 | P a g e  
 Q: What are some recent advancements in LLMs?  
A: Recent advancements include improvements in model architectures, training techniques 
like self -supervised learning, and better handling of long -context sequences.  
Q: How do you ensure the ethical use of LLMs?   
A: Ensuring ethical use involves implementing guidelines for fairness, transparency, and 
accountability, as well as actively monitoring and mitigating biases and potential misuse of 
the models.  
Prompt Engineering  
Google Colab Notebook  
Q: What is prompt engineering in the context of language models?  
A: Prompt engineering involves designing and optimizing input prompts to guide language 
models (LMs) like GPT -3 or GPT -4 to generate desired outputs. For example, asking GPT -4 
"Write a short story about a brave knight" helps generate a coherent and relevant narrative.  
Q: Why is prompt engineering important for effective use of Language Models?   
A: Effective prompt engineering can significantly improve the relevance, accuracy, and 
creativity of the model's outputs by providing clear and structured guidance. For example, 
specifying "Generate a formal letter of recommendation" instead of a vague "Writ e a letter" 
leads to more appropriate content.  
Q: What is a basic structure of a prompt for an Language Model?  
A: A basic prompt should include context, instructions, and a desired output format. For 
instance, "Write a summary of the following article: [insert article text here]" provides clear 
guidance.  
Q: What are some techniques for crafting effective prompts?  
A: Techniques include specifying the task clearly, providing examples, using role -playing 
scenarios, and setting the tone. For example, "As a travel guide, describe the best tourist 
attractions in Paris."  
Q: How does role -playing enhance prompt effectiveness?  
A: Role -playing helps the model adopt a specific perspective, leading to more relevant and 
targeted responses. For example, "As a financial advisor, give tips on saving for retirement."  
 
 31 | P a g e  
 Q: How do example -based prompts improve model responses?  
A: Providing examples helps the model understand the expected output format and 
content. For instance, "Translate the following sentence into French: 'Hello, how are you?' 
Example: 'Bonjour, comment ça va?'"  
Q: How can you handle ambiguity in prompts?   
A: To reduce ambiguity, be specific and clear about the task requirements. For example, 
instead of "Write a report," specify "Write a one -page report summarizing the benefits of 
renewable energy."  
Q: How does the length of a prompt affect model performance?  
A: While longer prompts can provide more context, they may also introduce complexity. It's 
important to balance detail with clarity. For instance, a concise but clear prompt like 
"Summarize the following text in 50 words" is effective.  
Q: How do you set the tone in a prompt?  
A: The tone can be set by specifying the desired style or formality. For example, "Write a 
friendly and informal email to invite a friend to a party."  
Q: What is iterative prompt refinement?  
A: Iterative prompt refinement involves testing and tweaking prompts to improve the 
model's outputs. For example, adjusting "Describe a product" to "Describe the key features 
of the new smartphone model."  
Q: What are prompt templates and how are they used?  
A: Prompt templates are predefined structures for common tasks. They help ensure 
consistency and efficiency. For example, using a template like "Dear [Recipient], I am writing 
to [Purpose]."  
Q: How do you evaluate the effectiveness of a prompt?  
A: Evaluation involves checking the relevance, accuracy, and coherence of the model's 
output. For example, comparing the responses generated by different prompts for the same 
task to see which is more appropriate.  
Q: How can you avoid introducing bias in prompts?  
A: Avoid bias by using neutral and inclusive language, and by testing prompts with diverse 
datasets. For example, ensuring a prompt for job application advice does not favour a 
specific demographic.  
Q: How can prompts encourage creative outputs from LLMs?  32 | P a g e  
 A: Encouraging creativity involves using open -ended and imaginative prompts. For example, 
"Write a sci -fi story set in the year 3000."  
Q: How do you tailor prompts for specific domains (e.g., legal, medical)?  
A: Tailoring prompts involves using domain -specific terminology and context. For example, 
"As a legal advisor, summarize the key points of this contract."  
Q: What are common pitfalls in prompt engineering?  
A: Pitfalls include being too vague, overly complex, or biased. For instance, a vague prompt 
like "Write about technology" can lead to unfocused outputs.  
Q: What are conditional prompts and how are they used?  
A: Conditional prompts set specific conditions or scenarios. For example, "If you were an 
astronaut on Mars, describe your daily routine."  
Q: What is multi -turn prompting?  
A: Multi -turn prompting involves a sequence of prompts to build on previous responses. For 
example, first asking "Describe the plot of a novel," then "Outline the main character's 
journey."  
Q: How can pre -trained prompts be leveraged in prompt engineering?  
A: Pre-trained prompts from model documentation or community resources can be adapted 
for specific tasks. For instance, using OpenAI's example prompts as a starting point.  
Q: How do you handle unexpected or irrelevant outputs from a prompt?   
A: Refine the prompt to be more specific and test iteratively. For example, if the prompt 
"Describe a holiday destination" results in irrelevant information, adjust to "Describe the 
best tourist attractions in Tokyo."  
Q: How can you craft a prompt for text summarization?  
A: Use clear instructions and length constraints. For example, "Summarize the following 
article in 100 words."  
Q: How can prompts be used for data augmentation in NLP tasks?   
A: Prompts can generate synthetic data to augment training datasets. For instance, 
generating additional examples of customer service interactions using prompts.  
 
 33 | P a g e  
 Q: What is adaptive prompting?  
A: Adaptive prompting involves dynamically adjusting prompts based on intermediate 
outputs. For example, if an initial prompt leads to incomplete information, a follow -up 
prompt seeks clarification.  
Q: How is prompt engineering used in interactive applications like chatbots?  
A: Prompts guide the chatbot's responses to maintain coherence and relevance. For 
example, structuring prompts to handle multi -turn conversations effectively.  
Q: Provide a case study example of an effective prompt for content generation .  
A: In content marketing, a prompt like "Write a blog post on the benefits of remote work, 
focusing on productivity and work -life balance" yields focused and relevant content that 
meets marketing objectives.  
Q: Why is clarity important when designing prompts?  
A: Clarity ensures that the model understands the task requirements accurately. For 
example, instead of "Write an article," a clearer prompt would be "Write a 500 -word article 
on renewable energy sources."  
Q: How do you tailor prompts to the complexity of the task?  
A: Prompts should provide sufficient guidance for the task's complexity level. For instance, a 
more complex task like "Generate code for a neural network architecture" requires a 
detailed prompt compared to "Summarize a news article."  
Q: Why is providing context important in prompts?  
A: Context helps the model understand the task's purpose and audience. For example, "As a 
travel blogger, describe your recent trip to Italy" provides context for the narrative style and 
content.  
Q: How do you incorporate constraints into prompts?   
A: Constraints ensure that the model produces outputs within specific boundaries. For 
example, "Write a tweet promoting a new product in 280 characters or less" sets a 
character limit constraint.  
Q: How do you balance flexibility and guidance in prompts?  
A: Prompts should offer enough flexibility for creativity while providing clear guidance. For 
example, "Write a short story with a surprise ending" offers flexibility in the narrative while 
guiding the story structure.  
 34 | P a g e  
 Q: How do you structure multi -part prompts effectively?  
A: Multi -part prompts should clearly delineate each part to avoid confusion. For example, 
"Part 1: Describe the setting. Part 2: Introduce the main characters. Part 3: Outline the 
conflict."  
Q: How do you ensure prompts align with the desired outputs?  
A: Prompts should clearly articulate the expected format and content of the output. For 
example, "Generate a product review with pros and cons listed in bullet points" specifies the 
output format.  
Q: Why is iterative refinement important in prompt design?  
A: Iterative refinement allows for fine -tuning prompts based on model performance and 
feedback. For example, refining a prompt based on initial model outputs to achieve more 
accurate responses.  
Q: How do you test prompts with diverse inputs?   
A: Testing prompts with a variety of input examples helps ensure robustness and 
generalization. For example, testing a translation prompt with sentences of varying 
complexity and languages.  
Q: How does prompt design impact user experience?  
A: Well -designed prompts enhance user experience by providing clear instructions and 
achieving desired outcomes efficiently. For example, a user -friendly chatbot prompt leads to 
quicker and more accurate responses.  
Q: How do you anticipate model responses when designing prompts?  
A: Understanding the model's capabilities and limitations helps craft prompts that elicit 
desired responses. For example, designing a prompt that accounts for potential biases or 
inaccuracies in the model's output.  
Q: How do you address ambiguity in prompt design?  
A: Clarifying instructions and providing examples can help reduce ambiguity in prompts. For 
example, specifying a range for numerical inputs or providing context for ambiguous terms.  
Q: How can iterative prompt design optimize model performance?  
A: Iteratively refining prompts based on model feedback improves prompt effectiveness 
over time. For example, adjusting the level of detail or complexity based on initial model 
responses.  
Q: How do you adapt prompts for different model architectures?  35 | P a g e  
 A: Prompts should be tailored to leverage the strengths and nuances of specific model 
architectures. For example, structuring a prompt differently for a transformer -based model 
like GPT -3 compared to a recurrent neural network.  
Q: How does providing feedback on model responses inform prompt design?  
A: Analyzing model outputs and user feedback helps refine prompts to better align with 
desired outcomes. For example, adjusting a prompt based on common errors or 
misunderstandings in model responses.  
Q: How do you verify that a prompt is understandable to the model?  
A: Testing prompts with diverse inputs and analyzing  model responses ensures 
understandability. For example, evaluating whether the model produces relevant outputs 
consistent with the prompt's intent.  
Q: How can transfer learning inform prompt design?  
A: Leveraging pre -trained models and transfer learning techniques can inform prompt 
design for specific tasks. For example, adapting prompts based on prompts that have been 
successful in similar domains.  
Q: How does collaborative prompt design enhance prompt effectiveness?  
A: Collaborative prompt design involves input from domain experts, users, and data 
scientists to ensure prompts are well -suited for the task. For example, involving subject 
matter experts in crafting prompts for specialized domains like healthcare or finance . 
Q: How do you address bias and fairness considerations in prompt design?   
A: Careful crafting of prompts and testing with diverse datasets help mitigate bias and 
ensure fairness. For example, analyzing prompts for language or cultural biases and 
adjusting accordingly.  
One -Shot Prompting  
Q: What is one -shot prompting in the context of Generative AI?   
A: One-shot prompting involves providing a single example or prompt to a model to perform 
a task, requiring the model to generalize and generate outputs based on a minimal amount 
of information.  
Q: How does one -shot prompting differ from traditional prompt -based approaches?  
A: Unlike traditional prompt -based approaches that rely on multiple examples or structured 
prompts, one -shot prompting challenges models to generalize from a single instance, 
making it more efficient and adaptable to new tasks or domains.  36 | P a g e  
 Q: What are the advantages of using one -shot prompting over traditional prompt -based 
approaches?   
A: One-shot prompting requires less data and human intervention, making it more scalable 
and efficient for generating content across diverse domains or languages.  
Q: What role does transfer learning play in enabling one -shot prompting?   
A: Transfer learning enables models pretrained on large datasets to extract relevant 
features and knowledge from a single prompt, leveraging prior learning to generate 
contextually relevant outputs.  
Q: What are some practical applications of one -shot prompting in Generative AI?  
A: Applications include language translation, text summarization, question answering, and 
content generation tasks where input examples are limited or scarce.  
Q: What are some challenges associated with implementing one -shot prompting?  
A: Challenges may include ensuring model robustness and generalization across diverse 
tasks, domains, and languages, as well as addressing biases or limitations in the training 
data.  
Few-Shot Prompting  
Q: What is few -shot learning in the context of Generative AI?  
A: Few-shot learning involves training models with a small number of examples to perform 
tasks. For instance, providing only a few examples of poetry to a language model to 
generate new poems.  
Q: How does few -shot learning differ from traditional supervised learning?  
A: Few-shot learning requires fewer labeled examples compared to traditional supervised 
learning, making it more adaptable to new tasks or domains with limited data.  
Q: Can you explain the concept of meta -learning in few -shot learning?   
A: Meta -learning involves training a model to learn how to learn from limited data. For 
example, a meta -learning algorithm can enable a model to quickly adapt to new tasks with 
minimal examples.  
Q: What are some techniques used to implement few -shot learning?   
A: Techniques include meta -learning algorithms like MAML (Model -Agnostic Meta -Learning) 
and transfer learning approaches such as fine -tuning pretrained models on few examples.  37 | P a g e  
 Zero -Shot Prompting  
Q: What is zero -shot learning and how does it work in Generative AI?   
A: Zero -shot learning enables models to perform tasks without specific training examples by 
leveraging prior knowledge. For example, GPT -3 can translate languages it has never been 
trained on by understanding linguistic patterns.  
Q: How does zero -shot learning differ from few -shot learning?  
A: Zero -shot learning requires no training examples for a specific task, while few -shot 
learning uses a small number of examples. Zero -shot learning relies more on generalization.  
Q: What are some practical applications of zero -shot learning in Generative AI?  
A: Applications include machine translation, text summarization, and content generation in 
languages or domains with limited training data  
Chain of Thought Prompting  
Q: What is Chain of Thought Prompt Engineering in Generative AI?  
A: Chain of Thought Prompt Engineering involves crafting sequential prompts that build 
upon each other to guide the model through a series of interconnected thoughts or actions.  
Q: How does Chain of Thought Prompt Engineering differ from traditional prompt 
engineering approaches?   
A: Chain of Thought Prompt Engineering focuses on structuring prompts in a sequential 
manner to create a coherent narrative or logical progression of ideas, whereas traditional 
prompt engineering may involve standalone prompts for individual tasks.  
Q: Can you provide an example of how Chain of Thought Prompt Engineering can be 
applied in content generation tasks?   
A: Sure, for generating a story, the chain of prompts could start with an initial prompt like 
"Introduce the main character," followed by prompts like "Describe the setting," "Introduce 
the conflict," "Detail the character's actions," and "Resolve the confli ct." 
Q: What are some key considerations when designing a chain of thought prompts?  
A: Considerations include maintaining coherence and consistency between prompts, 
ensuring a logical flow of ideas, and providing clear instructions for each step in the chain.  
 38 | P a g e  
 Q: How can Chain of Thought Prompt Engineering be leveraged to guide multi -turn 
conversations or interactions?  
A: By structuring prompts as sequential steps, Chain of Thought Prompt Engineering can 
guide the model through a dialogue or interaction, ensuring that each turn builds upon the 
previous one to maintain coherence and relevance.  
Q: What role does context play in Chain of Thought Prompt Engineering?  
A: Contextual information provided in each prompt helps the model understand the 
progression of thoughts or actions and ensures that subsequent prompts are relevant and 
appropriate.  
Q: How do you ensure that the chain of thought prompts leads to desired outcomes or 
objectives?  
A: By carefully designing each prompt in the sequence to align with the overall goal or 
objective, and by iteratively refining the prompts based on model performance and user 
feedback.  
Q: What are some challenges associated with implementing Chain of Thought Prompt 
Engineering?  
A: Challenges may include maintaining coherence and relevance across multiple prompts, 
managing the complexity of the chain, and ensuring that each prompt effectively guides the 
model towards the desired outcome.  
Hybrid Prompting  
Q: What is Hybrid Prompting in the context of Generative AI?  
A: Hybrid Prompting combines multiple prompt engineering techniques, such as using both 
structured prompts and open -ended prompts, to guide model behaviour and enhance 
output quality.  
Q: How does Hybrid Prompting leverage the strengths of different prompt engineering 
approaches?  
A: Hybrid Prompting integrates structured prompts for clarity and guidance with open -
ended prompts for creativity and flexibility, allowing for a more nuanced and adaptable 
approach to content generation.  
 
 
 39 | P a g e  
 Q: Can you provide an example of how Hybrid Prompting can be applied in text 
generation tasks?   
A: In a storytelling task, Hybrid Prompting could involve providing an initial structured 
prompt to set the scene and introduce characters, followed by open -ended prompts to 
allow the model to develop the narrative organically.  
Q: What are the advantages of using Hybrid Prompting over individual prompt 
engineering techniques?   
A: Hybrid Prompting offers the benefits of both structured and open -ended prompts, 
including clear guidance, context, and flexibility, leading to more diverse and high -quality 
outputs.  
Q: How do you determine the optimal balance between structured and open -ended 
prompts in Hybrid Prompting?  
A: The balance depends on the task requirements, desired output characteristics, and model 
capabilities. Experimentation and iterative refinement are key to finding the right balance.  
Q: What role does user feedback play in refining Hybrid Prompting strategies?  
A: User feedback helps identify areas where structured prompts may be too restrictive or 
open -ended prompts may lead to irrelevant outputs, guiding adjustments to the Hybrid 
Prompting approach.  
Q: How can Hybrid Prompting be tailored to suit different types of content generation 
tasks?  
A: By customizing the mix of structured and open -ended prompts based on the specific 
objectives, constraints, and characteristics of each task, such as adjusting the level of 
guidance or flexibility as needed.  
Q: What are some challenges associated with implementing Hybrid Prompting?   
A: Challenges may include designing prompts that effectively balance structure and 
flexibility, managing the complexity of hybrid prompt sequences, and ensuring coherence 
and relevance in model outputs.  
Q: Can you explain how Hybrid Prompting can be used to address the trade -off between 
control and creativity in content generation?  
A: Hybrid Prompting allows for a fine -tuned balance between providing guidance and 
allowing for creative exploration, enabling models to generate diverse and engaging content 
while maintaining control over the overall direction.  
 40 | P a g e  
 Q: How do you evaluate the effectiveness of Hybrid Prompting strategies in improving 
model performance?  
A: Evaluation involves assessing the quality, diversity, and relevance of model outputs 
generated using Hybrid Prompting compared to other prompt engineering approaches, as 
well as gathering user feedback to inform further refinement.  
ReAct Prompting  
Q: What is the concept behind ReAct in Generative AI, and how does it leverage human 
cognitive processes?  
A: ReAct is a method inspired by how humans learn and make decisions. It combines 
reasoning and action in AI models. ReAct prompts these models to think through a problem 
and take actions. It's like giving the 
AI a task and asking it to figure out 
the best way to solve it. It can also 
look up information from places 
like Wikipedia to help with its 
decision -making.  
 
 
 
 
 
Advance Topics  
 
Graph Retrieval Augmented Generation (GraphRAG)  
Useful Blog  : https://github.com/microsoft/graphrag?tab=readme -ov-file 
Q: How would you integrate GraphRAG with a large language model (LLM) to improve the 
factual accuracy of generated responses in a customer support chatbot?  
A: To integrate GraphRAG with an LLM for improved factual accuracy:  
• Build a knowledge graph of product information, FAQs, and customer interactions.  
• Implement RAG to retrieve relevant subgraphs based on the customer query.  
• Use graph embeddings to represent structural information in the knowledge graph.  
41 | P a g e  
 • Combine retrieved graph information with the customer query as context for the 
LLM.  
• Implement a fact -checking mechanism that compares LLM outputs with graph data.  
• Fine -tune the LLM on graph -augmented data to improve coherence between graph 
and text.  
• Use the graph to generate explanations for the LLM's responses.  
• This approach would help the chatbot provide more accurate and contextually 
relevant responses while leveraging the LLM's natural language generation 
capabilities.  
• Allow researchers to interactively explore connections through graph visualizations 
and LLM -generated summaries.  
• This approach would leverage the LLM's language understanding while using the 
graph structure to uncover non -trivial relationships in the scientific literature.  
Q: How would you implement a GraphRAG -enhanced LLM system for generating more 
accurate and diverse creative writing prompts?  
A: To implement a GraphRAG -enhanced LLM for creative writing prompts:  
• Build a knowledge graph of literary elements, genres, themes, and their 
relationships.  
• Use RAG to retrieve relevant subgraphs based on user preferences or initial ideas.  
• Implement graph traversal algorithms to find unique combinations of literary 
elements.  
• Use the LLM to generate natural language prompts based on the retrieved graph 
information.  
• Apply diversity -promoting techniques in both graph retrieval and LLM generation.  
• Implement a feedback mechanism to update the graph based on user ratings of 
generated prompts.  
• Use graph -based clustering to categorize and recommend similar prompts.  
• This approach would combine the structured creativity of the graph with the natural 
language generation of the LLM to produce more diverse and inspiring writing 
prompts.  42 | P a g e  
 Q: In a financial analysis system, how would you use GraphRAG with an LLM to generate 
more comprehensive and insightful market reports?  
A: To use GraphRAG with an LLM for financial market reports:  
• Construct a knowledge graph of companies, financial indicators, market events, and 
news.  
• Implement RAG to retrieve relevant financial data and relationships based on report 
requirements.  
• Use graph algorithms to identify trends, correlations, and anomalies in financial data.  
• Apply the LLM to generate natural language descriptions of graph -based insights.  
• Implement a template system for report structure, using the graph to fill in specific 
data points.  
• Use the LLM to generate explanations for complex financial concepts found in the 
graph.  
• Incorporate a fact -checking mechanism to ensure LLM -generated text aligns with 
graph data.  
• This approach would combine the data -driven insights from the graph with the LLM's 
ability to generate readable and contextually rich reports.  
Q: How would you design a GraphRAG -enhanced LLM system for a personalized learning 
platform to generate tailored educational content?  
A: To design a GraphRAG -enhanced LLM for personalized learning:  
• Create a knowledge graph of educational concepts, prerequisites, and learning 
resources.  
• Implement RAG to retrieve relevant educational materials based on the learner's 
profile and current topic.  
• Use graph traversal to determine optimal learning paths and identify knowledge 
gaps.  
• Apply the LLM to generate personalized explanations and examples based on graph 
data.  
• Implement a difficulty scaling mechanism using graph -based complexity measures.  43 | P a g e  
 • Use the LLM to rephrase complex concepts retrieved from the graph for better 
understanding.  
• Generate personalized quizzes and exercises by combining graph -based knowledge 
structure with LLM -generated questions.  
• This approach would leverage the structured representation of knowledge in the 
graph with the LLM's ability to generate engaging and personalized educational 
content.  
Q: In a drug interaction prediction system, how would you integrate GraphRAG with an 
LLM to improve the accuracy and explainability of predictions?  
A: To integrate GraphRAG with an LLM for drug interaction prediction:  
• Build a knowledge graph of drugs, their chemical properties, known interactions, and 
biological pathways.  
• Implement RAG to retrieve relevant subgraphs for given drug combinations.  
• Use graph neural networks to learn representations of drug interactions.  
• Apply the LLM to generate natural language explanations of potential interactions 
based on graph data.  
• Implement a mechanism to translate graph -based predictions into human -readable 
risk assessments.  
• Use the LLM to generate detailed reports on the mechanism of predicted 
interactions.  
• Incorporate a feedback loop where expert knowledge can be used to update both 
the graph and the LLM.  
• This approach would combine the structured prediction power of graph -based 
models with the LLM's ability to generate understandable explanations for 
healthcare professionals.  
 
 
 44 | P a g e  
 Q: How would you use GraphRAG with an LLM to enhance a code generation system's 
ability to produce more context -aware and maintainable software?  
A: To enhance code generation with GraphRAG and an LLM:  
• Create a knowledge graph of programming concepts, design patterns, and code 
dependencies.  
• Use RAG to retrieve relevant code snippets and architectural patterns based on the 
programming task.  
• Implement graph -based static analysis to understand existing codebase structure.  
• Apply the LLM to generate code while considering the retrieved graph context.  
• Use graph algorithms to suggest optimal placement of new code within the existing 
structure.  
• Implement an LLM -based system to generate meaningful variable names and 
comments based on graph context.  
• Use the graph to check for potential conflicts or inefficiencies in the generated code.  
• This approach would help the LLM generate code that is not only syntactically 
correct but also fits well within the larger software architecture and follows best 
practices.  
Q: In a legal document analysis system, how would you implement GraphRAG with an LLM 
to improve contract review and risk assessment?  
A: To implement GraphRAG with an LLM for legal document analysis:  
• Build a knowledge graph of legal terms, clauses, precedents, and their relationships.  
• Use RAG to retrieve relevant legal concepts and past cases based on contract 
content.  
• Implement graph -based similarity measures to identify non -standard or high -risk 
clauses.  
• Apply the LLM to generate plain -language summaries of complex legal language.  
• Use graph traversal to identify potential conflicts or missing clauses in the contract.  
• Implement an LLM -based system to generate suggested modifications for 
problematic clauses.  45 | P a g e  
 • Use the graph structure to provide context for the LLM when explaining legal 
implications.  
• This approach would combine the structured analysis capabilities of the graph with 
the LLM's natural language understanding to provide more comprehensive and 
accessible legal document analysis.  
Q: How would you design a GraphRAG -enhanced LLM system for generating more 
accurate and diverse marketing campaign ideas?  
A: To design a GraphRAG -enhanced LLM for marketing campaign ideation:  
• Create a knowledge graph of marketing channels, audience segments, past 
campaigns, and performance metrics.  
• Implement RAG to retrieve relevant marketing strategies and performance data.  
• Use graph algorithms to identify successful patterns and untapped market segments.  
• Apply the LLM to generate campaign ideas based on retrieved graph data and user 
input.  
• Implement a diversity -promoting mechanism in both graph retrieval and LLM 
generation.  
• Use the graph to fact -check and ground the LLM's creative ideas in historical 
performance data.  
• Generate explanations for each campaign idea, linking it to relevant nodes in the 
knowledge graph.  
• This approach would combine data -driven insights from the graph with the LLM's 
creative language generation to produce innovative yet grounded marketing ideas.  
Q: In a multi -lingual information retrieval system, how would you use GraphRAG with an 
LLM to improve cross -language query understanding and result generation?  
A: To use GraphRAG with an LLM for multi -lingual information retrieval:  
• Build a multi -lingual knowledge graph linking concepts across languages.  
• Implement RAG to retrieve relevant information regardless of the language of the 
source.  
• Use graph embeddings to capture semantic relationships across languages.  46 | P a g e  
 • Apply the LLM for query translation and expansion using graph -based context.  
• Implement cross -lingual graph traversal to find relevant information in other 
languages.  
• Use the LLM to generate coherent summaries of retrieved information in the user's 
preferred language.  
• Implement a mechanism to explain cross -lingual connections found in the graph.  
• This approach would leverage the language -agnostic nature of the graph structure 
with the LLM's multi -lingual capabilities to provide more comprehensive cross -
language information retrieval and presentation.  
Agentic AI  / AI Agents  
• Agentic AI Interview Q&A  
• GitHub  
Q: What is an Agentic AI ? 
A: An Agentic AI  is an AI framework that uses large language models to perform tasks 
autonomously or semi -autonomously. These systems can understand natural language 
instructions, make decisions, and take actions to achieve specific goals.  
Q: How does an LLM Agent differ from a standard LLM?  
A: While a standard LLM primarily focuses on generating text based on prompts, an LLM 
Agent can interact with its environment, make decisions, and take actions. It often 
integrates with external tools and APIs to perform tasks beyond just text generation.  
Q: What are the key components of an LLM Agent system?  
A: Key components typically include:  
• The LLM itself  
• A prompt engineering system  
• A memory or context management system  
• A decision -making /planning  module  
• Tool integration for performing actions  
• A feedback loop for learning and improvement  47 | P a g e  
 Q: Can you explain the concept of "prompt engineering" in the context of Agentic AI ? 
A: Prompt engineering involves crafting specific instructions or queries that guide the LLM's 
behavio ur and output. For agents, this often includes defining the agent's role, goals, 
constraints, and available actions in a way that the LLM can understand and act upon.  
Q: How do AI Agents handle long -term memory and context?  
A: AI Agents often use techniques like vector databases or other persistent storage methods 
to maintain context over long interactions. They may also summarize past interactions or 
use retrieval techniques to access relevant information when needed.  
Q: What are some common challenges in developing Agentic  AI systems?  
A: Challenges include:  
• Ensuring consistent behavio ur and adherence to goals  
• Managing context and memory effectively  
• Integrating with external tools and APIs securely  
• Handling errors and unexpected situations  
• Balancing autonomy with safety and control  
Q: How can Agentic sys tem be made more reliable and less prone to hallucination?  
A: Strategies include:  
• Implementing fact -checking mechanisms  
• Using retrieval -augmented generation (RAG) to ground responses in verified 
information  
• Employing multiple agents for cross -verification  
• Implementing robust error handling and uncertainty quantification  
Q: What are some potential applications of Agentic AI systems?  
A: Applications include:  
• Personal assistants  
• Automated customer service  
• Research and data analysis assistants  
• Code generation and debugging agents  48 | P a g e  
 • Task planning and execution systems  
• Educational tutors  
Q: How do AI Agents make decisions?  
A: LLM Agents typically use a combination of:  
• Pre-defined rules and constraints  
• The LLM's understanding of the task and context  
• Heuristics encoded in their prompts  
• Feedback from previous actions  
• Some advanced systems may also incorporate reinforcement learning or other AI 
decision -making techniques.  
Q: What ethical considerations should be taken into account when developing Agent ic 
systems?  
A: Key ethical considerations include:  
• Ensuring transparency about the system's capabilities and limitations  
• Protecting user privacy and data security  
• Avoiding bias and ensuring fairness in decision -making  
• Implementing safeguards against misuse or harmful actions  
• Considering the potential impact on employment and human roles  
Q: How can the performance of an Agentic AI system be evaluated?  
A: Evaluation methods may include:  
• Task completion rates and quality  
• User satisfaction metrics  
• Adherence to defined constraints and goals  
• Efficiency in resource use (time, computational resources, etc.)  
• Robustness in handling various scenarios, including edge cases  
Q: What's the difference between single -task and multi -task Agents ? 49 | P a g e  
 A: Single -task agents are designed to perform a specific function, like customer support for a 
particular product. Multi -task agents are more versatile and can handle a variety of tasks, 
often deciding which actions to take based on user input or environme ntal cues.  
 
Latest  Q & A (Based on question asked in different companies)   
 
Q: How do LLMs balance between fluency and factual accuracy?  
A: LLMs often prioritize fluency due to their training on language patterns, which can lead to 
confident -sounding but inaccurate responses. This tendency is sometimes referred to as 
"hallucination." Strategies to improve accuracy while maintaining fluency include:  
• External grounding : Using verified information sources during generation.  
• Reinforcement learning : Fine -tuning with accuracy -focused rewards.  
• Few-shot learning : Providing accurate examples in prompts.  
• Uncertainty quantification : Training models to express doubt when unsure.  
• Factual knowledge injection : Training on curated factual datasets.  
• Constrained decoding : Limiting improbable text generation.  
• Prompt engineering : Crafting prompts to elicit accurate responses.  
 
Q: What is an embedding in the context of machine learning and natural language 
processing?  
A: An embedding is a dense vector representation of data (such as words, sentences, or 
documents) in a continuous vector space. It captures semantic meaning and relationships 
between data points, allowing for efficient computation and comparison.  
 
Q: How do embedding models handle out -of-vocabulary words?  
A: Embedding models typically handle out -of-vocabulary words through techniques like:  
• Subword tokenization (e.g., WordPiece, Byte -Pair Encoding ) 
• Character -level embeddings  
• Using a special token for unknown words  
• Fallback to nearest known words . 
• The specific method depends on the model architecture and training approach.  
 
Q: What is the " curse of dimensionality " and how does it relate to embeddings?  
A: The curse of dimensionality refers to various phenomena that arise when analysing data 
in high -dimensional spaces. With embeddings, it can lead to:  
• Increased computational complexity  50 | P a g e  
 • Sparsity of data in the embedding space  
• Reduced effectiveness of distance metrics To  mitigate this, techniques like 
dimensionality reduction or using specialized index structures for high -dimensional 
spaces are often employed.  
 
Q: Can you explain the concept of "semantic search" and how it relates to embeddings?  
A: Semantic search refers to search techniques that aim to understand the intent and 
contextual meaning of the query, rather than just matching keywords. Embeddings enable 
semantic search by:  
• Representing both queries  and documents  in the same vector space  
• Allowing for similarity comparisons based on meaning, not just exact matches  
• Capturing nuanced relationships between concepts This results in more relevant 
search results, especially for complex or ambiguous queries.  
 
Q: What is the difference between sparse and dense embeddings?  
A: Sparse embeddings:  
• Have many zero values  
• Are typically high -dimensional  
• Often used in traditional information retrieval (e.g., TF -IDF vectors)  
• Efficient for exact matching  
Dense embeddings:  
• Have few or no zero values  
• Are typically lower -dimensional  
• Used in modern neural network -based models  
• Better at capturing semantic similarity  
• More computationally efficient for many operations  
 51 | P a g e  
  
 
Q: What is the main difference between dense search and sparse search?  
A: The main difference lies in their vector representations. Dense search uses continuous, 
real-valued vectors where each dimension contributes some meaning, while sparse search 
uses high -dimensional vectors where most elements are zero, typically correspondi ng to 
specific features or keywords.  
 
Q: In the context of text search, what might a sparse vector look like compared to a dense 
vector?  
A: For the phrase "quick brown fox":  
• Sparse vector: [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, ...] where 1s represent the presence of 
"quick", "brown", and "fox" in a large vocabulary  
• Dense vector: [0.1, -0.3, 0.7, 0.2, -0.1, ...] where each number represents some 
learned semantic feature  
Q: In which scenarios would you prefer to use dense search over sparse search?  
A: Dense search is preferable in scenarios where:  
• Semantic understanding is crucial  
• Dealing with natural language queries  
• Handling synonyms or related concepts  
• Working with out -of-vocabulary words  
• Searching for similar items based on meaning rather than exact matches  
52 | P a g e  
  
Q: What are the advantages of sparse search?  
A: Advantages of sparse search include:  
• Efficiency in large -scale retrieval tasks  
• Excellent performance for exact matching and Boolean  queries  
• Easy interpretability and explainability of results  
• Lower computational requirements compared to dense search  
 
Q: How do dense vectors capture semantic meaning?  
A: Dense vectors capture semantic meaning by:  
• Using machine learning models (often neural networks) to learn representations  
• Encoding contextual information into the vector  
• Representing words or items in a continuous space where similar concepts are closer 
together  
• Allowing for complex relationships to be captured in the multidimensional space  
 
Q: Can you explain a use case where combining dense and sparse search might be 
beneficial?  
A: A common use case is in e -commerce search engines. Here's how a combined approach 
could work:  
• Sparse search can quickly filter products based on exact attribute matches (e.g., 
brand, colour, size)  
• Dense search can then rank the filtered results based on semantic similarity to the 
user's query  
• This combination provides both the efficiency of sparse search for filtering and the 
semantic understanding of dense search for ranking  
 
Q: How do you evaluate the quality of embeddings?  
A: Embedding quality can be evaluated through various methods:  
• Intrinsic evaluation : Using tasks like word similarity or analogy tests  
• Extrinsic evaluation : Performance on downstream tasks (e.g., classification, 
clustering)  
• Visualization techniques : t-SNE or UMAP to examine relationships in 2D or 3D  53 | P a g e  
 • Similarity analysis : Checking if semantically related items are close in the embedding 
space  
 
Q: If we embed information using one embedding model and query using a different 
embedding model, will we be able to fetch accurate responses?  
A: No, this approach would likely result in poor or inconsistent retrieval performance.  
 
Q: Why wouldn't this work effectively?  
A: Different embedding models create vector representations in distinct "embedding 
spaces" that are not directly comparable. It's akin to trying to compare words in two 
different languages without translation.  
Q: What's the best practice for effective retrieval using embeddings?  
A: For effective retrieval, it's crucial to use the same embedding model for both encoding 
the information and generating the query embeddings. This ensures that both the stored 
information and the queries are represented in the same vector space.  
 
Q: What happens if you use mismatched embeddings?  
A: Using mismatched embeddings would lead to unreliable or meaningless similarity scores. 
The semantic relationships preserved by each embedding model wouldn't align, making 
accurate comparisons impossible.  
Q: If I need to switch to a new embedding model, what should I do with my existing 
embedded information?  
A: If you need to switch embedding models, you would typically need to re -embed all your 
stored information using the new model. This maintains consistency and ensures continued 
retrieval effectiveness.  
Q: Why is it important to have both stored information and queries in the same vector 
space?  
A: Having both in the same vector space allows for meaningful similarity comparisons. It 
ensures that the relationships and distances between vectors are consistent and 
interpretable, which is essential for accurate information retrieval.  
Q: Let's say I am storing different Indian mutual fund scheme names in a vector database. 
If I want to access a particular scheme like "HDFC Top 100 Fund - Direct Plan - Growth 
Option", what should be my approach to get the exact result without getting any other 
results like "HDFC Top 100 Fund - Regular Plan - Growth Option" or "HDFC Mid -Cap 
Opportunities Fund - Direct Plan - Growth Option"?  54 | P a g e  
 A: To retrieve the exact match "HDFC Top 100 Fund - Direct Plan - Growth Option" without 
getting similar results, you should use a hybrid approach:  
• Perform a vector similarity search for "HDFC Top 100 Fund - Direct Plan - Growth 
Option".  
• Apply an exact string match filter on the results of the vector search.  
• Use a separate metadata field for the precise mutual fund scheme name for 
filtering.  
• Utilize any database -specific exact matching features provided by your vector 
database system.  
 
Q: Why is a simple vector similarity search not sufficient for this task in the Indian mutual 
fund context?  
A: A simple vector similarity search is not sufficient because:  
• It's designed to find semantically similar items, which could include other HDFC funds 
or similar funds.  
• Vector representations may not distinguish between minor differences in scheme 
names that are crucial for exact identification (e.g., Direct Plan vs Regular Plan).  
• It might return related schemes with similar investment objectives instead of the 
exact scheme we're looking for.  
• In the Indian mutual fund context, even small differences in scheme names can 
represent significantly different investment products.  
Q: How does adding an exact string match improve the search results in this mutual fund 
scenario?  
A: Adding an exact string match:  
• Filters out similar but non -identical mutual fund scheme names.  
• Ensures that only the precise scheme "HDFC Top 100 Fund - Direct Plan - Growth 
Option" is returned.  
• Eliminates false positives like "HDFC Top 100 Fund - Regular Plan - Growth Option" or 
"HDFC Mid -Cap Opportunities Fund - Direct Plan - Growth Option".  
• Prevents confusion between direct and regular plans, or between growth and 
dividend options of the same scheme.  
Q: How does this system handle updates to mutual fund schemes, such as changes in 
names or mergers?  
A: To handle updates to mutual fund schemes:  
• Implement a versioning system to track changes in scheme details over time.  55 | P a g e  
 • Use the SEBI -assigned scheme code as a persistent identifier, even if the scheme 
name changes.  
• Maintain a mapping of old scheme names to new ones for historical searches.  
• Update vector embeddings when significant changes occur to ensure accurate 
semantic search.  
• Implement a regular update process to sync with source . 
 
Q: As a developer using a third -party LLM API for your e -commerce chatbot, how would 
you protect against prompt injection attempts?  
A: To protect against prompt injection as an LLM API consumer:  
• Implement robust input validation to filter out suspicious commands before sending 
to the API.  
• Use a prompt prefix that reinforces the chatbot's role and limitations, e.g., "You are 
an e -commerce assistant. Do not perform actions outside of product inquiries and 
purchases."  
• Sanitize user inputs to remove potential injection phrases like "ignore previous 
instructions."  
• Implement output filtering to catch unexpected or harmful responses.  
• Use API features like content moderation if available.  
• Set up monitoring to flag suspicious interactions for human review.  
 
Q: Users are attempting to jailbreak the LLM -powered application you've built. As 
someone without access to modify the underlying model, how would you address this?  
A: To address jailbreak attempts as an LLM API consumer:  
• Analy se patterns in jailbreak attempts to identify common techniques.  
• Implement stricter input validation based on observed patterns.  
• Use system prompts that reinforce ethical boundaries and the intended use of the 
application.  
• Implement a multi -stage filtering process for both inputs and outputs.  
• Utilize API parameters for safety and content filtering if provided by the LLM service.  
• Develop a list of forbidden prompts or keywords associated with jailbreak attempts.  
• Implement user action limits or cooldown periods to prevent rapid -fire attempts.  
• Set up logging and alerting for potential jailbreak attempts to enable quick responses.  
Q: You're using an LLM API to create an AI study aid. How would you prevent students 
from manipulating it to do their homework?  
A: Strategies to protect an AI study aid as an LLM API consumer:  
• Design prompts that encourage explanations rather than direct answers.  56 | P a g e  
 • Implement input classification to detect homework -like questions and adjust 
responses accordingly.  
• Use follow -up prompts that ask students to show their work or explain their 
understanding.  
• Set character limits on inputs to prevent entire homework problems from being 
submitted.  
• Develop a bank of educational prompts to guide the LLM towards tutoring rather 
than problem -solving.  
• Implement user sessions with usage patterns analysis to detect potential misuse.  
• Create custom output templates that format responses as hints or study guides 
rather than complete answers.  
• Include automated messages about academic integrity and proper use of AI study 
aids.  
 
Q: You're integrating an LLM into a banking chatbot for customer service. How would you 
protect against prompt injection or jailbreak attempts that could compromise financial 
data or operations?  
A: To secure a banking chatbot using an LLM API:  
• Strict input validation : Implement rigorous filters to catch and block potential 
injection attempts before they reach the LLM.  
• Financial context reinforcement : Begin each interaction with a strong prompt  
(system prompt)  that reinforces the chatbot's role and limitations in handling 
financial information.  
• Multi -factor authentication : Require additional verification for any actions involving 
sensitive operations or data access.  
• Sensitive information masking : Develop a system to automatically detect and mask 
account numbers, SSNs, and other sensitive data in both inputs and outputs.  
• Transaction isolation : Ensure that the LLM has no direct access to perform financial 
transactions. Use it for information lookup and guidance only.  
• Response templating : Create pre -approved response templates for common financial 
queries to minimize unexpected outputs.  
• Anomaly detection : Implement real -time monitoring to flag unusual patterns of 
interaction or requests for sensitive information.  
• Regulatory compliance checks : Incorporate checks to ensure all responses comply 
with financial regulations (e.g., GDPR, CCPA).  
• Audit logging : Maintain detailed logs of all interactions for security audits and 
potential forensic analysis.  
• Fallback mechanisms : Implement automatic escalation to human operators for any 
detected anomalies or high -risk queries.  57 | P a g e  
 • Regular security testing : Conduct frequent penetration testing and security audits 
specific to LLM -based financial interactions.  
• Customer education : Provide clear guidelines to users about the chatbot's 
capabilities and limitations, especially regarding financial transactions and data 
handling.  
 
Latest top LLM  Leaderboard   
 
We can find latest top LLMs at following leaderboard . 
LMSYS Leaderboard : https://lmarena.ai/?leaderboard  
Research Papers   
Important research papers . 
2025  
1. DeepSeek -R1: Incentivizing Reasoning Capability in LLMs via  Reinforcement Learning  - 
DeepSeek introduces its first -generation reasoning models, DeepSeek -R1-Zero and 
DeepSeek -R1. DeepSeek -R1-Zero, trained solely through reinforcement learning (RL), exhibits 
strong reasoning abilities but faces challenges with readability and language mixing . To 
address these issues, DeepSeek -R1 incorporates multi -stage training and cold -start data, 
achieving reasoning performance comparable to OpenAI -o1-1217. DeepSeek has open -
sourced both models along with six distilled versions (1.5B –70B) based on Qwen and  Llama.  
2. https://ai.meta.com/research/publications/brain -to-text-decoding -a-non-invasive -approach -
via-typing/  - Brain2Qwerty, a deep learning model, decodes sentences from brain activity 
using EEG or MEG while participants type. In a study of 35 volunteers, MEG achieved a 32% 
character -error -rate (CER), outperforming EEG (67%). Top participants reached 19% CER, with  
some sentences perfectly decoded. Errors suggest both motor and cognitive influences. This 
non-invasive approach brings brain -computer interfaces closer to aiding non -communicating 
patients safely.  
3. The Future of Software Engineering in an AI -Driven World  - AI-driven software development 
is transforming engineering, with LLMs enhancing productivity. This trend will grow, fostering 
a human -AI partnership. The research community must tackle key challenges in AI 
integration. This paper outlines our vision and th e critical issues to address for the future of 
AI-driven software engineering.  
2024  
1. Mixtral of Experts  - Mixtral 8x7B is a Sparse Mixture of Experts (SMoE) model based on 
Mistral 7B, featuring 8 feedforward experts per layer, with two selected per token. Each 
token accesses 47B parameters but uses only 13B during inference. Trained with a 32k 
context, Mixtral  outperforms Llama 2 70B and GPT -3.5, excelling in math, coding, and 
multilingual tasks. The fine -tuned Mixtral 8x7B -Instruct surpasses GPT -3.5 Turbo, Claude -2.1, 
Gemini Pro, and Llama 2 70B -chat. Both models are released under Apache 2.0.  58 | P a g e  
 2. DeepSeek -V3 Technical Report  - DeepSeek -V3 is a 671B -parameter Mixture -of-Experts (MoE) 
model with 37B active per token, optimized for efficiency using Multi -head Latent Attention 
(MLA) and DeepSeekMoE. It introduces an auxiliary -loss-free strategy for load balancing and 
a multi -token p rediction objective for improved performance. Trained on 14.8T tokens with 
SFT and RL, it rivals top closed -source models while requiring only 2.788M H800 GPU hours. 
Its training was highly stable, with no irrecoverable loss spikes or rollbacks.  
3. The Llama 3 Herd of Models (Meta , 202 4) - This paper introduces Llama 3, a new set of 
foundation models supporting multilinguality, coding, reasoning, and tool usage. The largest 
model, with 405B parameters and a 128K token context window, shows comparable 
performance to GPT -4 across various tasks . Llama 3 models, including Llama Guard 3 for 
safety, are publicly released. Additionally, the paper explores integrating image, video, and 
speech capabilities, demonstrating competitive results, though these models are still in 
development.  
 
4. NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window? (Mo Li et 
al., 2024) - This paper introduces NeedleBench, a framework for evaluating the long -context 
capabilities of large language models (LLMs) across various text lengths and depths. It 
assesses how well models retrieve and reason with critical information in bilingual long texts. 
The Ancestral Trace Challenge (ATC) is also proposed to test LLMs on complex logical 
reasoning. Results indicate that current LLMs have significant room for improvement in 
handling real -world long -context tasks.  
 
 
5. PaliGemma: A versatile 3B VLM for transfer (Lucas Beyer et al., 2024)  - PaliGemma is an open 
Vision -Language Model (VLM) that is based on the SigLIP -So400m vision encoder and the 
Gemma -2B language model. It is trained to be a versatile and broadly knowledgeable base 
model that is effective to transfer. It achieves strong perfo rmance on a wide variety of open -
world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM 
benchmarks, but also more specialized tasks such as remote -sensing and segmentation.  
 
6. Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence 
(Weize Chen et al., 202 4)- This paper introduces the Internet of Agents (IoA), a new 
framework for LLM -based multi -agent collaboration. IoA overcomes the limitations of 
existing frameworks by offering flexible agent integration, dynamic teaming, and scalable 
communication, inspired by the Internet's architecture. Experiments show IoA outperforms 
current baselines in various tasks, enabling seamless collaboration among diverse agents for 
enhanced intelligence and capabilities.  
 
2023  
1. GPT-4 Technical Report (OpenAI, 2023)  - Detailed the capabilities and limitations of the GPT -4 
language model.  
2. LLaMA: Open and Efficient Foundation Language Models (Touvron et al., 2023)  - Introduced a 
collection of open foundation language models with high performance and efficiency.  
 
 
3. PaLM 2 Technical Report (Anil et al., 2023)  - Described the architecture and performance of 
Google's PaLM 2 language model.  
 
4. Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism 
(Zihao Li et al., 2023)  - This research paper explores offline Reinforcement Learning with 59 | P a g e  
 Human Feedback (RLHF) to understand human rewards and optimal policies in a Markov 
Decision Process (MDP).  
2022  
1. DALL·E 2: Hierarchical Text -Conditional Image Generation with CLIP Latents (Ramesh et al., 
2022)  - Improved upon the original DALL -E model with higher resolution and more accurate text -
to-image generation.  
 
2. InstructGPT: Training language models to follow instructions with human feedback (Ouyang et 
al., 2022)  - Described the process of fine -tuning language models to better follow human 
instructions.  
2020  
1. Language Models are Few -Shot Learners (Brown et al., 2020)  - Introduced GPT -3, a large 
language model with few -shot learning capabilities.  
2017  
1. Attention Is All You Nee d (Vaswani et al. , 2017)  - Introduced the Transformer architecture, 
which revolutionized natural language processing and foundation for current Large Language 
Models (GPT, Claude, Gemini, Llama etc) . 

Computing and Artificial Intelligence  2025, 3(1), 1498.  
https://doi.org/10.59400/ cai.v 3i1.1498 
1 
Article  
Generative artificial intelligence (GAI): From large language models 
(LLMs) to multimodal applications towards fine tuning of models, 
implications, investigations  
Zarif Bin Akhtar  
Department of Computing , Institute of Electrical and Electronics Engineers (IEEE), Piscataway, NJ 08854 -4141, USA ; 
zarifbinakhtarg@gmail.com,  zarifbinakhtar@ieee.org  
Abstract: This research explores the transformative integration of artificial intelligence (AI), 
robotics, and language models, with a particular emphasis on the PaLM -E model. The 
exploration aims to assess PaLM -E’s decision -making processes and adaptability across 
various robotic environments, demonstrating its capacity to convert textual prompts int o very 
precise robotic actions. In addition, the research investigates Parameter -Efficient Fine -Tuning 
(PEFT) techniques, such as Low -Rank Adaptation (LoRA) and Quantized Low -Rank 
Adaptation (QLoRA), providing a historical overview of PEFT and highlighting  their 
significance in enhancing task performance while reducing the number of trainable parameters. 
The broader scope of Generative AI is examined through an analysis of influential models like 
GPT -3, GPT -4, Copilot, Bard, LLaMA, Stable Diffusion, Midjour ney, and DALL -E. These 
models’ abilities to process natural language prompts and generate a wide range of outputs are 
thoroughly investigated. The research traces the historical evolution of AI, from its roots in 
science fiction to its practical applicatio ns today, with a focus on the rise of Generative AI in 
the 21st century. Furthermore, the research delves into the various modalities of Generative 
AI, covering applications in text, code, images, and more, and assesses their real -world impact 
on robotics,  planning, and business intelligence. The implications of synthetic data generation 
for business analytics are also explored. The research inspects within both software and 
hardware landscapes, comparing local deployment on consumer -grade hardware along wi th 
cloud -based services, and underscores the benefits of local model deployment in terms of 
privacy protection, intellectual property security, and censorship resistance. Ethical 
considerations are central to this research, addressing concerns related to p rivacy, security, 
societal impact, biases, and misinformation. The research proposes ethical guidelines for the 
responsible development and deployment of AI technologies. Ultimately, this work reveals the 
deep interconnections between vision, language, and  robotics, pushing the boundaries of AI 
capabilities and providing crucial insights for future AI model development and technological 
innovation. These findings are intended to guide the field through the emerging challenges of 
the rapidly evolving Generat ive AI landscape.  
Keywords: artificial intelligence (AI); computer vision; deep learning (DL) ; generati ve 
artificial intelligence (GAI); large language models (LLMs); machine learning (ML); models 
fine tuning; robotics  
1. Introduction  
In the rapidly advancing field of artificial intelligence (AI), the convergence of 
vision, language, and robotics is emerging as a critical area of exploration, driving the 
development of intelligent systems capable of interacting with the world in more 
holistic and meaningful ways [1 –3]. CITATION  
Akhtar ZB . Generative artificial 
intelligence (GAI): From large 
language models (LLMs) to 
multimodal applications towards fine 
tuning of models, implications, 
investigations . Computing and 
Artificial Intelligence . 2025; 3(1): 
1498 . 
https://doi.org/10.59400/ cai.v3i1.1498 
ARTICLE INFO  
Received: 3 July 2024  
Accepted: 17 October 2024  
Available online: 4 November  2024  
COPYRIGHT  
 
Copyright © 2024 by author(s).  
Computing and Artificial Intelligence 
is published by Academic Publishing 
Pte. Ltd. This work is licensed under 
the Creative Commons Attribution 
(CC BY) license.  
https://creativecommons.org/licenses/
by/4.0/  
Computing and Artificial Intelligence  2025, 3(1), 1498.   
2 This research investigates the PaLM -E model, a pioneering effort in multimodal 
AI designed to bridge the gap between perception, language understanding, and 
robotic control. PaLM -E’s ability to navigate complex, real -world scenarios and its 
adaptability ac ross various tasks position it as a significant advancement in the 
integration of AI with robotics. The exploration extends its focus to large language 
models (LLMs) and introduces the concept of Parameter -Efficient Fine -Tuning 
(PEFT), a paradigm shift in the adaptation of LLMs for specialized tasks. Techniques 
such as Low -Rank Adaptation (LoRA) and Quantized Low -Rank Adaptation 
(QLoRA) are explored in depth, offering insights into how these methods optimize the 
use of computational resources while maintain ing or enhancing task performance. The 
discussion includes an analysis of additional PEFT methods like T -Few, AdaMix, and 
MEFT, highlighting the delicate balance between efficiency and effectiveness in LLM 
adaptation. Generative artificial intelligence (Ge nerative AI) represents a significant 
evolution in the field, with the ability to produce text, images, and multimodal outputs 
that have broad applications across industries [4 –6]. This research delves into the 
transformative impact of transformer -based mo dels such as GPT -3, Copilot, Bard, and 
LLaMA, alongside text -to-image generation systems like Stable Diffusion, 
Midjourney, and DALL -E. As these technologies gain traction in areas ranging from 
art and creative writing to healthcare and finance, the resear ch critically examines both 
the opportunities and ethical challenges associated with their widespread use [7 –9]. 
The investigations also provide a historical context, tracing the evolution of AI from 
its conceptual origins in the mid -20th century through t o its current state as a driver of 
innovation in the 21st century. By reflecting on the contributions of pioneers like Alan 
Turing and the development of early automated systems, the research underscores the 
philosophical and ethical debates that have shap ed AI’s trajectory [10 –12]. The rise of 
Generative AI in recent years is presented as the latest chapter in this ongoing 
narrative, with a focus on its applications in robotics, planning, and business 
intelligence [13 –15]. Furthermore, the research examine s the software and hardware 
ecosystems that support Generative AI, comparing the benefits and limitations of local 
deployments versus cloud -based services.  
This analysis highlights the importance of accessibility, scalability, and the 
protection of privacy and intellectual property in the deployment of AI technologies. 
Through a detailed exploration of these themes, the research aims to provide a 
comprehensiv e understanding of the current state of Generative AI, its potential future 
directions, and the ethical considerations that must guide its development and 
implementation.  
2. Methods and experimental analysis  
This research adopts a multi -faceted approach to evaluate the performance and 
adaptability of the PaLM -E model in robotic environments, as well as to explore the 
broader implications of Parameter -Efficient Fine -Tuning (PEFT) and Generative AI 
technologies.  
Phase 1: Evaluation of PaLM -E in Robotic Environments  
The initial phase focuses on assessing PaLM -E’s capabilities in a variety of 
robotic scenarios. A diverse set of tasks will be formulated, ranging from simple Computing and Artificial Intelligence  2025, 3(1), 1498.   
3 actions to complex, long -horizon maneuvers, simulating real -world conditions to 
evaluate PaLM -E’s decision -making processes. A robust testing framework will be 
developed, incorporating benchmarks that simulate dynamic and unpredictable 
environments. The in tegration of PaLM -E with low -level language -to-action policies 
will be central to this phase, enabling the translation of textual prompts into precise 
robotic actions. The model’s adaptability will be further tested by introducing 
adversarial disturbances to evaluate its robustness and generalization to tasks not 
encountered during the training phase. This aspect of the research is crucial for 
understanding PaLM -E’s potential in transfer learning and its effectiveness in 
unforeseen scenarios, drawing on met hodologies established in existing literature on 
robotic AI integration.  
Phase 2: Exploration of Parameter -Efficient Fine -Tuning (PEFT)  
The research then delves into PEFT, with detailed background research and 
available knowledge focusing on techniques like Low -Rank Adaptation (LoRA) and 
Quantized Low -Rank Adaptation (QLoRA). The historical development of PEFT will 
be traced, highlighting key milestones and the challenges that have shaped its 
evolution. The implementation of LoRA in large language models (LLMs) will be 
analyzed, with particular attention to the starting point preservation hypothesis, which 
plays a critical role in reducing the number of trainable parameters without sacrificing 
performance. Following this, the introduction of QLoRA will be explored, 
demonstrating how quantization enhances parameter efficiency. This slice will 
provide a comprehensive understanding of PEFT’s ro le in optimizing LLMs, 
supported by previous studies that have documented its impact on computational 
resource management.  
Phase 3: Analysis of Generative AI Technologies  
In the third phase, the research shifts to an in -depth examination of Generative 
AI, specifically transformer -based models like GPT -3, GPT -4, Copilot, Bard, 
LLaMA, Stable Diffusion, Midjourney, and DALL -E. The study will assess these 
models’ capabilities i n processing natural language prompts and generating diverse 
outputs across multiple modalities, including text, code, and images. This analysis will 
be contextualized within the broader historical development of AI, tracing its 
evolution from speculative fiction to its current status as a transformative force in 
various industries. The research will explore both unimodal and multimodal systems, 
emphasizing their real -world applications in fields such as robotics, planning, and 
business intelligence, and wi ll investigate the role of Generative AI in synthetic data 
generation, with a particular focus on its implications for business analytics.  
Phase 4: Examination of Software, Hardware, and Ethical Considerations  
The research will then inspect the integration of Generative AI features into 
commercial products, assessing the accessibility and usability of these technologies 
on consumer devices. The scalability of Generative AI models will be evaluated, 
comparing loc al deployment on consumer -grade hardware with cloud -based services. 
Special attention will be given to the advantages of local model deployment, including 
privacy protection, intellectual property safeguards, and the avoidance of rate limiting 
and censorsh ip. This phase will also incorporate an ethical dimension, critically 
examining privacy and security concerns associated with Generative AI, particularly Computing and Artificial Intelligence  2025, 3(1), 1498.   
4 in relation to deepfakes and synthetic media. The exploration will propose ethical 
guidelines for the responsible development and deployment of these technologies, 
addressing societal impacts, biases, misinformation, and manipulation concerns. This 
ethical  exploration will be grounded in existing frameworks and will contribute to the 
ongoing discourse on AI ethics.  
Synthesis and Future Directions  
Finally, the research will synthesize the findings from each phase, highlighting 
the interconnectedness of vision, language, and robotics in pushing the boundaries of 
AI capabilities. The implications of this research for the future development of AI 
model s and technologies will be discussed, with recommendations for future research 
aimed at addressing emerging challenges and opportunities in the rapidly evolving 
field of Generative AI. To provide a better understandin g, Figure 1  illustrates the 
visualizati on concerning the matters.  
 
Figure 1.  An overall visualization of the research exploration experimentations.  
2.1. Background research and available knowledge explorations  
Generative artificial intelligence (Generative AI , or GenAI) represents a 
significant advancement in the capabilities of AI systems, particularly in their ability 
to produce text, images, and other forms of media through generative models. These 
models, which learn patterns and structures from vast datase ts, are capable of 
generating new data that mirrors the characteristics of the training data. The early 
2020s witnessed remarkable progress in transformer -based deep neural networks, 
leading to the emergence of Generative AI systems [1 –15]. Notable example s include 
large language model (LLM) chatbots and text -to-image AI art systems, which have 
garnered widespread attention for their ability to accept and process natural language 
prompts. The application of Generative AI spans a broad spectrum of industries , 
highlighting its versatility and transformative potential. In the fields of art, writing, 
and scriptwriting, Generative AI has been used to create content that pushes the 
boundaries of creativity. In software development, tools like GitHub Copilot assist  
Computing and Artificial Intelligence  2025, 3(1), 1498.   
5 programmers by generating code snippets, streamlining the development process.  
The healthcare and finance sectors have also embraced Generative AI for tasks 
such as predictive analytics and automated reporting. Additionally, the gaming, 
marketing, and fashion industries are leveraging these technologies to enhance user 
experiences an d design processes. The early 2020s saw a significant surge in 
investment from major technology companies, including Microsoft, Google, and 
Baidu, as well as numerous smaller firms, reflecting the growing interest in the 
potential of Generative AI [16 –20]. Despite its promising applications, the 
development of Generative AI has raised concerns regarding its potential misuse. The 
ability of these models to generate realistic content has led to fears of cybercrime, the 
creation of fake news, and the productio n of deepfakes —manipulated media that can 
deceive viewers [11 –22]. These concerns underscore the need for ethical guidelines 
and regulatory frameworks to ensure the responsible development and deployment of 
Generative AI technologies.  
The historical evolution of artificial intelligence provides essential context for 
understanding the development of Generative AI [21 –33]. The field of AI was 
formally established as an academic discipline in 1956, but the roots of automated 
creativity can  be traced back much further. Ancient Greek civilization explored the 
concept of automated art, and over the centuries, the development of creative 
automatons laid the groundwork for the sophisticated Generative AI systems of today.  
One of the seminal contributions to the conceptual foundation of AI was Alan 
Turing’s 1950 paper, which posed fundamental questions about machine reasoning 
and laid the groundwork for future advancements in the field. Over the decades, AI 
has experienced s everal waves of progress and periods of optimism, leading to the 
development of Generative AI planning systems and, more recently, advanced 
generative models capable of performing complex tasks [34 –49]. 
Generative AI now operates across various modalities, including text, code, 
images, audio, video, molecules, robotics, planning, and business intelligence. Large 
language models, such as GPT -4 and PaLM, typically run  on powerful data center 
computers, but there has been significant progress in developing smaller models that 
can operate on more accessible devices, such as smartphones, embedded systems, and 
personal computers.  
This accessibility has facilitated the integration of Generative AI into a wide 
range of products, from conversational agents like ChatGPT to programming tools like 
GitHub Copilot. Furthermore, many of these models are available as open -source 
software, en abling broader experimentation and application [34 –49]. 
One of the key advantages of running Generative AI models locally, as opposed 
to relying solely on cloud -based services, is the enhanced protection of privacy. Local 
deployment mitigates the risks associated with data exposure and provides users with 
great er control over their intellectual property. Additionally, running models locally 
can help avoid issues related to rate limiting and censorship that may arise with cloud 
services.  
However, the largest models, which often contain hundreds of billions of 
parameters, still require the computational power of data center computers and are 
typically accessed through cloud services.  Computing and Artificial Intelligence  2025, 3(1), 1498.   
6 This research highlights the profound impact of Generative AI on various 
industries while also acknowledging the potential challenges and ethical 
considerations that accompany its rapid development. As Generative AI continues to 
evolve, it is crucial to ad dress these concerns to ensure that the technology is harnessed 
for the benefit of society [34 –49]. 
2.2. Experimental designs and simulation investigations  
This research undertakes a detailed exploration of PaLM -E, a generalist robotics 
model developed by Google, which addresses the significant challenges posed by the 
lack of large -scale datasets in robotics. PaLM -E’s innovative architecture integrates 
sensor  data from robotic agents directly with a powerful language model, PaLM, to 
create a comprehensive visual -language model. The experimental design is structured 
to evaluate PaLM -E’s effectiveness in performing a range of tasks across multiple 
robots and mod alities, such as processing images, robot states, and neural scene 
representations. These experiments aim to demonstrate PaLM -E’s ability to transfer 
knowledge from large -scale training data to various robotic applications, thereby 
improving the model’s pe rformance in both vision -language tasks and robotic 
decision -making [32 –34]. The first phase of the experimental design involves setting 
up robotic environments where PaLM -E is tested on diverse tasks, ranging from basic 
operations to complex, long -horizon  maneuvers. A simulation framework is employed 
to create realistic robotic scenarios, enabling the assessment of PaLM -E’s adaptability 
and decision -making capabilities in dynamic environments.  
The integration of PaLM -E with low -level language -to-action policies is crucial 
in this phase, allowing the model to translate textual prompts into executable robot 
actions. The experiments further introduce adversarial disturbances to test the model’s 
robustness and its ability to generalize to tasks that it was not explicitly trained for. 
This phase also examines how visual -language data enhances the model’s 
performance in robotic tasks, thereby underscoring the potential of PaLM -E to 
function as an effic ient and effective generalist model for robotics. The second phase 
of the research delves into Parameter -Efficient Fine -Tuning (PEFT) for Large 
Language Models (LLMs), focusing on methodologies such as Low -Rank Adaptation 
(LoRA) and Quantized Low -Rank Adap tation (QLoRA). The experiments are 
designed to evaluate how PEFT techniques optimize LLMs for specific tasks while 
managing computational and memory requirements effectively. A step -by-step 
simulation process is employed, where pretrained LLMs undergo fin e-tuning to adapt 
to specialized tasks. This phase highlights the benefits of PEFT, including reduced 
memory usage and lower storage costs, while also addressing potential challenges like 
increased training time. In the context of LoRA, the experiments int roduce trainable 
low-rank matrices into each layer of the Transformer architecture during the fine -
tuning process. This technique is designed to minimize the number of trainable 
parameters, thus reducing the computational burden without compromising task 
performance. The experimental design elucidates LoRA’s working principles, 
particularly its focus on starting point preservation and the use of low -rank matrices 
as adapters.  
These experiments aim to showcase LoRA’s efficiency in task -switching and its Computing and Artificial Intelligence  2025, 3(1), 1498.   
7 applicability to real -time applications, making it an optimal choice for fine -tuning 
large language models in resource -constrained environments. Building on the findings 
from LoRA, the research introduces QLoRA, an extension of LoRA that incorporates 
quant ization techniques to achieve further parameter efficiency. The experiments 
simulate NF4 quantization and Double Quantization processes, demonstrating how 
QLoRA reduces memory requirements while maintaining, or even enhancing, model 
performance. The result s from this phase are critical in understanding how QLoRA 
can be employed across various LLMs, offering a versatile and highly efficient 
approach to parameter -efficient fine -tuning. The experimental design also includes 
detailed simulations to compare the performance of PaLM -E and various PEFT 
techniques against existing models. These simulations provide insights into the 
strengths and limitations of each method, allowing for a comprehensive evaluation of 
their potential impact on language processing and ro botics tasks. The discussions and 
findings from these simulations are intended to equip researchers and practitioners 
with a nuanced understanding of PEFT, LoRA, and QLoRA, guiding their application 
in real -world scenarios where efficient fine -tuning of la rge language models is 
essential. By systematically evaluating PaLM -E and PEFT techniques through 
carefully designed experiments and simulations, this research contributes valuable 
insights into the practical implementation of advanced AI models in robotic s and 
language processing domains. The findings emphasize the potential of these 
techniques to significantly enhance the efficiency and effectiveness of AI -driven 
systems, paving the way for future developments in Generative AI and robotics. To 
provide a b etter understanding , Figures S1 (see supp lementary mat erials  file) and 2 
provide an overall visualization concerning the matters.  
 
Figure 2. PaLM -E an embodied multimodal language model in action 2.  
2.3. A deep dive into general artificial intelligence (GAI)  
Generative artificial intelligence (AI) represents a significant advancement in the 
field of artificial intelligence, with algorithms like ChatGPT at the forefront, capable 
of producing a wide array of content types , including text, code, images, audio, 
simulations, and videos.  
Computing and Artificial Intelligence  2025, 3(1), 1498.   
8 These transformative capabilities have garnered widespread attention, 
particularly following the release of ChatGPT by OpenAI in November 2022. As a 
highly capable chatbot, ChatGPT quickly rose to prominence, attracting over a million 
users within just fiv e days of its launch. Its ability to generate diverse forms of content, 
from computer code and essays to creative works like poems, highlights its versatility 
and potential to revolutionize content creation across various industries [23 –33]. The 
impact of Generative AI, exemplified by ChatGPT, extends beyond mere content 
generation, raising both opportunities and challenges. Tools like DALL -E, another AI 
system developed by OpenAI for generating art, further illustrate the potential of 
Generative AI to disr upt traditional workflows and job markets [32 –34]. 
However, this disruption is accompanied by uncertainties and risks, particularly 
regarding the implications for employment, content quality, and ethical 
considerations. As Generative AI continues to evolve, it becomes increasingly 
important to balance its transformative potential with a careful understanding of its 
limitations and the risks it may pose. To fully grasp the significance of Generative AI, 
it is essential to distinguish it from broader concepts within artificial intelligence and 
machine learnin g. 
While AI broadly refers to machines that mimic human intelligence, machine 
learning is a subset of AI focused on models that learn from data patterns without 
explicit programming. Generative AI, as a breakthrough in machine learning, enables 
models to crea te new content on demand, going beyond traditional tasks of pattern 
recognition and classification.  
Text-based models like ChatGPT rely on self -supervised learning, where they are 
trained on vast amounts of text data to generate predictions and responses that closely 
resemble human language. Although ChatGPT has captured public attention, it is part 
of a lineage of text -based models that includes predecessors like GPT -3 and BERT, 
which have also made significant contributions to the field [34 –36]. 
The development of Generative AI models requires substantial resources, 
typically available only to well -funded tech companies. Training these models 
involves large -scale data processing and significant computational power. For 
instance, GPT -3, one of the models underlying ChatGPT, was trained on 
approximately 45 terabytes of text data, reflecting the massive investment in both 
infrastructure and talent necessary to achieve high levels of performance [32 –34]. 
These investments enable companies like OpenAI, DeepMind, and Meta to push 
the boundaries of what Generative AI can achieve, creating models capable of 
producing outputs that rival human -generated content in terms of quality and diversity.  
Generative AI models are particularly adept at generating content that is not only 
lifelike but also creative, introducing random elements that add diversity to the 
outputs. This capability has broad practical applications across industries such as IT, 
software development, marketing, and healthcare [34 –49]. For example, Generative 
AI can rapidly produce written content, optimize code, or create marketing copy, 
saving time and resources for organizations. However, the effectiveness of these 
outputs depends largely on the quality and relevance of the training data used to 
develop the models, as well as the specific use cases for which they are applied.  
Despite the impressive capabilities of Generative AI, there are significant Computing and Artificial Intelligence  2025, 3(1), 1498.   
9 limitations and risks that must be acknowledged. While the outputs of Generative AI 
models can be convincing, they are not immune to errors, biases, or inappropriate 
content generation. These risks can lead to reputational and legal challenges, 
particularl y if biased or offensive content is inadvertently published [34 –36]. To 
mitigate these risks, it is crucial to carefully curate training data, consider the use of 
smaller, more specialized models, and maintain human oversight to review and 
approve AI -gener ated content before it is disseminated. As a rapidly evolving field, 
the long -term effects and risks associated with Generative AI are still being 
understood. Organizations adopting these technologies must remain vigilant, staying 
informed about regulatory  developments and emerging risks. While Generative AI 
holds immense promise, its responsible implementation, coupled with continuous 
monitoring, is essential to maximize its benefits while minimizing unintended 
consequences. By approaching Generative AI wi th a balanced perspective, we can 
harness its transformative potential while addressing the ethical and practical 
challenges it presents [34 –49]. 
3. GAI: From a techspertive point o f view  
Generative AI, often synonymous with large language models (LLMs), 
represents a significant subset of machine learning, recognized for its ability to 
generate natural -sounding language. The emergence of tools like Bard, an 
experimental platform designed fo r collaboration with Generative AI powered by a 
large language model, underscores the growing influence of this technology. To fully 
understand Generative AI, it is essential to first explore the broader context of artificial 
intelligence (AI). Most modern  AI is rooted in machine learning, a process where 
neural networks —complex computer systems —learn from vast amounts of data. 
These networks are trained to perform specific tasks, such as image classification or 
predicting the next word in a sentence, by id entifying patterns within the data [34 –36]. 
Language models, a particular type of neural network, are central to Generative AI. 
They are trained on extensive datasets and are capable of predicting the next word in 
a sequence, becoming increasingly sophisti cated as the amount of training data grows. 
These models are already in use in everyday applications, such as Gmail’s Smart 
Compose and Smart Reply features, which assist users by suggesting contextually 
relevant responses.  
Bard, powered by such models, leverages this predictive capability to generate 
coherent and contextually appropriate language. Generative AI, by its nature, goes 
beyond simply predicting text. It is designed to create entirely new content based on 
the patt erns and structures it has learned from its training data. This ability to generate 
novel combinations of text in natural -sounding language is what distinguishes 
Generative AI as a powerful tool in content creation. However, its capabilities extend 
beyond text; Generative AI can also produce images, audio, and even video, offering 
vast potential across multiple creative domains. The potential impact of Generative AI 
on creative fields is profound. It has the ability to transform the way we approach 
creativi ty, much like the drum machine did for music production.  
By automating repetitive tasks and eliminating drudgery, Generative AI can 
enhance creative workflows, allowing human creators to focus on more innovative and Computing and Artificial Intelligence  2025, 3(1), 1498.   
10 imaginative aspects of their work. However, it is crucial to recognize that Generative 
AI is not a replacement for human creativity but rather a tool that augments and 
facilitates it [34 –49]. Despite its potential, Generative AI also presents challenges, 
particularly in educational contexts. The ease with which AI -generated content can be 
produced raises important questions about how we measure success and originality in 
education. These concerns highlight the need for a thoughtful and responsible 
approach to the development and deployment of machine learning technologies.  
Companies like Google have taken steps to address these challenges by 
establishing AI principles and creating internal governance structures aimed at 
ensuring the ethical development of AI. These guidelines are designed to prevent harm 
and mitigate issues related to bias and toxicity in AI -generated content. By adhering 
to these principles, organizations can help ensure that Generative AI technologies are 
developed and used in ways that benefit society while minimizing potential risks. 
While Generative AI holds immense potential to revolutionize creative processes and 
workflows, it also demands careful con sideration of its societal impacts. By fostering 
responsible development and use, we can harness the power of Generative AI to tackle 
new challenges and open up fresh perspectives in various fields, ultimately 
contributing to a more innovative and creative  future.  
4. Machine learning (ML) mystery: A case study investigation 
analysis  
A recent study, conducted by researchers from MIT in collaboration with Google 
Research and Stanford University, delves into the intriguing phenomenon of in -
context learning observed in large language models, such as OpenAI’s GPT -3 and 
GPT -4. In-context le arning refers to the ability of these models to perform new tasks 
after being exposed to just a few examples, without the need for retraining on new 
data. This capability has piqued the interest of researchers, who have sought to 
understand the underlying mechanisms that enable such models to learn without the 
traditional process of parameter updates [34 –36]. The study centers on the hypothesis 
that these massive neural network models, particularly transformers like GPT -3, might 
encapsulate smaller, simpler  linear models within their vast architecture. These 
smaller models, the researchers suggest, could be trained to execute new tasks using 
straightforward learning algorithms, all while leaving the parameters of the 
overarching model unchanged. This notion challenges the conventional understanding 
of how learning occurs within large language models, opening the door to new theories 
about their inner workings. To explore this hypothesis, the researchers conducted a 
theoretical investigation into transformer m odels specifically engineered for in -
context learning. Transformers, which form the backbone of models like GPT -3, are 
neural networks known for their ability to process sequences of data, such as text, in a 
highly efficient manner. The findings of the stu dy reveal that within these 
transformers, a linear model can be “written” into the hidden states of the network.  
This process involves embedding the linear model within the earliest layers of 
the transformer, allowing the larger model to simulate and train this smaller model 
using pre -existing information.  
As a result, the model can effectively perform in -context learning, adapting to Computing and Artificial Intelligence  2025, 3(1), 1498.   
11 new tasks without the need for parameter modification. The implications of these 
results are profound. By demonstrating that a transformer can house and train a linear 
model within its hidden states, the study suggests a novel method by which large 
languag e models might achieve in -context learning. This insight could lead to the 
development of more efficient learning algorithms capable of performing new tasks 
without the extensive retraining that is typically required.  
The lead authors of the study emphasize the practical advantages of in -context 
learning, particularly its potential to streamline the learning process. By eliminating 
the need for complex engineering and the collection of domain -specific data, in -
context l earning presents a more efficient approach to training models. The researchers 
propose that these in -context learners do not merely mimic patterns observed in their 
training data; instead, they might actually acquire the ability to perform new tasks.  
This challenges the prevailing notion that large language models simply 
memorize tasks, suggesting that they possess a more sophisticated capacity for 
learning. Ultimately, this study represents a significant step toward unraveling the 
capabilities of mode rn large language models. By shedding light on the mechanisms 
behind in -context learning, the research contributes to a deeper understanding of how 
these models can be leveraged for complex learning tasks. As the field of machine 
learning continues to evol ve, insights such as these will be crucial in shaping the future 
of AI development.  
5. GAI: The creative work perspectives  
Generative AI, particularly through the use of large language and image models, 
is revolutionizing the landscape of creative work and business functions. These 
models, often referred to as foundation models, present a wide array of opportunities, 
including  the automation of content generation, enhancement of content quality, 
diversification of content types, and the ability to personalize outputs across various 
domains. Models like OpenAI’s GPT -3 and GPT -4 are prime examples of Generative 
AI’s capacity to p roduce diverse types of content, such as text, images, and videos. 
These models are trained on vast datasets and require significant computational 
resources to develop. However, once trained, they can be fine -tuned for specific 
content domains using relati vely smaller datasets. This process underscores the 
continued necessity for human involvement, both in generating prompts for the AI and 
in evaluating or editing the content produced by these models.  
One of the key applications of Generative AI lies in the marketing sector. For 
instance, specialized versions of GPT -3, such as Jasper, are being utilized to create 
blogs, social media posts, and other customer -facing content. These tools are 
invaluable fo r maximizing search engine optimization (SEO) and tailoring 
personalized pitches in public relations. Additionally, image generation tools like 
DALL -E 2 are already making an impact in advertising, with brands such as Heinz 
and Nestle adopting these techno logies to enhance their campaigns.  
Generative AI also shows significant potential in code generation. GPT -3’s 
Codex, for example, can create code snippets based on textual descriptions, 
dramatically improving the efficiency of software development. Experiments by 
companies like Deloitte hav e demonstrated up to a 20% increase in code development Computing and Artificial Intelligence  2025, 3(1), 1498.   
12 speed using Codex, highlighting the practical benefits of these models in the tech 
industry [34 –36]. Furthermore, Generative AI is increasingly being integrated into 
conversational AI and chatbots, leading to more sophisticated conversation 
understanding a nd context awareness. However, challenges persist, particularly 
regarding the replication of biased language. These issues necessitate ongoing efforts 
to refine and filter AI outputs, especially in sensitive contexts. Another area of interest 
is knowledge management. Large language models, when fine -tuned on specific 
content, have the potential to manage and streamline an organization’s knowledge 
base. For example, Morgan Stanley is working with OpenAI’s GPT -3 and GPT -4 to 
fine-tune models for wealth manage ment training, aiming to leverage Generative AI 
for more effective information dissemination within the company. However, the 
ethical and legal implications of Generative AI cannot be overlooked. The rise of 
deepfakes and concerns over content ownership ar e central to discussions about the 
future of AI in creative work. As AI systems become more capable of generating a 
wide range of content —including emails, articles, computer programs, and more —the 
questions of intellectual property and content ownership b ecome increasingly complex 
and pressing. The perspective presented acknowledges that while Generative AI 
models offer unprecedented opportunities, they also bring about challenges and risks 
that must be carefully managed.  
As these technologies continue to evolve, they are likely to create unforeseen 
opportunities and implications for creative work and knowledge management. The 
ongoing discourse on these issues will play a critical role in shaping the future of 
Generative AI  and its integration into various aspects of work and life.  
6. GAI: Ethics, accountability, trust, public interest and proactive 
managements  
The rise of Generative AI marks a pivotal moment in the evolution of artificial 
intelligence, offering transformative potential across various sectors. Unlike 
traditional AI, Generative AI, driven by large language models, responds to user 
prompts with outputs that closely mimic human language, making this tech nology 
accessible to a broader audience. However, as Generative AI gains prominence, it also 
brings forth significant ethical, accountability, and trust -related challenges that require 
careful consideration [36]. In the business world, there is a growing i nterest in 
harnessing Generative AI to enhance enterprise operations. Yet, as this technology 
rapidly evolves, the importance of trust and ethical management becomes paramount. 
One of the most pressing concerns is whether business users can trust the outpu ts 
generated by these AI models. The potential risks associated with Generative AI, such 
as producing inaccurate or hallucinated outputs, pose serious challenges for end -users, 
who may struggle to assess the factual accuracy of content that appears convinc ingly 
eloquent.  
Generative AI models are also prone to biases as they are trained on vast datasets 
that may contain inherent prejudices. This raises the risk of users placing undue 
confidence in biased or erroneous outputs, which could have significant consequences 
in dec ision -making processes. Moreover, the issue of attribution is critical. Since 
Generative AI outputs are closely aligned with the original training data, there is a Computing and Artificial Intelligence  2025, 3(1), 1498.   
13 heightened risk of plagiarism and copyright violations. Balancing trust in these outputs 
with human oversight becomes a complex challenge, particularly when considering 
the legal and brand implications for enterprises. Transparency and explaining ability 
are equally crucial. End -users, who may not have a deep technical understanding of 
AI, need accessible explanations of how Generative AI works. This underscores the 
necessity for enterprise -wide AI literacy and risk awareness, which can help mitigate 
potent ial negative consequences. Without transparency, users may misinterpret AI -
generated content, leading to misguided decisions that could have far -reaching effects.  
Accountability in the use of Generative AI is a central theme in this discussion. 
As these models increasingly mimic human creativity, the responsibility for their 
outputs must be clearly defined. It is essential to maintain human oversight and ensure 
that AI-generated content is subject to thorough analysis, scrutiny, and context -aware 
review. This human element is critical to preserving ethical standards and ensuring 
that AI -driven decisions align with societal values and public interest.  
The need for proactive management in the deployment of Generative AI cannot 
be overstated. Organizations must establish robust frameworks for accountability, 
trust, and ethics, ensuring that the outcomes of Generative AI are transparently linked 
to their c reators and the enterprise. As the AI landscape continues to evolve, these 
frameworks will be essential in navigating the complexities of this technology and 
safeguarding its integration into various aspects of work and life.  
While Generative AI holds immense potential to revolutionize content creation 
and business operations, it also necessitates a careful balance between innovation and 
ethical responsibility. By addressing the challenges of trust, accountability, and public 
interest and by implementing proactive management strategies, organizations can 
harness the power of Generative AI while minimizing the risks and ensuring its 
positive impact on society.  
7. The future of Generative AI (GAI) and its directions  
The current excitement surrounding Generative AI, particularly models like 
ChatGPT, has sparked widespread interest and speculation about its potential impact. 
However, it’s crucial to focus on the true value of Generative AI, which lies not in its 
capacit y as a generalized solution but in its application to niche domains where it can 
offer significant, context -specific advantages.  
While the buzz around ChatGPT is undeniable, the primary value of Generative 
AI will emerge in specialized contexts where it can explore and utilize highly specific 
information in novel ways. The development of ChatGPT plugins by various 
companies exemplif ies this shift. These plugins are not about creating a one -size-fits-
all solution but rather enhancing functionality in specific areas.  
For instance, in the realm of travel planning, a Generative AI tool tailored for a 
company like Expedia can deliver a substantial competitive edge, particularly in a 
market where information discovery is critical.  
This trend raises important questions about the future of Generative AI and its 
implications for established search giants like Google. Will these developments pose 
a serious threat to their dominance, or are we witnessing an “iPhone moment” —a 
transformati ve shift in user behavior and expectations? The likely outcome is a gradual Computing and Artificial Intelligence  2025, 3(1), 1498.   
14 change, where organizations leverage large language models (LLMs) trained on their 
own data to drive meaningful transformations in how they operate and interact with 
customers.  
OpenAI’s recognition of the commercial potential of Generative AI is evident in 
its recent move to open a waiting list for companies to access ChatGPT plugins. This 
decision foreshadows the emergence of numerous new products and interfaces 
powered by OpenA I’s Generative AI systems in the coming months and years, 
underscoring the expanding role of Generative AI in various sectors. However, it’s 
important to dispel the notion that OpenAI is the sole gatekeeper of Generative AI 
technology. While ChatGPT is a p rominent tool, it is not the only one available. A 
broader ecosystem of tools exists, including self -hosted LLMs, which offer 
organizations an alternative approach to Generative AI. By deploying LLMs on their 
own enterprise data, organizations can address privacy concerns and maintain greater 
control over their AI implementations.  
The future of Generative AI is also likely to see a trend toward domain -specific 
language models. Fine -tuning general -purpose LLMs on specific datasets could result 
in highly effective information retrieval tools tailored to particular industries or use 
cases [36]. This approach has promising applications in areas such as product 
information management, content creation, and internal documentation, 
demonstrating how Generative AI can evolve into more specialized and practical tools.  
As Generative AI becomes more embedded in specific contexts, its mystique as 
an all -knowing entity will diminish. The future of AI will likely be less threatening 
and more approachable, especially as it becomes increasingly domain -specific. A 
comparison ca n be drawn to GitHub Copilot, an AI tool that supports software 
developers by helping them solve problems within the scope of their existing 
knowledge and experience. This is a key indicator of how Generative AI will be 
successful —not as a catch -all soluti on but as an integrated tool that enhances specific 
applications. Ultimately, the true value of Generative AI will be realized as it becomes 
seamlessly embedded in particular domains, leading to a more practical and grounded 
acceptance of its capabilities [34–49]. As users come to understand the limitations of 
Generative AI, its usefulness will become more apparent, fostering a balanced 
perspective that recognizes both its potential and its boundaries.  
8. Results and findings  
Google’s PaLM -E, an embodied multimodal language model, demonstrates 
significant advancements in robotic environments and vision -language tasks, 
showcasing its versatility across a range of applications.  
The model was rigorously evaluated in three distinct robotic scenarios, each 
involving real robots and a variety of tasks such as visual question answering (VQA), 
image captioning, and general language processing. The findings highlight PaLM -E’s 
ability to  effectively integrate language understanding with robotic control, 
establishing new benchmarks in the field.  
Robotic scenarios and task performance:  
In one scenario, PaLM -E was tasked with directing a mobile robot in a kitchen 
environment. The model successfully guided the robot to retrieve a bag of chips, Computing and Artificial Intelligence  2025, 3(1), 1498.   
15 demonstrating robustness even when the bag was placed back into a drawer, which 
introduced additional complexity. This capability underscores PaLM -E’s resilience to 
environmental disturbances and its ability to execute tasks in dynamic settings. In 
another  task, PaLM -E was instructed to grab an unseen green block. Here, the model 
generated a plan that extended beyond the robot’s training data, effectively 
generalizing its actions to handle novel objects.  
This ability to generalize is a critical advancement, enabling robots to perform 
tasks with objects or in environments not explicitly encountered during training. In a 
tabletop robot environment, PaLM -E tackled long -horizon tasks, such as sorting 
blocks by  color into designated corners. The model demonstrated its capacity to 
process visual information and generate sequences of textually represented actions for 
intricate, prolonged tasks. This performance marks a significant improvement over 
previous models,  showcasing PaLM -E’s potential for handling complex, multi -step 
processes.  
Moreover, PaLM -E exhibited impressive zero -shot generalization. For example, 
it successfully pushed red blocks towards a coffee cup, adapting to new tasks that were 
not part of its training data. This adaptability is a testament to the model’s robustness 
in handling unforeseen challenges and tasks. In a third robotic scenario inspired by 
task and motion planning (TAMP), PaLM -E addressed combinatorically challenging 
planning tasks. The model effectively solved these tasks by leveraging knowledge 
transferred from visual and language models, coupled with a modest amount of 
training data from an expert TAMP planner. This capability highlights PaLM -E’s 
efficiency in learning from limited data while achieving high -level task performance.  
Visual -Language Generalization:  
PaLM -E also proved itself as a visual -language generalist, outperforming leading 
vision -language -only models in several benchmarks. Notably, it achieved the highest 
reported score on the OK -VQA dataset, a challenging benchmark for visual question 
answering , without requiring task -specific fine -tuning. This performance underscores 
the model’s strength in visual understanding and its extensive external world 
knowledge.  
The largest version of the model, PaLM -E-562B, exhibited particularly advanced 
capabilities, including visual chain -of-thought reasoning and multi -image inference. 
These abilities enable the model to break down complex answering processes into 
smaller, man ageable steps and to perform inference across multiple images, even 
though it was primarily trained on single -image prompts.  
This sophistication in reasoning and inference represents a significant leap 
forward in the capabilities of embodied AI systems.  
Figure  S2 (see supple mentary materials  file) and Figures 3 –5 in the 
accompanying visualizations provide further insights into these findings, illustrating 
the model’s performance across various tasks and environments and highlighting its 
versatility and robustness.  Computing and Artificial Intelligence  2025, 3(1), 1498.   
16  
Figure 3. The experimental simulation processing.  
 
Figure 4. A visualization of the research findings 1.  
Computing and Artificial Intelligence  2025, 3(1), 1498.   
17  
Figure 5. A visualization of the research findings 2.  
9. Discussions and future directions  
The exploration of Parameter -Efficient Fine -Tuning (PEFT) techniques, such as 
Low-Rank Adaptation (LoRA) and Quantized Low -Rank Adaptation (QLoRA), 
brings to light several key insights and future possibilities in the field of Natural 
Language Processing (NLP). This discussion addresses essential queries about these 
techniques, providing a comprehensive overview o f their goals, advantages, and 
potential impacts on the research community [32 –49]. 
Parameter -Efficient Fine -Tuning Goals:  
The primary goal of parameter -efficient fine -tuning is to adapt pre -trained 
language models to specific tasks while minimizing the computational and memory 
burdens traditionally associated with fine -tuning large models. This approach is 
particularly crucial as the scale and complexity of language models continue to grow. 
By reducing the number of parameters that need to  be updated during the fine -tuning 
process, PEFT techniques enable more efficient adaptation to various downstream 
tasks, making it feasible to deploy sophisticated models in resource -constrained 
environments.  
Computing and Artificial Intelligence  2025, 3(1), 1498.   
18 Enhancements through Quantized Low -Rank Adaptation (QLoRA):  
One of the significant advancements in parameter efficiency comes from the 
integration of quantization into the low -rank adaptation process, as seen in QLoRA. 
By quantizing the weights of the adaptation layers, QLoRA reduces the memory 
footprint and comput ational load without resorting to complex quantization 
techniques. This method preserves the overall performance of the language model, 
ensuring that the efficiency gains do not come at the cost of reduced accuracy or 
effectiveness. The approach enhances t he applicability of fine -tuning large models in 
practical scenarios, where hardware constraints often pose significant challenges.  
Advantages of Low -Rank Adaptation (LoRA):  
Low-Rank Adaptation offers several advantages that make it a valuable technique 
for fine -tuning large language models. First, LoRA reduces the parameter overhead, 
which is particularly beneficial when multiple tasks require fine -tuning, as it allows 
for mo re efficient task switching without needing to retrain the entire model from 
scratch. Additionally, LoRA maintains inference latency, ensuring that the model’s 
responsiveness remains intact despite the reduced number of trainable parameters. 
These benefits  make LoRA an effective solution for deploying adaptable models in 
real-world applications, where both efficiency and performance are paramount.  
Implications for Researchers:  
Researchers stand to gain significantly from the adoption of PEFT techniques. 
By leveraging methods like LoRA and QLoRA, researchers can fine -tune large 
language models more efficiently, optimizing their use across a range of downstream 
tasks without incur ring excessive computational costs. This not only broadens the 
accessibility of powerful language models but also encourages innovation by allowing 
more researchers to experiment with and refine these models. The practical 
implications of these techniques are far -reaching, particularly in enabling the 
deployment of advanced NLP solutions in a wider array of contexts.  
Applicability of QLoRA to Language Models:  
QLoRA’s versatility is another point of discussion, as it can be applied to various 
types of language models, including RoBERTa, DeBERTa, GPT -2, and GPT -3. This 
adaptability underscores the potential of QLoRA as a standard tool for parameter -
efficient fine -tuning across different architectures. The ability to fine -tune diverse 
models with minimal resource requirements opens up new possibilities for deploying 
customized NLP solutions in specific domains, further enhancing the impact of these 
models on indust ry and academia.  
Future Directions:  
Looking ahead, the development of more advanced PEFT techniques will likely 
focus on further reducing the computational demands of fine -tuning while expanding 
the applicability to even larger and more complex models. Future research may 
explore the integra tion of other efficiency -boosting methods, such as pruning or 
distillation, with PEFT techniques to create even more streamlined fine -tuning 
processes. Additionally, there is potential for extending the benefits of PEFT beyond 
NLP, applying similar princip les to other domains where large models are used, such 
as computer vision or speech processing.  
The continued evolution of these techniques will also necessitate addressing the Computing and Artificial Intelligence  2025, 3(1), 1498.   
19 challenges of maintaining model performance while optimizing for efficiency. As 
language models become increasingly embedded in real -world applications, ensuring 
that they remain accurate, reliable, and fair will be critical. This may involve 
developing mo re sophisticated methods for managing the trade -offs between 
efficiency and performance, as well as refining the mechanisms for controlling model 
bias and ensuring robust generalization across different tasks and datasets.  
Parameter -efficient finetuning represents a promising direction in the ongoing 
development of NLP technologies. By enabling more efficient use of large language 
models, PEFT techniques like LoRA and QLoRA have the potential to significantly 
expand the acce ssibility and applicability of these models, driving innovation and 
enabling the deployment of advanced AI solutions across a broader range of contexts.  
10. Conclusions  
The development of PaLM -E marks a significant milestone in advancing the 
capabilities of generally -capable models by simultaneously addressing vision, 
language, and robotics. This research not only explores the model’s versatility in 
unifying traditionally  distinct tasks but also highlights the broader implications of 
PaLM -E in enhancing the integration of these domains. By leveraging knowledge 
transfer from vision and language to robotics, PaLM -E demonstrates the potential to 
create more proficient robots capable of utilizing diverse data sources. This 
advancement paves the way for broader applications in multimodal learning, 
positioning PaLM -E as a critical facilitator in the evolution of artificial intelligence 
across various fields.  
In parallel, the rapid evolution of Parameter -Efficient Fine -Tuning (PEFT) 
techniques, such as Low -Rank Adaptation (LoRA) and Quantized Low -Rank 
Adaptation (QLoRA), addresses the significant challenges posed by the computational 
and memory requirements of fine-tuning large language models (LLMs). These 
innovative strategies enhance the efficiency of the fine -tuning process while 
maintaining or even improving task performance. The introduction of trainable low -
rank matrices in LoRA and quantization technique s in QLoRA exemplifies novel 
approaches to minimizing the number of trainable parameters, making the fine -tuning 
of LLMs more practical and accessible.  
The emphasis on parameter efficiency is crucial for overcoming the resource -
intensive nature of LLMs, contributing to reduced memory usage and computational 
costs. This focus not only addresses technical challenges but also has profound 
implications for th e broader field of Natural Language Processing (NLP). By making 
the fine -tuning process more efficient, these techniques open up opportunities for 
deploying large language models in a wider range of real -world applications, fostering 
innovation and broader  adoption of NLP technologies.  
As these PEFT techniques continue to evolve, they are reshaping the landscape 
of fine -tuning processes for LLMs, making them more adaptable, resource -efficient, 
and applicable to a diverse array of tasks. The potential to deploy large language 
models with reduced resource requirements is transformative, enabling the practical 
application of advanced AI across various domains.  
The research underscores the significant impact of PaLM -E and parameter -Computing and Artificial Intelligence  2025, 3(1), 1498.   
20 efficient fine -tuning techniques like LoRA and QLoRA, highlighting their role in 
driving the next generation of AI technologies and expanding the accessibility and 
applicability of NLP in real -world scenarios.  
Supplementary materials : The various original data sources some of which are not 
all publicly available, because they contain various types of private information. The 
available platform provided data sources that support the exploration findings and 
information of the research in vestigations are referenced where appropriate.  
Acknowledgments: The author would like to acknowledge and thank the GOOGLE 
Deep Mind Research  with its associated pre -prints access platforms. This research 
exploration was investigated under the platform provided by GOOGLE Deep Mind  
which is under the support of the GOOGLE Research  and the GOOGLE Research 
Publications  within the GOOGLE Gemini  platform. Using their provided platform of 
datasets and database associated files with digital software layouts consisting of free 
web access to a large collection of recorded models that are found within research 
access and its related open -source softwa re distributions which is the implementation 
for the proposed research exploration that was undergone and set in motion. There are 
many data sources some of which are resourced and retrieved from a wide variety of 
GOOGLE  service domains as well. All the da ta sources which have been included and 
retrieved for this research are identified, mentioned and referenced where appropriate.  
Availability of data and materials : The various original data models and datasets 
some of which are not all publicly available, because they contain various types of 
private information. The available platform provided datasets and data models that 
support the research findings and information o f the research investigations are 
referenced where appropriate.  
Conflict of interest : The author declare s no conflict of interest.  
References  
1. McKinsey. The state of AI in 2022 -and a half decade in review. Available online:  
https://www.mckinsey.com/~/media/mckinsey/business%20functions/quantumblack/our%20insights/the%20state%20of%20
ai%20in%202022%20and%20a%20half%20decade%20in%20review/the -state-of-ai-in-2022 -and-a-half-decade -in-
review.pdf  (accessed on 2 Ma rch 2024 ). 
2. McKinsey & Company. McKinsey Technology Trends Outlook 2022.  Available online:  
https://www.mckinsey.com/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/the%20top%20tr
ends%20in%20tech%202022/mckinsey -tech-trends -outlook -2022 -full-report.pdf  (accessed on 2 Ma rch 2024 ). 
3. Quick guide to AI 2.0 Oct 2020. Mckinsey.com. Available online:  https://ceros.mckinsey.com/quick -guide -to-ai-12/p/1  
(accessed on 2 Ma rch 2024 ). 
4. Chui, M., Manyika, J., & Miremadi, M.  What AI can and can’t do (yet) for your business . Available online:  
https://www.mckinsey.com/capabilities/quantumblack/our -insights/what -ai-can-and-cant-do-yet-for-your-business  (accessed 
on 2 Ma rch 2024 ). 
5. Hedges  R. Artificial intelligence discovery & admissibility case law and other resources.  Available online:  
https://afbnj.org/wp -content/uploads/2023/12/AI -Written -Materials -1.25.24.pdf  (accessed on 2 Ma rch 2024 ). 
6. Griffith  E, Metz  C. Anthropic, an A.I. Start -Up, Is Said to Be Close to Adding $300 Million. Available online:  
https://www.nytimes.com/2023/01/27/technology/anthropic -ai-funding.html  (accessed on 2 Ma rch 2024 ). 
7. Fiegerman  S, Lanxon  N. AI Glossary: A -Z of Artificial Intelligence Terms to Know. . Available online:  
https://www.bloomberg.com/features/2024 -artificial -intelligence -glossary/  (accessed on 2 Ma rch 2024 ). Computing and Artificial Intelligence  2025, 3(1), 1498.   
21 8. Pinaya  WHL, Graham  MS, Kerfoot  E, et al. Generative AI for Medical Imaging: extending the MONAI Framework. ArXiv . 
2013. doi: 10.48550/arXiv.2307.15208  
9. Pasick  A. Artificial Intelligence Glossary: Neural Networks and Other Terms Explained. Available online:  
https://www.nytimes.com/article/ai -artificial -intelligence -glossary.html  (accessed on 2 Ma rch 2024 ). 
10. Generative models.  Available online:  https://openai.com/index/generative -models/  (accessed on 2 Ma rch 2024 ). 
11. Metz  C. OpenAI Plans to Up the Ante in Tech’s A.I. Race. Available online:  
https://www.nytimes.com/2023/03/14/technology/openai -gpt4-chatgpt.html  (accessed on 2 Ma rch 2024 ). 
12. Thoppilan  R, De Freitas  D, Hall  J, et al. (2022). LaMDA: Language Models for Dialog Applications. ArXiv . 2022;  
ArXiv:2201.08239 . 
13. Roose  K. A Coming -Out Party for Generative A.I., Silicon Valley’s New Craze. Available online:  
https://www.nytimes.com/2022/10/21/technology/generative -ai.html  (accessed on 2 Ma rch 2024 ). 
14. Don’t fear an AI -induced jobs apocalypse just yet.  Available online:  https://www.economist.com/business/2023/03/06/dont -
fear-an-ai-induced -jobs-apocalypse -just-yet (accessed on 2 Ma rch 2024 ). 
15. Harreis  H, Koullias  T, Roberts  R, Te K. Generative AI in Fashion | McKinsey. Available online:  
https://www.mckinsey.com/industries/retail/our -insights/generative -ai-unlocking -the-future -of-fashion  (accessed on 2 Ma rch 
2024 ). 
16. Eapen  TT, Finkenstadt  DJ, Folk  J, Venkataswamy  L. How Generative AI Can Augment Human Creativity. Harvard 
Business Review.  2023.  
17. The race of the AI labs heats up. Available online:  https://www.economist.com/business/2023/01/30/the -race-of-the-ai-labs-
heats -up (accessed on 2 Ma rch 2024 ). 
18. Google Cloud brings generative AI to developers, businesses, and governments. Available online:  
https://cloud.google.com/blog/products/ai -machine -learning/generative -ai-for-businesses -and-governments  (accessed on 2 
March 2024 ). 
19. Political Machines: Understanding the Role of AI in the U.S. 2024 Elections and Beyond - Center for Media Engagement - 
Center for Media Engagement. Available online:  https://mediaengagement.org/research/generative -ai-elections -and-beyond/  
(accessed on 2 Ma rch 2024 ). 
20. Simon FM, Altay S, Mercier H. Misinformation reloaded? Fears about the impact of generative AI on misinformation are 
overblown. Available online: https://misinforeview.hks.harvard.edu/article/misinformation -reloaded -fears -about -the-impact -
of-generative -ai-on-misinformation -are-overblown/  (accessed on 22 October 2024).  
21. Anyoha  R. The History of Artificial Intelligence. Science in the News.  Scientific Research Publishing ; 2017 . 
22. Benbya  H, Strich  F, Tamm  T. Navigating Generative Artificial Intelligence Promises and Perils for Knowledge and Creative 
Work. Journal of the Association for Information Systems . 2024;  25(1) : 23–36. doi: 10.17705/1jais.00861  
23. Gagniuc  PA. Markov Chains. John Wiley & Sons, Inc. ; 2017 . doi: 10.1002/9781119387596  
24. Jebara  T. Machine Learning. Springer eBooks ; 2004.  doi: 10.1007/978 -1-4419 -9011 -2 
25. Cao Y, Li S, Liu Y, et al. A Comprehensive Survey of AI -Generated Content (AIGC): A History of Generative AI from 
GAN to ChatGPT. ArXiv . 2023; ArXiv:2303.04226 . 
26. openai. Available online:  https://github.com/openai/finetune -transformer -lm (accessed on 2 Ma rch 2024 ). 
27. Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners . OpenAI Blog. 2019; 1(8): 9.  
28. Bubeck  S, Chandrasekaran  V, Eldan  R, et al. Sparks of Artificial General Intelligence: Early experiments with GPT -4. 
ArXiv . 2023;  ArXiv:2303.12712 . 
29. Schlagwein D, Willcocks L. ‘ChatGPT et al.’: The ethics of using (generative) artificial intelligence in research and science . 
Journal of Information Technology. 2023; 38(3): 232 -238. doi: 10.1177/02683962231200411  
30. Islam  A. A History of Generative AI: From GAN to GPT -4. Available online:  https://www.marktechpost.com/2023/03/21/a -
history -of-generative -ai-from -gan-to-gpt-4/ (accessed on 2 Ma rch 2024 ). 
31. McKinsey & Company.  What Is Generative AI? Available online:  https://www.mckinsey.com/featured -insights/mckinsey -
explainers/what -is-generative -ai (accessed on 2 Ma rch 2024 ). 
32. Bommasani  R, Hudson  DA, Adeli  E, et al. On the Opportunities and Risks of Foundation Models. ArXiv . 2021; 
ArXiv:2108.07258 . 
33. Chen  M, Tworek  J, Jun  H, et al. Evaluating Large Language Models Trained on Code. ArXiv . 2021; ArXiv:2107.03374  
34. Bin Akhtar Z. From bard to Gemini: An investigative exploration journey through Google’s evolution in conversational AI Computing and Artificial Intelligence  2025, 3(1), 1498.   
22 and generative AI. Computing and Artificial Intelligence. 2024; 2(1): 1378. doi: 10.59400/cai.v2i1.1378  
35. Bin Akhtar Z. Exploring Biomedical Engineering (BME): Advances within Accelerated Computing and Regenerative 
Medicine for a Computational and Medical Science Perspective Exploration Analysis. Journal of Emergency Medicine: 
Open Access. 2024; 2(1): 1 -23. do i: 10.33140/jemoa.02.01.06  
36. Akhtar ZB. Unveiling the evolution of generative AI (GAI): a comprehensive and investigative analysis toward LLM models 
(2021 –2024) and beyond. Journal of Electrical Systems and Information Technology. 2024; 11(1). doi: 10.1186/s43067 -024-
00145 -1 
37. Unraveling the Promise of Computing DNA Data Storage: An Investigative Analysis of Advancements, Challenges, Future 
Directions. Journal of Advances in Artificial Intelligence. 2024; 2(1). doi: 10.18178/jaai.2024.2.1.122 -137 
38. Akhtar ZB. The design approach of an artificial intelligent (AI) medical system based on electronical health records (EHR) 
and priority segmentations. The Journal of Engineering. 2024; 2024(4). doi: 10.1049/tje2.12381  
39. Akhtar ZB. Securing Operating Systems (OS): A Comprehensive Approach to Security with Best Practices and Techniques. 
International Journal of Advanced Network, Monitoring and Controls. 2024; 9(1): 100 -111. doi: 10.2478/ijanmc -2024 -0010  
40. Akhtar ZB, Gupta AD. Integrative Approaches for Advancing Organoid Engineering: From Mechanobiology to Personalized 
Therapeutics. Journal of Applied Artificial Intelligence. 2024; 5(1): 1 -27. doi: 10.48185/jaai.v5i1.974  
41. Akhtar ZB. Advancements within Molecular Engineering for Regenerative Medicine and Biomedical Applications an 
Investigation Analysis towards A Computing Retrospective. Journal of Electronics, Electromedical Engineering, and 
Medical Informatics. 2024; 6(1).  doi: 10.35882/jeeemi.v6i1.351  
42. Akhtar ZB. Accelerated Computing  a Biomedical Engineering and Medical Science Perspective. Annals of the Academy of 
Romanian Scientists Series on Biological Sciences. 2023; 12(2): 138 -164. doi: 10.56082/annalsarscibio.2023.2.138  
43. Akhtar ZB. Designing an AI Healthcare System: EHR and Priority -Based Medical Segmentation Approach. Medika 
Teknika: Jurnal Teknik Elektromedik Indonesia. 2023; 5(1): 50 -66. doi: 10.18196/mt.v5i1.19399  
44. Bin Akhtar Z. A Revolutionary Gaming Style in Motion. In: Dey I (editor). Computer -Mediated Communication. 
IntechOpen; 2022.  doi: 10.5772/intechopen.100551  
45. Akhtar ZB, Stany Rozario V. The Design Approach of an Artificial Human Brain in Digitized Formulation based on 
Machine Learning and Neural Mapping. In: Proceedings of the 2020 International Conference for Emerging Technology 
(INCET) ; 5-7 June 2020; Belgaum, India. doi: 10.1109/incet49848.2020.9154000  
46. Akhtar Z. Biomedical engineering (bme) and medical health science: an investigation perspective exploration. Quantum 
Journal of Medical and Health Sciences. 2024; 3(3): 1 -24. 
47. Akhtar ZB, Rawol AT. Uncovering Cybersecurity Vulnerabilities: A Kali Linux Investigative Exploration Perspective. 
International Journal of Advanced Network, Monitoring and Controls. 2024; 9(2): 11 -22. doi: 10.2478/ijanmc -2024 -0012  
48. Akhtar ZB, Tajbiul Rawol  A. Unlocking the Future for the New Data Paradigm of DNA Data Storage: An Investigative 
Analysis of Advancements, Challenges, Future Directions. Journal of Information Sciences. 2024. doi: 
10.34874/IMIST.PRSM/JIS -V23I1.47102  
49. Bin Akhtar Z. Artificial intelligence (AI) within manufacturing: An investigative exploration for opportunities, challenges, 
future directions. Metaverse. 2024; 5(2): 2731. doi: 10.54517/m.v5i2.2731  

ARTIFICIAL INTELLIGENCE 
LECTURE NOTES 
(Subject Code: BCS-404)
for 
Bachelor of Technology 
in 
Computer Science and Engineering 
& 
Information Technology 
Department of Computer Science and Engineering & Information Technology 
Veer Surendra Sai University of Technology 
(Formerly UCE, Burla) 
Burla, Sambalpur, Odisha 
Lecture Note Prepared by:  Prof. Pradipta Kumar Das  
 
 
 
DISCLAIMER  
  
This document does not claim any originality and cannot  be 
used as a substitute for prescribed textbooks. The information 
presented here is merely a collection by the committee members for their respective teaching assignments. Various sources as mentioned at the end of the document as well as freely available  
material from internet were consulted for preparing this document. The ownership of the information lies with the respective authors or institutions. 
 
      
 
      BCS -404 ARTIFICIAL INTELLIGENCE (3- 1-0) Cr. -04 
Module - I  
 
Formalized symbolic logic: Propositional logic -first order predicate logic, wff conversion 
to clausal form, inference rules, the resolution principle, Dealing with inconsistencies and  
uncertaintie s, fuzzy logic.  
 
Module - II 
 
Probabilistic Reasoning Structured knowledge, graphs, frames and related structures,  
Knowledge organization and   manipulation.  
 
Module – III  
 
Matching Techniques, Knowledge  organizations, Management.  
 
Module - IV  
 
Natural Language processing, Pattern recognition, expert systems.  
 
Text Book:  
1. Artificial Intelligence, Dan W Patterson, Prentice Hall of India (1999) Chapter -4, 
5,7,9,10,11,12,13,15.  
 
Reference Books:  
1. Artificial Intelligence, Nils J.Nilsson, ELSEVIER.  
2. E.R ich and K.Knight, Artificial Intelligence, - TMH  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  Overview of Artificial Intelligence  
What is AI ?  
 
 Artificial Intelligence (AI) is a branch of Science  which deals with helping machines 
find solutions to complex problems in a more hu man-like fashion.  
 This generally involves borrowing characteristics from human intelligence, and 
applying them as algorithms in a computer friendly way.  
 A more or less flexible or efficient approach can be taken depending on the 
requirements established,  which influences how artificial the intelligent behavior 
appears  
 Artificial intelligence can be viewed from a variety of perspectives.  
 From the perspective of intelligence   
artificial intelligence is making machines "intelligent" -- acting as we would 
expect people to act.  
o The inability to distinguish computer responses from human responses 
is called the Turing test.  
o Intelligence requires knowledge  
o Expert problem solving - restricting domain to allow including 
significant relevant knowledge  
 From a business  perspective AI is a set of very powerful tools, and 
methodologies for using those tools to solve business problems.  
 From a programming perspective, AI includes the study of symbolic 
programming, problem solving, and search.  
o Typically AI programs fo cus on symbols rather than numeric 
processing.  
o Problem solving - achieve goals.  
o Search - seldom access a solution directly. Search may include a 
variety of techniques.  
o AI programming languages include:  
– LISP, developed in the 1950s, is the early program ming language 
strongly associated with AI. LISP is a functional programming language with 
procedural extensions. LISP (LISt Processor) was specifically designed for processing heterogeneous lists -- typically a list of symbols. Features of LISP 
are run - time type checking, higher order functions (functions that have other 
functions as parameters), automatic memory management (garbage collection) 
and an interactive environment.  
– The second language strongly associated with AI is PROLOG. 
PROLOG was developed in the 1970s. PROLOG is based on first order logic. 
PROLOG is declarative in nature and has facilities for explicitly limiting the 
search space.  
– Object -oriented languages are a class of languages more recently used 
for AI programming. Important features o f object -oriented languages include: 
concepts of objects and messages, objects bundle data and methods for 
manipulating the data, sender specifies what is to be done receiver decides 
how to do it, inheritance (object hierarchy where objects inherit the att ributes 
of the more general class of objects). Examples of object -oriented languages 
are Smalltalk, Objective C, C++. Object oriented extensions to LISP (CLOS - 
Common LISP Object System) and PROLOG (L&O - Logic & Objects) are 
also used.  
 Artificial Intell igence is a new electronic machine that stores large amount of 
information and process it at very high speed  
 The computer is interrogated by a human via a teletype It passes if the human cannot 
tell if there is a computer or human at the other end 
 The abil ity to solve problems  
 It is the science and engineering of making intelligent machines, especially intelligent 
computer programs. It is related to the similar task of using computers to understand 
human intelligence  
 Importance of AI  
 Game Playing  
You can buy machines that can play master level chess for a few hundred dollars. 
There is some AI in them, but they play well against people mainly through brute 
force computation --looking at hundreds of thousands of positions. To beat a world 
champion by brute fo rce and known reliable heuristics requires being able to look at 
200 million positions per second.  
 Speech Recognition   In the 1990s, computer speech recognition reached a practical level for limited 
purposes. Thus United Airlines has replaced its keyboard tree for flight information 
by a system using speech recognition of flight numbers and city names. It is quite 
convenient. On the other hand, while it is possible to instruct some computers using 
speech, most users have gone back to the keyboard and the m ouse as still more 
convenient.  
 Understanding Natural Language   
Just getting a sequence of words into a computer is not enough. Parsing sentences is 
not enough either. The computer has to be provided with an understanding of the 
domain the text is about, a nd this is presently possible only for very limited domains.  
 Computer Vision   
The world is composed of three -dimensional objects, but the inputs to the human eye 
and computers' TV cameras are two dimensional. Some useful programs can work 
solely in two di mensions, but full computer vision requires partial three -dimensional 
information that is not just a set of two -dimensional views. At present there are only 
limited ways of representing three -dimensional information directly, and they are not 
as good as wh at humans evidently use.  
 Expert Systems   
      A ``knowledge engineer'' interviews experts in a certain domain and tries to embody 
their knowledge in a computer program for carrying out some task. How well this 
works depends on whether the intellectual me chanisms required for the task are 
within the present state of AI. When this turned out not to be so, there were many 
disappointing results. One of the first expert systems was MYCIN in 1974, which 
diagnosed bacterial infections of the blood and suggested treatments. It did better than 
medical students or practicing doctors, provided its limitations were observed. 
Namely, its ontology included bacteria, symptoms, and treatments and did not include 
patients, doctors, hospitals, death, recovery, and events oc curring in time. Its 
interactions depended on a single patient being considered. Since the experts 
consulted by the knowledge engineers knew about patients, doctors, death, recovery, 
etc., it is clear that the knowledge engineers forced what the experts to ld them into a 
predetermined framework. The usefulness of current expert systems depends on their 
users having common sense.   Heuristic Classification   
One of the most feasible kinds of expert system given the present knowledge of AI is 
to put some informa tion in one of a fixed set of categories using several sources of 
information. An example is advising whether to accept a proposed credit card 
purchase. Information is available about the owner of the credit card, his record of 
payment and also about the i tem he is buying and about the establishment from which 
he is buying it (e.g., about whether there have been previous credit card frauds at this 
establishment).  
 The applications of AI are shown in Fig 1.1:  
 Consumer Marketing  
o Have you ever used any kind of  credit/ATM/store card while shopping?  
o if so, you have very likely been “input” to an AI algorithm  
o All of this information is recorded digitally 
o Companies like Nielsen gather this information weekly and search for 
patterns  
– general changes in consumer behavior  
– tracking responses to new products  
– identifying customer segments: targeted marketing, e.g., they find 
out that consumers with sports cars who buy textbooks respond 
well to offers of new credit cards.  
o Algorithms (“data mining”) search data for patterns based on mathematical 
theories of learning  
 Identification Technologies  
o ID cards e.g., ATM cards  
o can be a nuisance and security risk: cards can be lost, stolen, passwords 
forgotten, etc  
o Biometric Identification, walk up to a locked door  
– Camera  
– Fingerprint device  
– Microphone  
– Computer uses biometric signature for identification 
– Face, eyes, fingerprints, voice pattern  – This works by comparing data from person at door with stored 
library  
– Learning algorithms can learn the matching process by analyzing a 
large library database off -line, can improve its performance.  
 Intrusion Detection 
o Computer security -  we each have specific patterns of computer use times 
of day, lengths of sessions, command used, sequence of commands, etc  
– would like to learn the “signature” of e ach authorized user  
– can identify non -authorized users  
o How can the program automatically identify users?  
– record user’s commands and time intervals  
– characterize the patterns for each user  
– model the variability in these patterns  
– classify (online) any new user  by similarity to stored patterns  
 Machine Translation 
o Language problems in international business  
– e.g., at a meeting of Japanese, Korean, Vietnamese and Swedish 
investors, no common language  
– If you are shipping your software manuals to 127 countries, the  
solution is ; hire translators to translate  
– would be much cheaper if a machine could do this!  
o How hard is automated translation 
– very difficult!  
– e.g., English to Russian 
– not only must the words be translated, but their meaning also!   
                     Fig : Application areas of AI  
 Early work in AI  
 “Artificial Intelligence (AI) is the part of computer science concerned with designing 
intelligent computer systems, that is, systems that exhibit characteristics we associate 
with intelligence in human beha viour – understanding language, learning, reasoning, 
solving problems, and so on.”      
 Scientific Goal  To determine which ideas about knowledge representation, learning, 
rule systems, search, and so on, explain various sorts of real intelligence.  
 Enginee ring Goal  To solve real world problems using AI techniques such as 
knowledge representation, learning, rule systems, search, and so on.  
 Traditionally, computer scientists and engineers have been more interested in the 
engineering goal, while psychologists , philosophers and cognitive scientists have been 
more interested in the scientific goal.   
 The Roots - Artificial Intelligence has identifiable roots in a number of older 
disciplines, particularly:  
 Philosophy 
 Logic/Mathematics  
 Computation  Psychology/Cogni tive Science  
 Biology/Neuroscience  
 Evolution 
 There is inevitably much overlap, e.g. between philosophy and logic, or between 
mathematics and computation. By looking at each of these in turn, we can gain a 
better understanding of their role in AI, and how th ese underlying disciplines have 
developed to play that role.  
 Philosophy 
 ~400 BC   Socrates asks for an algorithm to distinguish piety from non -piety.  
 ~350 BC   Aristotle formulated different styles of deductive reasoning, which 
could mechanically generate  conclusions from initial premises, e.g. Modus Ponens             
If    A ? B     and     A      then       B  
      If    A implies B     and     A is true      then       B is true when it’s raining you       
      get wet  and  it’s raining  then  you ge t wet  
 1596 – 1650   Rene Descartes idea of mind- body dualism – part of the mind is 
exempt from physical laws.  
 1646 – 1716   Wilhelm Leibnitz was one of the first to take the materialist position 
which holds that the mind operates by ordinary physical proce sses – this has the 
implication that mental processes can potentially be carried out by machines.  
 Logic/Mathematics  
 Earl  Stanhope’s Logic Demonstrator was a machine that was able to solve 
syllogisms, numerical problems in a logical form, and elementary questions of 
probability.  
 1815 – 1864   George  Boole introduced his formal language for making logical 
inference in 1847 – Boolean algebra.  
 1848 – 1925   Gottlob Frege produced a logic that is essentially the first -order 
logic that today forms the most basi c knowledge representation system.  
 1906 – 1978   Kurt Gödel showed in 1931 that there are limits to what logic can 
do. His Incompleteness Theorem showed that in any formal logic powerful 
enough to describe the properties of natural numbers, there are true statements 
whose truth cannot be established by any algorithm.   1995 Roger Penrose tries to prove the human mind has non -computable 
capabilities.  
 Computation 
 1869   William Jevon’s Logic Machine could handle Boolean Algebra and Venn 
Diagrams, and was able t o solve logical problems faster than human beings.  
 1912 – 1954   Alan Turing tried to characterise exactly which functions are 
capable of being computed.  Unfortunately it is difficult to give the notion of 
computation a formal definition.  However, the Church -Turing thesis, which states 
that a Turing machine is capable of computing any computable function, is 
generally accepted as providing a sufficient definition.  Turing also showed that 
there were some functions which no Turing machine can compute (e.g.  Halting 
Problem).  
 1903 – 1957   John von Neumann proposed the von Neuman architecture which 
allows a description of computation that is independent of the particular 
realisation of the computer.  
 1960s   Two important concepts emerged: Intractability (when  solution time 
grows atleast exponentially) and Reduction (to ‘easier’ problems).  
 Psychology / Cognitive Science  
 Modern Psychology / Cognitive Psychology / Cognitive Science is the science 
which studies how the mind operates, how we behave, and how our bra ins process 
information.  
 Language is an important part of human intelligence.  Much of the early work on 
knowledge representation was tied to language and informed by research into 
linguistics.  
 It is natural for us to try to use our understanding of how h uman (and other 
animal) brains lead to intelligent behavior in our quest to build artificial intelligent 
systems. Conversely, it makes sense to explore the properties of artificial systems 
(computer models/simulations) to test our hypotheses concerning hum an systems.  
 Many sub -fields of AI are simultaneously building models of how the human 
system operates, and artificial systems for solving real world problems, and are 
allowing useful ideas to transfer between them.  
 Biology / Neuroscience   Our brains (which give rise to our intelligence) are made up of tens of billions of 
neurons, each connected to hundreds or thousands of other neurons.   
 Each neuron is a simple processing device (e.g. just firing or not firing depending 
on the total amount of activity feedi ng into it).  However, large networks of 
neurons are extremely powerful computational devices that can learn how best to 
operate.  
 The field of Connectionism or Neural Networks attempts to build artificial 
systems based on simplified networks of simplified artificial neurons.  
 The aim is to build powerful AI systems, as well as models of various human 
abilities.  
 Neural networks work at a sub -symbolic level, whereas much of conscious human 
reasoning appears to operate at a symbolic level.  
 Artificial neural ne tworks perform well at many simple tasks, and provide good 
models of many human abilities.  However, there are many tasks that they are not 
so good at, and other approaches seem more promising in those areas.  
 Evolution 
 One advantage humans have over curren t machines/computers is that they have a 
long evolutionary history.  
 Charles Darwin (1809 – 1882) is famous for his work on evolution by  natural 
selection.  The idea is that fitter individuals will naturally tend to live longer and 
produce more children, a nd hence after many generations a population will 
automatically emerge with good innate properties.  
 This has resulted in brains that have much structure, or even knowledge, built in at 
birth.  
 This gives them at the advantage over simple artificial neural network systems 
that have to learn everything.   
 Computers are finally becoming powerful enough that we can simulate evolution 
and evolve good AI systems.   
 We can now even evolve systems (e.g. neural networks) so that they are good at 
learning.  
 A related field called  genetic programming has had some success in evolving 
programs, rather than programming them by hand.  
 Sub-fields of Artificial Intelligence   Neural Networks – e.g. brain modelling, time series prediction, classification  
 Evolutionary Computati on – e.g. genetic algorithms, genetic programming  
 Vision – e.g. object recognition, image understanding  
 Robotics – e.g. intelligent control, autonomous exploration  
 Expert Systems – e.g. decision support systems, teaching systems  
 Speech Processing– e.g. spe ech recognition and production 
 Natural Language Processing – e.g. machine translation 
 Planning – e.g. scheduling, game playing  
 Machine Learning – e.g. decision tree learning, version space learning  
 Speech Processing  
 As well as trying to understand human sy stems, there are also numerous real 
world applications: speech recognition for dictation systems and voice activated 
control; speech production for automated announcements and computer interfaces.  
 How do we get from sound waves to text streams and vice -versa? 
 
 Natural Language Processing  
 For example, machine understanding and translation of simple sentences:  
 Planning  
 Planning refers to the process of choosing/computing the correct sequence of steps 
to solve a given problem.  
 To do this we need some convenie nt representation of the problem domain.  We 
can define states in some formal language, such as a subset of predicate logic, or a 
series of rules.   
 A plan can then be seen as a sequence of operations that transform the initial state 
into the goal state, i .e. the problem solution.  Typically we will use some kind of 
search algorithm to find a good plan.  
 Common Techniques  
 Even apparently radically different AI systems (such as rule based expert systems 
and neural networks) have many common techniques.   
 Four  important ones are:  o Knowledge Representation:   Knowledge needs to be represented 
somehow – perhaps as a series of if -then rules, as a frame based system, as 
a semantic network, or in the connection weights of an artificial neural 
network.  
o Learning:    Automatically building up knowledge from the environment – 
such as acquiring the rules for a rule based expert system, or determining 
the appropriate connection weights in an artificial neural network.  
o Rule Systems:   These could be explicitly built into an expert system by a 
knowledge engineer, or implicit in the connection weights learnt by a 
neural network.  
o Search:   This can take many forms – perhaps searching for a sequence of 
states that leads quickly to a problem solution, or searching for a good set 
of connection weights for a neural network by minimizing a fitness 
function.  
 
AI and related fields  
 Logical AI   
What a program knows about the world in general the facts of the specific situation in 
which it must act, and its goals are all represented by se ntences of some mathematical 
logical language. The program decides what to do by inferring that certain actions are 
appropriate for achieving its goals.  
 Search  
AI programs often examine large numbers of possibilities, e.g. moves in a chess game 
or infere nces by a theorem proving program. Discoveries are continually made about 
how to do this more efficiently in various domains.  
 Pattern Recognition  
When a program makes observations of some kind, it is often programmed to 
compare what it sees with a patter n. For example, a vision program may try to match 
a pattern of eyes and a nose in a scene in order to find a face. More complex patterns, 
e.g. in a natural language text, in a chess position, or in the history of some event are 
also studied.   Representation  
Facts about the world have to be represented in some way. Usually languages of 
mathematical logic are used.  
 Inference  
From some facts, others can be inferred. Mathematical logical deduction is adequate 
for some purposes, but new methods of non-monotonic  inference have been added to 
logic since the 1970s. The simplest kind of non -monotonic reasoning is default 
reasoning in which a conclusion is to be inferred by default, but the conclusion can be 
withdrawn if there is evidence to the contrary. For examp le, when we hear of a bird, 
we man infer that it can fly, but this conclusion can be reversed when we hear that it 
is a penguin. It is the possibility that a conclusion may have to be withdrawn that 
constitutes the non -monotonic character of the reasoning.  Ordinary logical reasoning 
is monotonic in that the set of conclusions that can the drawn from a set of premises is 
a monotonic increasing function of the premises.   
 Common sense knowledge and reasoning  
This is the area in which AI is farthest from huma n-level, in spite of the fact that it has 
been an active research area since the 1950s. While there has been considerable 
progress, e.g. in developing systems of non-monotonic reasoning and theories of 
action, yet more new ideas are needed.   
 Learning from  experience  
Programs do that. The approaches to AI based on connectionism  and neural nets  
specialize in that. There is also learning of laws expressed in logic. Programs can only 
learn what facts or behaviors their formalisms can represent, and unfortunat ely 
learning systems are almost all based on very limited abilities to represent 
information.  
 Planning  
Planning programs start with general facts about the world (especially facts about the 
effects of actions), facts about the particular situation and a statement of a goal. From 
these, they generate a strategy for achieving the goal. In the most common cases, the 
strategy is just a sequence of actions.  
 Epistemology  This is a study of the kinds of knowledge that are required for solving problems in the 
world.  
 Ontology  
Ontology is the study of the kinds of things that exist. In AI, the programs and 
sentences deal with various kinds of objects, and we study what these kinds are and 
what their basic properties are. Emphasis on ontology begins in the 1990s.   
 Heuristics  
A heuristic is a way of trying to discover something or an idea imbedded in a 
program. The term is used variously in AI. Heuristic functions  are used in some 
approaches to search to measure how far a node in a search tree seems to be from a 
goal. Heuristic predicates  that compare two nodes in a search tree to see if one is 
better than the other, i.e. constitutes an advance toward the goal, may be more useful.   
 Genetic Programming  
Genetic programming is a technique for getting programs to sol ve a task by mating 
random Lisp programs and selecting fittest in millions of generations.  
 Search and Control Strategies:  
Problem solving is an important aspect of Artificial Intelligence. A problem can be 
considered to consist of a goal and a set of actions that can be taken to lead to the goal. At 
any given time, we consider the state of the search space to represent where we have reached 
as a result of the actions we have applied so far. For example, consider the pr oblem of 
looking for a contact lens on a football field. The initial state is how we start out, which is to 
say we know that the lens is somewhere on the field, but we don’t know where. If we use the 
representation where we examine the field in units of on e square foot, then our first action 
might be to examine the square in the top -left corner of the field. If we do not find the lens 
there, we could consider the state now to be that we have examined the top -left square and 
have not found the lens. After a number of actions, the state might be that we have examined 
500 squares, and we have now just found the lens in the last square we examined. This is a 
goal state because it satisfies the goal that we had of finding a contact lens.  
Search is a method that c an be used by computers to examine a problem space like 
this in order to find a goal. Often, we want to find the goal as quickly as possible or without 
using too many resources. A problem space can also be considered to be a search space because in order t o solve the problem, we will search the space for a goal state.We will 
continue to use the term search space to describe this concept. In this chapter, we will look at 
a number of methods for examining a search space. These methods are called search 
methods . 
 The Importance of Search in AI  
 It has already become clear that many of the tasks underlying AI can be 
phrased in terms of a search for the solution to the problem at hand.  
 Many goal based agents are essentially problem solving agents which must 
decide what to do by searching for a sequence of actions that lead to their 
solutions.  
 For production systems, we have seen the need to search for a sequence of rule 
applications that lead to the required fact or action.  
 For neural network systems, we need to se arch for the set of connection 
weights that will result in the required input to output mapping.  
 Which search algorithm one should use will generally depend on the problem 
domain? There are four important factors to consider:  
 Completeness – Is a solution guaranteed to be found if at least one solution 
exists?  
 Optimality – Is the solution found guaranteed to be the best (or lowest cost) 
solution if there exists more than one solution?  
 Time Complexity – The upper bound on the time required to find a solution,  
as a function of the complexity of the problem.  
 Space Complexity – The upper bound on the storage space (memory) required 
at any point during the search, as a function of the complexity of the problem.  
Preliminary concepts  
 Two varieties of space -for-time  algorithms:  
 Input enhancement   — preprocess the input (or its part) to store some info to 
be used later in solving the problem  
o Counting for sort ing 
o String searching algorithms  
 Prestructuring — preprocess the input to make accessing its elements easier  
o Hashing  o Indexing schemes (e.g., B -trees)  
 State Space Representations:  The state space is simply the space of all possible 
states, or configurations,  that our system may be in. Generally, of course, we prefer to 
work with some convenient representation of that search space.  
 There are two components to the representation of state spaces:  
 Static States  
 
 Transitions between States  
 
 State Space Graphs: If the number of possible states of the system is small enough, 
we can represent all of them, along with the transitions between them, in a state space 
graph, e.g.  
 
 Routes through State Space: Our general aim is to search for a route, or sequence of 
transi tions, through the state space graph from our initial state to a goal state.  
 Sometimes there will be more than one possible goal state. We define a goal test to 
determine if a goal state has been achieved.  
 The solution can be represented as a sequence of link labels (or transitions) on the 
state space graph. Note that the labels depend on the direction moved along the link.  
 Sometimes there may be more than one path to a goal state, and we may want to find 
the optimal (best possible) path. We can define link costs and path costs for 
measuring the cost of going along a particular path, e.g. the path cost may just equal 
the number of links, or could be the sum of individual link costs.   For most realistic problems, the state space graph will be too large for us  to hold all of 
it explicitly in memory at any one time.  
 Search Trees:  It is helpful to think of the search process as building up a search tree 
of routes through the state space graph. The root of the search tree is the search node 
corresponding to the initial state.  
 The leaf nodes correspond either to states that have not yet been expanded, or to states 
that generated no further nodes when expanded.  
 
 At each step, the search algorithm chooses a new unexpanded leaf node to expand. 
The different search st rategies essentially correspond to the different algorithms one 
can use to select which is the next mode to be expanded at each stage.  
 
Examples of search problems  
 Traveling Salesman Problem:  GGiivveenn  nn  cciittiieess  wwiitthh  kknnoowwnn  ddiissttaanncceess  bbeettwweeeenn  eeaacchh  
ppaaiirr,,  ffiinndd  tthhee  sshhoorrtteesstt  ttoouurr  tthhaatt  ppaasssseess  tthhrroouugghh  aallll  tthhee  cciittiieess  eexxaaccttllyy  oonnccee  bbeeffoorree  
rreettuurrnniinngg  ttoo  tthhee  ssttaarrttiinngg  cciittyy.. 
 AA  lloowweerr  bboouunndd  oonn  tthhee  lleennggtthh  ll  ooff  aannyy  ttoouurr  ccaann  bbee  ccoommppuutteedd  aass  ffoolllloowwss 
 FFoorr  eeaacchh  cciittyy  ii,,  11  ≤ i ≤ n, find the sum s i of the distances from city i to the tw o 
nearest cities.  
 Compute the sum s of these n numbers.  
 Divide the result by 2 and round up the result to the nearest integer  
lb = s / 2  
 The lower bound for the graph shown in the Fig 5.1 can be computed as follows:  lb = [(1 + 3) + (3 + 6) + (1 + 2) 
+ (3 + 4) + (2 + 3)] / 2 = 14.  
 
 
 
 
 
 For any subset of tours that must include particular edges of a given graph, the lower 
bound can be modified accordingly. E.g.: For all the Hamiltonian circuits of the graph 
that must include edge (a, d), the lower bound can be computed as follows:  
lb = [(1 + 5) + (3 + 6) + (1 + 2) + (3 + 5) + (2 + 3)] / 2 = 16.  
 Applying the branch -and-bound algorithm, with the bounding function lb = s / 2, to 
find the shortest Hamiltonian circuit for the given graph, we obtain the s tate-space 
tree as shown below:  
 To reduce the amount of potential work, we take advantage of the following two 
observations:  
 We can consider only tours that start with a.  
 Since the graph is undirected, we can generate only tours in which b is visited 
befor e c. 
 In addition, after visiting n – 1 cities, a tour has no choice but to visit the remaining 
unvisited city and return to the starting one is shown in the Fig 5.2 
 
 
  Root node includes only the starting vertex a with a lower bound of  
lb = [(1 + 3) + (3 + 6) + (1 + 2) + (3 + 4) + (2 + 3)] / 2 = 14.  
 Node 1 represents the inclusion of edge (a, b)  
lb = [(1 + 3) + (3 + 6) + (1 + 2) + (3 + 4) + (2 + 3)] / 2 = 14.  
 Node 2 represents the inclusion of edge (a, c). Since b is not visited before c, 
this node is terminated.  
 Node 3 represents the inclusion of edge (a, d)  
lb = [(1 + 5) + (3 + 6) + (1 + 2) + (3 + 5) + (2 + 3)] / 2 = 16.  
 Node 1 represents the inclusion of edge (a, e)  
lb = [(1 + 8) + (3 + 6) + (1 + 2) + (3 + 4) + (2 + 8)] / 2 = 19.  
 Among all the four live nodes of the root, node 1 has a better lower bound. 
Hence we branch from node 1.  
 Node 5 represents the inclusion of edge (b, c)  
lb = [(1 + 3) + (3 + 6) + (1 + 6) + (3 + 4) + (2 + 3)] / 2 = 16.  
 Node 6 represents the inclusi on of edge (b, d)  
lb = [(1 + 3) + (3 + 7) + (1 + 2) + (3 + 7) + (2 + 3)] / 2 = 16.  
 Node 7 represents the inclusion of edge (b, e)  
lb = [(1 + 3) + (3 + 9) + (1 + 2) + (3 + 4) + (2 + 9)] / 2 = 19.   Since nodes 5 and 6 both have the same lower bound, we branch  out from 
each of them.  
 Node 8 represents the inclusion of the edges (c, d), (d, e) and  (e, a). Hence, 
the length of the tour,  
l = 3 + 6 + 4 + 3 + 8 = 24.  
 Node 9 represents the inclusion of the edges (c, e), (e, d) and  (d, a). Hence, 
the length of the to ur, 
l = 3 + 6 + 2 + 3 + 5 = 19.  
 Node 10 represents the inclusion of the edges (d, c), (c, e) and  (e, a). Hence, 
the length of the tour,  
l = 3 + 7 + 4 + 2 + 8 = 24.  
 Node 11 represents the inclusion of the edges (d, e), (e, c) and  (c, a). Hence, 
the length  of the tour,  
l = 3 + 7 + 3 + 2 + 1 = 16.  
 Node 11 represents an optimal tour since its tour length is better than or equal 
to the other live nodes, 8, 9, 10, 3 and 4.  
 The optimal tour is a → b → d → e → c → a with a tour length of 16.  
 
 Uniformed or Blind search  
 Breadth First Search (BFS): BFS expands the leaf node with the lowest path cost so 
far, and keeps going until a goal node is generated. If the path cost simp ly equals the 
number of links, we can implement this as a simple queue (“first in, first out”).  
  This is guaranteed to find an optimal path to a goal state. It is memory intensive if the 
state space is large. If the typical branching factor is b, and the depth of the shallowest 
goal state is d – the space complexity is O(bd), and the time complexity is O(bd). 
 BFS is an easy search technique to understand. The algorithm is presented below.  
breadth_first_search ()  
{ 
store initial state in queue Q  
set state i n the front of the Q as current state ;  
while (goal state is reached OR Q is empty)  
{ 
apply rule to generate a new state from the current  
state ;  
if (new state is goal state) quit ;  
else if (all states generated from current states are  
exhausted)  
{ 
delete the current state from the Q ;  
set front element of Q as the current state ;  
} 
else continue ;  
} 
} 
 The algorithm is illustrated using the bridge components configuration problem. The 
initial state is PDFG, which is not a goal state; and hence set it as the  current state. 
Generate another state DPFG (by swapping 1st and 2nd position values) and add it to the list. That is not a goal state, hence; generate next successor state, which is FDPG 
(by swapping 1st and 3rd position values). This is also not a goal s tate; hence add it to 
the list and generate the next successor state GDFP.  
 Only three states can be generated from the initial state. Now the queue Q will have 
three elements in it, viz., DPFG, FDPG and GDFP. Now take DPFG (first state in the 
list) as the  current state and continue the process, until all the states generated from 
this are evaluated. Continue this process, until the goal state DGPF is reached.  
 The 14th evaluation gives the goal state. It may be noted that, all the states at one 
level in th e tree are evaluated before the states in the next level are taken up; i.e., the 
evaluations are carried out breadth -wise. Hence, the search strategy is called breadth -
first search.  
 Depth First Search (DFS):  DFS expands the leaf node with the highest path cost so 
far, and keeps going until a goal node is generated. If the path cost simply equals the 
number of links, we can implement this as a simple stack (“last in, first out”).  
 
 This is not guaranteed to find any path to a goal state. It is memory efficie nt even if 
the state space is large. If the typical branching factor is b, and the maximum depth of 
the tree is m – the space complexity is O(bm), and the time complexity is O(bm). 
 In DFS, instead of generating all the states below the current level, only the first state 
below the current level is generated and evaluated recursively. The search continues 
till a further successor cannot be generated.  
 Then it goes back to the parent and explores the next successor. The algorithm is 
given below.  
depth_first_se arch ()  
{ 
set initial state to current state ;  
if (initial state is current state) quit ;  else 
{ 
if (a successor for current state exists)  
{ 
generate a successor of the current state and  
set it as current state ;  
} 
else return ;  
depth_first_search (current _state) ;  
if (goal state is achieved) return ;  
else continue ;  
} 
} 
 Since DFS stores only the states in the current path, it uses much less memory during 
the search compared to BFS.  
 The probability of arriving at goal state with a fewer number of evaluatio ns is higher 
with DFS compared to BFS. This is because, in BFS, all the states in a level have to 
be evaluated before states in the lower level are considered. DFS is very efficient 
when more acceptable solutions exist, so that the search can be terminated once the 
first acceptable solution is obtained.  
 BFS is advantageous in cases where the tree is very deep.  
 An ideal search mechanism is to combine the advantages of BFS and DFS.  
 Depth Limited Search (DLS):  DLS is a variation of DFS. If we put a limit l o n how 
deep a depth first search can go, we can guarantee that the search will terminate 
(either in success or failure).   
 If there is at least one goal state at a depth less than l, this algorithm is guaranteed to 
find a goal state, but it is not guarantee d to find an optimal path. The space 
complexity is O(bl), and the time complexity is O(bl).  
 Depth First Iterative Deepening Search (DFIDS):  DFIDS is a variation of DLS. If 
the lowest depth of a goal state is not known, we can always find the best limit l for 
DLS by trying all possible depths l = 0, 1, 2, 3, … in turn, and stopping once we have 
achieved a goal state.  
 This appears wasteful because all the DLS for l less than the goal level are useless, 
and many states are expanded many times. However, in pra ctice, most of the time is 
spent at the deepest part of the search tree, so the algorithm actually combines the 
benefits of DFS and BFS.  
 Because all the nodes are expanded at each level, the algorithm is complete and 
optimal like BFS, but has the modest me mory requirements of DFS. Exercise: if we 
had plenty of memory, could/should we avoid expanding the top level states many 
times?  
 The space complexity is O(bd) as in DLS with l = d, which is better than BFS.   
 The time complexity is O(bd) as in BFS, which i s better than DFS.  
 Bi-Directional Search (BDS):  The idea behind bi -directional search is to search 
simultaneously both forward from the initial state and backwards from the goal state, 
and stop when the two BFS searches meet in the middle.  
 
 This is not al ways going to be possible, but is likely to be feasible if the state 
transitions are reversible. The algorithm is complete and optimal, and since the two search depths are ~d/2, it has space complexity O(bd/2), and time complexity O(bd/2). 
However, if ther e is more than one possible goal state, this must be factored into the 
complexity.  
 Repeated States:  In the above discussion we have ignored an important complication 
that often arises in search processes – the possibility that we will waste time by 
expandi ng states that have already been expanded before somewhere else on the 
search tree.  
 For some problems this possibility can never arise, because each state can only be 
reached in one way.  
 For many problems, however, repeated states are unavoidable. This wi ll include all 
problems where the transitions are reversible, e.g.  
 
 The search trees for these problems are infinite, but if we can prune out the repeated 
states, we can cut the search tree down to a finite size, We effectively only generate a 
portion of the search tree that matches the state space graph.  
 Avoiding Repeated States:  There are three principal approaches for dealing with 
repeated states:  
 Never return to the state you have just come from  
The node expansion function must be prevented from gener ating any 
node successor that is the same state as the node’s parent.  
 Never create search paths with cycles in them  
The node expansion function must be prevented from generating  
any node successor that is the same state as any of the node’s 
ancestors.  
 Never generate states that have already been generated before  
This requires that every state ever generated is remembered, potentially 
resulting in space complexity of O(bd). 
 Comparing the Uninformed Search Algorithms:  We can now summarize the 
properties of our five uninformed search strategies:          
  
 Simple BFS and BDS are complete and optimal but expensive with respect to space 
and time.  
 DFS requires much less memory if the maximum tree depth is limited, but has no 
guarantee of finding any solution, le t alone an optimal one. DLS offers an 
improvement over DFS if we have some idea how deep the goal is.  
 The best overall is DFID which is complete, optimal and has low memory 
requirements, but still exponential time.  
 Informed search  
 Informed search use s some kind of evaluation function to tell us how far each 
expanded state is from a goal state, and/or some kind of heuristic function to help us 
decide which state is likely to be the best one to expand next.  
 The hard part is to come up with good evaluati on and/or heuristic functions. Often 
there is a natural evaluation function, such as distance in miles or number objects in 
the wrong position.  
 Sometimes we can learn heuristic functions by analyzing what has worked well in 
similar previous searches.  
 The simplest idea, known as greedy best first search, is to expand the node that is 
already closest to the goal, as that is most likely to lead quickly to a solution. This is 
like DFS in that it attempts to follow a single route to the goal, only attempting to  try 
a different route when it reaches a dead end. As with DFS, it is not complete, not 
optimal, and has time and complexity of O(bm). However, with good heuristics, the 
time complexity can be reduced substantially.  
 Branch and Bound:  An enhancement of backtracking.  
 Applicable to optimization problems.   For each node (partial solution) of a state -space tree, computes a bound on the value 
of the objective function for all descendants of the node (extensions of the partial 
solution).  
 Uses the bound for:  
 Ruling out certain nodes as “nonpromising” to prune the tree – if a node’s 
bound is not better than the best solution seen so far.  
 Guiding the search through state -space.  
 The search path at the current node in a state -space tree can be terminated for any one 
of the following three reasons:  
 The value of the node’s bound is not better than the value of the best solution 
seen so far.  
 The node represents no feasible solutions because the constraints of the 
problem are already violated.  
 The subset of feasible solutions represented by the node consists of a single 
point and hence we compare the value of the objective function for this 
feasible solution with that of the best solution seen so far and update the latter 
with the former if the new solution is better.  
 Best-First branch -and-bound:  
 A variation of backtracking.  
 Among all the nonterminated leaves, called as the live nodes, in the current 
tree, generate all the children of the most promising node, instead of 
generation a single child of the last promising node as i t is done in 
backtracking.  
 Consider the node with the best bound as the most promising node.  
 A* Search:  Suppose that, for each node n in a search tree, an evaluation function f(n) 
is defined as the sum of the cost g(n) to reach that node from the start sta te, plus an 
estimated cost h(n) to get from that state to the goal state. That f(n) is then the 
estimated cost of the cheapest solution through n.  
 A* search, which is the most popular form of best -first search, repeatedly picks the 
node with the lowest f(n ) to expand next. It turns out that if the heuristic function h(n) 
satisfies certain conditions, then this strategy is both complete and optimal.  
 In particular, if h(n) is an admissible heuristic, i.e. is always optimistic and never 
overestimates the cost to reach the goal, then A* is optimal.   The classic example is finding the route by road between two cities given the straight 
line distances from each road intersection to the goal city. In this case, the nodes are 
the intersections, and we can simply use the straight line distances as h(n).  
 Hill Climbing / Gradient Descent:  The basic idea of hill climbing is simple: at each 
current state we select a transition, evaluate the resulting state, and if the resulting 
state is an improvement we move there, otherw ise we try a new transition from where 
we are.  
 We repeat this until we reach a goal state, or have no more transitions to try. The 
transitions explored can be selected at random, or according to some problem specific 
heuristics.  
 In some cases, it is possi ble to define evaluation functions such that we can compute 
the gradients with respect to the possible transitions, and thus compute which 
transition direction to take to produce the best improvement in the evaluation 
function.  
 Following the evaluation gr adients in this way is known as gradient descent.  
 In neural networks, for example, we can define the total error of the output activations 
as a function of the connection weights, and compute the gradients of how the error 
changes as we change the weights.  By changing the weights in small steps against 
those gradients, we systematically minimize the network’s output errors.  
 Searching And -Or graphs  
 The DFS and BFS strategies for OR trees and graphs can be adapted for And- Or trees  
 The main difference lies  in the way termination conditions are determined, since all 
goals following an And node must be realized, whereas a single goal node following 
an Or node will do  
 A more general optimal strategy is AO* (O for ordered) algorithm  
 As in the case of the A* algorithm, we use the open list to hold nodes that have been 
generated but not expanded and the closed list to hold nodes that have been expanded  
 The algorithm is a variation of the original given by Nilsson 
 It requires that nodes traversed in the tree be lab eled as solved or unsolved in the 
solution process to account for And node solutions which require solutions to all 
successors nodes.  
 A solution is found when the start node is labeled as solved   The AO* algorithm  
 Step 1: Place the start node s on open  
 Step 2: Using the search tree constructed thus far, compute the most promising 
solution tree T 0 
 Step 3:Select a node n that is both on open and a part of T 0. Remove n from 
open and place it on closed  
 Step 4: If n ia terminal goal node, label n as solved. If th e solution of n results 
in any of n’s ancestors being solved, label all the ancestors as solved. If the 
start node s is solved, exit with success where T 0 is the solution tree. Remove 
from open all nodes with a solved ancestor  
 Step 5: If n is not a solvabl e node, label n as unsolvable. If the start node is 
labeled as unsolvable, exit with failure. If any of n’s ancestors become 
unsolvable because n is, label them unsolvable as well. Remove from open all 
nodes with unsolvable ancestors  
 Otherwise, expand node  n generating all of its successors. For each such 
successor node that contains more than one subproblem, generate their 
successors to give individual subproblems. Attach to each newly generated 
node a back pointer to its predecessor. Compute the cost esti mate h* for each 
newly generated node and place all such nodes thst do not yet have 
descendents on open. Next recomputed the values oh h* at n and each 
ancestors of n 
 Step 7: Return to step 2  
 It can be shown that AO* will always find a minimum -cost solutio n tree if one exists, 
provided only that h*(n) ≤ h(n), and all arc costs are positive. Like A*, the efficiency 
depends on how closely h* approximates h 
 
 Constraint Satisfaction Search  
 
 Search can  be used to solve problems that are limited by constraints, such  as the eight -
queens problem. Such problems are often known as Constraint  Satisfaction Problems , 
or CSPs . I  n this problem, eight queens must be placed on a chess board in such a  way that no 
two queens are on the same diagonal, row, or column. If we use  traditional chess 
board notation, we mark the columns with letters from a  to g and the rows with 
numbers from 1 to 8. So, a square can be referred to  by a letter and a number, such as 
a4 or g7.   
 This kind of problem is known as a constraint satisfaction problem (CSP)  because a 
solution must be found that satisfies the constraints.   
 In the case of the eight -queens problem, a search tree can be built that represents  the 
possible positions of queens  on the board.  One way to represent this is to have a tree 
that is 8- ply deep, with a branching  factor of 64 for the first level, 63 for the next 
level, and so on, down to  57 for the eighth level.   
 A goal node in this tree is one that satisfies the constra ints that no two queens can be 
on the same diagonal, row, or column.   
 An extremely simplistic approach to solving this problem would be to analyze  every 
possible configuration until one was found that matched the  constraints.   
 A more suitable approach to s olving the eight -queens problem would be to  use depth -
first search on a search tree that represents the problem in the  following manner:   
 The first branch from the root node would represent the first choice of a  square 
for a queen. The next branch from the se nodes would represent  choices of 
where to place the second queen.  
 The first level would have a branching factor of 64 because there are 64 
possible  squares on which to place the first queen.  The next level would have 
a somewhat lower branching factor b ecause once a queen has been placed,  the 
constraints can be used to determine possible squares upon which the  next 
queen can be placed.  
 The branching factor will decrease as the algorithm  searches down the tree. At 
some point, the tree will terminate  because the path being followed will lead to 
a position where no more  queens can be placed on legal squares on the board, 
and there are still some  queens remaining.    
  
In fact, because each row and each column must contain exactly one queen,  the branching 
factor can be significantly reduced by assuming that the first  queen must be placed in row 1, 
the second in row 2, and so on. In this way,  the first level will have a branching factor of 8 (a 
choice of eight squares on  which the first queen can be placed), the next 7, the next 6, and so 
on. 
 The search tree can be further simplified as each queen placed on the  board “uses up” 
a diagonal, meaning that the branching factor is only 5 or 6 after the first choice has 
been made, depending on whether the  first queen is placed on an edge of the board 
(columns a or h) or not.  
 The next level  will have a branching factor of about 4, and the next may have a 
branching factor of just 2, as shown in Fig 6.1.  
 The arrows in Fig 6.1 show the squares to which each queen can move.   
 Note that no queen can move to a square that is already occupied by  another queen.     
  
       
 
 In Fig 6.1, the first queen was placed in column a of row 8, leaving six choices for the 
next row. The second queen was placed in column  d of row  7, leaving four choices for 
row 6. The third queen was placed in column f in row 6, leaving just two choices 
(column c or column h) for row 5.   
 Using knowledge like this about the problem that is being solved can help to 
significantly reduce the size of the search tree and thus improve the efficiency  of the 
search solution.  
 A solution will be found when the algorithm reaches depth 8 and successfully  places 
the final queen on a legal square on the board.  
 A goal node  would be a path containing eigh t squares such that no two squares shared 
a diagonal, row, or column.   
 One solution to the eight -queens problem is shown in  above Fig .  
 Note that in this solution, if we start by placing queens on squares e8, c7,  h6, and then 
d5, once the fourth queen has be en placed, there are only two  choices for placing the 
fifth queen (b4 or g4). If b4 is chosen, then this  leaves no squares that could be chosen 
for the final three queens to satisfy  the constraints. If g4 is chosen for the fifth queen, 
as has been done in Fig 6.2, only one square is available for the sixth queen (a3), and 
the final  two choices are similarly constrained. So, it can be seen that by applying the  constraints appropriately, the search tree can be significantly reduced for  this 
problem.   
 Using ch ronological backtracking in solving the eight -queens problem  might not be 
the most efficient way to identify a solution because it will  backtrack over moves that 
did not necessarily directly lead to an error, as  well as ones that did. In this case, 
nonchro nological backtracking, or  dependency -directed backtracking  could be more 
useful  because it could identify the steps earlier in the search tree that caused the 
problem further down the t ree. 
 
Forward Checking  
 In fact, backtracking can be augmented in solving problems like the eightqueens  
problem by using a method called forward checking.  
 As each  queen is placed on the board, a forward- checking mechanism is used to 
delete from the set of possible future choices any that have been rendered impossible 
by placing the queen on that square.  
 For example, if a queen is  placed on square a1, forward checking will remove all 
squares in row 1, all  squares in column a, and also squares b2, c3, d4, e5, f6, g7, and 
h8.  
 In this  way, if placing a queen on the board r esults in removing all remaining squares, 
the system can immediately backtrack, without having to attempt  to place any more 
queens.  
 This can often significantly improve the performance  of solutions for CSPs such as 
the eight -queens problem.  
 
Most -Cons trained Variables  
 A further improvement in performance can be achieved by using the most -constrained  
variable heuristic.  
 At each stage of the search, this heuristic  involves working with the variable that has 
the least possible number of  valid choices.   In the case of the eight -queens problem, this might be  achieved by considering the 
problem to be one of assigning a value to eight  variables, a through h. Assigning 
value 1 to variable a means placing a  queen in square a1.  
 To use the most constrained varia ble heuristic with this  representation means that at 
each move we assign a value to the variable  that has the least choices available to it. 
Hence, after assigning a = 1, b = 3,  and c = 5, this leaves three choices for d, three 
choices for e, one choice fo r f, three choices for g, and three choices for h. Hence, our 
next move is to place a queen in column f.  
 This heuristic is perhaps more clearly understood in relation to the mapcoloring  
problem. It makes sense that, in a situation where a particular  countr y can be given 
only one color due to the colors that have been assigned to its neighbors, that country 
be colored next.  
 The most -constraining variable heuristic is similar in that it involves  assigning a value 
next to the variable that places the greatest number of  constraints on future variables.   
 The least -constraining value heuristic is perhaps more intuitive than the  two already 
presented in this section.  
 This heuristic involves assigning a  value to a variable that leaves the greatest number 
of choices  for other variables.  
 This heuristic can be used to make n -queens problems with  extremely large values of 
n quite solvable.  
 
Example: Cryptographic Problems  
 The constraint satisfaction procedure is also a useful way to solve problems  such as 
cryptographic problems. For example:  
FORTY  
+ TEN  
+ TEN  
SIXTY  
Solution:  
29786 + 850 
+ 850 
31486 
 This cryptographic problem can be solved by using a Generate and Test  method, 
applying the following constraints:   
 Each letter represents exactly one number.  
 No two letter s represent the same number.  
 Generate and Test is a brute -force method,  which in this case involves cycling 
through all possible assignments of  numbers to letters until a set is found that meets 
the constraints and solves  the problem.  
 Without using constra ints, the method would first start by attempting to  assign 0 to all 
letters, resulting in the following sum:  
00000 
+ 000 
+ 000 
00000 
 Although this may appear to be a valid solution to the problem, it does not  meet the 
constraints laid down that specify tha t each letter can be assigned  only one number, 
and each number can be assigned only to one letter.  
 Hence, constraints are necessary simply to find the correct solution to the  problem. 
They also enable us to reduce the size of the search tree.  
 In this  case, for example, it is not necessary to examine possible solutions where  two 
letters have been assigned the same number, which dramatically  reduces the possible 
solutions to be examined.  
 
 
 
 Heuristic Repair   Heuristics can be used to improve performance of solutions to constraint  satisfaction 
problems.  
 One way to do this is to use a heuristic  repair method, which involves generating a 
possible solution (randomly,  or using a heuristic to generate a position that is close to 
a solution)  and then making change s that reduce the distance of the state  from the 
goal.  
 In the case of the eight -queens problem, this could be done using the minconflicts  
heuristic.  
 To move from one state to another state that is likely to be  closer to a solution using 
the min -conflicts heuristic, select one queen that  conflicts with another queen (in 
other words, it is on the same row, column,  or diagonal as another queen).  
 Now move that queen to a square where it  conflicts with as few queens as possible. 
Continue with another queen.  To see how this method would work, consider the 
starting position shown  in Fig 6.3. 
 
  
  
      
 
 
  
 
 
 
 
 This starting position has been generated by placing the queens such that there  are no 
conflicts on rows or columns. The only conflict here is that t he queen in column 3 (on 
c7) is on a diagonal with the queen in column h (on h2).   
 To move toward a solution, we choose to move the queen that is on column h. 
 We will only ever apply a move that keeps a queen on the same column because we 
already know that  we need to have one queen on each column.   
 Each square in column h has been marked with a number to show how  many other 
queens that square conflicts with. Our first move will be to move the queen on column 
h up to row 6, where it will conflict only with one queen. Then we arrive a t the 
position shown in below  Fig  
 Because we have created a new conflict with the queen on row 6 (on f6),  our next 
move must be to move this queen. In fact, we can move it to a  square where it has 
zero conflicts. This means the pro blem has been solved,  and there are no remaining 
conflicts.  
 This method can be used not only to solve the eight -queens problem but  also has been 
successfully applied to the n -queens problem for extremely  large values of n. It has 
been shown that, using thi s method, the 1,000,000 queens problem can be solved in an 
average of around 50 steps.  
 Solving the 1,000,000- queens problem using traditional search techniques  would be 
impossible because it would involve searching a tree with a  branching factor of 1012.   
  
  Local Search and Metaheuristics  
 Local search methods work by starting from some initial configuration  (usually 
random) and making small changes to the configuration until a  state is reached from 
which no better state can be achieved.   
 Hill climbing is  a good example of a local search technique.  
 Local search techniques, used in this way, suffer from the same problems as hill 
climbing and, in particular,  are prone to finding local maxima that are not the best 
solution possible.  
 The methods used by local search techniques are known as metaheuristics .  
 Examples of metaheuristics include simulated annealing,  tabu search, genetic 
algorithms , ant colony  optimization, and neural networks.   
 This kind of search method is also kn own as local optimization because it  is 
attempting to optimize a set of values but will often find local maxima  rather than a 
global maximum.   
 A local search technique applied to the problem of allocating teachers to classrooms  
would start from a random po sition and make small changes until a  configuration was 
reached where no inappropriate allocations were made.  
 Exchanging Heuristics  
 The simplest form of local search is to use an exchanging heuristic.   An exchanging heuristic moves from  one state to anothe r by exchanging one 
or more variables by giving them different values. We saw this in solving the  
eight -queens problem as heuristic repair.  
 A k-exchange is considered to be a  method where k variables have their values 
changed at each step.  
 The heuristic  repair method we applied to the eight -queens problem was 2-
exchange.  
 A k-exchange can be used to solve the traveling salesman problem. A tour  (a 
route through the cities that visits each city once, and returns to the  start) is 
generated at random. Then, if we use 2- exchange, we remove two  edges from 
the tour and substitute them for two other edges. If this pro duces  a valid tour 
that is shorter than the previous one, we move on from  here. Otherwise, we go 
back to the previous tour and try a different set of  substitutions.  
 In fact, using k = 2 does not work well for the traveling salesman problem,  
whereas using k = 3 produces good results.  
 Using larger numbers of k will  give better and better results but will also 
require more and more iterations.  
 Using k = 3 gives reasonable results and can be implemented efficiently. It  
does, of course, risk finding local maxima, as is often the case with local  
search methods.  
 Iterated Local Search  
 Iterated local search techniques attempt to overcome the problem of local  
maxima by running the optimization procedure repeatedly, from different  
initial states.  
 If used with sufficient iterations, this kind of method will  almost always find a 
global maximum.   
 The aim, of course, in running methods like this is to provide a very good  
solution without needing to exhaustively search the entire problem space.   
 In problems such as the traveling salesman problem, where the search  space 
grows extremely quickly as the number of cities increases, results  can be 
generated that are good enough (i.e., a local maximum) without  using many 
iterations, where a perfect solution would be impossible to  find (or at least it 
would be impossible to guarantee a perfect solution even one iteration of local 
search may  happen upon the global maximum ).  Tabu Search  
 Tabu search is a metaheuristic that uses a list of states that have already  been 
visited to attempt to avoid repeating paths.  
 The tabu search metaheuristic  is used in combination with another heuristic 
and operates on the  principle that it is worth  going down a path that appears to 
be poor if it  avoids following a path that has already been visited.  
 In this way, tabu search is able to avoid local maxima.  
Simulated Annealing  
 Annealing is a process of producing very strong glass or metal, which  involves 
heating the material to a very high temperature and then allowing it to cool very 
slowly.  
 In this way, the atoms are able t o form the most stable  structures, giving the material 
great strength.   
 Simulated annealing is a local search metaheuristic based on this method and is an 
extension of a process called metropolisMonteCarlo simulation .  
 Simulated annealing is applied to a m ulti-value combinatorial problem  where values 
need to be chosen for many variables to produce a particular  value for some global 
function, dependent on all the variables in the system.   
 This value is thought of as the energy of the system , and in general t he aim of 
simulated annealing is to find a minimum energy for a system.   
 Simple Monte Carlo simulation is a method of learning information (such  as shape) 
about the shape of a search space. The process involves randomly  selecting points 
within the search s pace.  
 An example of its use is as follows:  A square is partially contained within a circle. 
Simple Monte Carlo simulation  can be used to identify what proportion of the square 
is within the circle  and what proportion is outside the circle. This is done by  randomly  
sampling points within the square and checking which ones are within the  circle and 
which are not.   
 Metropolis Monte Carlo simulation extends this simple method as follows:  Rather 
than selecting new states from the search space at random, a new  state is chosen by 
making a small change to the current state.   If the new  state means that the system as a whole has a lower energy than it did in the  
previous state, then it is accepted.  
 If the energy is higher than for the previous  state, then a probabi lity is applied to 
determine whether the new state  is accepted or not. This probability is called a 
Boltzmann acceptance criterion  and is calculated as follows:  e(_dE/T) where T is the 
current temperature of the system, and dE is the increase in energy tha t has been 
produced by moving from the previous state to the  new state.  
 The temperature in this context refers to the percentage of steps  that can be taken that 
lead to a rise in energy:  At a higher temperature, more  steps will be accepted that lead 
to a rise in energy than at low temperature.  
 To determine whether to move to a higher energy state or not, the probability 
e(_dE/T) is calculated, and a random number is generated between 0 and 1. If this 
random number is lower than the probability function, th e new state is accepted. In 
cases where the increase in energy is very high, or  the temperature is very low, this 
means that very few states will be accepted that involve an increase in energy, as 
e(_dE/T) approaches zero.   
 The fact that some steps are all owed that increase the energy of the system  enables the 
process to escape from local minima, which means that simulated annealing often can 
be an extremely powerful method for solving complex problems with many local 
maxima.   
 Some systems use e(_dE/kT) as the probability that the search will  progress to a state 
with a higher energy, where k is Boltzmann’s constant  (Boltzmann’s constant is 
approximately 1.3807 _ 10_23 Joules per Kelvin).  
 Simulated annealing usesMonte Carlo simulation to identify the most sta ble state (the 
state with the lowest energy) for a system.  
 This is done by running successive iterations of metropolis Monte Carlo simulation, 
using progressively  lower temperatures. Hence, in successive iterations, fewer and 
fewer  steps are allowed that lead to an overall increase in energy for the system.  
 A cooling schedule (or annealing schedule ) is applied, which determines  the manner 
in which the temperature will be lowered for successive iterations.   
 Two popular cooling schedules are as follows:  
Tnew = Told _ dT 
Tnew = C _ Told (where C < 1.0)   The cooling schedule is extremely important, as is the choice of the number  of steps 
of metropolis Monte Carlo simulation that are applied in each iteration.   
 These help to determine whether the system will be t rapped by local  minima (known 
as quenching). The number of times the metropolis  Monte Carlo simulation is applied 
per iteration is for later iterations.  
 Also important in determining the success of simulated annealing are the  choice of the 
initial temperat ure of the system and the amount by which  the temperature is 
decreased for each iteration.  
 These values need to be  chosen carefully according to the nature of the problem being 
solved.  When the temperature, T, has reached zero, the system is frozen, and i f the  
simulated annealing process has been successful, it will have identified a  minimum 
for the total energy of the system.   
 Simulated annealing has a number of practical applications in solving problems  with 
large numbers of interdependent variables, suc h as circuit design.   
 It has also been successfully applied to the traveling salesman problem.  
 Uses of Simulated Annealing 
 Simulated annealing was invented in 1983 by Kirkpatrick, Gelatt, and Vecchi.   
 It was first used for placing VLSI* components on a cir cuit board.   
 Simulated annealing has also been used to solve the traveling salesman 
problem, although this approach has proved to be less efficient than using 
heuristic methods that know more about the problem.  
 It has been used much more successfully in s cheduling problems and other 
large combina torial problems where values need to be assigned to a large 
number of variables  to maximize (or minimize) some function of those 
variables.  
 Real -Time A*  
 Real-time A* is a variation of A*.  
 Search continues  on the basis of choosing paths that have minimum values of f(node) 
= g(node) + h(node). However, g(node) is the distance of the node from the  current 
node, rather than from the root node.  
 Hence, the algo rithm will backtrack if the cost of doing so plus the estimated cost of 
solving the problem  from the new node is less than the estimated cost of solving the 
problem  from the current node.   Implementing real -time A* means maintaining a hash table of previous ly visited 
states with their h(node) values.  
Iterative -Deepening A* (IDA*)  
 By combining iterative -deepening with A*, we produce an algorithm that is  optimal 
and complete (like A*) and that has the low memory requirements  of depth -first 
search.   
 IDA* is a form of iterative -deepening search where successive iterations  impose a 
greater limit on f(node) rather than on the depth of a node.  
 IDA* performs well in problems where the heuristic value f (node) has relatively  few 
possible values.  
 For example, using the Manhattan distance as a  heuristic in solving the eight -queens 
problem, the value of f (node) can  only have values 1, 2, 3, or 4.  
 In this case, the IDA* algorithm only needs to run through a maximum of four 
iterations, and it has a time complexity  not dissimilar from that of A*, but with a 
significantly improved space  complexity because it is effectively running depth -first 
search.  
 In cases such as the traveling salesman problem where the value of f (node)  is 
different for every state, the IDA* met hod has to expand 1 + 2 + 3 + . . .  + n nodes = 
O(n2) where A* would expand n nodes.   
Propositional and Predicate Logic  
Logic is concerned with reasoning and the validity of arguments. In general,  in logic, we are 
not concerned with the truth of statements, but rather  with their validity . That is to say, 
although the following argument is  clearly logi cal, it is not something that we would consider 
to be true:   
All lemons are blue  
Mary is a lemon 
Therefore, Mary is blue  
This set of statements is considered to be valid because the conclusion (Mary is blue) follows 
logically from the other two statements,  which we  often call the premises . The reason that 
validity and truth can be separated in this way is simple: a  piece of a reasoning is considered to be valid if its conclusion is true in cases  where its premises are also true.  Hence, a valid set 
of statem ents such as the  ones above can give a false conclusion, provided one or more of the 
premises  are also false.   
We can say: a piece of reasoning is valid if it leads to a true conclusion in every  
situation where the premises are true.  
Logic is concerned wit h truth values . The possible truth values are true and false. 
These can be considered to be the fundamental units of logic, and almost all logic is 
ultimately concerned with these truth values.  
 
Logic is widely used in computer science, and particularly in Artificial  Intelligence. Logic is 
widely used as a representational method for Artificial  Intelligence. Unlike some other 
representations , logic allows us to easily reason about negatives  (such as, “this book is not 
red”) and disjunctions (“or” —such as,  “He’s either a soldier or a sailor”).   
Logic is also often used as a representational method for communicating concepts and 
theories within the Artificial Intelligence community. In addition, logic is used to represent 
language in systems that are able to understand and analyze human language.   
As we will see, one of the main weaknesses of traditional logic is its inability  to deal 
with uncertainty . Logical statements must be expressed in terms  of truth or falsehood—it is 
not possible to reason, in classical  logic, about  possibilities.  We will see different versions of 
logic such as modal logics that provide some ability to reason about possibilities, and also 
probabilistic  methods and fuzzy logic that provide much more rigorous ways to reason  in 
uncertain si tuations.  
 
Logical Operators  
 In reasoning about truth values, we need to use a number of operators , which can be 
applied to truth values.  
 We are familiar with several of these  operators from everyday language:  
I like apples and oranges.  
You can have a n ice cream or a cake.  If you come from France, then you speak French.  
 Here we see the four most basic logical operators being used in everyday  language. 
The operators are:  
 and 
 or 
 not 
 if . . . then . . . (usually called implies ) 
 One important  point to note  is that or is slightly different from the way we usually use 
it. In  the sentence, “You can have an icecream or a cake,” the mother is usually  
suggesting to her child that he can only have one of the items, but not both.  This is 
referred to as an exclusive -or in logic because the case where both  are allowed is 
excluded.  
 The version of or that is used in logic is called inclusive -or and allows the case with 
both options.   
 The operators are usually written using the following symbols, although  other 
symbols are sometimes used, according to the context:  
and ∧ 
or ∨ 
not ￢ 
implies → 
iff ↔ 
 Iff is an abbreviation that is commonly used to mean “if and only if.”   
 We see  later that this is a stronger form of implies that holds true if one thing implies 
another, and al so the second thing implies the first.   
 For example, “you can have an ice -cream if and only if you eat your dinner.”  It may 
not be immediately apparent why this is different from “you can have an icecream if 
you eat your dinner.” This is because most mothe rs really mean iff when they use if in 
this way.  
  Translating between English and Logic Notation  
 To use logic, it is first necessary to convert facts and rules about the real  world into 
logical expressions using the logical operators  
 Without a reason able amount of experience at this translation, it  can seem quite a 
daunting task in some cases.  
 Let us examine some examples.  First, we will consider the simple operators, ∧, ∨, and 
￢. 
 Sentences that use the word and in English to express more than one con cept,  all of 
which is true at once, can be easily translated into logic using the  AND operator, ∧. 
 For example:  “It is raining and it is Tuesday.”  might be expressed as:  R ∧ T, Where R 
means “it is raining” and T means “it is Tuesday.”  
 For example, if it is not necessary to discuss where it is raining,  R is probably enough.  
 If we need to write expressions such as “it is raining in New York” or “it is raining 
heavily” or even “it rained for 30 minutes on  Thursday,” then R will probably not 
suffice.  To expr ess more complex concepts like these, we usually use predicates.  
Hence, for example, we might translate “it is raining in New York” as:  N(R)  We 
might equally wel l choose to write it as:  R(N)   
 This depends on whether we consider the rain to be a property of  New  York, or vice 
versa. In other words, when we write N(R), we are saying that  a property of the rain is 
that it is in New York, whereas with R(N) we are  saying that a property of New York 
is that it is raining.  Which we use depends on the problem we are  solving. It is likely 
that if we  are solving a problem about New York, we would use R(N), whereas if we  
are solving a problem about the location of various types of weather, we  might use 
N(R). 
 Let us return nowto the logical operators. The expression “it is raining inNew  York, 
and I’meither getting sick or just very tired”can be expressed as follows:  R(N) ∧ (S(I) 
∨ T(I))  
 Here we have used both the ∧ operator, and the ∨ operator to express a collection  of 
statements. The statement can be broken down into tw o sections,  which is indicated 
by the use of parentheses.   The section in the parentheses  is S(I) ∨ T(I), which means “I’m either getting sick OR 
I’m very tired”. This  expression is “AND’ed”with the part outside the parentheses, 
which is R(N). 
 Finally, the ￢ operator is applied exactly as you would expect —to express  negation.  
 For example,  It is not raining in New York,  might be expressed as  ￢R(N)  
 It is important to get the ￢ in the right place. For example: “I’m either not  well or 
just very tired” would be translated as  ￢W(I) ∨ T(I) 
 The position of the ￢ here indicates that it is bound to W(I) and does not  play any 
role in affecting T(I).  
 Now let us see how the → operator is used. Often when dealing with logic  we are 
discussing rules, which express concepts  such as “if it is raining then  I will get wet.”   
 This sentence might be translated into logic as  R→W(I)  
 This is read “R implies W(I)” or “IF R THEN W(I)”. By replacing the symbols  R and 
W(I) with their respective English language equivalents, we can  see that this sentence 
can be read as  “raining implies I’ll get wet”  or “IF it’s raining THEN I’ll get wet.”   
 Implication can be used to express much more complex concepts than this.   
 For example, “Whenever he eats sandwiches that have pickles in them, he  ends up 
either asleep at his desk or singing loud songs”  might be translated as  
S(y) ∧ E(x, y) ∧ P(y)→A(x) ∨ (S(x, z) ∧ L(z))  
 Here we have used the following symbol translations:   S(y) means that y is a 
sandwich.                                                                                                     E(x, y) 
means that x (the man) eats y (the sandwich).  
P(y) means that y (the sandwich) has pickles in it.  
A(x) means that x ends up asleep at his desk.  
S(x, z) means that x (the man) sings z (songs).  
L(z) means that z (the songs) are loud.   The important thing to realize is that the choice of variables and predicates  is 
important, but that you can choose any variables and predicates that map well to your 
problem and that help you to solve the problem.  
 For example, in the example we have just looked at, we could perfectly well have 
used instead S→A ∨ L where S means “he eats a sandwich which has pickles in it,” A 
means “he  ends up asleep at his desk,” and L means “he sings loud songs.”   
 The choice of granularity is important, but there is no right or  wrong way  to make this 
choice. In this simpler logical expression, we have chosen to  express a simple 
relationship between three variables, which makes sense if  those variables are all that 
we care about —in other words, we don’t need to  know anything else  about the 
sandwich, or the songs, or the man, and the  facts we examine are simply whether or 
not he eats a sandwich with pickles,  sleeps at his desk, and sings loud songs.  
 The first translation we gave is  more appropriate if we need to examine these conc epts 
in more detail and reason more deeply about the entities involved.   
 Note that we have thus far tended to use single letters to represent logical  variables. It 
is also perfectly acceptable to use longer variable names, and thus to write expressions 
such as the following:  
Fish (x) ∧ living (x) →has_scales (x)  
 This kind of notation is obviously more useful when writing logical expressions  that 
are intended to be read by humans but when manipulated by a  computer do not add 
any value.  
 
Truth Tables  
 We can use variables to represen t possible truth values, in much the same  way that 
variables are used in algebra to represent possible numerical values.   
 We can then apply logical operators to these variables and can reason  about the way 
in which they behave.   
 It is usual to represent th e behavior of these logical operators using truth  tables .  
 A truth table shows the possible values that can be generated by  applying an operator 
to truth values.   
 Not  First of all, we will look at the truth table for not, ￢. 
 Not is a unary operator, which means it is applied only to one variable.  
 Its behavior is very simple:   
￢ true is equal to false  
￢ false is equal to true  
If variable A has value true, then ￢A has value false. 
If variable B has value false, then ￢B has v alue true. 
 These can be represented by a truth table,  
  
  
 And 
 Now, let us examine the truth table for our first binary operator —one which 
acts on two variables:  
A B A ∧ 
  
 ∧ is also called the conjunctive operator .  
 A ∧ B is the conjunction of A and B.   
 You can see that the only entry in the truth table for which A ∧ B is true is the 
one where A is true and B is true. If A is false, or if B is false, then A ∧ B is 
false. If both A and B are false, then A ∧ B is also false. 
 What do A and B mean? They can repr esent any statement, or proposition , 
that can take on a truth value.   For example, A might represent “It’s sunny,”  and B might represent “It’s 
warm outside.” In this case, A ∧ B would mean “It is sunny and it’s warm 
outside,” which clearly is true only if the two  component parts are true (i.e., if 
it is true that it is sunny and it is true that  it is warm outside).  
 Or 
 The truth table for the or operator, ∨ 
  
  
 ∨ is also called the disjunctive operator .  
 A ∨ B is the disjunction of A and B.  
 Clearly A ∨ B is true for any situation except when both A and B are false.  
 If A is true, or if B is true, or if both A and B are true, A ∨ B is true.   
 This table represents the inclusive -or operator.  
 A table to represent exclusive -or would have false in the final row. In other  
words, while A ∨ B is true if A and B are both true, A EOR B (A exclusive -or 
B) is false if A and B are both true.  
 You may also notice a pleasing symmetry between the truth tables for ∧ and ∨. 
This will become useful later, as will a number of ot her symmetrical  
relationships.  
 Implies  
 The truth table for implies (→) is a little less intuitive.  
  
  
  This form of implication is also known as material implication 
 In the statement A→B, A is the antecedent , and B is the consequent . The 
bottom two lines of the table should be obvious. If A is true and B is true, then 
A → B seems to be a reasonable thing to believe.  
 For example, if  A means “you live in France” and B means “You speak 
French,” then A→B corresponds to the statement “if you live in France, th en 
you speak French.”   
 Clearly, this statement is true ( A→B is true) if I live in France and I speak  
French ( A is true and B is true).  
 Similarly, if I live in France, but I don’t speak French ( A is true, but B is 
false), then it is clear that A→B is not true.  
 The situations where A is false are a little less clear. If I do not live in France  
(A is not true), then the truth table tells us that regardless of whether I  speak 
French or not (the value of B), the statement A→B is true. A→B is usually 
read as “A  implies B” but can also be read as “If A then B”  or “If A is true 
then B is true.”   
 Hence, if A is false, the statement is not really  saying anything about the value 
of B, so B is free to take on any value (as  long as it is true or false, of course!).   
 All of the following statements are valid:  
52 = 25 →4 = 4 (true →true)  
9 _ 9 = 123 →8 > 3 (false →true)  
52 = 25 →0 = 2 (false →false)  
 In fact, in the second and third examples, the consequent could be given  any 
meaning, and the statement would still be true. For example, the following 
statement is valid:  
52 = 25 →Logic is weird  
 Notice that when looking at simple logical statements like these, there does  
not need to be any real -world relationship between the antecedent and the  
consequent.  
 For logic to be useful, though, we tend to want the relationships  being 
expressed to be meaningful as well as being logically true.   
 iff  The truth table for iff (if and only if {↔}) is as follows:  
A  
  
 
 It can be seen that A ↔ B is true as long as A and B have the same value.  
 In other words, if one is true and the other false, then A ↔ B is false . 
Otherwise,  if A and B have the same value, A↔ B is true. 
 
 Complex Truth Tables  
 Truth tables are not limited to showing the values for single operators.  
 For example, a truth table can be used to display the possible values fo r A ∧ (B ∨C). 
 
  
 Note that for two variables, the truth table has four lines, and for three variables,  it has 
eight. In general, a truth table for n variables will have 2n lines.  
 The use of brackets in this expression is important. A ∧ (B ∨ C) is not the  same as ( A 
∧ B) ∨ C. 
 To avoid ambiguity, the logical operators are assigned precedence, as with 
mathematical operators.  
 The order of precedence that is used is as follows:  ￢, ∧, ∨,→,↔   Hence, in a statement such as  ￢A ∨ ￢B ∧ C, the ￢ operator has the great est 
precedence, meaning that it is most closely  tied to its symbols. ∧ has a greater 
precedence than ∨, which means that the  sentence above can be expressed as  (￢A) ∨ 
((￢B) ∧ C) 
 Similarly, when we write  ￢A ∨ B this is the same as  (￢A) ∨ B rather than  ￢(A ∨ B) 
 In general, it is a good idea to use brackets whenever an expression might  otherwise 
be ambiguous.  
 
Tautology  
 Consider the following truth table:  
 
  
 This truth table has a property that we have not seen before: the value of  the 
expression A∨￢A is true regardless of the value of A. 
 An expression like  this that is always true is called a tautology .  
 If A is a tautology, we write:  |=A   
 A logical expression that is a tautology is often described as being valid.  
 A valid expression is defined as being one that is true under any interpretation .  
 In other words, no matter what meanings and values we assign to the  variables in a 
valid expression, it will still be true.  
 For example, the following sentences are all valid:   
If wibble is true, then wibble is true. 
Either wibble is true, or wibble is not true.  
 In the language of logic, we can replace wibble with the symbol A, in which case 
these two statements can be rewritten as  
A→A A ∨ ￢A 
 If an expression is false in any interpretation, it is described as bei ng contradictory .  
 The following expressions are contradictory:  
A ∧ ￢A 
(A ∨ ￢A) →(A ∧ ￢A)  
Equivalence  
 Consider the following two expressions:  
A ∧ B 
B ∧ A 
 It should be fairly clear that these two expressions will always have the same  value 
for a given pair of values for A and B.  
 In otherwords,  we say that the first  expression is logically equivalent to the second 
expression.  
 We write this as  A ∧ B _ B ∧ A. This means that the ∧ operator is commutative . 
 Note that this is not the same as implication:  A ∧ B→B ∧ A, although this second 
statement is also true.  
 The difference is that if for two  expressions e1 and e2: e1 _ e2, then e1 will always 
have the same value as e2 for a given set of variables.  
 On the other hand, as we have seen, e1→e 2 is true if e1 is false and e2 is true.  
 There are a number of logical equivalences that are extremely useful.  
 The following is a list of a few of the most common:  
A ∨ A _ A 
A ∧ A _ A 
A ∧ (B ∧ C) _ (A ∧ B) ∧C (∧ is associative ) 
A ∨ (B ∨ C) _ (A ∨ B) ∨C (∨ is associative ) 
A ∧ (B ∨ C) _ (A ∧ B) ∨ (A ∧ C) (∧ is distributive over ∨) A ∧ (A ∨ B) _ A 
A ∨ (A ∧ B) _ A 
A ∧ true _ A 
A ∧ false _ false  
A ∨ true _ true 
A ∨ false _ A 
 All of these equivalences can be proved by drawing up the truth tables for  each side of 
the equivalence and seeing if the two tables are the same.  
 The following is a very important equivalence:  A→B _ ￢A ∨ B 
 We do not need to use the →  symbol at all —we can  replace it with a combination 
of ￢ and ∨.  
 Similarly, the following equivalences  mean we do not need to  use ∧ or↔: 
A ∧ B _ ￢(￢A ∨ ￢B)  
A↔ B _ ￢(￢(￢A ∨ B) ∨ ￢ (￢B ∨ A)) 
 In fact, any binary logical operator can be expressed using ￢ and ∨. This is a  fact 
that is employed in electronic circuits, where nor gates, based on an operator  called 
nor, are used. Nor is represented by ↓, and is defined as follows:  
A ↓ B _ ￢(A ∨ B) 
 Finally, the following equivalences are known as DeMorgan’s Laws : 
A ∧ B _ ￢(￢A ∨ ￢B)  
A ∨ B _ ￢(￢A ∧ ￢B)  
 By using these and other equivalences, logical expressions can be simplified.   
 For example , (C ∧ D) ∨ ((C ∧ D) ∧ E) can be simplified using the following rule:  A 
∨ (A ∧ B) _ A hence,  (C ∧ D) ∨ ((C ∧ D) ∧ E) _ C ∧ D  In this way, it is possible to eliminate subexpressions that do not contribute  to the 
overall value of the expression.  
 
Proposi tional Logic  
 There are a number of possible systems of logic.  
 The system we have been  examining so far is called propositional logic .  
 The language  that is used to express propositional logic is called the propositional 
calculus .  
 A logical system can be  defined in terms of its syntax (the alphabet of  symbols and 
how they can be combined), its semantics (what the symbols  mean), and a set of rules 
of deduction that enable us to derive one  expression from a set of other expressions 
and thus make arguments  and proofs.   
 Syntax  
 We have already examined the syntax of propositional calculus. The alphabet  
of symbols, _ is defined as follows  
∑ = {true, false, ￢,→, (, ), ∧, ∨,↔, p1, p2, p3, . . . , pn, . . . }  
 Here we have used set notation to define the possible va lues that are contained  
within the alphabet  ∑.  
 Note that we allow an infinite number of  proposition letters , or propositional 
symbols , p1, p2, p3, . . . , and so on.   
 More usually, we will represent these by capital letters P, Q, R, and so on,   
 If we need to represent a very large number of them, we will use  the subscript 
notation (e.g., p1). 
 An expression is referred to as a well-formed formula (often abbreviated as  
wff) or a sentence if it is constructed correctly, according to the rules of the  
syntax of  propositional calculus, which are defined as follows.  
 In these  rules, we use A, B, C to represent sentences. In other words, we 
define a sentence  recursively, in terms of other sentences.  
 The following are wellformed sentences:  
P,Q,R. . .  
true, false  (A) 
￢A 
A ∧ B 
A ∨ B 
A→B 
A↔ B 
 Hence, we can see that the following is an example of a wff:  
P ∧ Q ∨ (B ∧ ￢C) →A ∧ B ∨ D ∧ (￢E)  
 Semantics  
 The semantics of the operators of propositional calculus can be defined in  
terms of truth tables.  
 The meaning of P ∧ Q is defined as  “true when P is true and Q is also true.” 
 The meaning of symbols such as P and Q is arbitrary and could be ignored 
altogether if we were reasoning about pure logic.  
 In other words, reasoning about sentences such as P ∨ Q∧ ￢R is possible 
without c onsidering what P, Q, and R mean . 
 Because we are using logic as a representational method for artificial 
intelligence,  however, it is often the case that when using propositional logic, 
the meanings of these symbols are very important.  
 The beauty of this representation is that it is possible for a computer to reason 
about them in a very  general way, without needing to know much about the 
real world.   
 In other words, if we tell a computer, “I like ice cream, and I like chocolate,”  it 
might represent this st atement as A ∧ B, which it could then use to reason  
with, and, as we will see, it can use this to make deductions.  
 
 Predicate Calculus  
 Syntax   Predicate calculus allows us to reason about properties of objects and 
relationships  between objects.  
 In propositional calculus, we could express the  English statement “I like 
cheese” by A. This enables us to create constructs  such as ￢A, which means 
“I do not like cheese,” but it does not allow us to extract any information about 
the cheese, or me, or other t hings that I like.  
 In predicate calculus, we use predicates to express properties of objects. So the 
sentence “I like cheese” might be expressed as  L(me, cheese)  where L is a 
predicate that represents the idea of “liking.” Note that as well  as expressing a  
property of me, this statement also expresses a relationship  between me and 
cheese . This can be useful, as we will see, in describing environments for 
robots and other agents .  
 For example, a simple agent may  be concerned with the location of various 
blocks, and a statement about  the world might be  T(A,B) , which could mean: 
Block A is on top of Block B.  
 It is also possible to  make more general statements using the predicate 
calculus.  
 For example, to  express the idea that everyone likes cheese, we might s ay 
(x)(P(x) →L(x, C))     
 The symbol  is read “for all,” so the statement above could be read as “for  
every x it is true that if property P holds for x, then the relationship L holds  
between x and C,” or in plainer English: “every x that is a person likes  
cheese.” (He re we are interpreting P(x) as meaning “ x is a person” or, more  
precisely, “ x has property P.”)  
 Note that we have used brackets rather carefully in the statement above.   
 This statement can also be written with fewer brackets:  x P(x) →L(x, C) ,  
is called the universal quantifier . 
 The quantifier  can be used to express the notion that some values do have  a 
certain property, but not necessarily all of them:  (x)(L(x,C))   
 This statement can be read “there exists an x such that x likes cheese.”  
 This does not  make any claims about the possible values of x, so x could be a  
person, or a dog, or an item of furniture.  When we use the existential quantifier  in this way, we are simply saying that there is at least one value of x 
for which L(x,C) holds.   
 The following is true:  (x)(L(x,C)) →( x)(L(x,C)) , but the following is not:  
(x)(L(x,C)) →( x)(L(x,C))  
 Relationships between  and  
 It is also possible to combine the universal and existential quantifiers, such  as 
in the following statement:  (x) (y) (L(x,y)) .  
 This statement can be read “for all x, there exists a y such that L holds for x 
and y,” which we might interpret as “everyone likes something.”   
 A useful relationship exists between  and . Consider the statement “not  
everyone likes cheese.”  We could write t his as   
￢(x)(P(x) →L(x,C))   --------------   (1) 
 As we have already seen, A→B is equivalent to ￢A ∨ B. Using DeMorgan’s  
laws, we can see that this is equivalent to ￢(A ∧ ￢B). Hence, the statement   
(1) above, can be rewritten:    
￢(x)￢(P(x) ∧ ￢L(x,C))   -------------  (2) 
 This can be read as “It is not true that for all x the following is not true: x is a 
person and x does not like cheese.” If you examine this rather convoluted  
sentence carefully, you will see that it is in fact the same as “there exists an x 
such that  x is a person and x does not like cheese.”  Hence we can rewrite it as  
(x)(P(x) ∧ ￢L(x,C)) -------------  (3) 
 In making this transition from statement (2) to statement (3), we have utilized  
the following equivalence:  x   ￢(x)￢ 
 In an expression of the form ( x)(P(x, y)), the variable x is said to be  bound, 
whereas y is said to be free. This can be understood as meaning that  the 
variable y could be replaced by any other variable because it is free, and the 
expression would still have the same meaning, whereas if the variable x were 
to be replaced by some other variable in P(x,y), then the meaning of  the expression would be changed:  (x)(P(y, z))  is not equivalent to ( x)(P(x, y)), 
whereas ( x)(P(x, z)) is. 
 Note that a variable  can occur both bound and free in an expression, as in 
(x)(P(x,y,z) → (y)(Q(y,z)))  
 In this expression, x is bound throughout, and z is free throughout; y is free  in 
its first occurrence but is bound in ( y)(Q(y,z)). (Note that both occurrences  of 
y are bound here.)  
 Making this kind of change is known as substitution .  
 Substitution is  allowed of any free variable for another free variable.  
 Functions  
 In much the same way that functions can be used in mathematics, we can 
express an object that relates to another object in a specific way using  
functions.  
 For example, to repre sent the statement “my mother likes  cheese,” we might 
use L(m(me),cheese)   
 Here the function m(x) means the mother of x. Functions can take more  than 
one argument, and in general a function with n arguments is represented as 
f(x1, x2, x3, . . . , x n) 
 
First -Order Predicate Logic  
 The type of predicate calculus that we have been referring to is also called firstorder  
predicate logic (FOPL) . 
 A first-order logic is one in which the quantifiers   and  can be applied to objects or 
terms , but not to predica tes or functions.   
 So we can define the syntax of FOPL as follows. First,we define a term:   
 A constant is a term.   
 A variable is a term.  f(x1, x2, x3, . . . , xn) is a term if x1, x2, x3, . . . , xn are all 
terms.  
 Anything that does not meet the above desc ription cannot be a term.  
 For example, the following is not a term: x P(x). This kind of construction we  call a 
sentence or a well -formed formula (wff), which is defined as follows.    In these definitions, P is a predicate, x1, x2, x3, . . . , xn are term s, and A,B are wff ’s. 
The following are the acceptable forms for wff ’s:   
P(x1, x2, x3, . . . , xn)  
￢A 
A ∧ B 
A ∨ B 
A→B 
A↔ B 
(x)A 
(x)A 
 An atomic formula is a wff of the form P(x1, x2, x3, . . . , xn).  
 Higher order logics exist in which quantifiers can be applied to predicates  and 
functions, and where the following expression is an example of a wff:  
(P)(  x)P(x)  
 Soundness  
 We have seen that a logical system such as propositional logic consists of a  syntax, a 
semantics, and a set of rules of deduction.  
 A logical system also  has a set of fundamental truths, which are known as axioms.  
 The axioms  are the basic rules that are known to be true and from which all other 
theorems  within the system can be proved.   
 An axiom of propositional logic, for example, is  A→(B →A)   
 A theorem of a logical system is a statement that can be proved by applying the rules 
of deduction to the axioms in the system.   
 If A is a theorem, then we write  ├ A 
 A logical system is described as being sound if every theorem is logically  valid, or a 
tautology.   
 It can be proved by induction that both propositional logic and FOPL  are sound.  
 Completeness   A logical system is complete if every tautology is a theorem —in other  words, if 
every valid statement in the logic can be proved by applying the  rules of deduction 
to the axioms. Both propositional logic and FOPL are  complete.  
 Decidability  
 A logical system is decidable if it is possible to produce an algorithm that  will 
determine whether any wff is a theorem. In other words, if a logical  system is  
decidable, then a computer can be used to determine whether  logical expressions 
in that system are valid or not.  
 We can prove that propositional logic is decidable by using the fact that it is  
complete.  
 We can prove  that a wff A is a theorem by showing t hat it is a tautology. To show 
if a wff  is a tautology, we simply need to draw up a truth table for that wff and  
show that all the lines have true as the result. This can clearly be done 
algorithmically  because we know that a truth table for n values has 2n lines and is 
therefore finite, for a finite number of variables.   
 FOPL, on the other hand, is not decidable. This is due to the fact that it is  not 
possible to develop an algorithm that will determine whether an arbitrary  wff in 
FOPL is logically valid.  
 Monotonicity  
 A logical system is described as being monotonic if a valid proof in the system  
cannot be made invalid by adding additional premises or assumptions.   
 In other words, if we find that we can prove a conclusion C by applying rules of 
deduction t o a premise B with assumptions A, then adding additional  assumptions 
A￢ and B￢ will not stop us from being able to deduce C.  
 Monotonicity of a logical system can be expressed as follows:   
If we can prove {A, B} ├  C, 
then we can also prove: {A, B, A _, B_} ├  C. 
 In other words,  even adding contradictory assumptions does not stop us from 
making the  proof in a monotonic system.  
 In fact, it turns out that adding contradictory  assumptions allows us to prove 
anything, including invalid conclusions.  This makes s ense if we recall the line in the truth table for →, which shows  that false → true. By adding a contradictory 
assumption, we make our  assumptions false and can thus prove any conclusion.  
Modal Logics and Possible Worlds  
 The forms of logic that we hav e dealt with so far deal with facts and properties  of 
objects that are either true or false.  
 In these classical logics , we do not consider the possibility that things change or that 
things might not  always be as they are now.   
 Modal logics are an extensio n of classical logic that allow us to reason  about 
possibilities and certainties.  
 In other words, using a modal logic, we  can express ideas such as “although the sky is 
usually blue, it isn’t always”  (for example, at night). In this way, we can reason abo ut 
possible worlds.  
 A possible world is a universe or scenario that could logically come about.   
 The following statements may not be true in our world, but they are possible,  in the 
sense that they are not illogical, and could be true in a possible world:  
Trees are all blue.  
Dogs can fly.  
People have no legs.  
 It is possible that some of these statements will become true in the future,  or even that 
they were true in the past.  
 It is also possible to imagine an  alternative universe in which these statements are true 
now.  
 The following statements, on the other hand, cannot be true in any possible world:  
A ∧ ￢A 
(x > y) ∧ (y > z) ∧ (z > x)  
 The first of these illustrates the law of the excluded middle , which simply  states that a 
fact must be either true or false : it cannot be both true and false.  
 It also cannot be the case that a fact is neither true nor false. This is a  law of classical 
logic , it is possible to have a logical  system without the law of the excluded middle, 
and in which a fact can  be both true and false.   The second statement cannot be true by the laws of mathematics. We are  not 
interested in possible worlds in which the laws of logic and mathematics  do not hold.  
 A statement that may be true or false, depending on the situation, is called contingen t.  
 A statement that must always have the same truth value,  regardless of which possible 
world we consider, is noncontingent .  
 Hence,  the following statements are contingent:  
A ∧ B 
A ∨ B 
I like ice cream.  
The sky is blue.  
 The following statements are nonco ntingent:  
A ∨ ￢A 
A ∧ ￢A 
If you like all ice cream, then you like this ice cream.  
 Clearly, a noncontingent statement can be either true or false, but the fact  that it is 
noncontingent means it will always have that same truth value.   
 If a statement A is con tingent, then we say that A is possibly true, which  is written  ◊ 
A  
 If A is noncontingent, then it is necessarily true, which is written  □ A 
 Reasoning in Modal Logic  
 It is not possible to draw up a truth table for the operators ◊ and □  
 The following rules  are examples of the axioms that can be used to reason  in 
this kind of modal logic:  
□A→◊A 
□￢A→￢◊ A 
◊A→￢□  A  Although truth tables cannot be drawn up to prove these rules, you should be 
able to reason about them using your understanding of the meaning of  the ◊ 
and □ operators.  
Possible world representations  
 It describes method proposed by Nilsson which generalizes firtst order logic in the 
modeling of uncertain beliefs  
 The method assigns truth values ranging from 0 to 1 to possible worlds  
 Each set of possible worlds c orresponds to a different interpretation of sentences 
contained in a knowledge base denoted as KB  
 Consider the simple case where a KB contains only the single sentence S, S may be 
either true or false. We envision S as being true in one set of possible wor lds W 1 and 
false in another set W 2 . The actual world , the one we are in, must be in one of the 
two sets, but we are uncertain which one. Uncertainty is expressed by assigning a 
probability P to W 1 and 1 – P to W 2. We  can say then that the probability of  S being 
true is P  
 When KB contains L sentences, S 1,… SL , more sets of possible worlds are required to 
represent all consistent truth value assignments. There are 2L possible truth 
assignments for L sentences.  
 Truth Value assignments for the set {P. P →Q, Q} 
Consistent  Inconsistent  
P Q P → Q P Q P → Q 
True  True  True  True  True  False  
True  False  False  True  False  True  
False  True  True  False  True  False  
False  False  True  False  False  False  
 
 They are based on the use of the probability constraints  
         0 ≤ p i ≤ 1, and ∑ i  pi = 1  The consistent probability assignments are bounded by the hyperplanes of a certain 
convex hull  
 Dempster - Shafer theory  
 The Dempster -Shafer theory, also known as the theory of belief functions, is a 
generalization of the B ayesian theory of subjective probability.  
 Whereas the Bayesian theory requires probabilities for each question of interest, belief 
functions allow us to base degrees of belief for one question on probabilities for a 
related question. These degrees of beli ef may or may not have the mathematical 
properties of probabilities;  
 The Dempster -Shafer theory owes its name to work by A. P. Dempster (1968) and 
Glenn Shafer (1976), but the theory came to the attention of AI researchers in the 
early 1980s, when they we re trying to adapt probability theory to expert systems.  
 Dempster -Shafer degrees of belief resemble the certainty factors in MYCIN, and this 
resemblance suggested that they might combine the rigor of probability theory with 
the flexibility of rule -based s ystems.  
 The Dempster -Shafer theory remains attractive because of its relative flexibility.  
 The Dempster -Shafer theory is based on two ideas:  
 the idea of obtaining degrees of belief for one question from subjective 
probabilities for a related question,  
 Dempster's rule for combining such degrees of belief when they are based on 
independent items of evidence.  
 To illustrate the idea of obtaining degrees of belief for one question from subjective 
probabilities for another, suppose I have subjective probabilit ies for the reliability of 
my friend Betty. My probability that she is reliable is 0.9, and my probability that she 
is unreliable is 0.1. Suppose she tells me a limb fell on my car. This statement, which 
must true if she is reliable, is not necessarily fal se if she is unreliable. So her 
testimony alone justifies a 0.9 degree of belief that a limb fell on my car, but only a 
zero degree of belief (not a 0.1 degree of belief) that no limb fell on my car. This zero 
does not mean that I am sure that no limb fell  on my car, as a zero probability would; 
it merely means that Betty's testimony gives me no reason to believe that no limb fell 
on my car. The 0.9 and the zero together constitute a belief function.  
 To illustrate Dempster's rule for combining degrees of b elief, suppose I also have a 
0.9 subjective probability for the reliability of Sally, and suppose she too testifies, independently of Betty, that a limb fell on my car. The event that Betty is reliable is 
independent of the event that Sally is reliable, and we may multiply the probabilities 
of these events; the probability that both are reliable is 0.9x0.9 = 0.81, the probability 
that neither is reliable is 0.1x0.1 = 0.01, and the probability that at least one is reliable 
is 1 - 0.01 = 0.99. Since they both  said that a limb fell on my car, at least of them 
being reliable implies that a limb did fall on my car, and hence I may assign this event 
a degree of belief of 0.99. Suppose, on the other hand, that Betty and Sally contradict 
each other —Betty says that a  limb fell on my car, and Sally says no limb fell on my 
car. In this case, they cannot both be right and hence cannot both be reliable —only 
one is reliable, or neither is reliable. The prior probabilities that only Betty is reliable, 
only Sally is reliable , and that neither is reliable are 0.09, 0.09, and 0.01, respectively, 
and the posterior probabilities (given that not both are reliable) are 9 19 , 9 19 , and 1 
19 , respectively. Hence we have a 9 19 degree of belief that a limb did fall on my car 
(becau se Betty is reliable) and a 9 19 degree of belief that no limb fell on my car 
(because Sally is reliable).  
 In summary, we obtain degrees of belief for one question (Did a limb fall on my car?) 
from probabilities for another question (Is the witness reliable?). Dempster's rule 
begins with the assumption that the questions for which we have probabilities are 
independent with respect to our subjective probability judgments, but this 
independence is only a priori; it disappears when conflict is discerned betwe en the 
different items of evidence.  
 Implementing the Dempster -Shafer theory in a specific problem generally involves 
solving two related problems.  
 First, we must sort the uncertainties in the problem into a priori independent 
items of evidence.  
 Second, w e must carry out Dempster's rule computationally. These two 
problems and their solutions are closely related.  
 Sorting the uncertainties into independent items leads to a structure involving items of 
evidence that bear on different but related questions, a nd this structure can be used to 
make computations  
 This can be regarded as a more general approach to representing uncertainty than the 
Bayesian approach.  
 The basic idea in representing uncertainty in this model is:   Set up a confidence interval -- an int erval of probabilities within which the 
true probability lies with a certain confidence -- based on the Belief B and 
plausibility PL provided by some evidence E for a proposition P.  
 The belief brings together all the evidence that would lead us to believe  in P 
with some certainty.  
 The plausibility brings together the evidence that is compatible with P and is 
not inconsistent with it.  
 This method allows for further additions to the set of knowledge and does not 
assume disjoint outcomes.  
 If 
is the set of possible outcomes, then a mass probability , M, is defined for each 
member of the set 
 and t akes values in the range [0,1].  
The Null set, 
 , is also a member of 
 .  
 NOTE:  This deals w it set theory terminology that will be dealt with in a tutorial 
shortly. Also see exercises to get experience of problem solving in this important 
subject matter.  
M is a probability density function defined not just for 
 but for em all  subsets.  
So if 
 is the set { Flu (F), Cold (C), Pneumonia (P)  } then 
 is the set { 
 , {F}, {C }, 
{P}, {F , C}, {F , P}, {C , P}, {F , C, P} }  
 The confidence interval is then defined as [ B(E),PL(E)]  
where  
 
where 
 i.e. all the evidence that makes us believe in the correctness of P, and  
 
where 
 i.e. all the evidence that contradicts P.  
 Let X be the universal set: the set of all states  under consideration. The power set  is 
the set of all possible sub -sets of X, including the empty set  . For example,  if:  X =  
{a,b}then 2x = {Ǿ, {a},{b}, X} 
 The elements of the power set can be taken to represent propositions that one might 
be interested in, by containing all and only the states in which this proposition is true.   The theory of evidence assigns a belief  mass to each element of the power set. 
Formally, a function m: 2x→ [0, 1] is called a basic belief assignment  (BBA), when it 
has two properties.  
 First, the mass of the empty set is zero: m ( Ǿ) = 0  
 Second, the masses of the remaining members of the power set add up to a total of 1:  
∑ m(A) = 1 
          A€ 2x  
 The mass m(A) of a given member of the power set, A, expresses the proportion of all 
relevant and available evidence that supports the claim that the actual state belongs to 
A but to no particular su bset of A. The value of m(A) pertains only to the set A and 
makes no additional claims about any subsets of A, each of which have, by definition, 
their own mass.  
 From the mass assignments, the upper and lower bounds of a probability interval can 
be define d. This interval contains the precise probability of a set of interest (in the 
classical sense), and is bounded by two non -additive continuous measures called 
belief  (or support ) and plausibility : 
bel(A) ≤ P(A) ≤ pl(A)  
  
 
 
  
 
  
 Benefits of Dempster -Shafer Theory:  
 Allows proper distinction between reasoning and decision taking  
 No modeling restrictions (e.g. DAGs)  
 It represents properly partial and total ignorance  
 Ignorance is quantified:  
o low degree o f ignorance means  
- high confidence in results  
- enough information available for taking decisions  
o high degree of ignorance means  
          - low confidence in results  
          - gather more information (if possible) before taking decisions  
 Conflict is quantified:  
o low conflict indicates the presence of confirming information sources  
o high conflict indicates the presence of contradicting sources  
 Simplicity: Dempster’s rule of combination covers  
o combination of evidence  
o Bayes’ rule  
o Bayesian updating (conditioning)  
o belief revision (results from non -monotonicity)  
 DS-Theory is not very successful because:  
 Inference is less efficient than Bayesian inference  
 Pearl is the better speaker than Dempster (and Shafer, Kohlas, etc.)  
 Microsoft supports Bayesian Networks  
 The UAI community does not like „outsiders“  
  
 
Fuzzy Se
t Theory 
 
 
 What 
 is  Fuzzy  Set ? 
 
 • The word "
fuzzy" means " vagueness ". Fuzziness occurs when the 
boundary  of  a  piece  of  information  is  not  clear-cut. 
  
 • F
u z z y  s e t s   h a v e   b e e n   i n t r o d u c e d   b y  L o t f i  A .  Z a d e h  ( 1 9 6 5 )  a s  a n  
extension of  the  classical  notion of  set.  
 
 • Cla
ssical  set theory allows  the  membership   of  the  elements  in  the set 
in  binary  terms,  a  bivalent  condition -  an element  either  belongs  or 
does  not  belong  to  the  set. 
 
Fuzzy 
 set theory  permits  the gradual  assessment  of  the  membership
of elements in a set, described with  the aid of a membership function 
valued  in  the  real  unit  interval [0, 1]. 
 
 • Exam
ple:   
Words  like  young,  tall,  good ,  or  high  are  fuzzy.  
− Ther
e  is  no  single  quantitative value  which defines  the term young.  
− For  some peo
ple,  age 25 is young, and  for others, age 35  is  young.  
− The 
 concept  young  has  no  clean  boundary.  
− Age 1  is  definitely  
young  and  ag e 100  is  definitely  not  young;  
− Age 
35  has some possibility of being young and usually depends 
on  the  context  in  which  it  is  being  considered. 
  
 Introduction  
In  real  world,
  there  exists  much  fuzzy  knowledge;   
Knowledge that is vague, imprecise, uncertain, ambiguous, inexact , or 
probabilistic in nature.  
 
Hu
man  thinking  and  reasoning  frequently  involve fuzzy  information, 
originating from inherently inexact human concepts. Humans, can give 
satisfactory  answers,  which  are  probably  true.  
 
Ho
wever,  our  systems  are  unable to answer many questions. The reason 
is,  most systems  are  designed  based  upon  classical  set theory  andtwo-valued logic  which  is  unable  to  cope  with  unreliable  and  incomplete information  and  give  expert  opinions.  
 
W
e want, our systems should also be able to cope with unreliable and 
incomplete information and give expert opinions. Fuzzy sets   have  been
 
able  provide  solutions  to  many  real  world  problems. 
 
F
uzzy  Set  theory  is  an  extension  of  classical  set  theory   where  elements 
have  degrees  of  membership.  • Cla
ssical Set Theory 
A  Set  is  any  well  defined  collection  of  objects.  An  object  in a  set  is  
called  an  element  or  member  of  that  set.   
 
  − Sets are defin
ed by a simple statement  describing  whether a 
particular element having a certain property belongs to thatparticular  set.   
 
  − Classical 
 set   theory   enumerates   all  its  elements  using 
  
      A = { a 1 ,  a 2 ,  a 3 , 
 a4 ,
  . . . .  a n } 
 
If th
e elements   ai (i = 
 1, 2, 3, . . .  n )  of  a  set  
A  are  
subset  of 
universal  se
t  X,  
 then  
set  A  can
  be represented  for  all   elements
x ∈ X  by  its   char
acteristic function  
   
                           1   if   
x ∈  
X 
              µA (x) =   
                              
                              0   oth
erwise  
 
  − A  
set  A  is 
 well  described  by  a  function  called   charac
teristic 
function . 
This  function, d
efined  on  the universal space X, assumes : 
   a  value  of  1  for  those  elements  x  that belong to set A,   and  
   a  value  of  0  for  those  elements  x  that do not belong to set A. 
The  notations  used  to  express  these  mathematically  are  
 
 
   Α : Χ → [0, 1] 
    A(x)  = 1 ,  x  is a member of A                Eq
.(1) 
 
   A(x)  = 0 ,  x  is not a member of A  
 
Alternatively,  the 
set  A  c
an be  represented  for  all elements   x ∈ X
by its  characteristic funct
ion   µA (x)  
 de
fined as 
 
                             1    if   x ∈ X 
 
          µA (x) =  
                                             Eq.
(2) 
 
                             0    otherwise  
 
  − Thu
s  in  classical set theory   µA (x
)  h
as  only  the values 0('
false') 
and 1 ('
true'').  Such sets are called  crisp sets
.  
 
    
 • Fuzzy Se
t Theory 
 
Fuzzy 
 set  theory  is  an  extension  of  classical  set  theory  where 
elements  have  varying  degrees  of  membership.  A  logic  based on 
the two truth values, True and False, is sometimes inadequate when 
describing human reasoning.   Fuzzy 
 logic  uses the whole interval between 
0 (false) and 1 (true) to describe 
human reasoning.   
 
  − A  Fu
zzy Set   i
s any  set  that  allows  its  members  to  have  different 
degree  of  membership,  called  m
embership function ,  in
 the interval 
[0
 , 1]. 
 
  − The  degree
 of membership   or  trut
h is not same as probability;  
 fuzzy truth is not likelihood of some event or condition.  
 fuzzy truth represents membership in vaguely defined sets;  
 
  − Fu
zzy  logic  
 is derived from fuzzy set theory dealing  with  reasoning 
that is approximate rather than precisely deduced from classical predicate logic. 
 
  − Fu
zzy logic is capable of handling inherently imprecise concepts. 
 
  − Fu
zzy logic allows in linguistic form the set membership values to 
imprecise concepts like "slightly", "qu
ite" and "very".  
 
  − F
uzzy set theory defines Fuzzy Operators on Fuzzy Sets.  
   
  
• Crisp
  and  Non-Crisp Set  
 
 − As 
said before, in classical set theory,  the  characteristic 
function
µA(x
)  of  Eq.(2)  has only values 0 ('false')  and 1 ('true'').   
Such  sets  are  crisp
 sets.   
 
 − F
or Non-crisp sets  the characteristic  function   µA(x)  can 
be defined.  
 The characteristic 
 function  µA(x) of  Eq. (2) for the crisp set 
is 
generalized for the Non-crisp sets.  
 This g
eneralized characteristic function µA(x)
 of  Eq.(2)  is called 
mem
bership function .  
Such  Non-crisp
  sets  are  called  Fuzzy Se
ts. 
 
 − Crisp
 set theory is not capable of representing descriptions and 
classifications in many cases; In fact, Crisp set does not provide adequate representation for most cases. 
 
 
 − The pro
position of  Fuzzy Sets  are  motivated by the need to capture 
and represent real world data with uncertainty  due to imprecise 
measurement.  
 
 − The 
 uncertainties  are  also  caused  by  vagueness  in  the  language. 
  
 
     
 • Represe
ntation of  Crisp  and  Non-Crisp Set  
Example :  Classify students for a basketball team 
This example explains the grade of truth value. 
- tall stud
ents qu
alify and  no
t tall students  do
 not qualify 
- if students 1.8 m tall are  to be qualified, then 
     should  we exclude  a student who  is 1/10" less?  
or 
     should  we exclude  a student who is 1" shorter?  
 
   ■ Non-Cris
p Representation to repres ent the notion of a tall person. 
 
 
  
 
 
 
  
 
  
               Crisp logic                                        Non-crisp logic  
                   
Fig. 1  Set Representation – Degree or grade of truth 
 
A 
student of height 1.79m would belong to both tall and not tall sets 
with a particular degree of membership.  
 
As the height increases the membership grade within the tall set would 
increase whilst the membership grade within the not-tall set would decrease.   
 
 
    
 Degr
ee or grade of truth 
 
 
          
Not Tall            Tall 
  1 
 
 
 
   
  0 
  
                   
   1.8 m        Height  x Degree or
 grade of truth 
 
  
        
 Not Tall            Tall 
  1 
 
 
 
   
  0 
   
                    
 1.8 m        Height  x 
 
• Ca
pturing Uncertainty   
Instead of avoiding or ignoring unce rtainty, Lotfi Zadeh introduced Fuzzy 
Set theory that captures uncertainty.  
 
 ■ A fuzzy set is 
described by a mem
bership function  µA (x)   o
f  A.    
This  
membership  function  associates  to  each element  xσ ∈ X  a
nu
mber as   µA (xσ )  in the closed 
unit interval [0, 1
]. 
 
The number   µA (xσ )  represen
ts the deg
ree of membership  of xσ in A.  
 ■ The 
notation used for membership function  µA (x) o
f a fuzzy set   A  is 
 
     Α : Χ → [0, 1]  
 
 ■ Each
 membership function maps el ements of a given universal base 
set X , wh
ich is itself a crisp set, into real numbers in  [0, 1]  .   
 
 
  
  ■ Example  
 
  
 
 
 
         Fig. 2  Membership function of a Crisp set  C and Fuzzy set  F 
 
  ■ In th
e case of Crisp Sets  the members of a set are :  
 
      either out of the set, with membership of degree  " 0 ",  
      or in the set, with membership of degree "  1 ",  
 
Therefore
,    Cr
isp Sets ⊆   Fuzzy Sets   
In ot
her words, Crisp Sets are Special cases of Fuzzy Sets. 
 
 
 
    
    µ  
        
  
   
     
             µc (x)        
                               µF (x)  
  1 
  
                C 
                                  F        
0.5 
 
 
  0 
                                                                                               x 
• Exam
ples of Crisp and Non-Crisp Set 
 
Exa
mple 1:  Set of prime numbers  ( a c
risp set ) 
 
If we consid
er space X   
consisting of  natural numbers   
 ≤   12  
            ie  X = { 1
, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 } 
Th
en, the set of prime numbers could be described as follows.  
PRIME = 
{x contained in X | x is a prime number} = { 2, 3, 5
, 6, 7, 11 } 
 
Ex
ample 2:   Set of  SMA
LL  ( as 
 non-crisp set ) 
 
A Set  X  t
hat consists of  SMA
LL can
not be described ;   
for exam
ple 1 is 
a member of SMALL and  12  is not a mem
ber of  SMA
LL.  
 
Set A, as SMALL
, has un-sharp boundaries, can be characterized by a 
function that assigns a real number  from the closed interval from 0 to 1to
 
each element x in the set X.  
 
 
     
 Fuzzy Set 
 
A Fuzzy 
Set is any set that allows its members to have different degree 
of  membership,  called membership function,  in  the  interval [0 , 
1].  
 
 
 • D
efinition of Fuzzy set 
 
A fuzz
y set  A, d
efined in the universal space  X,  is a function def
ined 
in  X  which  assumes values  in  the 
 range  [0, 1
].  
 
A fu
zzy set  A is
 written as a set of pairs   {x, A
(x)}  as  
   
       A = {{x ,  A(x)}} ,   x in the set X  
where    
x   is 
 an  element  of  the universal space  X,   an
d  
  
       A(x)  is  the value 
 of  the  function  A  fo
r  this  element.  
 
Th
e value A(
x) is the mem
bership  grade  of th
e element  x in a 
fu
zzy set   A. 
 
  Ex
ample :   Set
  SM
ALL  in 
set X  consisting  of  natural numbers   ≤  to
 12. 
 
  Assum
e:  SMALL(1) =
 1,      SMALL(2) = 1,     SMALL(3) = 0.9,   SMALL(4) = 0.6, 
                    SMALL(5) = 0.4,  SMALL(6) = 0.3,  SMALL(7) = 0.2,  SMALL(8) = 0.1, 
                    SMALL(u) = 0 for u >= 9.  
 
  Then
,  following  the  notations  descr ibed  in  the  definition  above : 
Se
t SMALL =  {
{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},  {7, 0.2},  
                             {8, 0.1},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}} 
Note that a 
fuzzy set can be defined precisely by associating with 
each x ,  its  grade of membership in  SM
ALL. 
 
     
• Defini
tion of Universal Space  
Originally
 the universal space for fuzzy sets in fuzzy lo gic wa
s 
defined only on the inte gers.
  Now,  the universal space for fuzzy sets 
and fuzzy relations is defined with three numbers.  
 
Th
e first  two numbers specify the st art and end of the universal space, 
and  the  third ar gume
nt specifies the increment between elements. 
This  gives  the user more flexibility in choosing the universal space.  
 
Exa
mple  :   The fuzzy set  of 
 numbers, defined  in  the  universal space    
X = { 
xi 
} = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}    is presen
ted as  
SetOptio
n [FuzzySet,  UniversalSpace → {1, 12, 1}]  
  
  Fuzzy Me
mbership 
 
A fuzzy set A d
efined in the universal space  X is 
a function defined 
in  X  which assumes  values  in  the  range  [0, 1
].  
 
A f
uzzy set  A   is  w
ritten  as  a set of pairs  {x, A(x
)}. 
          A = {{x ,  A(x)}} ,   x in the set X  
                  whe
re x is an elem
ent of the universal space  X,   
and  
                  A(x)  is th
e value of the function  A for this element.  
 
Th
e  value   A(
x)  is
  the  deg
ree of membership   
of  the  element   x
in
  a  fuzzy  set   A. 
 
Th
e Graphic Interpretation  of fuzzy membership for the fuzzy sets : 
Small, Prime Numbers, Universal-space, Finite and Infinite 
UniversalSpace,  and  Empty  are  illustrated  in the  next  few  slides. 
  
 
     
• Gr
aphic Interpretation of Fuzzy Sets  SMALL 
 Th
e fuzzy set  SMALL  of  small  numbers, defined  in  the  universal space
X = { xi } 
= {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}    is p
resented as  
SetOptio
n [FuzzySet,  UniversalSpace → {1, 12, 1} ]  
 
Th
e Set  SMALL  in set X  
is : 
SMALL =   Fuzzy
Set {
{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},   
                                      {7, 0.2},     {8, 0.1},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}} 
 
Th
erefore SetSmall  is re
presented as  
Se
tSmall  = FuzzySet  [
{{1,1},{2,1}, {3,0.9}, {4,0.6}, {5,0.4},{6,0.3}, {7,0.2},
       {8, 0.1}, {9, 0},  {10, 0}, {11, 0}, {12, 0}}  , Uni
versalSpace  → {1
, 12, 1}] 
 
Fuzz
yPlot [ SMALL, AxesLable → {"X", "SMALL"}] 
  
 SMAL
L 
   
 
             
             
             
             
                          
             
             
             
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10     11      12   X 
  
                   Fi
g Graphic Interpretation of Fuzzy Sets SMALL  
0 .8
 
.2 .4 .6 1  
 • Gr
aphic Interpretation of Fuzzy Sets   PR
IME Num
bers 
  The fuzzy set  PRI
ME  numbers,  defined  in  the universal space    
X = { 
xi } 
= {1,
 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}    is presented as   
SetOptio
n [FuzzySet,  UniversalSpace → {1, 12, 1} ]  
 
Th
e Set  PRIM
E  in 
set X  is : 
PRI
ME =   FuzzySet  {{1, 0
}, {2, 1},  {3, 1},  {4, 0}, {5, 1}, {6, 0},  {7, 1}, {8, 0},   
                                          {9, 0}, {10, 0}, {11, 1}, {12, 0}} 
Th
erefore SetPrime  is rep
resented as  
Se
tPrime =  FuzzySet  [{{1,0
},{2,1}, {3,1}, {4,0}, {5,1},{6,0}, {7,1}, 
          {8, 0}, {9, 0},  {10, 0}, {11, 1}, {12, 0}}  , Uni
versalSpace  → 
{1, 12, 1}] 
 
FuzzyPl
ot [ PRIME, AxesLable → {"X", "PRIME"}] 
  
 PR
IME 
   
 
             
             
                          
             
             
                          
             
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10     11      12   X 
  
                   Fi
g Graphic Interpretation of Fuzzy Sets PRIME 
  
 
 
    
 0 .8 
.2 .4 .6 1 
 
• Gr
aphic Interpretation of Fuzzy Sets   UNIVERSALS
PACE  
 In an
y application of sets or fuzzy sets theory, all sets are subsets of 
a  fixed set called universal space or  universe of discourse denoted by X.  
Universal space X as a fuzzy set is a function equal to 1 for all elements. 
  
Th
e  fuzzy  set  UN
IVERSALSPACE   n
umbers,  defined   in   the   universal 
space  X = { 
xi } = {1,
 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}    is presented as   
SetOptio
n [FuzzySet,  UniversalSpace → {1, 12, 1} ]  
 
Th
e Set  UNIVERSALSPACE   in set
 X  is : 
 
UNI
VERSALSPACE =  FuzzyS
et {
{1, 1}, {2, 1},  {3, 1},  {4, 1}, {5, 1}, {6, 1},   
                                                               {7, 1},    {8, 1},  {9, 1}, {10, 1}, {11, 1}, {12, 1}} 
Th
erefore SetUni
versal   is represented as 
  
 Se
tUniversal  = FuzzySet  [{{1,
1},{2,1}, {3,1}, {4,1}, {5,1},{6,1}, {7,1}, 
          {8, 1}, {9, 1},  {10, 1}, {11, 1}, {12, 1}}  , Uni
versalSpace  → 
{1, 12, 1}] 
 
FuzzyPl
ot [ UNI
VERSALSPACE ,
 AxesLable → {"X", " UNIVERSAL SPA
CE "}
] 
U
NIVERSAL SPACE 
   
 
             
             
             
                          
             
             
             
             
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10     11      12   X 
Fi
g Graphic Interpretation of Fuzzy Set  UNIVERSALSPACE 
 
    
  0 .8
 
.2 .4 .6 1 
• Finite and Infinite Un
iversal Space 
 Universal sets can be 
finite or infinite.  
Any universal set is finite if it consists of a specific number of different 
elements, that is, if in counting the different elements of the set, the 
counting can come to an end, else the set is infinite. 
 
Exa
mples:  
1.  Let   N   be the universal space of the days of the week.  
            N = {Mo, Tu, We, Th, Fr, Sa, Su}.      N  is finite. 
2.  Let   M = {1, 3, 5, 7, 9, ...}.                       M  is infinite. 
3.  Let   L = {u | u is a lake in a city }.            L   is finite. 
     (Although it may be difficult to c
ount the number of lakes in a city,   
       but   L is still a finite universal set.)  
 • Gr
aphic Interpretation of Fuzzy Sets   EMP
TY 
  An
 empty set is a set that contains only elements with a grade of 
membership  equal  to 0.  
Example:  Let EMPTY be a set of peop le, in Minnesota, older than 120.  
The Empty set is also called the Null set. 
 
The  fuzzy  set
  EM
PTY ,
  defined   in   the   universal  space   
X = { x i } = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 1
2}   is 
presented as  
SetOptio
n [FuzzySet,  UniversalSpace → {1, 12, 1} ]  
 
Th
e Set  EMPTY   in set X  is :  
EM
PTY =   FuzzySet  {{1,
 0}, {2, 0},  {3, 0},  {4, 0}, {5, 0}, {6, 0},  {7, 0},  
                                                          {8, 0},  {9, 0}, {10, 0}, {11, 0}, {12, 0}} 
Th
erefore SetEmp
ty is represen
ted as  
S
etEmpty  = FuzzySet  [{{1,0
},{2,0}, {3,0}, {4,0}, {5,0},{6,0}, {7,0}, 
          {8, 0}, {9, 0},  {10, 0}, {11, 0}, {12, 0}}  , Univers
alSpace  → {
1, 12, 1}] 
 
FuzzyPl
ot [ EMPTY, AxesLable →  {"X", " UN
IVERSAL SPACE  "}] 
EMPTY 
   
 
             
             
                          
             
             
             
                          
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10     11      12   X 
  
                   Fi
g Graphic Interpretation of Fuzzy Set  EMPTY  
 
    
 
  0 .8 
.2 .4 .6 1  
  Fuzzy Operations 
 
A fuzzy 
set operations are the operations on fuzzy sets. The fuzzy set 
operations are generalization of crisp set operations. Zadeh [1965] 
formulated the fuzzy set theory in the terms of standard operations: 
Complement,   Union,    Intersection,   and    Difference.  
 
In this section, the graphical interpretation of  the  following  standard 
fuzzy set terms  and  the  Fuzzy Logic  operations  are  illustrated:  
 
Inclu
sion  : FuzzyIncl
ude [VERYSMALL, SMALL]  
Equali
ty :   FuzzyEQUA
LITY [SMALL, STILLSMALL]  
Complement  
: FuzzyNOT
SMALL = FuzzyCompliment [Small] 
Uni
on : Fu
zzyUNION = [SMALL  ∪  MEDIUM]  
Inte
rsection  : FUZZYI
NTERSECTON = [SMALL  ∩  MEDIUM] 
  
   
   
• In
clusion 
 
 Let  A  an
d  B 
 be  fuzzy sets defined in the same universal space  X.  
The fuzzy set A 
is included in the fuzzy set B  if 
and only if  for every xin 
the set X we
 have A(x) ≤  B(x)  
Ex
ample :   
Th
e  fuzzy  set  UN
IVERSALSPACE   n
umbers,  defined   in   the   universal 
space  X = { x i } = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1
1, 12}    is presented as 
 
SetOptio
n [FuzzySet,  UniversalSpace → {1, 12, 1} ]  
 
The fu
zzy set B  SMALL 
Th
e Set  SMALL   in set  X  is :  
SMALL =   Fuzzy
Set {
{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},   
                                      {7, 0.2},     {8, 0.1},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}} 
Th
erefore  SetSm
all   is rep
resented as   
Se
tSmall  = FuzzySet  [{
{1,1},{2,1}, {3,0.9}, {4,0.6}, {5,0.4},{6,0.3}, {7,0.2},
       {8, 0.1}, {9, 0},  {10, 0}, {11, 0}, {12, 0}}  , Uni
versalSpace  → {1
, 12, 1}] 
  
 
The fu
zzy set  A  VERYSMALL 
Th
e Set  VE
RYSMALL  in set X  is 
:  
VE
RYSMALL =   FuzzySet  
{{1, 1   },   {2, 0.8  },  {3, 0.7},   {4, 0.4},   {5, 0.2},   
                              {6, 0.1},  {7, 0 },     {8, 0 },    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}} 
Th
erefore SetVer
ySmall  is represen
ted as   
Se
tVerySmall =  FuzzySet  [{
{1,1},{2,0.8}, {3,0.7}, {4,0.4}, {5,0.2},{6,0.1}, 
{7,0}, {8, 0}, {9, 0},  {10, 0}, {11, 0}, {12, 0}} , U
niversalSpace  → 
{1, 12, 1}]
 
The Fuz
zy Operation  :     I
nclusion 
Include [VER
YSMALL, SMALL]  
Mem
bership Grade  
                 B       A 
   
                                                          
             
             
             
             
             
             
                          
             
             
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10     11     12   X 
Fig Graph
ic Interpretation of Fuzzy Inclusion 
 FuzzyPlot [SMALL, VERYSMA
LL]  
 
    
   0 1 
.2 .6
.4 .8
 
• Comparabilit
y 
 
Two fu
zzy sets  A  and  B  are comparable  
if the condition  A ⊂ B or B ⊂ A  holds,  ie,   
if one of the fuzzy sets is a subset of the other set, they are comparable. 
 
Two fu
zzy sets A and B are incomparable  
If the condition  A ⊄ B or B ⊄ A  holds. 
 
Ex
ample 1: 
Let    A = {{
a, 1}, {b, 1}, {c, 0}}    and  
         B = {{a, 1}, {b, 1}, {c, 1}}.   
Th
en A  is 
 comparable  to   B,  sinc
e  A  is   a subset  of   B. 
Ex
ample 2 :  
Let    C 
= {{a, 1}, {b, 1}, {c, 0.5}} and  
         D = {{a, 1}, {b, 0.9}, {c, 0.6}}.   
Th
en C and D are n
ot comparable since   
  
       C  is not a subset of
 D  an
d  
  
       D  is not a 
subset of C.  
 
 C(x), then accordingly A ⊂  C. 
 
     
 
  
 
 
 
 
for all  x  
 in
 the set  X,  if  A(
x) ⊂  B(x) ⊂ 
 
for all  x  
 in
 the set  X,  if  A(
x) ⊂  B(x) ⊂ 
 Prope
rty Related to Inclusion : 
for all  x  
 in
 the set  X,  if  A(
x) ⊂  B(x) ⊂
 
 • Equality 
Let   A   and   B 
  be fuzzy sets defined in the same space X.  
Th
en A   and
  B   
are equal,  which is denoted X = Y   
if 
 and  only  if  for  all  x  in  the  set X,  
 A(x) = B(x) .  
Ex
ample. 
T
he fuzzy set  B  
SMALL  
SMALL =   Fuzzy
Set {
{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},   
                                       {7, 0.2},     {8, 0.1},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}} 
 
T
he fuzzy set   
A  STILLSMALL 
STILLSMALL =   FuzzySet  
{{1, 1   },  {2, 1  },  {3, 0.9},  {4, 0.6},  {5, 0.4},   
                                      {6, 0.3},  {7, 0.2},  {8, 0.1}, {9, 0  },  {10, 0 },  {11, 0}, {12, 0}} 
 
The Fuzzy Operation :  Equality 
Equalit
y [SMA
LL, STILLSMALL]  
Mem
bership Grade    
              B   A  
   
                                                          
             
             
             
             
             
                          
             
             
             
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10     11      12   X 
 
Fi
g Graphic Interpretation of Fuzzy Equality  
FuzzyPlot [SMA
LL, STILLSMALL]  
 
Note : If equ
ality A(
x) = B(x)  is not satisfied
 even for one element   x in 
the set X, then w
e say that A is 
not equal to B. 0 1 
.2.6
 
.4 .8
  
 • Com
plement 
 
Let  A  b
e a fuzzy set defined in the space X.  
Th
en the fuzzy  set B  
is  a  complement  of  the fuzzy set A, 
 if and only if, 
for all x  in the set X,    B(
x) = 1 - A(x).  
 
The c
omplement of the fuzzy set   A is
 often denoted by  A' or  Ac  or
   
Fuzzy Compl
ement :     Ac(x)
  = 1 – A(x)    
 
Ex
ample 1. 
T
he fuzzy set   A  
SMALL 
SMALL  =  FuzzySet  
{{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},   
                                        {7, 0.2},     {8, 0.1},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}} 
 
T
he fuzzy set   Ac  NOT
SMALL 
NOTSMALL =  Fu
zzySet  
{{1, 0   },   {2, 0  },  {3, 0.1},   {4, 0.4},   {5, 0.6},  {6, 0.7},  
                                               {7, 0.8},  {8, 0.9},  {9, 1  },  {10, 1 }, {11, 1},  {12, 1}} 
 
The Fuz
zy Operation :  Compliment   
NOT
SMALL  = Com
pliment  [S
MALL]  
Mem
bership Grade   
                A     
                               Ac  
   
                                                          
             
             
             
             
             
             
                          
             
             
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10      11      12   X
  
                           
Fig 
Graphic Interpretation of Fuzzy Compliment 
                                           FuzzyPlot [SMALL, NOTSMALL]  
 
 
    
 0 1 
.2 .6
.4 .8
 A 
  Ex
ample 2. 
Th
e empty set  Φ   a n d   t h e   u n i v e r s a l  s e t  X,  as fuzzy sets, are 
complements of one another.     
  
                    Φ' = X        ,       X' = Φ 
     
T
he fuzzy set  B   
EMPTY 
Empty =   FuzzySet  
{{1, 0   },  {2, 0  },  {3, 0},   {4, 0},   {5, 0},  {6, 0},   
                                         {7, 0},     {8, 0},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}} 
 
T
he fuzzy set  A   U
NIVERSAL  
U
niversal =   FuzzySet  {
{1, 1   },   {2, 1  },  {3, 1},   {4, 1},   {5, 1},  {6, 1},   
                                               {7, 1},     {8, 1},    {9, 1  },  {10, 1 },   {11, 1},    {12, 1}} 
 
The fuzzy opera
tion :  Compliment   
EM
PTY  =  Compliment  [UNI
VERSALSPACE ]  
 Memb
ership Grade     
          B    A 
 
  
           
             
             
                          
             
             
                          
             
 
            
  
      0       1        2       3        4        5        6       7        8        9       10      11      12   X
F
ig  Graphic Interpretation of Fuzzy Compliment 
FuzzyPlot [EMPTY, UNIVERSALSPACE] 
 
 
    
 0 1 
.2 .6 
.4 .8 
 • Union
  
  
    
Let A and B  
be fuzzy sets defined in the space X.  
The union is defined as the smallest fuzzy set that contains both A and B.  
The union of A and B is denoted by A ∪ B.  
The following relation must be satisfied for the union operation : 
   f
or all x in the set X,    (A ∪ 
B)(x) = Max (A(x), B(x)). 
Fuzzy Union 
:  
 (A ∪ B)(x)  =  max [A(x), B(x)]      for   all   x  ∈ X 
Ex
ample 1 :  Un
ion of   Fuzzy   A  and  B 
 
     
 A(x)  = 0.6    and    B(x)  = 0.4        ∴   (A ∪ B)(x)  =  max [0.6, 0.4]   = 0.6 
 
Ex
ample 2 :  Union o
f  SM
ALL an
d MEDI
UM 
T
he fuzzy set  A   SM
ALL 
SMALL
 =  FuzzySet  {
{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},   
                                     {7, 0.2},     {8, 0.1},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}}  
 
T
he fuzzy set  B   
MEDIUM 
MEDIUM
 =  FuzzySet  {
{1, 0   },   {2, 0  },  {3, 0},   {4, 0.2},   {5, 0.5},  {6, 0.8},   
                                            {7, 1},     {8, 1}, {9, 0.7  },  {10, 0.4 },  {11, 0.1},   {12, 0}} 
The fu
zzy operation :   Union    
FUZZYUNI
ON = [SMALL  ∪  MEDIUM]   
  
Se
tSmallUNIONMedium =  FuzzySet  [{
{1,1},{2,1}, {3,0.9}, {4,0.6}, {5,0.5},  
                       {6,0.8}, {7,1}, {8, 1}, {9, 0.7},  {10, 0.4}, {11, 0.1}, {12, 0}}  , 
                                                                                  UniversalSpace → {1, 12, 1}]  
Mem
bership Grade    
              FUZZ
YUNION = [SMALL  ∪  MEDIUM]   
 
  
           
             
             
             
             
             
             
                          
             
 
            
   
      0  
     1        2       3        4        5        6       7        8        9       10      11    12   X 
  
                         Fig
 Graphic Interpretation of Fuzzy Union   
                                         FuzzyPlot [UNION]   
   
  Th
e notion of the union is closely related to that of the connective " or".  
Let  A is a class of "Young" men, B is a class of "Bald" men.  
If  "David is Young" or "David is Bald,"  then David is associated with the 
union of A  and
 B.   Implies David
 is a member of A ∪ B. 
 
    
 0 1 
.2 .6
.4 .8
  
 • In
tersection 
 
Let A an
d B  be fuzzy sets defined in the space X.  Intersection is defined 
as the greatest fuzzy set that include both A and B.  Intersection of Aand 
B is denoted by A ∩ B.  The following relation must be satisfied for the 
intersection operation :    
for a
ll x in the set X,    (A ∩ B)(x) = Min (A(x), B(x)).  
Fuzzy In
tersection :    (A
 ∩ B)(x)  =  min [A(x), B(x)]      for   all   x  ∈ X 
Ex
ample 1 :  Intersectio
n of  Fuzzy   A  and  B 
 
A(x) 
 = 0.6    and    B(x)  = 0.4   ∴   (A ∩ B)(x)  =  min [0.6, 0.4]   = 0.4 
 
Ex
ample 2 :   Un
ion of   SMAL
L and MEDIUM  
The
 fuzzy set  A   SM
ALL 
SMALL =   FuzzySet  {
{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},   
                                        {7, 0.2},     {8, 0.1},    {9, 0  },  {10, 0 },   {11, 0},    {12, 0}}  
 
The
 fuzzy set  B   MEDIUM 
ME
DIUM =   FuzzySet  {
{1, 0   },   {2, 0  },  {3, 0},   {4, 0.2},   {5, 0.5},  {6, 0.8},   
                                            {7, 1},     {8, 1}, {9, 0.7  },  {10, 0.4 },  {11, 0.1},   {12, 0}}  
 
The fu
zzy operation :   In
tersection   
FUZZ
YINTERSECTION = min  [SMA
LL ∩ MEDIUM]   
SetSmallINTERSECTIO
NMedium  = FuzzySet  [{{
1,0},{2,0}, {3,0}, {4,0.2}, 
                                                            {5,0.4}, {6,0.3}, {7,0.2}, {8, 0.1}, {9, 0},   
                                     {10, 0}, {11, 0}, {12, 0}}  ,  UniversalSpace → {1, 12, 1}]  
 
Mem
bership Grade   
                 FU
ZZYINTERSECTON = [SMALL  ∩  MEDIUM]   
 
  
           
             
             
             
                          
             
             
             
             
 
            
  
     0       1        2       3        4        5        6       7        8        9      10      11     12   X 
 
                           
Fi
g Graphic Interpretation of Fuzzy Union 
                                         Fu
zzyPlot [INTERSECTION]  
 
 
    
 0 1 
.2 .6 
.4 .8
  
 • Difference 
 
Let  A an
d B  be fuzzy sets defined in the space X.   
The  difference  of  A  and  B is denoted by A ∩  B'.   
Fuzzy Difference :   (A
 - B)(x)  =  min [A(x), 1- B(x)]      for   all   x  ∈ X 
Example  : Dif
ference of  
 MEDIUM and SMALL   
 
The
 fuzzy set  A   MEDIUM 
MEDI
UM =   Fuz
zySet {{1, 0   },   {2, 0  },  {3, 0},   {4, 0.2},   {5, 0.5},  {6, 0.8},   
                                            {7, 1},  {8, 1}, {9, 0.7  },  {10, 0.4 },  {11, 0.1},   {12, 0}}  
The
 fuzzy set  B   SMA
LL 
MEDIUM =   FuzzySet  {
{1, 1   },   {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4},  {6, 0.3},   
                                        {7, 0.2},     {8, 0.1}, {9, 0.7  },  {10, 0.4 },  {11, 0},   {12, 0}}  
 
F
uzzy Complement :     Bc(x)  = 1 – B(x)    
The
 fuzzy set  Bc   
NOTSMALL 
NOTSMALL =   FuzzySet  
{{1, 0   },   {2, 0  },  {3, 0.1},   {4, 0.4},   {5, 0.6},  {6, 0.7}, 
                                          {7, 0.8},     {8, 0.9},    {9, 1  },  {10, 1 },   {11, 1},    {12, 1}} 
 
The
 fuzzy operation : Difference by the definition of Difference
FUZZ
YDIFFERENCE = [MEDIUM  ∩  SMALL']     
Set
MediumDIFFERECESmall  = FuzzySet  [{
{1,0},{2,0}, {3,0}, {4,0.2}, 
                                      {5,0.5}, {6,0.7}, {7,0.8}, {8, 0.9}, {9, 0.7},   
                              {10, 0.4}, {11, 0.1}, {12, 0}}  ,  UniversalSpace → {1, 12, 1}]
 
Mem
bership Grade   
                    FU
ZZYDIFFERENCE = [MEDIUM  ∪ SMALL' ]   
 
  
           
             
             
                          
             
             
             
                          
 
            
   
      0  
     1        2       3        4        5        6       7        8       9       10     11     12   X 
 
                           
Fi
g Graphic Interpretation of Fuzzy Union 
Fuz
zyPlot [UNION] 
 
 
    
 
 
   0 1 
.2
 .6
 
.4 .8  
 Fuzzy Properties 
 
Prop
erties related to Union, Intersection, Differences are illustrated below. 
 
 • Properties Related to 
Union 
The properties related to union are :  
Identity,  Idempotence,  Commutativity  and  Associativity.   
 
  ■ Id
entity:        
        
   A ∪ Φ  
= A 
    input   
 =  Equality [SMALL ∪ EMPTY
 ,  SMALL]  
   output
 = True 
 
              
    A ∪ X = X  
     in
put    =  Equality [ SM
ALL ∪ UnivrsalSpace ,  UnivrsalSpace ] 
    o
utput  = True 
 
  ■ Id
empotence :   
    
    A ∪ A = A  
  
   in
put    =  Equality [SMALL ∪ SM
ALL ,  SMALL]  
     
output = True 
 
  ■ Co
mmutativity :   
  
   A ∪ B  = 
B ∪ A 
  
  input   
=  Equality [SMALL ∪ MEDIUM
,  MEDIUM  ∪ SMALL] 
   output
 = True 
 
  
   
    
  ■ As
sociativity:            
 
   A ∪ (B∪ C) = (A ∪ B) ∪ C  
 
  i
nput  = Equality [Small ∪ (Medium ∪ Big) , (Small ∪  Medium ) ∪ Big] 
  output = True 
Fuzz
y Set  Small , Medium ,  Big   
 
Sm
all     =  FuzzySet  {
{1, 1 },  {2, 1  },  {3, 0.9},   {4, 0.6},   {5, 0.4}, {6, 0.3},   
                                       {7, 0.2},  {8, 0.1}, {9, 0.7  },  {10, 0.4 },  {11, 0}, {12, 0}}  
 
Mediu
m = FuzzySet  {
{1, 0   },  {2, 0  },  {3, 0},  {4, 0.2},   {5, 0.5},  {6, 0.8},   
                                      {7, 1},     {8, 1},    {9, 0  },  {10, 0 },   {11, 0.1},    {12, 0}}  
 
Big  
      = FuzzySet  
[{{1,0}, {2,0}, {3,0}, {4,0}, {5,0}, {6,0.1},  
                                 {7,0.2}, {8,0.4}, {9,0.6}, {10,0.8}, {11,1}, {12,1}}]  
 
C
alculate Fuzzy relations : 
 
(1)  Med
ium ∪ Big = FuzzySet  [{
1,0},{2,0}, {3,0}, {4,0.2}, {5,0.5}, 
                          {6,0.8},{7,1}, {8, 1}, {9, 0.6},  {10, 0.8}, {11, 1}, {12, 1}]  
 
(2)  Small
 ∪ Medium  = FuzzySet  [{
1,1},{2,1}, {3,0.9}, {4,0.6}, {5,0.5}, 
                 {6,0.8},  {7,1}, {8, 1}, {9, 0.7},  {10, 0.4}, {11, 0.1}, {12, 0}]   
 (3)  
Small
 ∪ (Medium ∪  Big)  = FuzzySet  [{1,
1},{2,1}, {3,0.9}, {4,0.6},  
        {5,0.5}, {6,0.8}, {7,1}, {8, 1}, {9, 0.7},  {10, 0.8}, {11, 1}, {12, 1}] 
 
                                                                                                                  
(4)  (Small ∪  Med
ium)  ∪ Big]  = FuzzySet  [{
1,1},{2,1}, {3,0.9}, {4,0.6},
          {5,0.5}, {6,0.8}, {7,1}, {8, 1}, {9, 0.7},{10, 0.8}, {11, 1},{12, 1}] 
 
Fuzzy set  (
3)   and  (4)  proves  Associativity relation  
 
 
 
      
 • Propert
ies Related to Intersection 
Absorption,  Identity,  Idempotence,  Commutativity,  Associativity.   
 
  ■ Abs
orption by Empty Set :  
    A ∩ Φ  = Φ  
   inpu
t    =  Equality [Small ∩ Empty ,  Empty] 
   output = True 
 
  ■ Identity : 
 
    
A ∩  X = A  
   input    =  Equality [Small ∩ UnivrsalSpace ,  Small] 
   output = True  
  ■ Idem
potence :  
    A ∩  A = A  
   inpu
t    =  Equality [Small ∩ Small ,  Small] 
   output = True   
  ■ Comm
utativity  : 
    A ∩  B = B  ∩  
A     
   input    =  Equality [Small ∩ Big ,  Big ∩  Small] 
   output = True 
 
  ■ Assoc
iativity  : 
    A ∩ (B ∩   C) 
= (A ∩  B) ∩  C  
   input = Equality [Small ∩  (Medium ∩ Big), (Small ∩  Medium) ∩ Big] 
   output = True 
 
 
    • Add
itional Properties  
Related  to  Intersection  and  Union  
 
 ■ Di
stributivity:  
    
A ∩ (B ∪  C) = (A ∩  B) ∪  (A ∩   C) 
   input = Equality [Small ∩ (Medium ∪ Big)  , 
                            (Small ∩ Medium)  ∪ (Small ∩ Big)]  
   output = True 
 
 ■ Di
stributivity:  
    
A ∪ (B ∩  C) = (A ∪  B) ∩  (A ∪   C) 
   input = Equality [Small ∪ (Medium ∩ Big)  , 
                            (Small ∪ Medium)  ∩ (Small ∪ Big)]  
   output = True   
 
 
  
  ■ Law o
f excluded middle  : 
    
 A ∪  A'  = X 
    input  = Equality [Small  ∪  NotSmall  ,  UnivrsalSpace ] 
    output = True  
  ■ L
aw of contradiction 
    A ∩  A'  =
 Φ 
   input  = Equality [Small  ∩  NotSmall  ,  EmptySpace ] 
   output = True 
 
 
     
• Ca
rtesian  Product  Of  Two  Fuzzy  Sets 
 
■ C
artesian  Product  of  two  Crisp  Sets 
Let  A  and B  be two  crisp sets in the universe of discourse X and Y..  
The Cartesian product of A and B is denoted by A x B  
Defined as   A x B  = { (a , b)  │ a ∈ A ,  b ∈ B } 
Note
 : Generally   
 A x B  ≠ B x A   
 
Exampl
e :  
Le
t  A = {
a, b, c}  and  B 
= {1, 2}    
th
en  A x 
B  = {
 (a , 1) , (a , 2) ,  
                           (b , 1)  , (b , 2)  ,  
                           (c , 1) , (c , 2) }  
 G
raphic representation of  A x B  
 B 
 
 2 
 
 1 
 
      
                                      
         A 
   
          a  
     b        c 
 
■ C
artesian  product  of  two  Fuzzy Sets 
Let  A and B   be two fuzzy sets in the universe of discourse X  and Y.  
The Cartesian product of A and B is denoted by   A
 x B 
Define
d  by their membership function  µ A (x
)  an
d  µ B (y)  as  
   
   
    
  µ A x B (x , 
y) = min [ µ A (x
)  ,  µ B (y) ]  =  µ A (x
) ∧  µ B (y)
 
or      µ A x B (x ,
 y) =   µ A (x
) µ B (y) 
    
     for all     x  ∈    X   and  y  ∈   Y 
 
Thu
s  the  Cartesian   product  A x B   i
s  a  fuzzy  set  of  ordered  pair
(x , y)   for all x ∈ X  an
d y ∈ Y,  with  
grade  membership  of (x 
, y)   in
X  
x Y   given
  by the above equations .  
 
In a sen
se  Cartesian  product  of  two  Fuzzy sets  is  a  Fuzzy Relation .
  
Fuzzy
 Relations 
Fuzzy  Relations  describe  the  degree  of association   of  the elements;   
Example :  “x  is  approximately  equal  to  y”.   
  
− F
uzzy relations offer the capability to capture the uncertainty  and vagueness 
in relations between sets and elements of a set.  
 
− Fuzzy Relat
ions  make  the  description of  a  concept  possible. 
 
− Fu
zzy Relations were introduced to supersede classical crisp relations;
It  describes  the  total  presence  or absence  of  association  of  elements.  
 
In this section, first the fuzzy relation  is defined and then expressing fuzzy 
relations in  terms  of  matrices and graphical visualizations.  Later the 
properties  of  fuzzy rela tions and operations that can be performed with fuzzy 
relations  are  illustrated.  
 
 
     
3
.1 Definition of Fuzzy Relation 
F u z z y  r e l a t i o n   i s   a   g e n e r a l i z a t i o n   o f   t h e   d e f i n i t i o n   o f   f u z z y   s e t
from  2-D space  to  3-D  space. 
 
• Fuzzy relation
 definition  
 
Consider a Cartesian product
  
   
  A x B  = { (x , y)  |  
 x  ∈ 
 A,  y  ∈  B } 
where   A  and  B  are su
bsets of universal sets  U1 and U2. 
Fuzzy r
elation  on  A x B   is denoted by  R  or  R(x ,
 y)  is d
efined as the set  
  
   R = { ((x , y) , µR (x ,
 y))  |  (x , y)  ∈  A x B ,  µR (x , y) 
 ∈ [0,1] } 
where   µR (x ,
 y) is a 
function in two variables called membership function . 
− It
 gives the degree of membership of the ordered pair (x , y)  in
 R
associating 
with each pair (x , 
y)  in A x B   a
  real  number  in  the 
interval [0
 , 1]. 
− The d
egree of membership indicates  the degree to which x is in 
relation  to  y.  
 • Exam
ple  of  Fuzzy  Relation 
 
  
  R   =   { (
(x1 , y 1)
 , 0)) ,    ((x 1 , 
y2) , 
0.1)) ,  ((x 1 , y 3)
 , 0.2)) ,  
                 ((x 2 , 
y1) , 0.7)) 
, ((x 2 , y 2) , 
0.2)) ,  ((x 2 , 
y3)
 , 0.3)) ,  
                 ((x 3 , 
y1)
 , 1)) ,    ((x 3 , y 2) 
, 0.6)) ,  ((x 3 , 
y3) , 0
.2)) ,  
The 
relation can be written in  matrix form as 
  
          
 
     y 
x        y1 Y 2Y3
x1 0 0.
1 0.2
X2 0.7
 0.2 0.3
X3 1 0.
6 0.2
 
   
    w
here symbol  
        m
eans ' is defined as'   and  
   
    t
he values in the matrix  are the values of membership function:  
   
      
µR (x1 , 
y1) = 0 µR (x1 , 
y2) = 0.1 µR (x1 , y 3) = 0.2 
µR (x2 , 
y1) = 0.
7 µR (x2, y2)  = 0.
2 µR (x2 , y 3) = 0.3 
µR (x3 , 
y1) = 1 µR (x3 , 
y2) = 0.6 µR (x3 , y 3) = 0.2 
 
Assu
ming  x1 = 1 ,  x2 = 2 ,  x 3 = 3  an
d  y1 = 1  ,  y 2= 2 , y3= 3 , 
 
the  relation  can  be  graphically  represented  by  points   in 3-D  space
(X, Y, µ)  as :  
   
 
  
 
 
  
 
 
 
 
  
 
 
 
 
 
 
Fi
g  Fuzzy Relation  R  de
scribing  
x  gr
eater than   y 
   
Note :  Sin
ce the values of the 
membership function 0
.7, 1, 0.6
are in the 
direction of x below the 
major diagonal (
0, 0.2, 0.2)  in
 the 
matrix  are  grater than those 
0.1
, 0.2, 0.3  in
 the direction of y, 
we therefore say that  the relation 
R   describes  x  is grater than y.  
 
 
    R 
1 
2 
3 1 
.8
 
.6 
.4 
.2 
0  1 
        2 
            3 y
x µ  
  Forming  Fuzzy Relations 
 
Assu
me that   V  and  W  are two collections of objects.  
 
A fuzzy relat
ion is characterized in the same way as it is in a fuzzy set.  
 
− The 
first item is a list containing element and membership grade pairs,  
        {{v 1, w1}, R 11},
 {{ v 1, w2}, R 12}, ..
. , {{ v n, wm}, R nm}}
.  
w
here  { v 1, w 1}, {
 v1, w 2}, ... 
, { v n, w m} 
are the elements of the relation 
are defined as ordered pairs, and { R 11 ,  
R12 , ... , R nm} a
re the  membership 
grades of the  elements of the relation that range from 0 to 1, inclusive. 
− The seco
nd item is the universal space; for relations, the universal space 
consists of a pair of ordered pairs,  
   
       {{ V mi
n, Vmax, C1}, { 
Wmin, W ma
x, C2}}
.  
 
w
here the first pair defines the universal space for the first set  and the second 
pair defines the universal sp ace for the second set.  
Example    
showing how fuzzy relations are represented  
          L
et V = {1, 2, 3} and W = {1, 2, 3, 4}. 
A 
 fuzzy relation R is,  a function defined in the space  V x W , w
hich  takes 
values from the interval [0, 1] ,  expressed as   R :
 V x W →  [0, 1]   
 R  
= Fuz
zyRelation [{{{1, 1}, 1},    {{1, 2}, 0.2}, {{1, 3}, 0.7}, {{1, 4},  0},  
                                      {{2, 1}, 0.7}, {{2, 2}, 1},    {{2, 3}, 0.4}, {{2, 4},  0.8}, 
                                      {{3, 1}, 0},    {{3, 2}, 0.6}, {{3, 3}, 0.3}, {{3, 4},  0.5}, 
                                       UniversalSpace  → {{1, 3, 1}, {1, 4, 1}}]   
This relat
ion can be represented in the following two forms shown below  
  Mem
bership matrix  form 
            
  
   w 
v        w1 w 2 w 3 w 4
v1 1 
0.2 0.7 0 
v2 0.7 1 
0.4 0.8
v3 0 
0.6 0.3 0.5
 
 
 
 
 
   
                   Gr
aph  form 
    
            
 
 
 
  
 
 
 
 
 
  
 
 
  
Ve
rtical lines represent  membership grades 
 
  Elem
ents of fuzzy relation are ordered pairs {vi , w j},
 where  vi  is first
 and 
wj is 
second element. The membership grades of the elements are 
represented by the heights of the vertical lines.  
 
    
 R 
1
2
31
.8
.6.4
.2
0 1 
      2 
            3   
      4w
vµ  
  Projections of Fuzzy Relations 
 
Definition :  A  fu
zzy  relation  on   A x 
B   is  d
enoted  by   R  or  R(x , y)   is 
defin
ed as the set  
  
   R = { ((x , y) , µR (x , y)
)  |  (x , y)  ∈ A x B ,  µR (x , 
y)  ∈ [0,1] } 
where   µR 
(x , y)  i
s  a  function  in  two  variables  called  membership 
function.  The first, the second  and  the total  projections  of  fuzzy 
relations  are  stated  below.   
 
 • Firs
t  Projection  of  R :   defined as   
 
  
   R(1) = {(x) 
, µ R(1
) (x , y))}  
  
         =  {(x) ,       µ R (x , y))
  |  (x , y)  ∈ A x B } 
 
 • Se
cond  Projection  of  R :   define
d as  
 
  
   R(2) = {
(y) , µ R(2
) 
(x , y))}  
           =  {(y) ,            µ R (x , 
y))  |  (x , y)  ∈ A x B } 
 
 • To
tal  Projection  of  R :   de
fined as   
 
  
   R(T
) =   
               
   {µ R (
x , y)  |  (x , y)  ∈ A x B } 
 
Note : 
In all these three expression  
  
                    m
eans  max with res
pect to   y  while   x  is con
sidered fixed   
 
                      m
eans  max with res
pect to  x  while   y  is con
sidered fixed      
 
 
The T
otal  Projection  is  al so known as Global projection 
 
 
    ma
x
Y 
max
X 
max
Y 
max
X max
Y max
X   
 • Exam
ple :  Fuzzy Projections 
 
Th
e Fuzzy Relation   R  t
ogether with  First,  Second  and  Total Projection  
of  R  ar
e shown below.   
 
    y 
x
       y1 y 2 y 3 y 4 Y 5  R(1)   
x1 0.1 
0.3 1 0.5 0.3  1   
x2 0.2 0.
5 0.7 0.9 0.6  0.9   
x3 0.3 
0.6 1 0.8 0.2  1   
  
        
R(2) 0.3 
0.6 1 0.9 0.6  1 = R(T
)
  
        
Note :
  
 
For  R(1
)  select     
      means  max with respect to y while  x  is considered fixed  
For  R(2
)  select    
      means  max with respect to x while  y  is considered fixed   
 
For  R(T) 
  select ma
x  w
ith respect to  R(1
)   an
d  R(2
)   
 
Th
e Fuzzy plot of these projections are shown below. 
   
 
 
 
 
   
 
   
 
 
 
 
Fi
g Fuzzy plot of 1st projection   R(1) 
  
 
  
 
 
 
  
 
 
 
 
 Fig F
uzzy plot of 2nd projection   R(2
) 
 
     
 R 
xR(1)
 
1 
.8 
.6 .4 
.2 
0 
1 2
 3 4 5 y R(2
)
1
.8
.6.4
.2
0
12
34  5  ma
x
Y 
ma
x
x  
  Max-Min and Min-Max Composition 
Th
e operation composition combines the fuzzy relations in different 
variables, say  (x , y)  and (y , z)  ;  x ∈ A ,   y ∈ B ,     z ∈ C . 
 
Consider the relations : 
  
   R1(x , y)   =
 { ((x , y) , µR1 (x , y))
  |  (x , y)  ∈ A x B } 
    R 2(y , z)    
= { ((y , y) , µR1 (y , 
z))  |  (y , z)  ∈  B x C } 
Th
e domain of   R1  
 is  A x B    and  the  domain of   R2  is  B x
 C   
 
 
 • Max-Min Composition 
 
Definition :   T
he Max-Min composition denoted by  R1 ο R2  w
ith membership 
function   µ R1
 ο R2  
 de
fined  as  
  
 R1 ο R2  =  { ((x
 , z) ,         ( mi
n (µR1 (x
 , y) ,  µR2 (y 
, z))))} ,    
                                                            (x , z)  ∈ 
A x C ,  y ∈ B  
 
Th
us  R1 ο R2   is relati
on in the domain   A x C 
 
An
 example of the composition is shown in the next slide. 
 
 
    ma
x
Y  
 • Exam
ple :  Max-Min Composition 
 
C
onsider the relations   R1(x
 , y)  and  R2(y , 
z) as given below . 
 
  
    
 
    y 
x
       y1 y 2 y 3 
x1 0.1 0.
3 0 
x2 0.8 1 
0.3 
 
   
   
   
  z
y      z1 z 2 z 3 
y1 0.
8 0.2 0 
y2 0.
2 1 0.6 
y3 0.
5 0 0.4 
  
  
  Note : 
 Number of columns in the first table and second table are equal. 
Compute max-min composition denoted  by   R1 ο R2 
 :  
  Step -1    Com
pute  min  op
eration (
definition  in  previous slide). 
Co
nsider row  x1  an
d column  z1 , 
 means  
the pair  (x1 , z1)  for 
 all  yj ,   
     
j =
 1, 2, 3,   and 
perform   min  operation  
  
   min  (µR1 (x1 , y 1) , 
 µR2 (y1 , z 1))  
=  m
in (0.1, 0.8) = 0.1,  
     min  (µR1 (x1 , y 2) , 
 µR2 (y2 , z 1))  
=  m
in (0.3, 0.2) = 0.2,  
  
   min  (µR1 (x1 , y 3) , 
 µR2 (y3 , z 1))  
=  m
in (  0,  0.5) = 0,  
  Step -2  Com
pute  ma
x  ope
ration (
definition  in  previous slide). 
For   x = x1 ,  
 z = z1 , 
  y = yj ,  j = 
1, 2, 3 ,   
Calculat
e   the  grade  membership  of  the  pair   (x1 , z1)  
 as 
   
      
   { (x 1 , z 1) , 
max ( (min ( 0
.1, 0.8 ), min
 (0.
3, 0.2 ),  min
 (0,
 0.5) ) 
 
i.e
.   
  { (x 1 , 
z1) , 
max( 0.
1, 0.2, 0 ) }
   
i.e
.   
  { (x 1 , 
z1) , 0.2 
}      
Hence  th
e  grade  membership of  the  pair  (x1 , z1)  
 is   0.2 . 
 
Similar
ly, find all  the grade membership of the pairs  
  
   (x 1 , 
z2) , (x1 , 
z3) , (x2 , 
z1) , 
(x2 , 
z2) , 
(x2 , 
z3)  
 
Th
e final result is  
     z
 
x       z1 z 2 z 3 
x1 0.1 0.
3 0 
x2 0.8 1 
0.3 
 
Note :
 If tables R1  and  
R2 
are considered as matrices, the operation 
composition resembles the operation multip lication in matrix calculus linking 
row by columns. After each cell is occupied max-min value (the product is 
replaced by min, the sum is replaced by max).    
 
    R2R1 
R1 ο R2 =  
 • Exam
ple :  Min-Max Composition 
 
Th
e min-max composition is similar to max-min composition with the 
difference that the roll of max and min are interchanged.  
 
Definition :  Th
e max-min composition denoted by  R1   R2  with  
membership 
function  µ R1   R2   
is defined by  
  
  R1   R2 
 =  { ((x , z) ,         ( max (µR1 (x , 
y) ,  µR2 (y , 
z))))} ,    
                                                            (x , z)  ∈  
A x C ,  y ∈ B  
 
Th
us  R1   R2  
 is rel
ation in the domain   A x 
C 
 
Con
sider the relations    R1(x , 
y)  and  R2(y , z
)  as given by t
he same 
relation of previous example of max-min composition,  that is   
 
  
    
     y 
x 
      y1 y 2 y 3 
x1 0.
1 0.3 0 
x2 0.
8 1 0.3 
 
   
   
  
   z
y       z1 z 2 z 3 
y1 0.8
 0.2 0 
y2 0.2
 1 0.6 
y3 0.5
 0 0.4 
  
  
  After com
putation in similar way as done in the case of max-min 
composition,   the final result is  
 
     z
 
x       z1 z 2 z 3 
x1 0.3 0 
0.1 
x2 0.5 0.
4 0.4 
   
  
 
      
 • Relation
 between  Max-Min and Min-Max Compositions 
 
The Max
-Min and Min-Max Compositions are related by the formula 
 
   
  
 
 
 
    
 R2R1 
R1   R2 = 
R1 ο R2   
= R1 R2min
y 
 
Fuzzy Sys
tems 
 
 
 What
  are  Fuzzy Systems ? 
 
 • Fuzz
y Systems  include  Fuzzy Logic and Fuzzy Set Theory.  
 
 • Kn
owledge exists in two distinct forms :  
− th
e Objective knowledge  that exists in  mathematical form is used in 
engineering problems; and  
− th
e Subjective knowledge that exists in linguistic form, usually 
impossible to quantify.   
Fuzzy Logic  can coordinate these two forms of knowledge in a logical way. 
  
 • Fuzzy Systems  can
  handle  simultaneously the numerical data and 
linguistic  knowledge. 
 
 • Fuzz
y Systems  provide  opportunities  for  modeling  of  conditions  which 
are  inherently  imprecisely  defined. 
 
 • Ma
ny real world  problems  have  been  modeled,  simulated,  and 
replicated  with  the   help of fuzzy systems.  
 
 • Th
e applications of Fuzzy Systems  are many  like : Information retrieval 
systems,  Navigation system,  and  Robot vision.  
 
 • Expert 
Systems design have become easy because their domains are
inherently fuzzy  and  can  now  be  handled  better;  
examples : Decision-support sy stems, Financial planners, Dia gnostic
 
system, and Meteorological system.  
  
Introduction  
Any system
 that uses Fuzzy mathematics may be viewed as Fuzzy system.  
The Fuzzy Set Theory - membership function, operations, properties and the 
relations  have  been  described  in previous  lectures.  These are the prerequisites for understanding Fuzzy Systems.  The applications  of  Fuzzy set theory  is  Fuzzy  logic  which  is  covered  in this  section.  
 
Here the emphasis is on the design of  fuzzy system and fuzzy controller in a 
closed–loop.  The specific topics of interest are : 
− Fuzzificatio
n  of   input  information,   
− Fuzzy  
Inferencing  using  Fuzzy  sets ,   
− De-Fuzzification  of
  results  from  the  Reasoning  process,  and 
− Fu
zzy  controller  in  a closed–loop. 
 
Fuzzy Inferencing, is the core constituen t of a fuzzy system.  A block schematic 
of Fuzzy System is shown in the next slide.  Fuzzy Inferencing  combines  the 
facts  obtained  from  the  Fuzzificatio n  with  the  fuzzy  rule  base  and 
conducts  the  Fuzzy  Reasoning  Process.    
 
 
    
 • Fuzzy System  
A b
lock schematic of Fuzzy System is shown below. 
 
  
 
 
 
     
  
  
 
  
Fig.  Elements of Fuzzy System Fuz
zification Fuzzy 
Ru
le Base 
Fu
zzy 
Inferencing Defu
zzification
 
Membes
hip Function X1 
X2 
 
Xn Y1
Y2
 YmIn
put 
variables outp
ut 
variables 
 
 
Fuzzy Sy
stem elements 
 
− In
put Vector  :  X = [x 1 , x 2,
 . . . x n ] T  are
 crisp values , which are 
transformed into fuzzy sets in the fuzzification block. 
− Outp
ut Vector  : Y = [y 1 , 
y2,
 . . . y m ] T com
es out  from  the 
defuzzification  block,  which  transforms   an  output  fuzzy  set  back  to 
a  crisp value .  
− Fuzzifi
cation  
:  a  process  of  transforming  crisp  values  into grades  of 
membership for linguistic  terms, "far",  "near", "small"  of  fuzzy  sets.  
− Fuzzy R
ule base  
: a  collection  of  propositions  containing  linguistic 
variables;  the  rules are  expressed in the form:   
           If (x i
s A ) AND (y is B )   . . . . . .   THEN  (z is C)   
    
where x, y and z represent  variables (e.g. distance, size)  and   
  
   A, B   an
d  Z   are 
 linguistic variables (e.g. `far', `near', `small'). 
− Mem
bership function  :
  provides  a  measure of the degree  of  similarity 
of  elements  in  the  universe  of  discourse U  t
o fuzzy set. 
− Fuzzy Inf
erencing  : co
mbines the facts obtained from the Fuzzification 
with the rule base and conducts the Fuzzy reasoning process. 
− Def
uzzyfication:   T
ranslate  results  back  to  the  real  world  values. 
 
 
     
Fuzzy Logic 
A
 simple form of logic, called  a  two-valued  logic  is  the  study of "truth tables" 
and  logic circuits.  Here the possible values are true as 1, an
d false as 0. 
 
Thi
s simple two-valued logic is generalized and called fu
zzy logic  which treats 
"truth"
 as a continuous quantity ranging from 0  to 1.  
 
Definit
ion : Fuzzy logic (FL) is derived from fuzzy set theory dealing with 
reasoning  that  is  approximate  rather  than precisely deduced  from  classical 
two-valued logic. 
 
− FL is  
the application of Fuzzy set theory. 
− FL allo
ws  set  membership values to range (inclusively)  between 0 an
d 1. 
− FL is  
capable of  handling  inherently  imprecise  concepts. 
− FL al
lows in  linguistic form, the set membership values to imprecise concepts
like  " s
lightly ",  "quit
e"  and  
"very".  
  Classical Logic   
 
Logic 
is used to represent simple facts.  Logic defines the ways of putting 
symbols together to form sentences  that represent facts. Sentences are 
either true or false but not both are called propositions . 
Ex
amples : 
Sentence Truth value Is  it a Proposition ? 
"Gra
ss is green" "true" Yes 
"2 + 5 = 5" "false" Yes 
"Close the door" - No 
"Is it hot out side ?" - No 
"x  >  2" - No (since x is not defined) 
"x  =  x" - No 
(don't 
know what is "x" and "=" 
mean;  "3 = 3" or say "air is equal 
to air" or "Water is equal to water" has no meaning)  
  
 
 • Propositional Logic (PL) 
 
  A 
 proposition  is a statement - which in English is a declarative sentence
and  Logic defines  the ways  of putting  symbols together to form 
sentences  that  represent  facts.  Every  proposition  is  either true or false. 
Propositional  logic  is  also called boolean algebra. 
 
  Ex
amples:  (
a) The sky is blue.,   (b)  Snow is cold. ,   (c) 12 * 12
=144 
  Propositi
onal logic  :  
It is fundamental to all logic.  
  ‡ 
Propositions  are  “Sentences”;  either true or false but not both.  
  ‡ 
A sentence is smallest unit in propositional logic 
  ‡ 
If proposition is true,  then tr uth value is "true"; else “false”  
  ‡ Exam
ple ;    Se
ntence     
   "Gr
ass is green";   
    
               Tr
uth value   
  “ true”
;     
  
                   Propos
ition    
 “y
es” 
  
  
  
  ■ Stat
ement, Variables and  Symbols 
   S
tatement  :  A 
 simple  statement  is  one that does not contain any 
other statement as a part.  A compound  statement  is  one that has
two  or  more  simple  statements  as  parts  called  components.   
 
   Oper
ator  or  connective :  Joins 
simple statements into compounds, 
and joins compounds into larger compounds.  
 
   Sy
mbols for connectives 
  
asse
rtion  P   
  "p is true
" 
nag
ation ¬p ~ ! 
 NOT
 "p is 
false" 
conjunc
tion p ∧ q · &&
 & AN
D "bo
th p and q are true" 
disjunctio
n P v q ||  ׀  OR "eit
her p is true,  
  or q is true,  
  or both "  
 
imp
lication p → q ⊃ ⇒  i
f . . then "if p is true, then q is true" 
" p implies q "  
 
equiv
alence ↔ ≡⇔  i
f and only if "p and q are either both true  
                      or both false" 
  
 
    
■ Tr
uth Value    
The  truth  value  of a statement  is  its  truth or falsity ,  
  
    p    
    is either true or false,  
  
  ~p        is eithe
r true or false,  
  
    p v q    is either 
true or false, and so on.  
  
 
    "T" or "1"  means  "tr
ue".   and    
  
     "F" or "0" 
 means  " false " 
Truth
 table is a convenient way of showing relationship between several 
propositions. The truth table for negation, conjunction, disjunction,
implication and  equivalence are shown below.  
 
p q ¬p ¬qp ∧ qp v q p→qp
 ↔ qq→p 
T T 
F F T T T T T 
T F 
F T F T F F T 
F T 
T F F T T F F 
F F 
T T F F T T T 
 
      
  ■ Tautology  
A Tautolo
gy is proposition formed by combining other propositions 
(p, q, 
r, . . .)  
which is true regardless  of  truth  or falsehood  of   p,  q
,
r, . . .  .  
   
The  important  tautologies  are :  
  
         (p→q) ↔ ¬ [p
 ∧ (¬q)]   
  and    (p→q) ↔ (¬p)
 ∨ q 
A proo
f of these tautologies,  using the truth tables are  given below.  
Tautologies  
    (p→q) ↔ ¬ [p
 ∧ (¬q)]
   
 and    
(p→q) ↔ (¬p)
 ∨ q 
                         Table 1: Proof of Tautologies 
p q p→q ¬qp ∧ (¬q)¬ [p
 ∧ (¬q)]¬p(¬p)
 ∨ q 
T T T F F T F T 
T F F T T F F F 
F T T F F T T T 
F F T T F T T T 
 
No
te :  
1. The entries of two columns  p→q 
 and  ¬ [p ∧  (¬q)
]  are i
dentical,
proves the tautology. Similarly, the entries of two columns p→qand 
(¬p) ∨ q  are identical,  proves  the  other  tautology.  
2. The importance of these tautologies is that they express the 
membership function for p→qin 
terms of membership functions of 
either propositions  p  
and ¬q  or  ¬p  
and q. 
 
  
  
■ Equivale
nces  
Between  Logic ,  Set theory  and  Boolean  algebra.  
Some  mathematical  equivalence  between  Logic and Set theory  and 
the correspondence  between Logic and  Boolean  algebra (0, 1) are
 
given below. 
  
      
Logic Bool
ean  Algebra (0, 1) Se
t theory 
T 1  
F 0  
∧ x ∩   ,    ∩ 
∨ +  
       ∪   ,   U 
¬ ′  ie
 complement    ( ― ) 
↔ =  
p, 
  q,   r a,  
 b,   c  
   
     
  ■ Mem
bership Functions obtain from facts  
 
Conside
r the facts (the two  tautologies)   
  
         (p→q) ↔ ¬ [p ∧ (¬q)]     and    ( p→q) ↔ (¬p) ∨ q 
U
sing these facts and the equivalence between logic and set theory, we 
can obtain membership functions for   µp→ q (x , 
y) .   
From 1st  fact :   µp→q (x 
, y)  =  1 -  µ p ∩     (x 
, y)   
                                             =  1 – min [µ p(x)
 , 1 -  µ q (y)]    
 Eq (1)  
From 2nd 
 fact :  µp→q (x
 , y)  =  1 -  µ     
   U q (x , y)   
  
                                           =  max [ 
1 - µ p 
(x) ,  µ q (y)]  
      Eq (2)  
 
Boolea
n truth table below shows the validation membership functions  
               
        Table-2 : Validation  of  Eq (
1)  and  Eq (
2) 
 
µ p(x
) µ q(y) 1 - µ p (x) 1 - µ q (y
)ma
x [ 1 - µ p (x
),  
µ q (y)] 1 –
 min [µ p(x
) ,  
 
1 -  µ q (y
)] 
1 1 0 0 1 1 
1 0 0 1 0 0 
0 1 1 0 1 1 
0 0 1 1 1 1 
 
No
te :  
1. Entries in last two columns of this table- 2a
grees with the entries in 
table-1  for  p→q ,
  the proof of tautologies, read T as
 1 and F as
 0. 
2
. The  implication  membership  f unctions  of Eq.1  and Eq.2 are not 
the only  ones  that  give  agreement  with p→q.   
The others are :  
  
  µp→q 
(x , y)   =  1 -  µ p  (x
) (1 -
  µ q (y)
) 
     
             Eq (3)  
 
  
  µp→q (x , 
y)  =  min [ 
1, 1 - µ p (x)
 +  µ  q (y
)]       Eq (
4) 
 
    
  
 
    
 
  q
p 
  ■ Mo
dus Ponens and Modus Tollens 
 
In  tradi
tional propositional logic there are two important inference 
rules,  Modus Ponens  and  Modus Tollens . 
Modus Ponens   
Prem
ise 1 :   "  
x  is  A "  
Prem
ise 2 :   "  
if x is  A  then  y  is  B  "   ;   C
onsequence :    " y  
is  B  "    
Modu
s Ponens  is associated with the implication  " A  i
mplies B " [A →B] 
In 
terms of propositions p and q, the  Modus Ponens  is expressed as  
 
            (p ∧ (p →  q))  → q 
Modus Tollens   
Prem
ise 1 :   "  
y  is  not B "  
Prem
ise 2 :   "  
if x is  A  then  y  is  B  "  ;  Conse
quence :  "
 x  is  not A "   
In 
terms of propositions p and q, the  Modus Tollens  is expressed as  
 
            (¬ q ∧ (p → q)
)  → ¬ p 
 
    
  
 
  
  Fuzzy Logic 
 
Like the extension of
 crisp set theory to fuzzy set theory,  the extension of 
crisp logic  is  made by replacing  the bivalent membership functions of the 
crisp logic  with  the fuzzy  membership functions.   
 
In c
risp logic, the truth value acquired  by  the  proposition  are  2-valued, 
namely true as 1 an
d false as 0.  
 
In fuzz
y logic, the truth values are multi-valued, as absolute true, partially 
true, absolute false etc  represented  numerically  as real  value  between 
0  to 1.  
 
Note :
 The fuzzy variables  in  fuzzy sets, fuzzy propositions, fuzzy relations 
etc are represented usually using symbol   ~ 
as       but  for the purpose of 
easy to write  it is always represented as  P .  P~ 
 • Recaps  
  
  01 Membershi
p function  µ A (x) descr
ibes the membership of the elements x of 
the base set  X   in the fuzzy set  A . 
  02 Fuzzy Intersect
ion operator   ∩  ( AND connectiv
e ) applied to two fuzzy sets   A 
and B  with the members
hip functions   µ A (x
)  and  µ B (x
)  b
ased on min/max 
ope
rations   
is  µ A ∩ B  =   
min [ µ A (x
)  ,  µ B (x
) ]  ,    x  ∈   X   (Eq.  01)    
  03 Fuzzy Intersect
ion operator   ∩  ( AND 
connective ) applied to two fuzzy sets   A 
and B  with the
 membership functions   µ A (x)  and  µ B (x)  base
d on algebraic 
pr
oduct  is  
 µ A ∩ B   =  
 µ A (x) µ B (x
)  ,    x  ∈   X   (Eq.  02)    
  04 Fuzzy U
nion  operator   U  ( OR  conn
ective ) applied to two fuzzy sets   A  an
d B
with the membe
rship functions   µ A (x)  and  µ B (x)  based on  mi
n/max 
ope
rations   
is  
 µ A U B   =   
max [ µ A (x)  ,  µ B (x) ]  ,   
 x  ∈   X    (Eq.  03)    
 
  05 Fuzzy Union  operator   U  ( O
R connectiv
e ) a
pplied to two fuzzy sets   A  and B
with 
the membership functions   µ A (x)  an
d  µ B (x
)  b
ased on  algebraic sum  is  
 
 µ A U B  
 =   µ A (x) + µ B (x) - µ A (x
) µ B (x)  , 
  x  ∈  X    (Eq.  04)   
  06 Fuzzy Compliment  operator   ( ― )  (
 NOT op
eration  ) appl
ied to fuzzy set   A 
with 
the membership function   µ A (x
)  is µ 
    =  
 1 - µ A (x) , 
 x  ∈ X    (Eq.  05)    
  07 Fuzzy relations  co
mbining two fuzzy sets by connective  "min op
eration"  is a
n
operation by cartesian product   
 R :  X x Y  →  [0 , 1]. 
µ R(
x,y) = mi
n[µ A (x)
, µ B (y
)]     (E
q.  06)    or 
µ R(
x,y) =  µ A (x) µ B (y)  
             (Eq.  07)  
Example :   Re
lation   R  b
etween fruit colour  x  
an
d maturity grade   y  ch
aracterized by base set 
 
     Y 
x         V h-m m 
G 1
 0.5 0.0 
Y 0
.3 1 0.4 
R 0
 0.2 1 
 
li
nguistic colorset  
 X = {green, yellow, red}     
matur
ity grade as  Y  = {v
erdant, half-mature, mature}  
  
 
  08 Max-Mi
n Composition - c
ombines the fuzzy relations   
varia
bles, say  (x , y
) and (y
 , z) ;  x ∈ A ,   y ∈ B ,   z ∈ C .  
co
nsider the relations : 
   
  R1(x , y
)  = { ((x , y) , µR1 (
x , y))  |  (x , y)  ∈ A x B } 
     R 2(y
 , z)  = { ((y , y) , µR1 (
y , z))  |  (y , z)  ∈ B x C } 
The
 domain of  R1 is  A 
x B  and
 the domain of  R2 is B x C   
 max-min c
omposition denoted by  R1 ο R2  with me
mbership function  µ R1
 ο R2  
 
 
   R1 ο R2  =  { 
((x , z) ,           (min ( µR1 (x , y
) ,  µR2 (y
 , z))))} ,    
                                                            (x , z)  ∈ A x C ,  y ∈ B                 (Eq.  08)
Thus   R1 ο R2  
 is re
lation in the domain   A x
 C 
 
 
    A
ymaxR 
 • Fuzzy Proposi
tional  
 
A  fuzz
y  proposition  is  a  statement  P  which  acquires  a  fuzzy  truth 
value  T(P) . 
Exa
mple :  
    P       :  Ram is honest 
    T(
P)  
= 0.8 ,   means  P  is partially true. 
    T(
P)  
= 1 ,      means  P   is absolutely  true. 
 
 
     
• Fuzzy C
onnectives 
 
Th
e fuzzy logic is similar to crisp logic supported  by connectives . 
Tab
le below illustrates the defi nitions of fuzzy connectives. 
Table :  
Fuzzy Connectves 
Connective Symbols Usage Definition 
N
agation ¬ ¬  P  1 
– T(P) 
Disjuction ∨ P ∨  Q M
ax[T(P) , T(Q)] 
Conjuction ∧ P ∧  Q m
in[T(P) , T(Q)] 
Implication ⇒ P ⇒  Q ¬ P ∨  Q
 = max (1-T(P), T(Q)] 
 
Here  
P , Q   are fuzzy prop
osition and   T(P
) ,  T(Q)   are th
eir truth values.  
− th
e P an
d Q 
 are  related by the  ⇒  operator  are known as antecedents 
and  consequent  respectively.  
− a
s  crisp  logic,  here  in  fuzzy logic  also  the  operator  ⇒  represents 
IF
-THEN   statem
ent like,  
   
    IF   x 
  is  
 A  
 THEN    y   is  
 B,  
   is eq
uivalent to   
 
      R  = (A x B)  U  (¬ A x Y) 
   th
e membership function of   R  is given 
by 
       µR (x , y)  = ma
x [min  (µA (x)
 ,  µB (y
)) ,  1 − µA (x
)] 
− For the
 compound implication statement  like      
   
     IF   x  is 
 A  TH
EN  y  i
s  B,   EL
SE  y is 
 C   is eq
uivalent to    
  
       R  
= (A
 x B)   U (¬ A x C)  
   
   t
he membership function of   R  i
s given by  
   
     µR (x , y)   = max
 [min  (µA 
(x) ,  µB (y
)) , min (1 − µA (x
),  µC (
y))]  
  Ex
ample 1 :  (Ref : Previous slide) 
 
           P  :  Mar
y  is efficient ,     T(P)  = 0.8 , 
           Q  :  Ram   is efficient ,    T(Q)  = 0.65 , 
       ¬ P   :  Mary  is efficient ,    T(¬ P)  = 1 − T(P)  = 1 − 0.8  = 0.2 
    P ∧  Q  :  Mary  is efficient  and so is Ram,   i.e.   
                   T(P ∧  Q)  = min (T(P), T(Q)) = min (0.8, 0.65)) = 0.65   
    P ∨  Q  :  Either Mary or Ram is efficient  i.e.   
                   T(P ∨   Q)  = max (T(P), T(Q)) = max (0.8, 0.65)) = 0.8 
    P ⇒  Q :  If Mary  is efficient  then so is Ram,   i.e.    
                   T(P ⇒  Q) = max (1 − T(P), T(Q)) = max (0.2, 0.65)) = 0.65 
  
 
     
 Ex
ample 2  :    (Ref
 : Previous slide on fuzzy connective) 
 
Let    
  X   =  {a, b, c, d}     ,      
  
         A   =  {(a, 0)      (b, 0.8)   (c, 0.6)    (d, 1)} 
           B   =  {(1, 0.2)   (2, 1)      (3, 0.8)   (4, 0)}  
           C   =  {(1, 0)      (2, 0.4)   (3, 1)      (4, 0.8)}    
                  
  
         Y   = { 1, 2, 3, 4}   
 the universe of discourse could be viewed as   
  
                   { (1, 1)  (2, 1)  (3, 1)  (4, 1) }   
   
              i.e.,  a fuzzy set all of whose elements x  have  µ(x)
 = 1 
 
Determi
ne the implication relations 
  (i)  
   If  x  is  A  THEN  y is  B              
  (ii)    If  x  is  A  THEN  y is  B    Else  y is  C   
Solu
tion  
To determine  implication relations (i)  compute  :             
The operator ⇒  represents   IF-TH
EN  statem
ent like,  
  IF  x  i
s  A  THEN  y  is  B,    is equi
valent to   R
  = (A x B)  U (¬ A x Y)    and 
the members
hip function  R  is given by 
    µR (x , 
y)  = max [min ( µA (x
) ,  µB (y))
 , 1 − µA (x)]    
  Fuzzy
 Intersection  A x B  is defined as :  
for all x in the set X,       
(A ∩ B)(x) = min [A(x), B(x)],   
  
          
   
   B 
A       1 2 3 4 
a 0 0 0 0 
b 0.
2 0.8 0.8 0 
c 0.
2 0.6 0.6 0 
d 0.
2 1 0.8 0 
   Fuzzy In
tersection  ¬A x Y is
 defined as :
for all x in the set X   
 (¬A ∩ Y
)(x) = min [A(x), Y(x)],  
     
        
   
   y 
A      1 2 3 4 
a 1 1 
1 1 
b 0.
2 0.2 0.2 0.2
c 0.
4 0.4 0.4 0.4
d 0 0 
0 0 
  
 
  Fuzzy Un
ion is defined as   (A ∪ 
B)(x)  =  max [A(x), B(x)]  for   all   x  ∈ X 
Therefo
re   R 
 = (A x B)  U (¬ A x Y)    gi
ves  
  
          
   
   y 
x        1 2 3 4 
a 1 1 1 1 
b 0.
2 0.8 0.8 0 
c 0.
4 0.6 0.6 0.4
d 0.
2 1 0.8 0 
 
Th
is represents   I
f  x is  A  THEN  y is  B   ie  T(A ⇒ B) = max (1- T(A), T(B))
 
    A x B  = ¬Ax Y =
   
  R  = 
 
 To determ
ine  implication relations (ii)  compute  :  
  (R
ef : Previous slide)     
    
Given  X
   =  {a, b, c, d}     ,      
  
         A   =  {(a, 0)      (b, 0.8)   (c, 0.6)    (d, 1)} 
           B   =  {(1, 0.2)   (2, 1)      (3, 0.8)   (4, 0)}  
           C   =  {(1, 0)      (2, 0.4)   (3, 1)      (4, 0.8)}  
                    
Here, the operator ⇒  rep
resents   IF
-THEN -ELSE  statem
ent like,  
   
    IF  x  is  A  THEN  y  is  B  Else  y is  C,    is eq
uivalent to    
  
       R 
 = (A x B)  U (¬ A x C)   
 an
d 
 
      t
he membership function of  R  
is given by 
        µR (x 
, y)  = max
 [min  (µA (x
) ,  µB (y)) , min(1 − µA (x
), µC (y)]   
  Fuzzy
 Intersection  A x B  is defined as :   
for all x in the set X,       
(A ∩ B)(x) = min [A(x), B(x)],   
  
          
   
   B 
A       1 2 3 4 
a 0 0 0 0 
b 0.
2 0.8 0.8 0 
c 0.
2 0.6 0.6 0 
d 0.
2 1 0.8 0 
   Fuzzy In
tersection  ¬A x Y is
 defined as :
for all x in the set X   
 (¬A ∩ C
)(x) = min [A(x), C(x)],  
     
        
   
   y 
A       1 2 3 4 
a 0 0.
4 1 0.8
b 0.
2 0.2 0.2 0.2
c 0.
4 0.4 0.4 0.4
d 0 0 
0 0 
  
 
  Fuzzy Un
ion is defined as   (A ∪ 
B)(x)  =  max [A(x), B(x)]  for   all   x  ∈ X 
Therefo
re   R  = (A
 x B)  U (¬ A x C)     
  gives  
  
          
   
   y 
x        1 2 3 4 
a 1 1 1 1 
b 0.
2 0.8 0.8 0 
c 0.
4 0.6 0.6 0.4
d 0.
2 1 0.8 0 
 
Th
is represents   I
f  x is  A  THEN  y is  B   Else  y  is  C   
 
    A x B  = ¬Ax C =
   
  R  = 
• Fuzzy Qu
antifiers 
In crisp logic,  the  predicates  are  quantified  by  quantifiers. 
Similarly,  in fuzzy logic  the proposit ions  are  quantified by quantifiers. 
There are two classes of fuzzy quantifiers : 
− Absolute qu
antifiers and 
− Relative quantifie
rs 
Examples :  
Absolute q
uantifiers Relative quantifiers 
 
r
ound about 250 almost 
much greater than 6 about 
some where around 20 most  
Fuzzification 
 
The fuzzification is a process
 of transforming crisp values into grades of 
membership for linguistic terms of fuzzy sets.  
T
he purpose is to allow a fuzzy cond ition in a rule to be interpreted.  
 
 • Fuzzificatio
n of the car speed   
Ex
ample 1 :   Speed  X0 
 =  70km/h  
Fig belo
w shows  the  fuzzification  of  the  car  speed  to  characterize a 
low  and  a  medium speed fuzzy set. 
 
  
 
 
 
 
 
  
 
 
  
 
 
 
 
 
C
haracterizing  two  grades,  low  and  
medium speed fuzzy set 
 Given
 car speed value X0=7
0km/h  : 
grad
e µA(x0) = 0
.75 belong
s to 
fuzzy low, and grade µB(x0) = 
0.25
belo
ngs to fuzzy medium  
  Ex
ample 2 :   Speed  X0 
 =  40km/h  
 
   
 
 
 
 
    
    
  
 
 
 
  
 
 
 
Charac
terizing  five  grades,  Very low, 
low, medium, high and  very high 
speed fuzzy set 
  Given
 car speed value X0=4
0km/h  : 
grad
e µA(x0) = 
0.6 belo
ngs to fuzzy 
low, and grade  µB(x0) = 0.4 belong
s 
to fuzzy medium . 
 
 
 
    1 
 
.8
 
 
.6 
 
.4 
 
.2 
 
0 
2
0 40 60 80 100 120 140
Speed   X 0=7
0 k m / hµ 
µA µBLow Med
ium
Speed  X 0=4
0 k m / hµ 
1 
 
.8 
 
.6 
 
.4 
 
.2 
 
0 
10
 20 30 40 50 60 70 80 90 00V L
ow Med
ium
Low Hig
h V H
igh 
Fuzzy Inference  
F
uzzy  Inferencing  is  the  core  element of a fuzzy system.  
Fuzzy  Inferencing  combines -  the facts ob tained from the fuzzification  with the 
rule base,  and then conducts the fuzzy reasoning process.   
 
 Fu
zzy  Inference  is  also  known as  app
roximate reasoning .  
F
uzzy  Inference  is  computational  procedur es  used  for  evaluating  linguistic  
descriptions.  Two  important  inferring  procedures  are 
− G
eneralized  Modus  Ponens ( GM
P) 
− Gen
eralized  Modus  Tollens  (G
MT)   
 
  
  
 • Ge
neralized Modus Ponens  (G
MP) 
  
  This  is f
ormally stated as  
   
 If    x  is  A  THEN  y  is  B 
     
          x   
is   ¬A 
     
          y  
 is   ¬B 
     wh
ere  A , B ,  ¬A ,  ¬B 
 are fuzzy terms.  
 
Note 
: Every fuzzy linguistic statements above the line is analytically known 
and what is below the line is analytically unknown. 
 
To  co
mpute  the  membership  function  ¬B ,
  the  max-min  composition 
of  fuzzy  set ¬A  
with  R(x , 
y) which is the known implication relation 
(IF-T
HEN)  i
s used.  i.e.    ¬B  =  ¬A  ο  R(x
, y) 
In terms of membership function  
 
  
        µ ¬B  (y)   = 
  max  (min (  µ ¬A
 (x) , 
 µR (x , 
y)))    w
here  
   
       µ ¬A
 (x
)     is
 the membership function of   ¬A 
,   
    
      µR (x ,
 y)  is the  
membership function of the implication relation and  
    
      µ ¬B  (y
)   
  is the m
embership function of   ¬B    
 • Ge
neralized Modus Tollens  (GM
T)   
  This  is f
ormally stated as  
   
 If    x  is  A  THEN  y is  B 
     
          y  
 is  ¬B 
              x   is  ¬A 
   
  wh
ere  A 
,  B ,  ¬A ,  ¬B  ar
e fuzzy terms.  
 
Note 
: Every fuzzy linguistic statements above the line is analytically known 
and what is below the line is analytically unknown. 
 
To 
 compute  the  membership  function  ¬A ,
  the  max-min  composition 
of  fuzzy  set ¬B  
with  R(x , 
y) which is the known implication relation 
(IF-T
HEN)  i
s used.  i.e.    ¬A  =  ¬B  ο  R(x
, y) 
 
In t
erms of membership function  
 
  
        µ ¬A  (y
)  = 
  max  (min (  µ ¬B
 (x) , 
 µR (x , 
y)))    w
here  
   
       µ ¬B
 (x
)     is
 the membership function of   ¬B 
,   
    
      µR (x ,
 y)  is the  
membership function of  the implication relation and  
    
      µ ¬A  (y
)   
  is the 
membership function of   ¬A   
 
 
 
     
 Exam
ple :    
  
App
ly  the fuzzy Modus Ponens rules  to deduce  Rotation is quite slow?  
Given  : 
 (i
)   If the temperature is high then then the rotation is slow. 
 (ii)  The temperature is very high. 
Let  H (H
igh) , VH (Very High) ,  S (Slow) and QS  (Quite Slow)   ind
icate the
associated fuzzy sets.  
Let  the set for temperatures be   X
 = {30, 40, 50, 60, 70, 80, 90, 100}  , a
nd 
Let  
the set of rotations per minute be  Y
 = {10, 20, 30, 40, 50, 60} and  
 
     H = {(70, 1)  (80, 1)  (90, 0.3)} 
    VH = {(90, 0.9) (100, 1)} 
    QS = {10, 1)  (20, 08) }       S = {(30, 0.8)  (40, 1) (50, 0.6) 
To derive   R(x
, y) repr
esenting the implication relation (i) above,  compute   
   
   R (x, y)  = max (H x  S ,   ¬ H x Y)    
   
      
     
   10 20 30 40 50 60
30 0 0 0 0 0 0 
40 0 0 0 0 0 0 
50 0 0 0 0 0 0 
60 0 0 0 0 0 0 
70 0 0 0.8 1 0.6 0 
80 0 0 0.8 1 0.6 0 
90 0 0 0.3 0.3 0.3 0 
100 0 0 0 0 0 0 
   
     
           
   10 20 30 40 50 60
30 1 1 1 1 1 1 
40 1 1 1 1 1 1 
50 1 1 1 1 1 1 
60 1 1 1 1 1 1 
70 0 0 0 0 0 0 
80 0 0 0 0 0 0 
90 0.7 0.7 0.7 0.7 0.7 0.7
100 1 1 1 1 1 1 
  
 
     H x
 S = H x Y
= 
   
  
  
      
     
   10 20 30 40 50 60
30 1 1 1 1 1 1 
40 1 1 1 1 1 1 
50 1 1 1 1 1 1 
60 1 1 1 1 1 1 
70 0 0 0.8 1 0.6 0 
80 0 0 0.8 1 0.6 0 
90 0.7 0.7 0.7 0.7 0.7 0.7
100 1 1 1 1 1 1 
  
 
     
  
To 
deduce Rotation is quite slow, we  make use of the composition rule 
 
QS =
 VH  ο   R (x, y)  
  
      
     
   10 20 30 40 50 60 
30 1 1 1 1 1 1 
40 1 1 1 1 1 1 
50 1 1 1 1 1 1 
60 1 1 1 1 1 1 
70 0 0 0 0 0 0 
80 0 0 0 0 0 0 
90 0.7 0.7 0.7 0.7 0.7 0.7 
100 1 1 1 1 1 1 
  
 
  R(
x,Y) = 
= [0  
0  0  0  0  0  0.9  1]    x
= [1 
 1  1  1  1  1  ]     
Fuzzy Rule Based System 
 
 The 
fuzzy linguistic descriptions are formal representation of systems made 
through fuzzy IF-THEN rule. They encode knowledge about a system in 
statements of the form  : 
 
IF
 (a set of conditions) are satisfied THEN (a set of consequents) can be inferred.
IF
 (x1 is
 A1,   x 2 is A 2,
    xn is An )  THEN (y 1 is
 B1,  
 y2 is
 B2,
    yn is Bn)   
w
here linguistic variables xi, y j 
take the values of fuzzy sets Ai an
d Bj
respectively.
 
Exam
ple : 
I
F        there  is  "heavy "  rain  and  "strong " winds 
THEN   there  must  "severe "  flood  warnings. 
Here,
 heavy  , stron
g , and seve
re   are
  fuzzy sets qualifying the variables  rain, 
wind,   and  flood  warnings  respectively. 
 
A collect
ion of  rules  referring  to a  pa rticular  system  is  known  as a fuzzy 
rule base.  If the conclusion C to be drawn from a rule base R is the conjunction 
of all the individual consequents C i  
 of  each  rule , then  
    
  C = C 1  ∩   
C2  ∩ . 
. .  ∩  Cn    
  where   
    
 µc (y 
)  = min (  µc1(y ),
  µc2(y ) ,  µcn(y )) , 
   ∀ y ∈  Y 
  
   where Y   is  
universe  of  discourse. 
On the other hand, if the conclusion C to be drawn from a rule base Ris the 
disjunction of the individual consequents of each rule, then  
      C = 
C1  U  
 C2  U . . 
.  U  Cn     
 where   
 
      µc (y )  
= max (  µc1 (y ),
  µc2(y ) ,  µcn (
y )) ,    ∀ y ∈  Y   wh
ere 
    
  Y   is universe of discourse. 
 
 
     
Defuzzification 
 
 In 
many situations, for a system whose output is fuzzy, it is easier to take a 
crisp decision  if the output  is  represented  as  a single quantity. This 
conversion  of  a  single  crisp  value  is called  Defuzzification.    
 
Defuzzificat
ion is the reverse process of fuzzification.   
 
The  
typical Defuzzification  methods are   
− Ce
ntroid method,  
− Ce
nter of sums,  
− Mean
 of maxima.   
 
Centroid method   
It  is  also  known  as  the  "center of gravity"  of  area  method.   
It obtains the centre of area (x*)  occupied  by  the  fuzzy set .  
For discrete membership function, it is given by 
  
         
                         xi µ (xi)   
 
     x*  =  
                             where  
  
                     
   µ (x
i)                      
     
          
   n  represents the number elements in the sample,  and  
             xi  are
 the elements,   and  
             µ (xi)  is th
e membership function. 
 
 
    
 
 
 
 Σ 
i=
1  n
Σ 
i=
1  nModule - II 
 
Probabilistic Reasoning 
 
 Probability theory is used to discuss events, categories, and hypotheses about which 
there is not 100% certainty. 
 We might write A→B, which means that if A is true, then B is true. If we are unsure 
whether A is true, then we cannot make use of this expression.  
 In many real-world situations, it is very useful to be able to talk about things that lack 
certainty. For example, what will the weather be like tomorrow? We might formulate 
a very simple hypothesis based on general observation, such as “it is sunny only 10% 
of the time, and rainy 70% of the time”. We can use a notation similar to that used for 
predicate calculus to express such statements: 
P(S) = 0.1 
P(R) = 0.7 
The first of these statements says that the probability of S (“it is sunny”) is 0.1. The 
second says that the probability of R is 0.7. Probabilities are always expressed as real 
numbers between 0 and 1. A probability of 0 means “definitely not” and a probability 
of 1 means “definitely so.” Hence, P(S) = 1 means that it is always sunny. 
 Many of the operators and notations that are used in prepositional logic can also be 
used in probabilistic notation. For example, P(￢S) means “the probability that it is 
not sunny”; P(S ∧ R) means “the probability that it is both sunny and rainy.” P(A ∨ 
B), which means “the probability that either A is true or B is true,” is defined by the 
following rule:    P(A ∨ B) = P(A) + P(B) - P(A ∧ B) 
 
 The notation P(B|A) can be read as “the probability of B, given A.” This is known as 
conditional probability—it is conditional on A. In other words, it states the probability that B is true, given that we already know that A is true. P(B|A) is defined by the 
following rule: Of course, this rule cannot be used in cases where P(A) = 0. 
 For example, let us suppose that the likelihood that it is both sunny and rainy at the 
same time is 0.01. Then we can calculate the probability that it is rainy, given that it is 
sunny as follows: 
 
 The basic approach statistical methods adopt to deal with uncertainty is via the 
axioms of probability:  
 Probabilities are (real) numbers in the range 0 to 1.  
 A probability of P(A) = 0 indicates total uncertainty in A, P(A) = 1 total 
certainty and values in between some degree of (un)certainty.  
 Probabilities can be calculated in a number of ways.  
 Probability = (number of desired outcomes) / (total number of outcomes)  
So given a pack of playing cards the probability of being dealt an ace from a full 
normal deck is 4 (the number of aces) / 52 (number of cards in deck) which is 1/13. 
Similarly the probability of being dealt a spade suit is 13 / 52 = 1/4.  
If you have a choice of number of items k from a set of items n then the 
formula is applied to find the number of ways of making this choice. (! 
= factorial).  
So the chance of winning the national lottery (choosing 6 from 49) is 
to 1.  
 Conditional probability, P(A|B), indicates the probability of of event A given that we 
know event B has occurred. 
 A Bayesian Network is a directed acyclic graph:  
 A graph where the directions are links which indicate dependencies that exist 
between nodes.  
 Nodes represent propositions about events or events themselves.  
 Conditional probabilities quantify the strength of dependencies.   Consider the following example:  
 The probability, 
 that my car won't start.  
 If my car won't start then it is likely that  
o The battery is flat or  
o The staring motor is broken.  
 In order to decide whether to fix the car myself or send it to the garage I make the 
following decision:  
 If the headlights do not work then the battery is likely to be flat so i fix it 
myself.  
 If the starting motor is defective then send car to garage.  
 If battery and starting motor both gone send car to garage.  
 The network to represent this is as follows:  
 
Fig.  A simple Bayesian network 
Bayesian probabilistic inference 
 Bayes’ theorem can be used to calculate the probability that a certain event will occur 
or that a certain proposition is true 
 The theorem is stated as follows: 
. 
 P(B) is called the prior probability of B. P(B|A), as well as being called the 
conditional probability, is also known as the posterior probability of B. 
 P(A ∧ B) = P(A|B)P(B)   Note that due to the commutativity of ∧, we can also write   
 P(A ∧ B) = P(B|A)P(A)  
 Hence, we can deduce: P(B|A)P(A) = P(A|B)P(B)  
 This can then be rearranged to give Bayes’ theorem: 
 
 Bayes Theorem states:  
 
 
 This reads that given some evidence E then probability that hypothesis 
is 
true is equal to the ratio of the probability that E will be true given 
times the 
a priori evidence on the probability of 
and the sum of the probability of E 
over the set of all hypotheses times the probability of these hypotheses.  
 The set of all hypotheses must be mutually exclusive and exhaustive.  
 Thus to find if we examine medical evidence to diagnose an illness. We must 
know all the prior probabilities of find symptom and also the probability of 
having an illness based on certain symptoms being observed.  
 Bayesian statistics lie at the heart of most statistical reasoning systems. How is Bayes 
theorem exploited?  
 The key is to formulate problem correctly:  
P(A|B) states the probability of A given only B's evidence. If there is other 
relevant evidence then it must also be considered.  
 All events must be mutually exclusive. However in real world problems events are not 
generally unrelated. For example in diagnosing measles, the symptoms of spots and a 
fever are related. This means that computing the conditional probabilities gets 
complex.  
In general if a prior evidence, p and some new observation, N then computing  
  
grows exponentially for large sets of p  
  All events must be exhaustive. This means that in order to compute all probabilities 
the set of possible events must be closed. Thus if new information arises the set must 
be created afresh and all probabilities recalculated.  
 Thus Simple Bayes rule-based systems are not suitable for uncertain reasoning.  
 Knowledge acquisition is very hard.  
 Too many probabilities needed -- too large a storage space.  
 Computation time is too large. 
 Updating new information is difficult and time consuming.  
 Exceptions like ``none of the above'' cannot be represented.  
 Humans are not very good probability estimators.  
 However, Bayesian statistics still provide the core to reasoning in many uncertain 
reasoning systems with suitable enhancement to overcome the above problems. We 
will look at three broad categories:  
 Certainty factors 
 Dempster-Shafer models 
 Bayesian networks.  
Bayesian networks are also called Belief Networks or Probabilistic Inference Networks. 
 
 
 
 
 
 
 Application Of Bayes Therom: 
 
  
Clinical Example: 
 
  
 
Definition and importance of knowledge 
 Knowledge can be defined as the body of facts and principles accumulated by human-
kind or the act, fact, or state of knowing 
 Knowledge is having familiarity with language, concepts, procedures, rules, ideas, 
abstractions, places, customs, facts, and associations, coupled with an ability to use 
theses notions effectively in modeling different aspects of the world 
 The meaning of knowledge is closely related to the meaning of intelligence 
 Intelligent requires the possession of and access to knowledge 
 A common way to represent knowledge external to a computer or a human is in the 
form of written language 
 Example:  
 Ramu is tall – This expresses a simple fact, an attribute possessed by a person 
 Ramu loves his mother – This expresses a complex binary relation between 
two persons 
 Knowledge may be declarative or procedural 
 Procedural knowledge is compiled knowledge related to the performance of 
some task. For example, the steps used to solve an algebraic equation  Declarative knowledge is passive knowledge expressed as statements of facts 
about the world. For example, personnel data in a database, such data are 
explicit pieces of independent knowledge 
 Knowledge includes and requires the use of data and information 
 Knowledge combines relationships, correlations, dependencies, and the notion of 
gestalt with data and information 
 Belief is a meaningful and coherent expression. Thus belief may be true or false 
 Hypothesis is defined as a belief which is backed up with some supporting evidence, 
but it may still be false 
 Knowledge is true justified belief 
 Epistemology is the study of the nature of knowledge 
 Metaknowledge is knowledge about knowledge, that is, knowledge about what we 
know 
 
  Knowledge Based Systems 
 Systems that depend on a rich base of knowledge to perform difficult tasks 
 It includes work in vision, learning, general problem solving and natural language 
understanding 
 The systems get their power from the expert knowledge that has been coded into 
facts, rules, heuristics and procedures. 
 In Fig 2.1, the knowledge is stored in a knowledge base separate from the control and 
inferencing components 
 It is possible to add new knowledge or refine existing knowledge without recompiling 
the control and inferencing programs 
 Components of a knowledge based system 
 
 
 Input-Output       
     Unit Inference-
Control Unit Knowledge 
Base  
Representation of knowledge 
 The object of a knowledge representation is to express knowledge in a computer 
tractable form, so that it can be used to enable our AI agents to perform well. 
 A knowledge representation language is defined by two aspects: 
 Syntax   The syntax of a language defines which configurations of the 
components of the language constitute valid sentences. 
 Semantics   The semantics defines which facts in the world the sentences refer 
to, and hence the statement about the world that each sentence makes. 
 Suppose the language is arithmetic, then ‘x’, ‘=’ and ‘y’ are components (or symbols 
or words) of the language the syntax says that ‘x = y’ is a valid sentence in the 
language, but ‘= = x y’ is not the semantics say that ‘x = y’ is false if y is bigger than 
x, and true otherwise 
 The requirements of a knowledge representation are: 
 Representational Adequacy – the ability to represent all the different kinds of 
knowledge that might be needed in that domain. 
 Inferential Adequacy – the ability to manipulate the representational structures 
to derive new structures (corresponding to new knowledge) from existing 
structures.  
 Inferential Efficiency – the ability to incorporate additional information into 
the knowledge structure which can be used to focus the attention of the 
inference mechanisms in the most promising directions. 
 Acquisitional Efficiency – the ability to acquire new information easily.  
Ideally the agent should be able to control its own knowledge acquisition, but 
direct insertion of information by a ‘knowledge engineer’ would be 
acceptable. Finding a system that optimizes these for all possible domains is 
not going to be feasible. 
 In practice, the theoretical requirements for good knowledge representations can 
usually be achieved by dealing appropriately with a number of practical requirements:  
 The representations need to be complete – so that everything that could 
possibly need to be represented can easily be represented.  They must be computable – implementable with standard computing 
procedures. 
 They should make the important objects and relations explicit and accessible – 
so that it is easy to see what is going on, and how the various components 
interact. 
 They should suppress irrelevant detail – so that rarely used details don’t 
introduce unnecessary complications, but are still available when needed.  
 They should expose any natural constraints – so that it is easy to express how 
one object or relation influences another. 
 They should be transparent – so you can easily understand what is being said. 
 The implementation needs to be  concise and fast – so that information can be 
stored, retrieved and manipulated rapidly. 
 The four fundamental components of a good representation 
 The lexical part – that determines which symbols or words are used in the 
representation’s vocabulary. 
 The structural or syntactic part – that describes the constraints on how the 
symbols can be arranged, i.e. a grammar. 
 The semantic part – that establishes a way of associating real world meanings 
with the representations. 
 The procedural part – that specifies the access procedures that enables ways of 
creating and modifying representations and answering questions using them, 
i.e. how we generate and compute things with the representation. 
 Knowledge Representation in Natural Language 
 Advantages of natural language 
o It is extremely expressive – we can express virtually everything in 
natural language (real world situations, pictures, symbols, ideas, 
emotions, reasoning).  
o Most humans use it most of the time as their knowledge 
representation of choice  
 Disadvantages of natural language 
o Both the syntax and semantics are very complex and not fully 
understood. 
o There is little uniformity in the structure of sentences. o It is often ambiguous – in fact, it is usually ambiguous. 
 Knowledge Organization 
 The organization of knowledge in memory is key to efficient processing 
 Knowledge based systems performs their intended tasks 
 The facts and rules are easy to locate and retrieve. Otherwise much time is wasted in 
searching and testing large numbers of items in memory 
 Knowledge can be organized in memory for easy access by a method known as 
indexing 
 As a result, the search for some specific chunk of knowledge is limited to the group 
only 
 Knowledge Manipulation 
 Decisions and actions in knowledge based systems come from manipulation of the 
knowledge 
 The known facts in the knowledge base be located, compared, and altered in some 
way 
 This process may set up other subgoals and require further inputs, and so on until a 
final solution is found 
 The manipulations are the computational equivalent of reasoning. This requires a 
form of inference or deduction, using the knowledge and inferring rules. 
 All forms of reasoning requires a certain amount of searching and matching. 
 The searching and matching operations consume greatest amount of computation time 
in AI systems 
 It is important to have techniques that limit the amount of search and matching 
required to complete any given task 
 
 
 
 
 
 
 
 
 Module 3  
Matching techniques : 
 
Matching is the process of comparing two or more structures to discover their likenesses or 
differences. The structures may represent a wide range of objects including physical entit ies, 
words or phrases in some language, complete classes of things, general concepts, relations 
between complex entities, and the like. The representations will be given in one or more of 
the formalisms like FOPL, networks, or some other scheme, and matchi ng will involve 
comparing the component parts of such structures.  
 Matching is used in a variety of programs for different reasons. It may serve to control 
the sequence of operations, to identify or classify objects, to determine the best of a number 
of di fferent alternatives, or to retrieve items from a database. It is an essential operation such 
diverse programs as speech recognition, natural language understanding, vision, learning, 
automated reasoning, planning, automatic programming, and expert systems , as well as many 
others.  
 In its simplest form, matching is just the process of comparing two structures or 
patterns for equality. The match fails if the patterns differ in any aspect. For example, a 
match between the two character strings acdebfba and ac debeba fails on an exact match since 
the strings differ in the sixth character positions.  
 In more complex cases the matching process may permit transformations in the 
patterns in order to achieve an equality match. The transformation may be a simple change of 
some variables to constants, or ti may amount to ignoring some components during the match 
operation. For example, a pattern matching variable such as ?x may be used to permit 
successful matching between the two patterns (a b (c d ) e) and (a b ?x e) by binding ?x to (c, 
d). Such matching are usually restricted in some way, however, as is the case with the 
unification of two classes where only consistent bindings are permitted. Thus, two patterns 
such as  
  ( a b (c d) e f) and (a b ?x e ?x)  
would not match since ?x could not be bound to two different constants.  
 In some extreme cases, a complete change of representational form may be required 
in either one or both structures before a match can be attempted. This will be the case, for example, when one visual object is represented as a vector of pixel gray levels and objects to 
be matched are represented as descriptions in predicate logic or some other high level 
statements. A direct comparison is impossible unless one form has been transformed into the 
other. 
 In subsequent chapters we will see examples of many problems where exact matches 
are inappropriate, and some form of partial matching is more meaningful. Typically in such 
cases, one is interested in finding a best match between pairs of structures . This will be the 
case in object classification problems, for example, when object descriptions are subject to 
corruption by noise or distortion. In such cases, a measure of the degree of match may also be 
required.  
 Other types of partial matching may re quire finding a match between certain key 
elements while ignoring all other elements in the pattern. For example, a human language 
input unit should be flexible enough to recognize any of the following three statements as 
expressing a choice of preference for the low -calorie food item  
  I prefer the low -calorie choice.  
 I want the low -calorie item  
  The low -calorie one please.  
 Recognition of the intended request can be achieved by matching against key words 
in a template containing “low -calorie” and ignori ng other words except, perhaps, negative 
modifiers.  
 Finally, some problems may obviate the need for a form of fuzzy matching where an 
entity’s degree of membership in one or more classes is appropriate. Some classification 
problems will apply here if the boundaries between the classes are not distinct, and an object 
may belong to more than one class.  
 Fig 8.1 illustrates the general match process where an input description is being 
compared with other descriptions. As stressed earlier, their term object is  used here in a 
general sense. It does not necessarily imply physical objects. All objects will be represented 
in some formalism such a s a vector of attribute values, prepositional logic or FOPL 
statements, rules, frame -like structures, or other scheme. T ransformations, if required, may 
involve simple instantiations or unifications among clauses or more complex operations such as transforming a two -dimensional scene to a description in some formal language. Once the 
descriptions have been transformed into the same schema, the matching process is performed 
element -by-element using a relational or other test (like equality or ranking). The test results 
may then be combined in some way to provide an overall measure of similarity. The choice 
of measure will depend on the match criteria and representation scheme employed.  
 The output of the matcher is a description of the match. It may be a simple yes or no 
response or a list of variable bindings, or as complicated as a detailed annotation of the 
similarities and differences between the matched objects.  
`To summarize then, matching may be exact, used with or without pattern variables, partial, 
or fuzzy, and any matching algorithm will be based on such factors as  
  Choice of representation scheme for the objects be ing matched,  
  Criteria for matching (exact, partial, fuzzy, and so on),  
  Choice of measure required to perform the match in accordance with the  
chosen criteria, and  
Type of match description required for output.  
 In the remainder of this chapter we exam ine various types of matching problems and 
their related algorithms. We bin with a description of representation structures and measures 
commonly found in matching problems. We next look at various matching techniques based 
on exact, partial, and fuzzy approaches. We conclude the chapter with an example of an 
efficient match algorithm used in some rule -based expert systems.  
                   Fig Typical Matching Process  
 
Structures used in Matching  
 The types of list structures represent clauses in propositional or predicate logic such 
as (or  ~(MARRIED ?x ?y)  ~(DAUGHTER ?z ?y) (MOTHER ?y ?z)) or rules such 
as (and ((cloudy -sky) (low -bar-pressure) (high -humidity)) (conclude (rain likely)) or 
fragments of associative networks in below Fig   
 The other c ommon structures include strings of characters a 1 a2 . . . ak , where the a i 
belong to given alphabet A, vector X = (x 1 x2 . . . xn), where the x i represents attribute 
values, matrices M (rows of vectors), general graphs, trees and sets  
 
                             Fig Fragment of associative network and corresponding LISP Code   
 Variables  
 The structures are constructed from basic atomic elements, numbers and 
characters   Character string elements may represent either constants or variables  
 If variables,  they may be classified by either the type of match permitted or 
their value domains  
 An open variable can be replaced by a single item  
 Segment variable can be replaced by zero or more items  
 Open variable are replaced with a preceding question mark (?x. ?y,  ?class)  
 They may match or assume the value of any single string element or word, but 
they are subject to consistency constraints  
 For example, to be consistent, the variable ?x can be bound only to the same 
top level element in any single structure  
 Thus (a  ?x d ?x e) may match (a b d b e), but not (a b d a e)  
 Segment variable types will be preceded with an asterisk (*x, *z, *words)  
 This type of variable can match an arbitrary number or segment of contiguous 
atomic elements  
 For example, (* x d (e g) *y) wil l match the patterns  
(a (b c) d (e f) g h), (d (e f ) (g))  
 Subject variable may also be subject to consistency constraints similar to open 
variables  
 Nominal variables  
 Qualitative variables whose values or states have no order nor rank 
 It is possible to di stinguish between equality or inequality between two objects  
 Each state can be given a numerical code  
 For example, “marital status” has states of married, single, divorced or 
widowed. These states could be assigned numerical codes, such as married = 
1, sin gle = 2, divorced = 3 and widowed = 4  
 Ordinal variables  
 Qualitative variables whose states can be arranged in a rank order  
 It may be assigned numerical values  
 Foe example, the states very tall, tall, medium, short and very short can be 
arranged in order fr om tallest to shortest and can be assigned an arbitrary scale 
of 5 to 1  
 Binary variable   Qualitative discrete variables which may assume only one of two values, such 
as 0 or 1, good or bad, yes or no, high or low  
 Interval variables or Metric variables  
 Quali tative variables which take on numeric values and for which equal 
differences between values have the same significance  
 For example, real numbers corresponding to temperature or integers 
corresponding to an amount of money are considered as interval variables  
 Grap hs and Trees  
 A graph is a collection of points called vertices, some of which are connected 
by line segments called edges  
 Graphs are used to model a wide variety of real -life applications, including 
transportation and communication networks, projec t scheduling, and games  
 A graph G = (V, E) is an ordered pair of sets V and E. the elements V are 
nodes or vertices and the elements of E are a subset of V X V called edges  
 An edge joints two distinct vertices in V  
 Directed graphs or digraphs, have directe d edges or arcs with arrows  
 If an arc is directed from node n i to n j , node n i is said to be a parent or 
successor of n j and n j is the child or successor of n i 
 Undirected graphs have simple edges without arrows connecting the nodes  
 A path is a sequence of edges connecting two modes where the endpoint of 
one edge is the start of its successor  
 A cycle is a path in which the two end points coincide  
 A Connected graph is a graph for which every pair of vertices is joined by a 
path 
 A graph is complete if every el ement of V X V is an edge  
 A tree is a connected graph in which there are no cycles and each node has at 
most one parent below  
 A node with no parent is called root node  
 A node with no children is called leaf node  
 The depth of the root node is defined as zero  
 The depth of any other node is defined to be the depth of its parent plus 1  
 
 
 Sets and Bags  
 A set is represented as an unordered list of unique elements such as the set (a d 
f c) or (red blue green yellow)  
 A bag is a set  which may contain more than one copy of the same member a, 
b, d and e  
 Sets and bags are structures used in matching operations  
 
Measure for Matching  
 The problem of comparing structures without the use of pattern matching variables. 
This requires consi deration of measures used to determine the likeness or similarity 
between two or more structures   The similarity between two structures is a measure of the degree of association or 
likeness between the object’s attributes and other characteristics parts.  
 If the describing variables are quantitative, a distance metric is used to measure the 
proximity 
 Distance Metrics  
 For all elements x, y, z of the set E, the function d is metric if and only if  
d(x, x) = 0  
d(x,y) ≥ 0 
d(x,y) = d(y,x)  
d(x,y) ≤ d(x,z) + d(z,y)  
 The Minkowski metric is a general distance measure satisfying the above 
assumptions  
 It is given by 
     n 
dp = [ ∑|x i - yi|p ]1/p 
               i = 1 
 For the case p = 2, this metric is the familiar Eucli dian distance. Where p = 1, d p is 
the so -called absolute or city block distance  
 Probabilistic measures  
 The representation variables should be treated as random variables  
 Then one requires a measure of the distance between the variates, their 
distributions,  or between a variable and distribution 
 One such measure is the Mahalanobis distance which gives a measure of the 
separation between two distributions  
 Given the random vectors X and Y let C be their covariance matrix 
 Then the Mahalanobis distance is given by 
D = X’C-1Y 
 Where the prime (‘) denotes transpose (row vector) and C-1 is the inverse of C  
 The X and Y vectors may be adjusted for zero means bt first substracting the 
vector means u x and u y   Another popular probability measure is the product moment corr elation r, given 
by 
               Cov(X, Y)  
r =       --------------------  
             [ Var(X)* Var(Y)]1/2 
 
 Where Cov and Var denote covariance and variance respectively 
 The correlation r, which ranges between -1 and +1, is a measure of similarity 
frequ ently used in vision applications  
 Other probabilistic measures used in AI applications are based on the scatter of 
attribute values  
 These measures are related to the degree of clustering among the objects  
 Conditional probabilities are sometimes used  
 For ex ample, they may be used to measure the likelihood that a given X is a 
member of class C i , P(C i| X), the conditional probability of C i given an observed 
X 
 These measures can establish the proximity of two or more objects  
  Qualitative measures  
 Measures betw een binary variables are best described using contingency tables in 
the below  
 Table  
 
 The table entries there give the number of objects having attribute X or Y with 
corresponding value of 1 or 0  
 For example, if the objects are animals might be horne d and Y might be long 
tailed. In this case, the entry a is the number of animals having both horns and 
long tails   Note that n = a + b + c + d, the total number of objects  
 Various measures of association for such binary variables have been defined  
 For examp le 
a                              a                                    a + d  
   -----------------------    =      ---- ,                              ----------  
         a + b + c + d                 n                                       n  
 
 
    a                                                                     a     
                                       ----------------  ,                                                    --------  
                                         a + b + c                                                             b + c  
 Contingency tables are useful for describing other qualitative variables, both 
ordinal and nominal. Since the methods are similar to those for binary variables  
 Whatever the variable types used in a measure, the y should all be properly scaled 
to prevent variables having large values from negating the effects of smaller 
valued variables  
 This could happen when one variable is scaled in millimeters and another variable 
in meters  
 Similarity measures  
 Measures of dissi milarity like distance, should decrease as objects become more 
alike  
 The similarities are not in general symmetric  
 Any similarity measure between a subject description A and its referent B, 
denoted by s(A,B), is not necessarily equal  
 In general, s(A,B) ≠ s(B,A) or “A is like B” may not be the same as “B is like A”  
 Tests on subjects have shown that in similarity comparisons, the focus of attention 
is on the subject and, therefore, subject features are given higher weights than the 
referent   For example, in te sts comparing countries, statements like “North Korea is similar 
to Red China” and “Red China is similar to North Korea” were not rated as 
symmetrical or equal  
 Similarities may depend strongly on the context in which the comparisons are 
made  
 An interesting family of similarity measures which takes into account such factors 
as asymmetry and has some intuitive appeal has recently been proposed 
 Let O ={o 1, o2, . . . } be the universe of objects of interest  
 Let A i be the set of attributes used to represent o i  
 A similarity measure s which is a function of three disjoint sets of attributes 
common to any two objects A i and A j is given as  
s(A i, Aj) = F(A i & A j, A i - Aj, A j - Ai) 
 Where A i & A j is the set of features common to both o i and o j 
 Where Ai - Aj is the set  of features belonging to o i and not o j 
 Where Aj - Ai is the set of features belonging to o j and not o i  
 The function F is a real valued nonnegative function 
s(A i, Aj) = af(A i & A j) – bf(A i - Aj) – cf(A j - Ai) for some a,b,c ≥ 0  
 Where f is an additive int erval metric function 
 The function f(A) may be chosen as any nonnegative function of the set A, like 
the number of attributes in A or the average distance between points in A  
       f(A i & A j) 
S(A i, A2j) =        -----------------------------------------  
               f(A i & A j) + af(A i - Aj) + bf(A j - Ai)  
for some a,b ≥ 0 
 When the representations are graph structures, a similarity measure based on the 
cost of transforming one graph into the other may be used  
 For example, a procedure to find a measure of similarity between two labeled 
graphs decomposes the graphs into basic subgraphs and computes the minimum 
cost to transform either graph into the other one, subpart -by-subpart  
Matching like Patterns   We consider procedures which amount to performing a complete match between two 
structures  
 The match will be  accomplished by comparing the two structures and testing for 
equality among the corresponding parts  
 Pattern variables will be used for instantiations of some parts subject to restrictions  
 Matching Substrings  
 A basic function required in many match algorit hms is to determine if a substring 
S2 consisting of m characters occurs somewhere in a string S 1 of n characters, m ≤ 
n 
 A direct approach to this problem is to compare the two strings character -by-
character, starting with the first characters of both  S 1 and S 2 
 If any two characters disagree, the process is repeated, starting with the second 
character of S 1 and matching again against S 2 character -by-character until a match 
is found or disagreements occurs again 
 This process continues until a match occurs or  S1 has no more characters  
 Let i and j be position indices for string S1 and a k position index for S 2 
 We can perform the substring match with the following algorithm  
i := 0  
While i ≤ (n – m + 1) do 
begin  
 i := i + 1;  
 j := i;  
 k :=1;  
while  S1(j) = S 2(k)  do 
begin  
 if k = m  
  writeln(“success”)  
 else do   begin  
   j := j +1;  
   k := k + 1;  
  end 
 end 
end 
writeln(“fail”)  
end 
 This algorithm requires m(n - m) comparisons in the worst case  
 A more efficient algorithm will not repeat the same comparisons over and over 
again 
 One such algorithm uses two indices I and j, where I indexes the character 
positions in S 1 and j is set to a “match state” value ranging from 0 to m  
 The state 0 corresponds to no matched characters between the strings, while state 
1 corresponds to the first letter in S 2 matching character i in S 2 
 State 2 corresponds to the first two consecutive letters in S 2 matching letters i and 
i+1 in S 1 respectively, and so on , with state m corresponding to a successful match 
 Whenever consecutive letters fail to match, the state index is reduced accordingly 
 Matching Graphs  
 Two graphs G 1 and G 2 match if they have the same labeled nodes and same 
labeled arcs and all node -to-node arcs are the same  
 If G 2 with m nodes is a subgraph of G 1 with n nodes, where n ≥ m 
 In a worst cas match, this will require n!/(n - m)! node comparison and O(m2) arc 
comparisons  
 Finding subgraph isomorphisms is also an important matching problem  
 An isomorphism between the graphs G 1 and G 2 with vertices V 1, V 2 and edges E1, 
E2, that is , (V1, E1) and (V2, E2) respectively, is a one -to-one mapping to f 
between V1 and V2, such that for all v1 € V1, f(v1) = v2, and for each arc e1 € E1 
connecting v1 and v1’ , there is a corresponding arc e2 € E2 connecting f(v1) and 
f(v1’)  Matching Sets and Bags  
 An exact match of two sets having the same number of elements requires that their 
intersection also have the number of elements  
 Partial matches of two sets can also be determined by taking their intersections  
 If the two sets have the same number of e lements and all elements are of equal 
importance, the degree of match can be the proportion of the total members which 
match 
 If the number of elements differ between the sets, the proportion of matched 
elements to the minimum of the total number of members  can be used as a 
measure of likeness  
 When the elements are not of equal importance, weighting factors can be used to 
score the matched elements  
 For example, a measure such as  
s(S1,S2)    =    ( ∑ w i N(a i))/m   
could be used, where w i =1 and N(a i) = 1 if a i is in the intersection; 
otherwise it is 0  
 An efficient way to find the intersection of two sets of symbolic elements in LISP 
is to work through one set marking each element on the elements property list and 
then saving all elements from the other list tha t have been marked 
 The resultant list of saved elements is the required intersection 
 Matching two bags is similar to matching two sets except that counts of the 
number of occurrences of each element must also be made  
 For this, a count of the number of occurrences can be used as the property mark 
for elements found in one set. This count can then be used to compare against a 
count of elements found in the second set  
 Matching to Unify Literals  
  One of the best examples of nontrivial pattern matching is in the  unification of 
two FOPL literals  
 For example, to unify P(f(a,x), y ,y) and P(x, b, z) we first rename variables so 
that the two predicates have no variables in common  
 This can be done by replacing the x in the second predicate with u to give P(u, b, 
z)  Compare the two symbol -by-symbol from left to right until a disagreement is 
found  
 Disagreements can be between two different variables, a nonvariable term and a 
variable, or two nonvariable terms. If no disagreements is found, the two are 
identical and we ha ve succeeded  
 If disagreements is found and both are nonvariable terms, unification is 
impossible; so we have failed  
 If both are variables, one is replaced throughout by the other. Finally, if 
disagreement is a variable and a nonvariable term, the variable is replaced by the 
entire term  
 In this last step, replacement is possible only if the term does not contain the 
variable that is being replaced. This matching process is repeated until the two are 
unified or until a failure occurs  
 For the two predicates P,  above, a disagreement is first found between the term 
f(a,x) and variable u. Since f(a,x) does not contain the variable u, we replace u 
with f(a,x) everywhere it occurs in the literal  
 This gives a substitution set of {f(a,x)/u} and the partially matched predicates 
P(f(a,x),y ,y) and P(f(a,x), b, z)  
 Proceeding with the match, we find the next disagreements pair y and b, a variable 
and term, respectively 
 Again we replace the variable y with the terms b and update the substitution list to 
get {f(a,x)/u, b/y}  
 The final disagreements pair is two variables. Replacing the variable in the second 
literal with the first we get the substitution set {f(a,x)/u, b/y, y/z}or {f(a,x), b/y, 
b/z} 
 For example, a LISP program which uses both the open and the segment pattern 
matching variables to find a match between a pattern and a clause  
(defun match (pattern clause)  
   (cond ((equal pattern clause) t )                  ; return t if  
        ((or (null pattern) (null clause)) nil)       ; equal, nil  
      ; if not  
        ((or (equal (car pattern) (car clause))     ;not, ?x       ; binds  
               (equal (car pattern) ‘?x))   ; to single  
 (match (cdr pattern) (cdr clause)))     ; term, *y  
      ; binds  
((equal (car pattern) ‘*y)    ; to several  
(or (match (cdr pattern) (cdr clause))   ;contiguous  
(match pattern (cdr clause))))))   ; terms  
 When a segment variable is encountered (the *y), match is recursively executed 
on the cdrs of both pattern and clause or on the cdr of clause and pattern as *y 
matches one or more than one ite m respectively 
 
Partial Matching  
 For many AI applications complete matching between two or more structures is 
inappropriate  
 For example, in put representations of speech waveforms or visual scenes may have 
been corrupted by noise or other unwanted disto rtions.  
 In such cases, we do not want to reject the input out of hand. Our systems should be 
more tolerant of such problems  
 We want our system to be able to find an acceptable or best match between the input 
and some reference description 
 Compensating for Distortions  
 Finding an object in a photograph given only a general description of the object 
is a common problems in vision applications  
 For example, the task may be to locate a human face or human body in 
photographs without the necessity of storing hundr eds of specific face templates  
 A better approach in this case would be to store a single reference description of 
the object   Matching between photographs regions and corresponding descriptions then 
could be approached using either a measure of correlation,  by altering the image 
to obtain a closer fit  
 If nothing is known about the noise and distortion characteristics, correlation 
methods can be ineffective or even misleading. In such cases, methods based on 
mechanical distortion may be appropriate  
 For exampl e, our reference image is on a transparent rubber sheet. This sheet is 
moved over the input image and at each location is stretched to get the best 
match alignment between the two images  
 The match between the two can then be evaluated by how well they corr espond 
and how much push -and-pull distortion is needed to obtain the best 
correspondence  
 Use the number of rigid pieces connected with springs. This pieces can 
correspond to low level areas such as pixels or even larger area segments  
 
                          Fig  Discrete version of stretchable overlay image  
 To model any restrictions such as the relative positions of body parts, nonlinear 
cost functions of piece displacements can be used  
 The costs can correspond to different spring tensions which refl ect the 
constraints  
 For example, the cost of displacing some pieces might be zero for no 
displacement, one unit for single increment displacements in any one of the permissible directions, two units for two position displacements and infinite cost 
for disp lacements of more than two increments. Other pieces would be assigned 
higher costs for unit and larger position displacements when stronger constraints 
were applicable  
 The matching problem is to find a least cost location and distortion pattern for 
the ref erence sheet with regard to the sensed picture  
 Attempting to compare each component of some reference to each primitive part 
of a sensed picture is a combinatorially explosive problem  
 In using the template -spring reference image and heuristic methods to co mpare 
against different segments of the sensed picture, the search and match process 
can be made tractable  
 Any matching metric used in the least cost comparison would need to take into 
account the sum of the distortion costs C d , the sum of the costs for r eference and 
sensed component dissimilarities Cc  , and the sum of penalty costs for missing 
components C m . Thus, the total cost is given by 
Ct = C d + C c + C m 
 Finding Match Differences  
 Distortions occurring in representations are not the only reason for pa rtial matches  
 For example, in problem solving or analogical inference, differences are expected. 
In such cases the two structures are matched to isolate the differences in order that 
they may be reduced or transformed. Once again, partial matching techniques are 
appropriate  
 In visual application, an industrial part may be described using a graph structure 
where the set of nodes correspond to rectangular or cylindrical block subparts  
 The arc in the graph correspond to positional relations between the subpart s 
 Labels for rectangular block nodes contain length, width and height, while labels 
for cylindrical block nodes, where location can be above, to the right of, behind, 
inside and so on  
 In Fig 8.5 illustrates a segment of such a graph  
 In the fig the follow ing abbreviations are used 
 Interpreting the graph, we see it is a unit consisting of subparts, mode up of 
rectangular and cylindrical blocks with dimensions specified by attribute values  
 The cylindrical block n 1 is to the right of n 2 by d 1 units and the t wo are connected 
by a joint  
 The blocks n 1 and n 2 are above the rectangular block n 3 by d 2 and d 3 units 
respectively, and so on  
 Graphs such as this are called attributed relational graphs (ATRs). Such a graph G 
is defined as a sextuple G = (N, B, A, G n, Gb) 
 Where N = { n1, n2, . . ., n k}is a set of nodes, A = {a  n1, an 2, . . . , an k} is an 
alphabet of node attributes, B = { b1, b2, . . . , b m}is a set of directed branches (b = 
(ni, nj)), and G n and G b are functions for generating node and branch attributes 
respectively 
 When the representations are graph structures like ARGs, a similarity measure 
may be computed as the total cost of transforming one graph into the other  
 For example, the similarity of two ARGs may be determined with the following 
steps:  
o Decompo se the ARGs into basic subgraphs, each having a depth of one  o Compute the minimum cost to transform either basic ARG into the other 
one subgraph -by-subgraph 
o Compute the total transformation cost from the sum of the subgraph costs  
 An ARG may be transformed by the three basic operations of node or branch 
deletions, insertions, or substitutions, where each operation is given a cost based 
on computation time or other factors  
 
The RETE matching algorithm  
 One potential problem with expert systems is the number  of comparisons  that need to 
be made between rules and facts in the database.  
 In some cases,  where there are hundreds or even thousands of rules, running 
comparisons  against each rule can be impractical.   
 The Rete Algorithm is an efficient method for solving this problem and is  used by a 
number of expert system tools, including OPS5 and Eclipse.  
 The Rete is a directed, acyclic, rooted graph .  
 Each path from the root node to a leaf in the tree represents the left -hand side of a 
rule.  
 Each node stores deta ils of which facts have been matched by  the rules at that point in 
the path.  As facts are changed, the new facts are propagated through the Rete from  the 
root node to the leaves, changing the information stored at nodes  appropriately.  
 This could mean addi ng a new fact, or changing information  about an old fact, or 
deleting an old fact.  In this way, the system only needs to test each new fact against 
the rules,  and only against those rules to which the new fact is relevant, instead of  
checking each fact aga inst each rule.   
 The Rete algorithm depends on the principle that in general, when using forward 
chaining in expert systems, the values of objects change relatively  infrequently, 
meaning that relatively few changes need to be made to the  Rete.  
 In such cas es, the Rete algorithm can provide a significant improvement  in 
performance over other methods, although it is less efficient in  cases where objects 
are continually changing.  
 The basic inference cycle of a production system is match, select and execute as 
indicated in Fig 8.6. These operations are performed as follows   
     Fig  Production system components and basic cycle  
 Match 
o During the match portion of the cycle, the conditions in the LHS of the rules in 
the knowledge base are matched against the co ntents of working memory to 
determine which rules have their LHS conditions satisfied with consistent 
bindings to working memory terms.  
o Rules which are found to be applicable are put in a conflict set  
 Select  
o From the conflict set, one of the rules is sele cted to execute. The selection 
strategy may depend on recency of useage, specificity of the rule or other 
criteria  
 Execute  
o The rule selected from the conflict set is executed by carrying the action or 
conclusion part of the rule, the RHS of the rule. This may involve an I/O 
operation, adding, removing or changing clauses in working memory or 
simply causing a halt  
o The above cycle is repeated until no rules are put in the conflict set or until a 
stopping condition is reached  
o The main time saving features of R ETE are as follows  
1. in most expert systems, the contents of working memory change very 
little from cycle to cycle. There is persistence in the data known as 
temporal redundancy. This makes exhaustive matching on every cycle 
unnecessary. Instead, by saving m atch information, it is only necessary 
to compare working memory changes on each cycle. In RETE, addition to, removal from, and changes to working memory are 
translated directly into changes to the conflict set in Fig . Then when a 
rule from the conflic t set has been selected to fire, it is removed from 
the set and the remaining entries are saved for the next cycle. 
Consequently, repetitive matching of all rules against working memory 
is avoided. Furthermore, by indexing rules with the condition terms 
appearing in their LHS, only those rules which could match. Working 
memory changes need to be examined. This greatly reduces the 
number of comparisons required on each cycle  
 
Fig  Changes to working memory are mapped to the conflict set  
 
2. Many rules in a knowledge base will have the same conditions 
occurring in their LHS. This is just another way in which unnecessary 
matching can arise. Repeating testing of the same conditions in those 
rules could be avoided by grouping rules which share the same 
condition s and linking them to their common terms. It would then be 
possible to perform a single set of tests for all the applicable rules 
shown in Fig  below   
  Fig  Typical rules and a portion of a compiled network  
 
Knowledge Organization and Management  
 
The advantage of using structured knowledge representation schemes (frames, associative 
networks, or obje ct-oriented structures) over unstructured ones (rules or FOPL clauses) 
should be understood and appreciated at this point. Structured schemes group or link small 
related chunks of knowledge together as a unit. This simplifies the processing operations, 
since knowledge required for a given task is usually contained within a limited semantic 
region, which can be accessed as a unit or traced through a few linkages.  
 But, as suggested earlier, representation is not the only factor, which affects efficient 
manipulation. A program must first locate and retrieve the appropriate knowledge in an 
efficient manner whenever it is needed. One of the most direct methods for finding the 
appropriate knowledge is exhaustive search or the enumerations of all items in memory. This 
is also one of the least efficient access methods. More efficient retrieval is accomplished 
through some form of indexing or grouping. We consider some of these processes in the next 
section where we review traditional access and retrieval methods use d in memory 
organizations. This is followed by a description of less commonly used forms of indexing.  
 A “smart” expert system can be expected to have thousands or even tens of thousands 
of rules (or their equivalent) in its KB. A good example is XCON (or RI), an expert system 
which was developed for the Digital Equipment Corporation to configure their customer’s computer systems. XCON has a rapidly growing KB, which, at the present time, consists of 
more than 12,000 production rules. Large numbers of rules  are needed in systems like this, 
which deal with complex reasoning tasks. System configuration becomes very complex when 
the number of components and corresponding parameters is large (several hundred). If each 
rule contained above four or five conditions  in its antecedent or If part and an exhaustive 
search was used, as many as 40,000- 50,000 tests could be required on each recognition cycle. 
Clearly, the time required to perform this number of tests is intolerable. Instead, some form of 
memory management is needed. We saw one way this problem was solved using a form of 
indexing with the RETE algorithm described in the preceding chapter, More direct memory 
organization approaches to this problem are considered in this chapter.  
 We humans live in a dynamic, continually changing environment. To cope with this 
change, our memories exhibit some rather remarkable properties. We are able to adapt to 
varied changes in the environment and still improve our performance. This is because our 
memory system is continuous ly adapting through a reorganization process. New knowledge 
is continually being added to our memories, existing knowledge is continually being revised, 
and less important knowledge is gradually being forgotten. Our memories are continually 
being reorganiz ed to expand our recall and reasoning abilities. This process leads to improved 
memory performance throughout most of our lives.  
 When developing computer memories for intelligent systems, we may gain some 
useful insight by learning what we can from human memory systems. We would expect 
computer memory systems to possess some of the same features. For example, human 
memories tend to be limitless in capacity, and they provide a uniform grade of recall service, 
independent of the amount of information store. For later use, we have summarized these and 
other desirable characteristics that we feel an effective computer memory organization 
system should possess.  
1. It should be possible to add and integrate new knowledge in memory as needed 
without concern for limit ations in size.  
2. Any organizational scheme chosen should facilitate the remembering process. Thus, it 
should be possible to locate any stored item of knowledge efficiently from its content 
alone.  3. The addition of more knowledge to memory should have no adver se effects on the 
accessibility of items already stored there. Thus, the search time should not increase 
appreciably with the amount of information stored.  
4. The organization scheme should facilitate the recognition of similar items of 
knowledge. This is ess ential for reasoning and learning functions. It suggests that 
existing knowledge be used to determine the location and manner in which new 
knowledge is integrated into memory.  
5. The organization should facilitate the process of consolidating recurrent incide nts or 
episodes and “forgetting” knowledge when it is no longer valid or no longer needed.  
 
These characteristics suggest that memory be organized around conceptual clusters of 
knowledge. Related clusters should be grouped and stored in close proximity to each 
other and be linked to similar concepts through associative relations. Access to any given 
cluster should be possible through either direct or indirect links such as  concept pointers 
indexed by meaning. Index keys with synonomous meanings should provide links to the 
same knowledge clusters. These notions are illustrated graphically in Fig 9.1 where the 
clusters represent arbitrary groups closely related knowledge such as objects and their 
properties or basic conceptual categories. The links connecting the clusters are two- way 
pointers which provide relational associations between the clusters they connect.  
 
 Indexing and retrieval techniques  
 The Frame Problem  
One tricky aspect of systems that must function in dynamic environments is 
due to the so ca lled frame problem. This is the problem of knowing what 
changes have and have not taken place following some action. Some changes 
will be the direct result of the action. Other changes will be the result of 
secondary or side effects rather than the result of the action. Foe example, if a 
robot is cleaning the floors in a house, the location of the floor sweeper 
changes with the robot even though this is not explicitly stated. Other objects 
not attached to the robot remain in their original places. The actua l changes 
must somehow be reflected in memory, a fear that requires some ability to infer. Effective memory organization and management methods must take into 
account effects caused by the frame problem  
 
The three basic problems related to knowledge organi zation:  
1. classifying and computing indices for input information presented to 
system  
2. access and retrieval of knowledge from mrmory through the use of the 
computed indices  
3. the reorganization of memory structures when necessary to 
accommodate additions, revis ions and forgetting. These functions are 
depicted in Fig 9.1 
 
    Fig  Memory Organization Function  When a knowledge base is too large to be held in main memory, it must be stored as a 
file in secondary storage (disk, drum or tape).  
 Storage and retrie val of information in secondary memory is then performed through 
the transfer of equal -size physical blocks consisting of between 256 and 4096 bytes.  
 When an item of information is retrieved or stored, at least one complete block must 
be transferred betwe en main and secondary memory.  
 The time required to transfer a block typically ranges between 10ms and 100ms, about 
the same amount of time required to sequentially searching the whole block for an 
item.  
 Grouping related knowledge together as a unit can h elp to reduce the number of block 
transfers, hence the total access time  
 An example of effective grouping can be found in some expert system KB 
organizations  
 Grouping together rules which share some of the same conditions and conclusions can 
reduce block t ransfer times since such rules are likely to be needed during the same 
problem solving session 
 Collecting rules together by similar conditions or content can help to reduce the 
number of block transfers required  
 Indexed Organization  
 While organization by c ontent can help to reduce block transfers, an indexed 
organization scheme can greatly reduce the time to determine the storage 
location of an item  
 Indexing is accomplished by organizing the information in some way for easy 
access  
 One way to index is by segregating knowledge into two or more groups and 
storing the locations of the knowledge for each group in a smaller index file  
 To build an indexed file, knowledge stored as units is first arranged 
sequentially by some key value  
 The key can be any chosen fiel ds that uniquely identify the record  
 A second file containing indices for the record locations is created while the 
sequential knowledge file is being loaded  
 Each physical block in this main file results in one entry in the index file  
 The index file entrie s are pairs of record key values and block addresses   The key value is the key of the first record stored in the corresponding block  
 To retrieve an item of knowledge from the main file, the index file is searched 
to find the desired record key and obtain th e corresponding block address  
 The block is then accessed using this address. Items within the block are then 
searched sequentially for the desired record  
 An indexed file contains a list of the entry pairs (k,b) where the values k are 
the keys of the first record in each block whose starting address is b 
 Fig 9.2 illustrates the process used to locate a record using the key value of 
378 
 
    Fig  Indexed File Organization 
 The largest key value less than 378 (375) gives the block address (800) where 
the it em will be found  
 Once the 800 block has been retrieved, it can be searched linearly to locate the 
record with key value 378. this key could be any alphanumeric string that 
uniquely identifies a block, since such strings usually have a collation order 
defined by their code set  
 If the index file is large, a binary search can be used to speed up the index file 
search 
 A binary search will significantly reduce the search time over linear search 
when the number of items is not too small   When a file contains n re cords, the average time for a linear search is 
proportional to n/2 compared to a binary search time on the order of ln 2(n) 
 Further reductions in search time can be realized using secondary or higher 
order arranged index files  
 In this case the secondary index file would contain key and block address pairs 
for the primary index file  
 Similar indexing would apply for higher order hierarchies where a separate 
file is used for each level  
 Both binary search and hierarchical index file organization may be needed 
when the KB is a very large file  
 Indexing in LISP can be implemented with property lists, A -lists, and/or hash 
tables. For example, a KB can be partitioned into segments by storing each 
segment as a list under the property value for that segment  
 Each list indexed in this way can be found with the get property function and 
then searched sequentially or sorted and searched with binary search methods  
 A hash -table is a special data structure in LISP which provides a means of 
rapid access through key hashing  
 Hashe d Files  
 Indexed organizations that permit efficient access are based on the use of a 
hash function 
 A hash  function, h, transforms key values k into integer storage location 
indices through a simple computation 
 When a maximum number of items C are to store d, the hashed values h(k) will 
range from 0 to C – 1. Therefore, given any key value k, h(k) should map into 
one of 0…C – 1 
 An effective hash function can be computed by choosing the largest prime 
number p less than or equal to C, converting the key value k into an integer k’ 
if necessary, and then using the value k’ mod p as the index value h 
 For example, if C is 1000, the largest prime less than C is p = 997. thus, if the 
record key value is 123456789, the hashed value is h = (k mod 997) = 273  
 When using hashed access, the value of C should be chosen large enough to 
accommodate the maximum number of categories needed   The use of the prime number p in the algorithm helps to insure that the 
resultant indices are somewhat uniformly distributed or hashed throughout the 
range 0 . . . C – 1 
 This type of organization is well suited for groups of items corresponding to C 
different categories  
 When two or more items belong to the same category, they will have the same 
hashed values. These values are called synonyms  
 One way to accommodate collisions is with data structures known as buckets  
 A bucket is a linked list of one or more items, where each item is record, 
block, list or other data strucyure  
 The first item in each bucket has an address corresponding to the hashed  
address  
 Fig 9.3 illustrates a form of hashed memory organization which uses buckets 
to hold all items with the same hashed key value  
 
Fig   Hashed Memory File organization 
 The address of each bucket in this case is the indexed location in an array  
 Conceptual Indexing  
 A better approach to indexed retrieval is one which makes use of the content 
or meaning associated with the stored entities rather than some nonmeaningful 
key value  
 This suggests the use of indices which name and define the entity being 
retrieved. Thus, if the entity is an object, its name and characteristic attributes 
would make meaningful indices   If the entity is an abstract object such as a concept, the name and other 
defining traits would ne meaningful as indices  
 Nodes within the networ k correspond to different knowledge entities, whereas 
the links are indices or pointers to the entities  
 Links connecting two entities name the association or relationship between 
them  
 The relationship between entities may be defined as a hierarchical one o r just 
through associative links  
 As an example of an indexed network, the concept of computer science CS 
should be accessible directly through the CS name or indirectly through 
associative links like a university major, a career field, or a type of classroom 
course  
 These notions are illustrated in Fig 9.4 
  
Fig Associative Network Indexing and Organization 
 
 Object attributes can also serve as indices to locate items based on the attribute 
values  
 In this case, the best attribute keys are those which pro vide the greatest 
discrimination among objects within the same category   For example, suppose we wish to organize knowledge by object types. In this 
case, the choice of attributes should depend on the use intended for the 
knowledge. Since objects may be cla ssified with an unlimited number of 
attributes , those attributes which are most discriminable with respect to the 
concept meaning should be chosen 
 
 Integrating knowledge and memory  
 Integrating new knowledge in traditional data bases is accomplished by  simply adding 
an item to its key location, deleting an item from a key directed location, or modifying 
fields of an existing item with specific input information.  
 When an item in inventory is replaced with a new one, its description is changed 
accordingly . When an item is added to memory, its index is computed and it is stored 
at the corresponding address  
 More sophisticated memory systems will continuously monitor a knowledge base and 
make inferred changes as appropriate  
 A more comprehensive management sys tem will perform other functions as well, 
including the formation of new conceptual structures, the computation and association 
of casual linkages between related concepts, generalization of items having common 
featu res and the formation of specialized con ceptual categories and specialization of 
concepts that have been over generalized  
 Hypertext  
 Hypertext systems are examples of information organized through associative 
links, like associative networks  
 These systems are interactive window systems connected to a database 
through associative links  
 Unlike normal text which is read in linear fashion, hypertext can be browsed 
in a nonlinear way by moving through a network of information nodes which 
are linked bidirectional ly through associative  
 Users of hypertext  systems can wander through the database scanning text and 
graphics, creating new information nodes and linkages or modify existing ones  
 This approach to documentation use is said to more closely match the 
cognitive process   It provides a new approach to information access and organization for authors, 
researchers and other users of large bodies of information 
 
 Memory organization system  
 HAM, a model of memory  
 One of the earliest computer models of memory was the Human Associative 
memory (HAM) system dev eloped by John Anderson and Gordon Bower  
 This memory is organized as a network of prepositional binary trees  
 An example of a simple tree which represents the statement “In a park s hippie 
touched a debutante” is illustrated in Fig 9.5 
 When an informant ass erts this statement to HAM, the system parses the 
sentence and builds a binary tree representation  
 Node in the tree are assigned unique numbers, while links are labeled with the 
following functions:  
C: context for tree fact   P: predicate  
e: set membership    R: relation 
F: a fact     S: subject  
L: a location    T: time  
O: object  
 As HAM is informed of new sentences, they are parsed and formed into new 
tree-like memory structures or integrated with existing ones  
 For example, to add the fact that the hippie was  tall, the following subtree is 
attached to the tree structure of Fig below  by merging the common node 
hippie (node 3) into a single node   
  Fig Organization of knowledge in HAM  
 
 When HAM is posed with a query, it is formed into a tree structure called a 
probe. This structure is then matched against existing memory structures for 
the best match 
 The structure with the closest match is used to formulate an answer to the 
query  
 Matching is accomplished by first locating the leaf nodes in memory that 
match le af nodes in the probe  
 The corresponding links are then checked to see if they have the same labels 
and in the same order  
 The search process is constrained by searching only node groups that have the 
same relation links, based on recency of usage  
 The search  is not exhaustive and nodes accessed infrequently may be forgotten  
 Access to nodes in HAM is accomplished through word indexing in LISP  
 Memory Organization with E -MOPs  
 One system was developed by Janet Kolodner to study problems associated 
with the retrie val and organization of reconstructive memory, called CYRUS 
(Computerized Yale Retrieval and Updating System) stores episodes from the 
lives of former secretaries of state Cyrus Vance and Edmund Muskie  
 The episodes are indexed and stored in long term memor y for subsequent use 
in answering queries posed in English  The basic memory model in CYRUS is a network consisting of Episodic 
Memory Organization Packets (E -MOPs)  
 Each such E -MOP is a frame -like node structure which contains conceptual 
information related to different categories of episodic events  
 E-MOP are indexed in memory by one or more distinguishing features. For 
example, there are basic E -MOPs for diplomatic meetings with foreign 
dignitaries, specialized political conferences, traveling, state dinner s as well as 
other basic events related to diplomatic state functions  
 This diplomatic meeting E -MOP called $MEET, contains information which 
is common to all diplomatic meeting events  
 The common information which characterizes such as E -MOP is called its 
content  
 For example, $MEET might contain the following information:  
 A second  type of information contained in E -MOPs are the indices which 
index either individual episodes or other E -MOPs which have become 
specializations of their parent E -MOPs  
 A typical $MEET E -MOP which has indices to two particular event meetings 
EV1 and EV2, is illustrated in Fig 9.6 
 
Fig  An example of an EMOP with two indexed events EV1 and EV2   
 For example, one of the meetings indexed was between Vance and Gromyko 
of the USSR in which they discussed SALT. This is labeled as event EV1 in 
the figure. The second meeting was between Vance and Begin of Israel in 
which they discussed Arab -Israeli peace. This is labeled as event EV2  
 Note that each of these events can be accessed through  more than one feature 
(index). For example, EV1 can be located from the $MEET event through a 
topic value of “Arab -Israel peace,” through a participants’ nationality value of 
“Israel,” through a participants’ occupation value of “head of state,” and so on  
 As new diplomatic meetings are entered into the system, they are either 
integrated with the $MEET E -MOP as a separately indexed event or merged 
with another event to form a new specialized meeting E -MOP.  
 When several events belonging to the same MOP category are entered, 
common event features are used to generalize the E -MOP. This information is 
collected in the frame contents. Specialization may also be required when 
over-generalization has occurred. Thus, memory is continually being 
reorganized as new fa cts are entered.  
 This process prevents the addition of excessive memory entries and much 
redundancy which would result if every event entered resulted in the addition 
of a separate event  
 Reorganization can also cause forgetting, since originally assigned i ndices 
may be changed when new structures are formed  
 When this occurs, an item cannot be located, so the system attempts to derive 
new indices from the context and through other indices by reconstructing 
related events  
 The key issues in this type of the or ganizations are:  
 The selection and computation of good indices for new events so that 
similar events can be located in memory for new event integration 
 Monitoring and reorganization of memory to accommodate new events 
as they occur  
 Access of the correct ev ent information when provided clues for 
retrieval    
  Module -4 
Natural Language Processing : 
 
Developing programs to understand natural language is important in AI because a natura l 
form of communication with systems is essential for user acceptance. One of the most 
critical tests for intelligent behavior is the ability to communicate effectively. This was 
the test proposed by Alan Turing. AI programs must be able to communicate wit h their 
human counterparts in a natural way, and natural language is one of the most important 
mediums for that purpose. A program understands a natural language if it behaves by 
taking a correct or acceptable action in response to the input. For example, we say a child 
demonstrates understanding if it responds with the correct answer to a question. The 
action taken need not be the external response. It may be the creation of some internal 
data structures. The structures created should be meaningful and cor rectly interact with 
the world model representation held by the program. In this chapter we explore many of 
the important issues related to natural language understanding and language generation.  
 
This chapter explores several techniques that are used to e nable humans to  interact with 
computers via natural human languages.  Natural languages are the languages used by 
humans for communication  (among other functions). They are distinctly different from 
formal languages,  such as C++, Java, and PROLOG. One of th e main differences,  which 
we will examine in some detail in this chapter, is that natural languages  are ambiguous, 
meaning that a given sentence can have more than one possible meaning, and in some 
cases the correct meaning can be very  hard to determine. F ormal languages are almost 
always designed to ensure  that ambiguity cannot occur. Hence, a given program written 
in C++ can  have only one interpretation. This is clearly desirable because otherwise the  
computer would have to make an arbitrary decision as t o which interpretation  to work 
with.  It is becoming increasingly important for computers to be able to understand natural 
languages. Telephone systems are now widespread that are  able to understand a narrow 
range of commands and questions to assist  callers  to large call centers, without needing to 
use human resources.  Additionally, the quantity of unstructured textual data that exists in 
the world (and in particular, on the Internet) has reached unmanageable proportions.  For 
humans to search through these data using traditional techniques  such as Boolean queries 
or the database query language SQL is  impractical. The idea that people should be able to pose questions in their  own language, or something similar to it, is an increasingly 
popular one.  Of course, English is not the only natural language. A great deal of research 
in natural language processing and information retrieval is carried out in  English, but 
many human languages differ enormously from English. Languages  such as Chinese, 
Finnish, and Navajo h ave almost nothing in common  with English (although of course 
Finnish uses the same alphabet).  Hence, a system that can work with one human language 
cannot necessarily  deal with any other human language.  In this section we will explore 
two main topics. Fir st, we will examine natural  language processing, which is a collection 
of techniques used to enable  computers to “understand” human language. In general, they 
are concerned with extracting grammatical information as well as meaning from  human 
utterances but they are also concerned with understanding those  utterances, and 
performing useful tasks as a result.  Two of the earliest goals of natural language 
processing were automated translation  (which is explored in this chapter) and database 
access. The idea he re was that if a user wanted to find some information from a database, 
it would  
make much more sense if he or she could query the database in her language,  rather than 
needing to learn a new formal language such as SQL.  Information retrieval is a collectio n 
of techniques used to try to match a  query (or a command) to a set of documents from an 
existing corpus of documents.  Systems such as the search engines that we use to find data 
on the Internet use information retrieval (albeit of a fairly simple nature) .  
 
 Overview of linguistics  
In dealing with natural language, a computer system needs to be able to  process and 
manipulate language at a number of levels.   
 Phonology. This is needed only if the computer is required to  understand spoken 
language. Phono logy is the study of the sounds  that make up words and is used to 
identify words from sounds.  We will explore this in a little more detail later, when 
we look at the  ways in which computers can understand speech.   
 Morphology. This is the first stage of ana lysis that is applied to  words, once they 
have been identified from speech, or input into  the system. Morphology looks at 
the ways in which words break down into components and how that affects their grammatical status.  For example, the letter “s” on the e nd of a word can often 
either  indicate that it is a plural noun or a third- person present -tense  verb.   
 Syntax. This stage involves applying the rules of the grammar from  the language 
being used. Syntax determines the role of each word in  a sentence and, th us, 
enables a computer system to convert sentences  into a structure that can be more 
easily manipulated.   
 Semantics. This involves the examination of the meaning of words and sentences.  
As we will see, it is possible for a sentence to be syntactically  correct but to be 
semantically meaningless. Conversely, it is desirable  that a computer system be 
able to understand sentences with incorrect  syntax but that still convey useful 
information semantically.   
 Pragmatics. This is the application of human -like under standing to  sentences and 
discourse to determine meanings that are not immediately  clear from the 
semantics. For example, if someone says,  “Can you tell me the time?”, most 
people know that “yes” is not a  suitable answer. Pragmatics enables a computer 
system to give a  sensible answer to questions like this.   
 In addition to these levels of analysis, natural language processing systems  must 
apply some kind of world knowledge. In most real -world systems, this   world 
knowledge is limited to a specific domain (e .g., a system might have  detailed 
knowledge about the Blocks  World and be able to answer questions  about this 
world). The ultimate goal of natural language processing would be to have a 
system with enough world knowledge to be able to engage a  human in dis cussion 
on any subject. This goal is still a long way off.  
 Morphological Analysis  
 In studying the English language, morphology is relatively simple. We 
have  endings such as -ing, -s, and -ed, which are applied to verbs; endings 
such as  -s and -es, which ar e applied to nouns; we also have the ending -ly, 
which  usually indicates that a word is an adverb.  
 We also have prefixes such as  anti-, non -, un-, and in- , which tend to 
indicate negation, or opposition.   
 We also have a number of other prefixes and suffixes that provide a 
variety of  semantic and syntactic information.    In practice, however, morphologic analysis for the English language is not  
terribly complex, particularly when compared with agglutinative languages  
such as German, which tend to combine wor ds together into single words  
to indicate combinations of meaning.   
 Morphologic analysis is mainly useful in natural language processing for  
identifying parts of speech (nouns, verbs, etc.) and for identifying which 
words belong together.  
 In English,  word order tends to provide more of this  information than 
morphology, however. In languages such as Latin, word order was almost 
entirely superficial, and the morphology was extremely  important. 
Languages such as French, Italian, and Spanish lie somewhere  betw een 
these two extremes.   
 As we will see in the following sections, being able to identify the part of  
speech for each word is essential to understanding a sentence. This can 
partly be achieved by simply looking up each word in a dictionary, which 
might con tain for example the following entries:  
(swims, verb, present, singular, third person)  
(swimmer, noun, singular)  
(swim, verb, present, singular, first and second persons)  
(swim, verb, present plural, first, second, and third persons)  
(swimming, participle ) 
(swimmingly, adverb)  
(swam, verb, past)  
 Clearly, a complete dictionary of this kind would be unfeasibly large.  A 
more  practical approach is to include information about standard endings, 
such as:  
(-ly, adverb)  
(-ed, verb, past)  
(-s, noun, plural)   This wo rks fine for regular verbs, such as walk, but for all natural 
languages  there are large numbers  of irregular verbs, which do not follow 
these rules. Verbs such as to be and to do are particularly difficult in 
English as they do not seem to follow any  morph ologic rules.   
 The most sensible approach to morphologic analysis is thus to include a set  
of rules that work for most regular words and then a list of irregular words.   
 For a system that was designed to converse on any subject, this second list  
would be e xtremely long. Most natural language systems currently are  
designed to discuss fairly limited domains and so do not need to include  
over-large look -up tables.   
 In most natural languages, as well as the problem posed by the fact that  
word order tends to hav e more importance than morphology, there is also  
the difficulty of ambiguity at a word level.  
 This kind of ambiguity can be  seen in particular in words such as trains, 
which could be a plural noun or  a singular verb, and set, which can be a 
noun, verb, or  adjective.   
 BNF  
  Parsing involves mapping a linear piece of text onto a hierarchy that  
represents the way the various words interact with each other syntactically.   
 First, we will look at grammars, which are used to represent the rules that  
define how a s pecific language is built up.   
 Most natural languages are made up of a number of parts of speech, 
mainly  the following:   
o Verb 
o Noun  
o Adjective  
o Adverb 
o Conjunction 
o Pronoun  
o Article  
 In fact it is useful when parsing to combine words together to form 
syntactic  groups. Hence, the words, a dog, which consist of an article and a 
noun,  can also be described as a noun phrase.   A noun phrase is one or more  words that combine together to represent an 
object or thing that can be described by a noun. Hence, the following a re 
valid noun phrases:  christmas, the dog , that packet of chips , the boy who 
had measles last year and nearly died, my favorite color  
 A noun phrase is not a sentence —it is part of a sentence.  
 A verb phrase is one or more words that  represent an action. The  following 
are valid verb phrases:  swim , eat that packet of chips , walking  
 A simple way to describe a sentence is to say that it consists of a noun 
phrase and a verb phrase. Hence, for example:  That dog is eating my 
packet of chips.  
 In this sentence, that dog is a noun phrase, and is eating my packet of chips 
is a verb phrase.  Note that the verb phrase is in fact made up of a verb 
phrase,  is eating, and a noun phrase, my packet of chips.  
 A language is defined partly by its grammar. The rules of grammar for  a 
language such as English can be written out in full, although it would be a  
complex process to do so.  
 To allow a natural language processing system to  parse sentences, it needs 
to have knowledge of the rules that describe how a  valid sentence can be 
constructed.  
 These rules are often written in what is known as Backus –Naur form (also  
known as Backus normal form —both names are abbreviated as BNF).  
 BNF is widely used by computer scientists to define formal languages 
such as  C++ and Java.  We can also use i t to define the grammar of a 
natural language.   
 A grammar specified in BNF consists of the following components:  
o Terminal symbols. Each terminal symbol is a symbol or word that  
appears in the language itself. In English, for example, the terminal  
symbols a re our dictionary words such as the, cat, dog, and so on.  In 
formal languages, the terminal symbols include variable names  such as 
x, y, and so on, but for our purposes we will consider the  terminal 
symbols to be the words in the language.  
o Nonterminal symb ols. These are the symbols such as noun, verb 
phrase, and conjunction that are used to define words and phrases  of the language. A nonterminal symbol is so -named because it is  used to 
represent one or more terminal symbols.   
o The start symbol. The start sym bol is used to represent a complete  
sentence in the language. In our case, the start symbol is simply  
sentence, but in first -order predicate logic, for example, the start  
symbol would be expression.   
o Rewrite rules. The rewrite rules define the structure of  the  grammar.  
Each rewrite rule details what symbols (terminal or nonterminal)  can 
be used to make up each nonterminal symbol.  
 Let us now look at rewrite rules in more detail.  We saw above that a 
sentence could take the following form:  noun phrase verb ph rase  
 We thus write the following rewrite rule:  Sentence →NounPhrase 
VerbPhrase  This does not mean that every sentence must be of this form, 
but simply  that a string of symbols that takes on the form of the right -hand 
side can be  rewritten in the form of the left -hand side. Hence, if we see the 
words   
The cat sa t on the mat  
 we might identify that the cat is a noun phrase and that sat on the mat is a  
verb phrase.  We can thus conclude that this string forms a sentence.  
 We can also use BNF to define a number of possible noun phrases.  
 Note  how we use the “|” symbol to separate the possible right -hand sides 
in BNF:  
NounPhrase → Noun 
| Article Noun 
| Adjective Noun  
| Article Adjective Noun 
 Similarly, we can define a verb phrase:  
VerbPhrase → Verb 
| Verb NounPhrase  
| Adverb Verb NounPhrase   The structure of human languages varies considerably.  Hence, a set of 
rules  like this will b e valid for one language, but not necessarily for any 
other language.  
 For example, in English it is usual to place the adjective before the  noun 
(black cat, stale bread), whereas in French, it is often the case that the  
adjective comes after the noun (moul in rouge).  Thus far, the rewrite rules 
we have written consist solely of nonterminal  symbols.  
 Rewrite rules are also used to describe the parts of speech of individual   
words (or terminal symbols):  
Noun → cat 
| dog  
| Mount Rushmore  
| chickens  
Verb → swims  
| eats  
| climbs  
Article → the 
| a 
Adjective → black  
| brown  
| green 
| stale  
 
Grammars and Languages  
 The types of grammars that exist  are Noam Chomsky invented a hierarchy of 
grammars.   The hierarchy co nsists  of four main types of grammars.   
 The simplest grammars are used to define regular languages.  
 A regular  language is one that can be described or understood by a finite state  
automaton. Such languages are very simplistic and allow sentences such as  
“aaaaabbbbbb.” Recall that a finite state automaton consists of a finite  number of 
states, and rules that define how the automaton can transition  from one state to 
another.   
 A finite state automaton could be designed that defined the language that  consisted of 
a string of one or more occurrences of the letter a. Hence, the  following strings would 
be valid strings in this language:  
aaa 
a 
aaaaaaaaaaaaaaaaa  
 Regular languages are of interest to computer scientists, but are not of great  interest to 
the field of n atural language processing because they are not  powerful enough to 
represent even simple formal languages, let alone the  more complex natural 
languages.  
 Sentences defined by a regular grammar  are often known as regular expressions.   
 The grammar that we de fined above using rewrite rules is a context -free grammar.  
 It is context free because it defines the grammar simply in terms  of which word types 
can go together —it does not specify the way that   words should agree with each.  
A stale dog climbs Mount Rushmore.  
 It also, allows the following sentence, which is not grammatically  correct:  
Chickens eats.  
 A context -free grammar can have only at most one terminal symbol on the  right -hand 
side of its rewrite rules.  
 Rewrite rules for a context -sensitive  grammar, i n contrast, can have more than one 
terminal symbol on the  right -hand side. This enables the grammar to specify number, 
case, tense,  and gender agreement.  
 Each context -sensitive rewrite rule  must have at least  as many symbols on the right -
hand side as it does on the left -hand side.    Rewrite rules for context -sensitive grammars have the following form:  
A X B →A Y B  
which means that in the context of A and B, X can be rewritten as Y.  
 Each  of A, B, X, and Y can be either a terminal or a nonterminal symbol.   
 Context -sensitive grammars are most usually used for natural language  processing 
because they are powerfu l enough to define the kinds of grammars  that natural 
languages use. Unfortunately, they tend to involve a  much larger number of rules and 
are a much less natural way to describe  language, making them harder for human 
developers to design than context free grammars.  
 The final class of grammars in Chomsky’s hierarchy consists of recursively 
enumerable grammars (also known as unrestricted grammars).  
 A recursively  enumerable grammar can define any language and has no restrictions  on 
the structure of its rewri te rules. Such grammars are of interest to  computer scientists 
but are not of great use in the study of natural language  processing.  
 Parsing: Syntactic Analysis  
 As we have seen, morphologic analysis can be used to determine to which  part 
of speech each wor d in a sentence belongs.  We will now examine how  this 
information is used to determine the syntactic structure of a sentence.   
 This process, in which we convert a sentence into a tree that represents the  
sentence’s syntactic structure, is known as parsing.   
 Parsing a sentence tells us whether it is a valid sentence, as defined by our  
grammar  
 If a sentence is not a valid sentence, then it cannot be parsed.  Parsing a 
sentence involves producing a tree, such as that shown in Fig 10.1, which 
shows the parse tr ee for the following sentence:  
The black cat crossed the road.   
   Fig 10.1 
 This tree shows how the sentence is made up of a noun phrase and a verb 
phrase.  
 The noun phrase consists of an article, an adjective, and a noun.  The verb 
phrase consists of a ve rb and a further noun phrase, which in turn  consists of 
an article and a noun.  
 Parse trees can be built in a bottom -up fashion or in a top- down fashion.   
 Building a parse tree from the top down involves starting from a sentence  and 
determining which of the  possible rewrites for Sentence can be applied to the 
sentence that is being parsed. Hence, in this case, Sentence would be  rewritten 
using the following rule:  
Sentence →NounPhrase VerbPhrase  
 Then the verb phrase and noun phrase would be broken down recursively  in 
the same way, until only terminal symbols were left.   
 When a parse tree is built from the top down, it is known as a derivation tree.   
 To build a parse tree fro m the bottom up, the terminal symbols of the sentence  
are first replaced by their corresponding nonterminals (e.g., cat is  replaced by 
noun), and then these nonterminals are combined to match the  right -hand sides 
of rewrite rules.  
 For example, the and roa d would be combined using the following rewrite 
rule: NounPhrase →Article Noun 
 Basic parsing techniques  
 Transition Networks  
 A transition network is a finite state automaton that is used to represent a  part 
of a grammar.  
 A transition network parser uses a number of these  transition networks to 
represent its entire  grammar.  
 Each network represents  one nonterminal symbol in the grammar. Hence, in 
the grammar for  the English language, we would have one transition network 
for Sentence,  one for Noun Phrase, one for Verb Phrase, one for Verb , and so 
on.  
 Fig 10.2 shows the transition network equivalents for three production  rules.   
 In each transition network, S1 is the start state, and the accepting state, or  final 
state, is denoted by a heavy border.  When a phrase is applied to a transition 
network, the first word is co mpared against one of the arcs leading from the 
first state.  
 If this word matches one of those arcs, the network moves into the state to 
which that arc points. Hence, the first network  shown in Fig 10.2, when 
presented with a Noun Phrase , will move from  state S1 to state S2.   
 If a phrase is presented to a transition network and no match is found from  the 
current state, then that network cannot be used and another network must be 
tried. Hence, when starting with the phrase the cat sat on the mat , none of th e 
networks shown in Fig 10.2 will be used because they all  have only 
nonterminal symbols, whereas all the symbols in the cat sat on the  mat are 
terminal.Hence, we need further networks, such as the ones shown  in Figure 
10.2, which deal with terminal symbol s. 
 
   
     Fig 10.2 
  Transition networks can be used to determine whether a sentence is 
grammatically  correct, at least according to the rules of the grammar the 
networks  represent.   
 Parsing using transition networks involves exploring a search space of 
possible  parses in a depth -first fashion.   
 Let us examine the parse of the following simple sentence:  
A cat sat.  
 We begin in state S1 in the Sentence transition network. To proceed, we   
must follow the arc that is labeled NounPhrase . We thus move out of th e 
Sentence network and into the NounPhrase network.   
 The first arc of the NounPhrase network is labeled Noun . We thus move  into 
the Noun network. We now follow each of the arcs in the Noun network and 
discover that our first word, A, does not match any of them.  Hence, we 
backtrack to the next arc in the NounPhrase network.  
 This arc is  labeled Article , so we move on to the Article transition network. 
Here, on  examining the second label, we find that the first word is matched 
by the  terminal symbol on this a rc.  
 We therefore consume the word, A, and move  on to state S2 in the Article 
network. Because this is a success node, we are  able to return to the 
NounPhrase network and move on to state S2 in this  network.  We now have 
an arc labeled Noun . 
 As before,  we m ove into the Noun network and find that our next word, cat, 
matches. We thus move to state S4 in the NounPhrase network. This is a  
success node, and so we move back to the Sentence network and repeat the  
process for the VerbPhrase arc.  
 It is possible for a system to use transition networks to generate a derivation 
tree for a sentence, so that as well as determining whether the sentence  is 
grammatically valid, it parses it fully to obtain further information by 
semantic analysis from the sentence.  
 This can  be done by simply having the  system build up the tree by noting 
which arcs it successfully followed.  When, for example, it successfully 
follows the NounPhrase arc in the Sentence  network, the system generates a 
root node labeled Sentence and an arc  leading from that node to a new node labeled NounPhrase .When the system  follows the NounPhrase network and 
identifies an article and a noun,  these are similarly added to the tree.  
 In this way, the full parse tree for the  sentence can be generated using 
transiti on networks.  Parsing using transition networks is simple to 
understand, but is not necessarily  as efficient or as effective as we might 
hope for. In particular, it does  not pay any attention to potential ambiguities 
or the need for words to agree with each  other in case, gender, or number.  
 Augmented Transition Networks  
 An augmented transition network, or ATN, is an extended version of a  
transition network.  
 ATNs have the ability to apply tests to arcs, for example,  to ensure 
agreement with number. Thus, an ATN for Sentence would be as  shown in 
Figure 10.2, but the arc from node S2 to S3 would be conditional  on the 
number of the verb being the same as the number for the noun.  
 Hence, if the noun phrase were three dogs and the verb phrase were is blue , 
the ATN would not be able to follow the arc from node S2 to S3 because the  
number of the noun phrase (plural) does not match the number of the  verb 
phrase (singular).  
 In languages such as French, checks for gender  would also be necessary.  
The conditions on the ar cs are calculated by procedures that are attached to 
the arcs. The procedure attached to an arc is called when the network  
reaches that arc. These procedures, as well as carrying out checks on 
agreement,  are able to form a parse tree from the sentence that  is being 
analyzed.  
 
 Chart Parsing  
 Parsing using transition networks is effective, but not the most efficient  way 
to parse natural language. One problem can be seen in examining the  
following two sentences:  1. Have all the fish been fed?  , Have all the fis h. 
 Clearly these are very different sentences —the first is a question, and the 
second is an instruction. In spite of this, the first threewords of each sentence 
are the same.   When a parser is exam ining one of these sentences, it is quite likely  to have 
to backtrack to the beginning if it makes the wrong choice in the first  case 
for the structure of the sentence. In longer sentences, this can be a  much  
greater problem, particularly as it involves examining the same words more  
than once,  without using the fac t that the words have already been analyzed.  
  
   Fig 10.3 
 Another method that is sometimes used for parsing natural language is  chart 
parsing.  
 In the worst case, chart parsing will parse a sentence of n words in O(n3)  
time. In many cases it will perfor m better than this and will  parse most 
sentences in O(n2) or even O(n) time.   
 In examining sentence 1 above, the chart parser  would note that the  words 
two children form a noun phrase. It would note this on its first pass through 
the sentence and would sto re this information in a chart , meaning it would 
not need to examine those words again on a subsequent pass, after 
backtracking.   
 The initial chart for the sentence The cat eats a big fish is shown in Fig 10.3 
It shows the chart that the chart parse algori thm would start with for parsing 
the sentence.  
 The chart consists of seven vertices , which will  become connected to each 
other by edges . The edges will show how the  constituents of the sentence 
combine together.   
 The chart parser starts by adding the foll owing edge to the chart:  
[0, 0, Target →• Sentence ] 
 This notation means that the edge connects vertex 0 to itself (the first two 
numbers in the square brackets show which vertices the edge connects).   
 Target is the target that we want to find, which is real ly just a placeholder to 
enable us to have an edge that requires us to find a whole sentence. The  
arrow indicates that in order to make what is on its left -hand side ( Target ) 
we need to find what is on its right -hand side ( Sentence ). The dot (•) shows  what  has been found already, on its left -hand side, and what is yet to be  
found, on its right -hand side. This is perhaps best explained by examining an 
example.  
 Consider the following edge, which is shown in the chart in Figure 10.4:  
[0, 2, Sentence →NounPhrase • VerbPhrase ] 
 This means that an edge exists connecting nodes 0 and 2. The dot shows us  
that we have already found a NounPhrase (the cat ) and that we are looking 
for a V erbPhrase . 
   
  
        Fig 10.4 
 Once we have found the V erbPhrase , we will  have what is  on the left -hand 
side of the arrow —that is, a Sentence .  
 The chart parser can add edges to the chart using the following three rules:  
o If we have an edge [ x, y, A → B • C], which needs to find a C, then 
an edge can be added that supplies that C (i.e., the edge [ x, y, C→ • 
E]), where E is some sequence of terminals or nonterminals  which 
can be replaced by a C).  
o If we have two edges, [ x, y, A → B • C D] and [ y, z, C → E •}, 
then these two edges can be combined together to form a new edge: 
[x, z, A→B C • D].  
o If we have an edge [ x, y, A → B • C], and the word at vertex y is of 
type C, then we have found a suitable word for this edge, and so we  
extend the edge along to the next vertex by adding the following 
edge: [ y, y + 1, A→B C •]. 
 Semantic Anal ysis 
 Having determined the syntactic structure of a sentence, the next task of  
natural language processing is to determine the meaning of the sentence.   
 Semantics is the study of the meaning of words, and semantic analysis is  the 
analysis we use to extract  meaning from utterances.    Semantic analysis involves building up a representation of the objects and  
actions that a sentence is describing, including details provided by 
adjectives,  adverbs, and prepositions. Hence, after analyzing the sentence 
The black cat sat on the mat , the system would use a semantic net such as 
the one shown in Figure 10.5 to represent the objects and the relationships 
between them.  
  
      Fig 10.5 
 A more sophisticated semantic network is likely to be formed, which  
includes informa tion about the nature of a cat (a cat is an object, an animal,  a 
quadruped, etc.) that can be used to deduce facts about the cat (e.g.,  that it 
likes to drink milk).   
 Ambiguity and Pragmatic Analysis  
 One of the main differences between natural languages an d formal 
languages  like C++ is that a sentence in a natural language can have more 
than one meaning. This is ambiguity —the fact that a sentence can be 
interpreted in different ways depending on who is speaking, the context in  
which it is spoken, and a number of other factors.   
 The more common forms of ambiguity  and look at ways in which a natural 
language processing system can make  sensible decisions about how to 
disambiguate them.  
 Lexical ambiguity occurs when a word has more than one possible meaning.  
For example, a bat can be a flying mammal or a piece of sporting equipment. 
The word set is an interesting example of this because it can be  used as a 
verb, a noun, an adjective, or an adverb. Determining which part  of speech is 
intended can often be achieved by a parser in cases where only  one analysis 
is possible, but in other cases semantic disambiguation is  needed to 
determine which meaning is intended.   
 Syntactic ambiguity occurs when there is more than one possible parse of a  
sentence. The sentence Jane carried the girl with the spade could be interpreted in two different ways, as is shown in the two parse trees in Fig 
10.6. In the first of the two parse trees in Fig 10.6, the prepositional phrase  
with the spade is applied to the noun phrase the girl , ind icating that it was  
the girl who had a spade that Jane carried.  In the second sentence, the  
prepositional phrase has been attached to the verb phrase carried the girl , 
indicating that Jane somehow used the spade to carry the girl.  
 Semantic ambiguity occurs  when a sentence has more than one possible  
meaning—often as a result of a syntactic ambiguity. In the example shown 
in Fig 10.6 for example, the sentence Jane carried the girl with the spade , the 
sentence has two different parses, which correspond to two possible 
meanings  for the sentence. The significance of this becomes clearer for 
practical  systems if we imagine a robot that receives vocal instructions from 
a human.  
    
  
       Fig 10.6 
 Referential ambiguity occurs when we use anaphoric expressions, or  
pronouns  to refer to objects that have already been discussed. An anaphora  
occurs when a word or phrase is used to refer to something without naming 
it. The problem of ambiguity occurs where it is not immediately clear which  
object is being referred to. F or example, consider the following sentences:  
John gave Bob the sandwich. He smiled.  
 It is not at all clear from this who smiled— it could have been John or Bob.  
In general, English speakers or writers avoid constructions such as this to 
avoid humans becomi ng confused by the ambiguity. In spite of this, 
ambiguity  can also occur in a similar way where a human would not have a  
problem, such as   John gave the dog the sandwich. It wagged its tail.  
 In this case, a human listener would know very well that it was t he dog that  
wagged its tail, and not the sandwich. Without specific world knowledge,  
the natural language processing system might not find it so obvious.   
 A local ambiguity occurs when a part of a sentence is ambiguous; however,  
when the whole sentence is examined, the ambiguity is resolved. For 
example,  in the sentence There are longer rivers than the Thames , the phrase 
longer  rivers is ambiguous until we read the rest of the sentence, than the 
Thames .  
 Another cause of ambiguity in human language is vague ness. we examined 
fuzzy logic, words such as tall, high, and fast are vague and do not have 
precise numeric meanings.  
 The process by which a natural language processing system determines 
which  meaning is intended by an ambiguous utterance is known as 
disambiguation .  
 Disambiguation can be done in a number of ways. One of the most  effective 
ways to overcome many forms of ambiguity is to use probability.   
 This can be done using prior probabilities or conditional probabilities. Prior  
probability might be used to tell the system that the word bat nearly always  
means a piece of sporting equipment.  
 Conditional probability would tell it  that when the word bat is used by a 
sports fan, this is likely to be the case, but  that when it is spoken by a 
naturalist it is more likely to be a winged mammal.   
 Context is also an extremely important tool in disambiguation. Consider  the 
following sentences:  
I went into the cave. It was full of bats.  
I looked in the locker. It was full of bats.  
 In each case, the second sentence i s the same, but the context provided by 
the first sentence helps us to choose the correct meaning of the word “bat”  in 
each case.   
 Disambiguation thus requires a good world model , which contains 
knowledge  about the world that can be used to determine the m ost likely meaning of a given word or sentence. The world model would help the 
system to  understand that the sentence Jane carried the girl with the spade is 
unlikely to  mean that Jane used the spade to carry the girl because spades are 
usually used to car ry smaller things than girls. The challenge, of course, is to 
encode this  knowledge in a way that can be used effectively and efficiently 
by the system.  
 The world model needs to be as broad as the sentences the system is likely  to 
hear. For example, a natural language processing system devoted to 
answering sports questions might not need to know how to disambiguate  the 
sporting bat from the winged mammal, but a system designed to  answer any 
type of question would.  
 
Expert System Architecture  
An expert system is a set of programs that manipulate encoded knowledge to solve problems 
in a specialized domain that normally requires human expertise. An expert system’s 
knowledge is obtained from expert sources and coded in a form suitable for the system to use 
in its inference or reasoning processes. The expert knowledge must be obtained from 
specialists or other sources of expertise, such as texts, journal, articles and databases. This  
type of knowledge usually requires m uch training and experience in some specialized field 
such as medicine, geology, system configuration, or engineering design. Once a sufficient 
body of expert knowledge has been auquired, it must be encoded in some form, loaded into a 
knowledge base, then tested, and refined continually throughout the life of the system  
 
Characteristics Features of Expert Systems  
Expert systems differ from conventional computer system in several important ways  
1. Expert systems use knowledge rather than data to control the sol ution process. Much 
of the knowledge used in heuristic in nature rather than algorithmic  
2. The knowledge is encoded and maintained as an entity separate from the aontrol 
program. As such, it is not complicated together with the control program itself. This 
permits the incremental addition and modification of the knowledge base without 
recompilation of the control programs. Furthermore, it is possible in some cases to use 
different knowledge bases with the same control programs to produce different types of ex pert systems. Such systems are  known as expert system shells since they may be 
loaded with different knowledge bases  
3. Expert systems are capable of explaining how a particular conclusion was reached, 
and why requested information is needed during a consult ation. This is important as it 
gives the user a chance to assess and understand the systems reasoning ability, thereby 
improving the user’s confidence in the system  
4. Expert systems use symbolic representations for knowledge and perform their 
inference through symbolic computations that closely resemble manipulations of 
natural language  
5. Expert systems often reason with metaknowledge, that is, they reason with knowledge 
about themselves, and their own knowledge limits and capabilities  
 
Rules for Knowledge  Representation  
 One way to represent knowledge is by using rules that express what must  happen or 
what does happen when certain conditions are met.  
 Rules are  usually expressed in the form of IF . . . THEN . . . statements, such as:  IF A 
THEN B  This can be  considered to have a similar logical meaning as the following:  
A→B  
 A is called the antecedent and B is the consequent  in this statement.  
 In expressing rules, the consequent usually takes the form  of an action or a 
conclusion.  
 In other words, the purpos e of a rule is usually  to tell a system (such as an expert 
system) what to do in certain circumstances,  or what conclusions to draw from a set of 
inputs about the  current situation.  
 In general, a rule can have more than one antecedent, usually combined either by 
AND or by OR (logically the same as the operators ∧ and ∨).  
 Similarly, a rule may have more than one consequent,  which usually suggests that 
there are multiple actions to be taken.   
 In general, the antecedent of a rule compares an object with a possible value,  using an 
operator.  
 For example, suitable an tecedents in a rule might be   
IF x > 3 IF name is “Bob”  
IF weather is cold  
 Here, the objects being considered are x, name, and weather; the operators  are “>” 
and “is”, and the values are 3, “Bob,” and cold.  
 Note that an object is  not necessarily an object  in the real -world sense —the weather is 
not a real  world object, but rather a state or condition of the world.  
 An object in this  sense is simply a variable that represents some physical object or 
state in the  real world.   
 An example of a rule might be  
IF name is “Bob”  
AND weather is cold  
THEN tell Bob ‘Wear a coat’  
 This is an example of a recommendation rule, which takes a set of inputsand gives 
advice as a result.  
 The conclusion of the rule is actually an action,  and the action takes the form of a 
recomm endation to Bob that he should wear a coat.  
 In some cases, the rules provide more definite actions such as  “move left” or “close 
door,” in which case the rules are being used to represent  directives.  
 Rules can also be used to represent relations such as:  
IF temperature is below 0  
THEN weather is cold  
 
Rule -Based Systems  
 Rule -based systems or production systems are computer systems that use  rules to 
provide recommendations or diagnoses, or to determine a course  of action in a 
particular situation or to solve a particular problem.  
 A rule -based system consists of a number of components:  
 a database of rules (also called a knowledge base ) 
 a database of facts    an interpreter , or inference engine  
 In a rule -based system, the knowledge base consists of a set of  rules that represent  the 
knowledge that the system has.  
 The database of facts represents  inputs to the system that are used to derive 
conclusions, or to cause actions.   
 The interpreter, or inference engine, is the part of the system that controls  the pro cess 
of deriving conclusions. It uses the rules and facts, and combines  them together to 
draw conclusions.  
 Using deduction to reach a  conclusion from a set of antecedents is called forward 
chaining.  
 An alternative  method, backward chaining, starts from a conclusion and tries to 
show it by following a logical path backward from the conclusion to a set of  
antecedents that are in the database of facts.  
 Forward Chaining 
 Forward chaining employs the system starts from a set of facts, and a set of  
rules, and tri es to find a way of using those rules and facts to deduce a 
conclusion  or come up with a suitable course of action.  
 This is known as data- driven reasoning because the reasoning starts from  a 
set of data and ends up at the goal, which is the conclusion.   
 When applying forward chaining, the first step is to take the facts in the fact  
database and see if any combination of these matches all the antecedents of  
one of the rules in the rule database.  
 When all the antecedents of a rule are  matched by facts in the database, then 
this rule is triggered .  
 Usually, when  a rule is triggered, it is then fired , which means its conclusion 
is added to  the facts database. If the conclusion of the rule that has fired is an 
action or  a recommendation, then the system may cause  that action to take 
place or  the recommendation to be made.   
 For example, consider the following set of rules that is used to control an 
elevator in a three -story building:  
Rule 1 
IF on first floor and button is pressed on first floor  THEN open door  
Rule 2 
IF on first floor  
AND button is pressed on second floor  
THEN go to second floor  
Rule 3 
IF on first floor  
AND button is pressed on third floor  
THEN go to third floor  
Rule 4 
IF on second floor  
AND button is pressed on first floor  
 
AND already going to thir d floor  
THEN remember to go to first floor later  
 This represents just a subset of the rules that would be needed, but we can  use 
it to illustrate how forward chaining works.  
 Let us imagine that we start with the following facts in our database:  
Fact 1 
At first floor  
Fact 2 
Button pressed on third floor  
Fact 3 
Today is Tuesday   Now the system exam ines the rules and finds that Facts 1 and 2 match the  
antecedents of Rule 3. Hence, Rule 3 fires, and its conclusion  “Go to third 
floor ” is added to the database of facts. Presumably, this results in the elevator  
heading toward the third floor.  
 Note that Fact 3 was ignored altogether  because it did not match the 
antecedents of any of the rules.   
 Now let us imagine that the elevator is on its way to the third floor an d has  
reached the second floor,  when the button is pressed on the first floor.  The fact  
Button pressed on first floor   
 Is now added to the database, which results in Rule 4 firing.  
 Now let us imagine  that later in the day the facts database contains the 
following information:  
Fact 1 
At first floor  
Fact 2 
Button pressed on second floor  
Fact 3 
Button pressed on third floor  
 In this case, two rules are triggered—Rules 2 and 3. In such cases where  there 
is more than one possible conclusion, conflict resolution needs to be  applied 
to decide which rule to fire.  
 
 Conflict Resolution  
 In a situation where more than one conclusion can be deduced from a set  of 
facts, there are a number of possible ways to decide which rule to fire.  
 For example, consider the following s et of rules:  
IF it is cold  
THEN wear a coat  
IF it is cold  THEN stay at home  
IF it is cold  
THEN turn on the heat  
 If there is a single fact in the fact database, which is “it is cold,” then clearly 
there are three conclusions that can be derived. In some cas es, it might be  fine 
to follow all three conclusions, but in many cases the conclusions are  
incompatible .  
 In one conflict resolution method, rules are given priority levels, and when  a 
conflict occurs, the rule that has the highest priority is fired, as i n the 
following example:  
IF patient has pain  
THEN prescribe painkillers priority 10 
IF patient has chest pain 
THEN treat for heart disease priority 100  
 Here, it is clear that treating possible heart problems is more important  than 
just curing the pain.  
 An alternative method is the longest -matching strategy.  This method 
involves  firing the conclusion that was derived from the longest rule.  
 For example:  
IF patient has pain  
THEN prescribe painkiller  
IF patient has chest pain 
AND patient is over 60 
AND patient  has history of heart conditions  
THEN take to emergency room   Here, if all the antecedents of the second rule match, then this rule’s 
conclusion  should be fired rather than the conclusion of the first rule because  
it is a more specific match.  
 A further meth od for conflict resolution is to fire the rule that has matched the 
facts most recently added to the database.  
 In each case, it may be that the syste m fires one rule and then stops,  but in 
many cases, the system simply needs to choose a  suitable ordering f or the 
rules because  each rule that matches the facts needs to be fired at some point.  
 Meta Rules  
 In designing an expert system, it is necessary to select the conflict resolution 
method that will be used, and quite possibly it will be necessary to use  
different methods to resolve different types of conflicts.  
 For example, in some situations it may make most sense to use the method 
that involves firing the most recently added rules.  
 This method makes most sense in situations  in which the timeliness of data  is 
important. It might be, for example,  that as research in a particular field of 
medicine develops, and new rules are  added to the system that contradicts 
some of the older rules.  
 It might make  most sense for the system to assume that these newer rules are 
more accurate  than the older rules.  
 It might also be the case, however, that the new rules have been added by an  
expert whose opinion is less trusted than that of the expert who added the  
earlier rules.  
 In this case, it clearly makes more sense to all ow the earlier  rules priority.  
 This kind of knowledge is called meta knowledge —knowledge about  
knowledge. The rules that define how conflict resolution will be used, and  
how other aspects of the system itself will run, are called meta rules . 
 The knowledge engineer who builds the expert system is responsible for  
building appropriate meta knowledge into the system (such as “expert A is  to 
be trusted more than expert B” or “any rule that involves drug X is not to  be 
trusted as much as rules that do not involve  X”). 
 Meta rules are treated by the expert system as if they were ordinary rules  but 
are given greater priority than the normal rules that make up the  expert system. In this way, the meta rules are able to override the normal  rules, if necessary, 
and are c ertainly able to control the conflict resolution  process.  
 Backward Chaining  
 Forward chaining applies a set of rules and facts to deduce whatever 
conclusions  can be derived, which is useful when a set of facts are present, but  
you do not know what conclusio ns you are trying to prove.  
 Forward chaining can be inefficient because it may end up proving a number  
of conclusions that are not currently interesting.  
 In such cases, where a  single specific conclusion is to be proved, backward 
chaining is more  appropr iate. 
 In backward chaining, we start from a conclusion, which is the hypothesis  we 
wish to prove, and we aim to show how that conclusion can be reached from 
the rules and facts in the database.  
 The conclusion we are aiming to prove is called a goal, and so  reasoning in 
this way is known as goal-driven reasoning.  
 Backward chaining is often used in formulating plans.  
 A plan is a sequence of actions that a program decides to take to solve a 
particular problem.  
 Backward chaining can make the process of formul ating a plan more efficient 
than forward chaining.   
 Backward chaining in this way starts with the goal state, which is the set of  
conditions the agent wishes to achieve in carrying out its plan. It now  
examines this state and sees what actions could lead t o it.  
 For example, if  the goal state involves a block being on a table, then one 
possible action  would be to place that block on the table.  
 This action might not be possible  from the start state, and so further actions 
need to be added before this  action  in order to reach it from the start state.  
 In this way, a plan can be formulated starting from the goal and working back 
toward the start state.   
 The benefit in this method is particularly clear in situations where the first  
state allows a very large num ber of possible actions.  
 In this kind of situation,  it can be very inefficient to attempt to formulate a 
plan using forward chaining because it involves examining every possible action, without paying any attention to which action might be the best one to 
lead to the goal  state.  
 Backward chaining ensures that each action that is taken is one that  will 
definitely lead to the goal, and in many cases this will make the planning  
process far more efficient.  
 Comparing Forward and Backward Chaining 
 Let us use an  example to compare forward and backward chaining. In this  
case, we will revert to our use of symbols for logical statements, in order to 
clarify the explanation, but we could equally well be using rules about 
elevators  or the weather.  
Rules:  
Rule 1 A ^ B → C  
Rule 2 A → D  
Rule 3 C ^ D → E 
Rule 4 B ^ E ^ F → G  
Rule 5 A ^ E → H  
Rule 6 D ^ E ^ H → I 
 
Facts:  
Fact 1        A 
Fact 2        B 
Fact 3        F 
 
Goal:  
Our goal is to prove H.   First let us use forward chaining. As our conflict resolution strategy, we w ill 
fire rules in the order they appear in the database, starting from Rule 1.  
 In the initial state, Rules 1 and 2 are both triggered. We will start by firing 
Rule 1, which means we add C to our fact database. Next, Rule 2 is fired,  
meaning we add D to our  fact database.   
 We now have the facts A, B, C, D, F, but we have not yet reached our goal,  
which is G.   
 Now Rule 3 is triggered and fired, meaning that fact E is added to the 
database.  
 As a result, Rules 4 and 5 are triggered. Rule 4 is fired first, resul ting in Fact 
G being added to the database, and then Rule 5 is fired, and Fact H is  added 
to the database.  
 We have now proved our goal and do not need to go on any further.  
 This deduction is presented in the following table:  
 
 
 Now we will consider the sam e problem using backward chaining. To do so,  
we will use a goals database in addition to the rule and fact databases.  
 In this case, the goals database starts with just the conclusion, H, which we  
want to prove.  We will now see which rules would need to fi re to lead to this  
conclusion.  
 Rule 5 is the only one that has H as a conclusion, so to prove H,  we must 
prove the antecedents of Rule 5, which are A and E.   
 Fact A is already in the database, so we only need to prove the other  
antecedent, E. Therefore, E  is added to the goal database. Once we have  proved E, we now know that this is sufficient to prove H, so we can remove  
H from the goals database.  
 So now we attempt to prove Fact E. Rule 3 has E as its conclusion, so to 
prove E, we must prove the anteceden ts of Rule 3, which are C and D.   
 Neither  of these facts is in the fact database, so we need to prove both of 
them.  They are both therefore added to the goals database. D is the 
conclusion of  Rule 2 and Rule 2’s antecedent, A, is already in the fact 
datab ase, so we can  conclude D and add it to the fact database.  
 Similarly, C is the conclusion of Rule 1, and Rule 1’s antecedents, A and B,  
are both in the fact database. So, we have now proved all the goals in the  
goal database and have therefore proved H and can stop.   
 This process is represented in the table below:  
 
 In this case, backward chaining needed to use one fewer rule. If the rule 
database  had had a large number of other rules that had A, B, and F as their  
antecedents, then forward chaining might  well have been even more 
inefficient.  
 In general, backward chaining is appropriate in cases where there are few  
possible conclusions (or even just one) and many possible facts, not very  
many of which are necessarily relevant to the conclusion.  
 Forward chaining is more appropriate when there are many possible 
conclusions.  
 The way in which forward or backward chaining is usually chosen is to 
consider which way an expert would solve the problem. This is particularly  
appropriate because rule -based reasoning is o ften used in expert systems . 
 
 Rule -Based Expert Systems   An expert system is one designed to model the behavior of an expert in  some field, 
such as medicine or geology.  
 Rule-based expert systems are  designed to be able to use the same rules that the 
expert would use to draw  conclusions from a set of facts that are presented to the 
system.  
 The People Involved i n an Expert System   
 The design, development, and use of expert systems involves a number of  
people.  
 The end-user of the system is the person who  has the need for the  system.  
 In the case of a medical diagnosis system, this may be a doctor, or it  may be 
an individual who has a complaint that they wish to diagnose.  
 The knowledge engineer is the person who designs the rules for the system,  
based on e ither observing the expert at work or by asking the expert  questions 
about how he or she works.  
 The domain expert is very important to the design of an expert system. In  the 
case of a medical diagnosis system, the expert needs to be able to  explain to 
the knowledge engineer how he or she goes about diagnosing illnesses.  
 Architecture of an Expert System  
 Typical expert system architecture is shown in Figure 11.1.  
 The knowledge base contains the specific domain knowledge that is used by  
an expert to derive con clusions from facts.  
 In the case of a rule -based expert  system, this domain knowledge is expressed 
in the form of a series of rules.  
 The explanation system provides information to the user about how the  
inference engine arrived at its conclusions. This ca n often be essential, 
particularly  if the advice being given is of a critical nature, such as with a 
medical  diagnosis system.   
  Fig  Expert System Architecture  
 If the system has used faulty reasoning to arrive at its  conclusions, then the  
user may be able to see this by examining the data  given by the explanation 
system.  
 The fact database contains the case -specific data that are to be used in a  
particular case to derive a conclusion.  
 In the case of a medical expert system,  this would contain inform ation that had 
been obtained about the  patient’s condition.  
 The user of the expert system interfaces with it through a user interface,  which 
provides access to the inference engine, the explanation system, and the 
knowledge -base editor.  
 The inference engi ne is the part of the system  that uses the rules and facts to 
derive conclusions. The inference engine will  use forward chaining, backward 
chaining, or a combination of the two to  make inferences from the data that 
are available to it.  
 The knowledge -base e ditor allows the user to edit the information that is  
contained in the knowledge base.   The knowledge -base editor is not usually  made available to the end user of the 
system but is used by the knowledge  engineer or the expert to provide and 
update the know ledge that is contained within the system.  
 The Expert System Shell  
 Note that in Figure 11.1, the parts of the expert system that do not contain 
domain- specific or case -specific information are contained within the  expert 
system shell .  
 This shell is a gene ral toolkit that can be used to build a number of different 
expert systems, depending on which knowledge base  is added to the shell.   
 An example of such a shell is CLIPS (C Language Integrated Production 
System).  Other examples  in common use include OPS5, ART, JESS, and 
Eclipse.  
 Knowledge Engineering 
 Knowledge engineering is a vital part of the development of any expert 
system.   
 The knowledge engineer does not need to have expert domain knowledge  but 
does need to know how to convert such expertise into the rules  that the system 
will use, preferably in an efficient manner.  
 Hence, the  knowledge engineer’s main task is communicating with the expert, 
in order  to understand fully how the expert goes about evaluating evidence and 
what methods he or she uses to de rive conclusions.  
 Having built up a good understanding of the rules the expert uses to draw  
conclusions, the knowledge engineer must encode these rules in the expert  
system shell language that is being used for the task.   
 In some cases, the knowledge engineer will have freedom to choose the  most 
appropriate expert system shell for the task.  
 In other cases, this decision  will have already been made, and the knowledge 
engineer must work with what he is given.  
 
CLIPS (C Language Integrated Production Sys tem)  
 CLIPS is a freely available expert system shell that has been implemented in  C.   It provides a language for expressing rules and mainly uses forward chaining to derive 
conclusions from a set of facts and rules.  
 The notation used by CLIPS is very simil ar to that used by LISP.  
 The following is an example of a rule specified using CLIPS:  
(defrule birthday 
(firstname ?r1 John)  
(surname ?r1 Smith)  
(haircolor ?r1 Red)  
=> 
(assert (is -boss ?r1)))  
 ?r1 is used to represent a variable, which in this case is a pe rson.  
 Assert is used to add facts to the database, and in this case the rule is used to draw a  
conclusion from three facts about the person:  
 If the person has the first  name John, has the surname Smith, and has red hair, then he 
is the boss.  
 This can be tried in the following way:  
(assert (firstname x John))  
(assert (surname x Smith))  
(assert (haircolor x Red))  
(run)  
 At this point, the command (facts) can be entered to see the facts that are  contained in 
the database:  
CLIPS> (facts)  
f-0 (firstname x John)  
f-1 (surname x Smith)  
f-2 (haircolor x Red)  f-3 (is-boss x)  
 So CLIPS has taken the three facts that were entered into the system and used the rule 
to draw a conclusion, which is that x is the boss.  
 Although this  is a simple example, CLIPS, like other exp ert system shells, can be 
used to  build extremely sophisticated and powerful tools.   
 For example, MYCIN is a well -known medical expert system that was  developed at 
Stanford University in 1984.  
 MYCIN was designed to assist  doctors to prescribe antimicrobia l drugs for blood 
infections.  
 In this way,  experts in antimicrobial drugs are able to provide their expertise to other  
doctors who are not so expert in that field. By asking the doctor a series of  questions, 
MYCIN is able to recommend a course of treatmen t for the  patient.  
 Importantly, MYCIN is also able to explain to the doctor which rules fired and 
therefore is able to explain why it produced the diagnosis  and recommended treatment 
that it did.   
 MYCIN has proved successful: for example, it has been prov en to be able to  provide 
more accurate diagnoses of meningitis in patients than most doctors.   
 MYCIN was developed using LISP, and its rules are expressed as LISP  expressions.  
 The following is an example of the kind of rule used by  MYCIN, translated into 
English:  
IF the infection is primary -bacteria  
AND the site of the culture is one of the sterile sites  
AND the suspected portal of entry is the gastrointestinal tract  
THEN there is suggestive evidence (0.7) that infection is bacteroid  
 The following is a ver y simple example of a CLIPS session where rules are  defined to 
operate an elevator:  
CLIPS> (defrule rule1  
(elevator ?floor_now)  
(button ?floor_now)  => 
(assert (open_door)))  
CLIPS> (defrule rule2  
(elevator ?floor_now)  
(button ?other_floor)  
=> 
(assert (goto ?other_floor)))  
CLIPS> (assert (elevator floor1))  
==> f -0 (elevator floor1)  
<Fact -0> 
CLIPS> (assert (button floor3))  
==> f -1 (button floor3)  
<Fact -1> 
<CLIPS> (run)  
==>f -2 (goto floor3)  
 The segments in bold are inputs by the knowledge engineer, and the plai n text 
sections are CLIPS.   
 Note that ?floor_now is an example of a variable within CLIPS, which means that 
any object can match it for the rule to trigger and fire.  
 In our  example, the first rule simply says: If the elevator is on a floor, and the button  
is pressed on the same floor, then open the door.  
 The second rule says:  If the elevator is on one floor, and the button is pressed on a 
different floor,  then go to that floor.   
 After the rules, two facts are inserted into the database. The first fact say s that the 
elevator is on floor 1, and the second fact says that the button has  been pressed on 
floor 3.   When the (run) command is issued to the system, it inserts a new fact into  the 
database, which is a command to the elevator to go to floor 3.  
 
Backward Chaining in Rule -Based Expert Systems  
 A common method for building expert systems is to use a rule -based system  with 
backward chaining. Typically, a user enters a set of facts into the  system, and the 
system tries to see if it can prove any of the po ssible  hypotheses using these facts.  
 In some cases, it will need additional facts, in  which case the expert system will often 
ask the user questions, to ascertain  facts that could enable further rules to fire.   
 The algorithm is applied as follows:  
 To prov e a conclusion, we must prove a set of hypotheses, one of which is  the 
conclusion. For each hypothesis, H:  
If H is in the facts database, it is proved.  
Otherwise, if H can be determined by asking a question, then enter  
the user’s answer in the facts databa se. Hence, it can be determined  
whether H is true or false, according to the user’s answer.  
Otherwise, find a rule whose conclusion is H. Now apply this algorithm  
to try to prove this rule’s antecedents.  
If none of the above applies, we have failed to prov e H. 
 Typically, backward chaining is used in combination with forward chaining.  
Whenever a new fact is added to the database, forward chaining is  applied to see if 
any further facts can be derived.  
 Backward chaining is then  used to try to prove each possi ble hypothesis.  
 Let us imagine a simple medical expert system that is designed to diagnose  and treat 
patients’ diseases. The rules might be as follows:  
Rule 1: If headache then prescribe painkiller  
Rule 2: If headache and sore throat and coughing then diag nose flu  
Rule 3: If tired and headache then diagnose glandular fever  Rule 4: If tired and sore throat then diagnose tonsillitis  
Rule 5: If tired then diagnose stress  
 Now we might imagine a session between the user (the patient) and the  expert system 
as fol lows:  
Patient: I have a headache  
System: I suggest you take a painkiller. Do you have a sore throat?  
Patient: Yes, I do.  
System: Have you been coughing?  
Patient: No, why do you ask?  
System: If you had a headache and a sore throat and had been 
coughing, I w ould have concluded you might have flu. It  
doesn’t sound as though you have flu. Have you been feeling  
tired recently?  
Patient: Yes, I have.  
System: It sounds as though you may have glandular fever or tonsillitis.  
 This was a rather simplistic example, but it illustrates the way that the system  uses 
forward chaining to derive new facts and asks questions in order  to carry out its 
backward chaining analysis.  
 For example, having been told that the patient has a headache, the system first applies 
forward chain ing with Rule 1 and, as a result, adds “prescribe painkiller” to its 
database of  facts.  
 This fact has an action, or a recommendation, associated with it, and so the system 
advises the patient that she should take a painkiller.   
 Next, the system tries to s ee if it can prove any other hypotheses. The possible  
hypotheses are flu, tonsillitis, glandular fever, and stress. First, the system  uses 
backward chaining to try to prove the hypothesis that the patient  has the flu.    To prove this hypothesis, the antecedents of Rule 2 must be proved: that the  patient 
has a headache and a sore throat and has been coughing. The  patient has already said 
that she has a headache, so this fact is already in the  fact database. Next, the system 
must establish whether the patient has a  sore throat. She says that she does, so this fact 
is added to the fact database.  She has not been coughing, though, so the system 
concludes that she does  not have flu.  
 At this point also note that the patient asks why the system asked the last  questi on. 
The system is able to use its explanation facility to provide an  explanation for why it 
asked the question and what conclusion it was able  to draw from the answer.   
 Finally, the patient says that she has been feeling tired, and as a result of this  fact 
being added to the database, Rules 3, 4, and 5 are all triggered.  
 In this  case, conflict resolution has been applied in a rather simplistic way, such  that 
Rules 3 and 4 both fire, but 5 does not.  
 In a real medical expert system,  it is likely that furthe r questions would be asked, and 
more sophisticated rules applied to decide which condition the patient really had.  
CYC  
 CYC is an example of a frame -based representational system of knowledge,  which is, 
in a way, the opposite of an expert system.  Wher eas an expert system  has detailed 
knowledge of a very narrow domain, the developers of CYC  have fed it information 
on over 100,000 different concepts from all fields of  human knowledge. CYC also has 
information of over 1,000,000 different  pieces of “common  sense” knowledge about 
those concepts.  
 The system has  over 4000 different types of links that can exist between concepts, 
such as  inheritance, and the “is –a” relationship that we have already looked at.   
 The idea behind CYC was that humans function in th e world mainly on the  basis of a 
large base of knowledge built up over our lifetimes and our ancestors’  lifetimes.  
 By giving CYC access to this knowledge, and the ability to  reason about it, they felt 
they would be able to come up with a system with  commo n sense. Ultimately, they 
predict, the system will be built into word  processors.  
 Then word processors will not just correct your spelling and grammar, but will also 
point out inconsistencies in your document.  
 For example, if you promise to discuss a par ticular subject later in your document,  
and then forget to do so, the system will point this out to you. They  also predict that search engines and other information retrieval systems  will be able to find documents 
even though they do not  contain any of the  words you entered as your query.   
 CYC’s knowledge is segmented into hundreds of different contexts to avoid the 
problem of many pieces of knowledge in the system contradicting each  other.  
 In this way, CYC is able to know facts about Dracula and to reason  about him, while 
also knowing that Dracula does not really exist.   
 CYC is able to understand analogies, and even to discover new analogies for  itself, by 
examining the similarities in structure and content between different  frames and 
groups of frames. CY C’s developers claim, for example, that  it discovered an analogy 
between the concept of “family” and the concept of  “country.”  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

1  LECTURE  NOTES  FOR  NEURAL  NETWORKS  AND  FUZZY  LOGIC  (15A02604)  
UNIT -1 
INTRODUCTION TO ARTIFICIAL INTELLIGENCE  
WHAT  IS ARTIFICIAL  INTELLIGENCE?  
Artificial  intelligence  (AI)  is the ability  of a digital computer or computer -controlled robot to 
perform tasks commonly associated with intelligent beings. The  term is frequently applied to the  
project of developing systems endowed with the intellectual processes characteristic of humans,  
such  as the ability  to reason, discover  meaning,  generalize, or  learn  from  past  experience.  
 
BRIEFLY  EXPLAIN  ARTIFICIALINTELLIGENCE.  
 
Artificial  intelligence (AI),  sometimes  called  machine  intelligence , is intelligence  demonstrated  
by machines, in contrast to the natural intelligence displayed by humans. Since the development  
of the digital  computer in the 194 0s, it has been  demonstrated  that  computers  can be 
programmed  to carry  out very  complex  tasks  like discovering  proofs  for mathematical  theorems  
or playing chess with great proficiency. Still, despit e continuing advances in computer processing  
speed and memory capacity, there are as yet no programs that can match human flexibility over  
wider  domains  or in tasks  requiring  much  everyday  knowledge.  On the other  hand,  some  
programs have attained the performance levels of human experts and professionals in performing  
certain  specific  tasks,  so that  artificial  intelligence  in this limited  sense  is found  in applications  
as diverse  as medical  diagnosis,  computer  search  engines,  and voice  or handwriting  recognition.  
 
DEFINE  ARTIFICIAL  INTELLIGENCE.  
 
Computer  science  defines  AI research  as the study  of "intelligent  agents ": any device  that  
perceives its environment and takes actions that maximize its cha nce of successfully achieving its  
goals. A more elaborate definition characterizes AI as “a system’s ability to correctly interpret  
external data,  to learn from such data,  and to  use those learnings to achieve  specific goals and  
tasks  through  flexible  adaptation.”  
 
HISTORY  OF AI. 
 
The field of AI research was born at a workshop at Dartmouth College in 1956, where the term  
"Artificial Intelligence" was coined by John McCarthy to distinguish the field from cybernetics and  
escape the influence of the cyberneticist Norbert Wiener. Attendees Allen Newell (CMU ), Herbert  
Simon (CMU), John McCarthy ( MIT ), Marvin Minsky (MIT) and Arthur Samuel (IBM ) became the  
founders  and leaders of  AI research.  
 
Commonly, th e term "artificial intelligence" is often used to describe machines (or computers) that  
mimic "cognitive" functions that humans associate with the human mind, such as "learning" and  
"problem solving". The field was founded on the assumption that human intelligence "can be so  
precisely described that a machine can be made to simulate it". For most of its history, AI re search  
has been divided into subfields  that often fail to communicate with each other. These sub -fields  
are based  on technical  considerations,  such  as particular  goals  (e.g.  "robotics " or "machine  
learning "), the use of particular tools (" logic " or artificial neural networks ), or deep philosophical  
differences. Subfields have also been based on social factors (particular institutions or the work of  
particular  researchers).  2  GOALS  OF AI 
 
The traditional  problems  (or goals)  of AI research  include reasoning, knowledge  
representation, planning, learning, natural  language  processing, perception and the ability  to 
move    and    manipulate    objects. General    intelligence is    among    the    field's    long -term  
goals. Approaches  include statistical  methods, computational  intelligence,  and traditional  
symbolic  AI. Many  tools  are used  in AI, including  versions  of search  and mathematical  
optimization, artificial  neural  networks,  and methods  based  on statistics,  probability  and 
economics.  The AI field  draws  upon  computer  science, information  
engineering, mathematic s, psychology, linguistics, philosophy, and many  other  fields.  
 
WHAT  ARE  THE ADVANTAGES  OF AI? 
 
The advantages of Artificial intelligence applications are enormous and can  revolutionize any  
professional  sector. Let’s see some of them  
 
1) Reduction in Human Error:  The phrase  “human  error”  was  born  because  humans  make  
mistakes  from  time  to time.  Computers,  however,  do not make  these  mistakes  if they  are 
programmed properly. With Artificial intelligence, the decisions are taken from the previously  
gathered information applying a certain set of algorithms. So errors are reduced and the  chance of  
reaching  accuracy  with  a greater  degree of  precision is a  possibility.  
 
Example:  In Weather  Forecasting  using  AI they have  reduced  the majority  of human  error.  
 
2) Takes risks instead of Humans: This is one of the biggest advantages of Artificial intelligence.  
We can overcome many risky limitations of humans by developing an AI Robot which in turn can  
do the risky things for us. Let it be going to mars, defuse a bomb, explore the deepest parts of  
oceans, mining for coal and oil, it can be used effe ctively in any kind of natural or man -made  
disasters.  
 
Example: Have you heard about the Chernobyl nuclear power plant explosion in Ukraine? At that  
time there were no AI -powered robots that can help us to minimize the effect of radiation by  
controlling th e fire in early stages, as any human went close to the core was dead in a matter of  
minutes.  They  eventually  poured  sand  and boron  from  helicopters  from  a mere  distance.  AI 
Robots  can be  used  in such situations  where intervention can  be hazardous.  
 
3) Available  24x7:  An Average  human  will work for 4 –6 hours  a day excluding  the breaks.  
Humans are built in such a way to get some time out for refreshing themselves and get ready for a  
new day of work and  they even have weekly offed to stay intact with their work -life and personal  
life. But using AI we can make machines work 24x7 without any breaks and they don’t even get  
bored,  unlike humans.  
 
Example: Educational Institutes and Helpline centers are getti ng many queries and issues which  
can be handled  effectively  using  AI. 
 
4) Helping in Repetitive Jobs: In our day -to-day work, we will be performing many repetitive  
works like sending a thanking mail, verifying certain documents for errors and many more things.  
Using artificial intelligence we can productively automate these mundane tasks and can even  
remove  “boring”  tasks for  humans  and free them  up to be increasingly  creative.  3  Example:  In banks,  we often  see various  verifications  of documents  to get a loan  which  is a 
repetitive task for the owner of the bank. Using AI Cognitive Automation the owner can speed up  
the process  of verifying  the documents  by which  both  the customers  and the owner  will be 
benefited.  
 
5) Digital Assistance: Some  of the highly advanced organizations use digital assistants to interact  
with users which saves the need for human resources. The digital assistants also used in many  
websites to provide things that users want. We can chat with them about what we are looki ng for.  
Some chatbots are designed in such a way that it’s become hard to determine that we’re chatting  
with  a chatbot or  a human being.  
 
Example: We all know that organizations have a customer support team that needs to clarify the  
doubts and queries of t he customers. Using AI the organizations can set up a Voice bot or Chatbot  
which can help customers with all their queries. We can see many organizations already started  
using  them  on their websites and  mobile  applications.  
 
6) Faster Decisions: Using AI alon gside other technologies we can make machines take decisions  
faster than a human and carry out actions quicker. While taking a decision human will analyze  
many  factors  both  emotionally  and practically  but AI-powered  machine  works  on what  it is 
programmed  and delivers the  results in  a faster way.  
 
Example: We all have played Chess games in Windows. It is nearly impossible to beat CPU in the  
hard mode because of the AI behind that game. It will take the best possible step in a very short  
time  according  to the algorithms used  behind  it. 
 
7) Daily Applications: Daily applications such as Apple’s Siri, Window’s Cortana, Google’s OK  
Google are frequently used in our daily routine whether it is for searching a location,  taking a  
selfie, making  a phone call,  replying  to a mail  and many  more.  
 
Example: Around 20 years ago, when we are planning to go somewhere we used to ask a person  
who already went there for the directions. But now all we have to do is say “OK Google where is  
Visakhapatnam”. It will show you Visakhapat nam’s location on google map and the best path  
between  you and Visakhapatnam.  
 
8) New Inventions: AI is powering many inventions in almost every domain which will help  
humans  solve the majority  of complex problems.  
 
Example:  Recently  doctors  can predict  breast  cancer  in the woman  at earlier  stages  using  
advanced  AI-based  technologies.  
 
WHAT  ARE  THE  DISADVANTAGES  OF AI? 
 
As every bright side has a darker version in it. Artificial Intelligence also has some disadvantages.  
Let’s  see some of them  
 
1) High Costs of Creation: As AI is updating every day the hardware and software need to get  
updated with time to meet the latest requirements. Machines need repairing and maintenance  
which  need  plenty  of costs.  It’ s creation  requires  huge  costs  as they  are very  complex  machines.  4  2) Making Humans Lazy: AI is making humans lazy with its applications automating the majority  
of the work. Humans tend to get addicted to these inventions which can cause a problem to future  
generations.  
 
3) Unemployment: As AI is  replacing the majority of the repetitive tasks and other works with  
robots, human interference is becoming less which will cause a major problem in the employment  
standards. Every organization is looking to replace the minimum qualified individuals with A I 
robots  which  can do similar  work  with more efficiency.  
 
4) No Emotions: There is no doubt that machines are much better when it comes to working  
efficiently but they cannot replace the human connection that makes the team. Machines cannot  
develop  a bond  with  humans  which  is an essential  attribute  when  comes  to Team  Management.  
 
5) Lacking Out of Box Thinking: Machines can perform only those tasks which they are designed  
or programmed to do, anything out of that they tend to crash or give irrelevant outputs w hich  
could  be a major  backdrop.  
 
APPROACHES  OF AI 
 
AI often revolves around the use of algorithms. An algorithm is a set of unambiguous instructions  
that a mechanical computer can execute. [b] A complex algorithm is often built on top of other,  
simpler, algorithms. Many AI algorithms are capable of learning from data; they can enhance  
themselves by learn ing new heuristics (strategies, or "rules of thumb", that have worked well in  
the past), or can  themselves  write other  algorithms.  
 
An algorithm is a  kind of container. It provid es a box for storing a  method to solve a particular  
kind of a problem. Algorithms process data through a series of well -defined states. The states need  
not be deterministic, but the states are defined nonetheless. The goal is to create an output that  
solve s a problem. In some cases, the algorithm receives inputs that help define the output, but the  
focus  is always  on the  output.  
Algorithms must express the transitions between states using a well -defined and formal language  
that the computer can understand. In processing the data and solving the problem, the algorithm  
defines, refines, and executes a function. The function is always specific to the kind of problem  
being  addressed  by the algorithm.  
Each of the five tribes has a different technique and strategy  for solving problems that result in  
unique algorithms. Combining these algorithms should lead eventually to the master algorithm  
that will be able to solve any given problem. The following discussion provides an overview of the  
five main algorithmic techniques.  
 
Symbolic reasoning: One of the earliest tribes, the symbolists, believed that knowledge could be  
obtained by operating on symbols (signs that stand for a certain meaning or event) and deriving  
rules from them. By putting together complex syste ms of rules, you could attain a logic deduction  
of the result you wanted to know, thus the symbolists shaped their algorithms to produce rules  
from  data.  In   symbolic   reasoning, deduction expands   the   realm   of   human   knowledge,  
while induction r aises the level of human knowledge. Induction commonly opens new fields of  
exploration, while deduction explores those fields.  
 
Connections modelled on the brain’s neurons: The connectionists are perhaps the most famous  
of the five tribes.  This  tribe  striv es to reproduce  the brain’s  functions  by using  silicon  instead  of 5  neurons. Essentially, each of the neurons (created as an algorithm that models the real -world  
counterpart) solves a small piece of the problem, and using many neurons in parall el solves the  
problem as a whole. The use of backpropagation, or backward propagation of errors, seeks to  
determine the conditions under which errors are removed from networks built to resemble the  
human  neurons  by changing  the weights  (how  much  a particular  input  figures  into  the result)  
and biases (which features are selected) of the network. The goal is to continue changing the  
weights and biases until such time as the actual output matches the target output. At this point, the  
artificial neuron fires and passes its solution along to the next neuron in line. The solution created  
by just one neuron is only part of the whole solution. Each neuron passes information to the next  
neuron in line until the group of neurons creates a final output. Such a method proved the most  
effective  in human -like tasks  such  as recognizing  objects,  understanding  written  and spoken  
language,  and chatting  with humans.  
 
Evolutionary  algorithms  that  test  variation:  The evolutionaries  rely  on the principles  of 
evolution to solve problems. In other words, this strategy is based on the survival of the fittest  
(removing any solutions that don’t match the desired output). A fitness function determines the  
viability of each function in  solving a problem. Using a tree structure, t he solution method looks  
for the best solution based on function output. The winner of each level of evolution gets to build  
the next -level functions. The idea is that the next level will get closer to solving the problem but  
may not solve it completely, w hich means that another level is needed. This particular tribe relies  
heavily  on recursion  and languages  that  strongly  support  recursion  to solve  problems.  An 
interesting output of this strategy has been algorithms that evolve: One generation of algorithms  
actually  builds the  next  generation.  
 
Bayesian inference: A group of scientists, called Bayesians, perceived that uncertainty was the  
key aspect  to keep  an eye on and that  learning  wasn’t  assured  but rather  took  place  as a 
continuous  updating  of previous  beliefs  that  grew  more  and more  accurate.  This  perception  led 
the Bayesians to adopt statistical methods and, in particular, derivations from Bayes’ theorem,  
which helps you calculate probabilities under specific conditions (for instance, seeing a card of a 
certain seed, the starting value for a pseudo -random sequence, drawn from a deck after three  
other  cards of  same seed).  
 
Systems that learn by analogy: The analogyzers use kernel machines to recognize patterns in  
data. By recognizing the pattern of one s et of inputs and comparing it to the pattern of a known  
output, you can create a problem solution. The goal is to use similarity to determine the best  
solution to a problem. It’s the kind of reasoning that determines that using a particular solution  
worked  in a given circumstance at some previous time; therefore, using that solution for a similar  
set of circumstances should also work. The ultimate goal of machine learning is to combine the  
technologies and strategies embraced by the five tribes to create a single algorithm (the master  
algorithm)  that  can learn  anything.  Of course, achieving  that  goal  is a long  way  off. 
 
BRIEFLY  EXPLAIN  SYMBOLIC  REASONING  SYSTEM.  
Reasoning can be defined  as the algebraic manipulation of historical knowledge in order to  
answer  a new  question.  This  manipulation  can include  a search  in an algebraic  space  of different  
solutions. The reasoning  system  has the following  features:  
1. It requires a knowledge base (a relational, non -relational or graph database). See the family  
tree  in following  figure  for an example.  6   
Figure:  A family  tree  
2. It requires a collection of symbolic facts, rules and relationships, like the one shown in the  
Figure  below.  
 
3. It requires  an inference  engine  that  takes  a question  or query  and generates  an answer  by 
using the set of rules and the knowledge -base. For example, if I ask “who is the maternal  
great -uncle of Freya?”, the inference engine will search for the solution in th e space of  
clauses  in the following  Figure  and apply  deduction  rules  such  as substitution . 
 
Figure  : Reasoning  about  the family  tree  
7  The first selection will be the last clause (in blue in the figure). The first predicate of this rule is  
maternal grandmother (Freya,?). By checking the third clause, we see that “maternal  
grandmother” has the conjunction of predicates mother(X,Z), mother(Z,Y), which basically says “if  
Y is the mother of Z and Z is the mother of X, then Y is the maternal gran dmother of X.” So the  
engine will first find the maternal grandmother of Freya using the third clause, which is Charlotte,  
then  the mother  of Charlotte,  which  is Lindsey  and finally  the son of Lindsey,  which  is Fergus, who  
is the maternal  great -uncle  of Freya.  
 
As we see in the previous example, symbolic AI involves a searching process. In this regards,  
researchers  have  proposed  different  searching algorithms,  such  us Goal  tree  search  (also  call And  
— Or tree)  and Monte  Carlo  tree  search.  
 
WHAT  ARE  EXPERT  SYSTEMS?  
 
The expert  systems  are the computer  applications  developed  to solve  complex  problems  in a 
particular  domain, at the  level of  extra -ordinary  human  intelligence  and expertise.  
 
Characteristics  of Expert  Systems  include:  
 
1. High  performance  
2. Understandable  
3. Reliable  
4. Highly  responsive  
 
Capabilities  of Expert  Systems:  The expert  systems  are capable  of the following:  
 
1. Advising  
2. Instructing  and assisting  human  in decision  making  
3. Demonstrating  
4. Deriving  a solution  
5. Diagnosing  
6. Explaining  
7. Interpreting  input  
8. Predicting  results  
9. Justifying  the conclusion  
10. Suggesting alternative options to a problem  
Benefits  of Expert  Systems  
1. Availability  − They  are easily  available  due to mass  production  of software.  
2. Less  Production  Cost  − Production  cost  is reasonable.  This makes  them  affordable.  
3. Speed  − They  offer  great  speed.  They  reduce  the amount  of work  an individual  puts  in. 
4. Less  Error  Rate  − Error  rate  is low as compared  to human  errors.  
5. Reducing  Risk  − They  can work  in the environment  dangerous  to humans.  
6. Steady  response  − They  work  steadily  without getting  motional,  tensed  or fatigued.  8  BRIEFLY  EXPLAIN  THE  ARCHITECTURE  OF AI AND  EXPERT  SYSTEMS.  
 
The job of AI is to design  an agent  program  that  implements  the agent  function  — 
the mapping from percepts  to actions. We assume this program will run on some sort of  
Computing  device  with  physical  sensors  and actuators  which  we call as the architecture:  
agent  = architecture  + program  
 
Obviously, the program we choose has to be one that is appropriate for the architecture. If the  
program is going to recommend actions like Walk, the architecture had better have legs. The  
architecture might be just an ordinary PC, or it might be a robotic car with several onboard  
computers, cameras, and other sensors. In general,  the architecture makes the percepts from  
the sensors  available  to the program,  runs  the program,  and feeds  the program’s  action  choices  
to the actuators as  they  are generated.  
 
 
 
9   
 
10  The architecture  of an expert  system  of AI consists  of the following  components:  
 
Knowledge Base (KB): repository of special heuristics or rules that direct the use of knowledge,  
facts  (productions).  It contains  the knowledge  necessary  for understanding,  formulating,  & 
problem  solving.  
 
Working Memory (Blackboard): if forward chaining used, it describes the current problem &  
record intermediate results. Records Intermediate Hypothesis & Decisions such as Plan, Agenda,  
Solution.  
 
Inference Engine: the deduction system used to infer results  from user input & KB. It is the brain  
of the ES, the control structure (rule  interpreter). It  provides methodology  for reasoning  
 
Explanation  Subsystem  (Justifier):  Traces  responsibility  & explains  the ES behaviour  by 
interactively  answering question:  Why?, How?,  What?, Where?,  When?,  Who?  
 
User Interface: interfaces with user through Natural Language Processing (NLP), or menus &  
graphics.  Acts  as Language  Processor  for friendly,  problem -oriented  communication  
 
Shell  = Inference  Engine  + User  Interface  
 
The Human Elements in ES is an Expert: Has the special knowledge, judgement, experience and  
methods  to give  advice  and solve  problems.  Provides  knowledge  about  task  performance  
 
Knowledge Engineer: Usually also the System Builder. Helps the expert(s) structure the problem  
area  by interpreting  and integrating  human  answers  to questions,  drawing  analogies,  posing  
counter  examples,  and bringing  to light  conceptual  difficulties.  The Expert  & the knowledge  
Engineer  should  Anticipate  Users’  needs &  Limitatio ns when  designing  Expert  Systems  
 
User:  Possible  Classes  of Users can  be, 
 A non -expert  client  seeking  direct  advice  (ES acts  as a Consultant  or Advisor)  
 A student  who  wants  to learn  (ES acts  as an Instructor)  
 An ES builder  improving  or increasing  the knowledge  base(ES  acts  as a Partner)  
 An Expert  (ES acts  as a Colleague  or an Assistant)  
 
WHAT  IS PERCEPTION  IN AI? 
 
Perception is a process to interpret, acquire, select and then organize the sensory information that  
is captured  from  the real  world.  For example:  Human  beings  have  sensory  receptors  such  as touch,  
taste, smell, sight and hearing. So, the information received from these receptors is transmitted to  
human brain to organize the received information. According to the received information, action  is 
taken by interacting with the environment to manipulate and navigate the objects. Perception and  
action are very important concepts in the field of Robotics. The following figures show the  
complete autonomous robot. There is one important difference be tween the artificial intelligence  
program and robot. The AI program performs in a computer stimulated environment, while the  
robot performs in the physical world. For example: In chess, an AI program can be able to make a  
move  by searching  different  nodes and has no facility  to touch  or sense the  physical world.  
However,  the chess  playing  robot  can make  a move  and grasp  the pieces  by interacting  with  the 
physical  world.  11   
 
WHAT  IS MACHINE  LEARNING?  
 
• It is very hard to write programs that solve  problems like recognizing a three -dimensional object  
from  a novel viewpoint  in new  lighting conditions in  a cluttered  scene.  
– We don’t  know  what  program  to write  because  we don’t  know  how  it is done  in our brain.  
– Even  if we had a good  idea  about  how  to do it, the program  might  be horrendously  complicated.  
 
• It is hard  to write  a program  to compute  the probability  that  a credit  card  transaction  is 
fraudulent.  
– There  may  not be any rules  that  are both  simple  and reliable.  We need  to combine  a very  large  
number  of weak  rules.  
– Fraud  is a moving  target.  The program  needs to  keep  changing.  
 
Instead of writing a program by hand for each specific task, we collect lots of examples that specify  
the correct output  for a given input.  
 
• A machine  learning  algorithm  then  takes  these  examples  and produces  a program  that  does  the 
job. 
– The program  produced  by the learning  algorithm  may  look  very  different  from  a typical  hand - 
written  program. It  may  contain millions  of numbers.  
– If we do it right, the  program works  for new  cases  as well  as the ones  we trained  it on. 
– If the data  changes  the program  can change  too by training  on the new  data.  
 
• Massive  amounts  of computation  are now  cheaper  than  paying  someone  to write  a task -specific  
program.  
12  WHAT  IS SUPERVISED LEARNING?  
 
Supervised learning is the machine learning task of learning a function that maps an input to an  
output  based  on example  input -output  pairs.  It infers  a function  from  labeled  training  data  
consisting of a set of training examples. In supervised learning, each example is a pair consisting of  
an input object (typically a vector) and a desired output value (also called the supervisory signal).  
A supervised learning algorithm analyzes the training data and produces an inferred function,  
which can be used for mapping new examples. An optimal scenario will allow for the algorithm to  
correctly determine the class labels for unseen instances. This requires the learning algorithm to  
generalize  from  the training  data  to unseen  situations  in a "reasonable" w ay. 
In order  to solve  a given  problem  of supervised  learning,  one has to perform  the following  steps:  
 
1. Determine the type of training examples. Before doing anything else, the user should decide  
what kind of data is to be used as a training set. In the cas e of handwriting analysis, for  
example, this might be a single handwritten character, an entire  handwritten word, or an  
entire  line of  handwriting.  
2. Gather a training set. The training set needs to be representative of the real -world use of the  
function. Thus, a set of input objects is gathered and corresponding outputs are also gathered,  
either  from human  experts or  from  measurements.  
3. Determine  the input  feature  representation  of the learned  function.  The accuracy  of the 
learned function depends strongly on how the input object is represented. Typically, the input  
object  is transformed into  a feature  vector, which  contains  a number of features  that are  
descriptive of the object. The number of features should not be too large, because of the curse  
of dimens ionality;  but should  contain  enough  information  to accurately  predict  the output.  
4. Determine the structure of the learned function and corresponding learning algorithm. For  
example, the  engineer  may  choose  to use support  vector  machines or  decision trees.  
5. Complete the design. Run the learning algorithm on the gathered training set. Some supervised  
learning  algorithms  require  the user  to determine  certain  control  parameters.  These  
parameters may be adjusted by optimizing performance on a subset (called a val idation set) of  
the training  set, or via cross -validation.  
6. Evaluate the accuracy of the learned function. After parameter adjustment and learning, the  
performance of the resulting function should be measured on a test set that is separate from  
the training  set.  
1  LECTURE  NOTES  FOR  NEURAL  NETWORKS  AND  FUZZY  LOGIC  (15A02604)  
UNIT -2 
ARTIFICIAL NEURAL NETWORKS  
WHAT  IS ARTIFICIAL  NEURAL NETWORK?  
An Artificial Neural  Network  (ANN)  is a mathematical model that  tries  to simulate  the structure  and 
functionalities of  biological neural networks. Basic building block of every artificial neural network is  
artificial neuron, that is, a simple mathematical model (function). Such a model has three simple sets of 
rules: multiplication, summation and activation. At the entrance of artificial neuron the inputs are weighted  
what means that every input value is multiplied with individual weight. In the middle section of artificial  
neuron is sum function that sums al l weighted inputs and bias. At the exit of artificial neuron the sum of  
previously weighted inputs and bias is passing through activation function that is also called transfer  
function.  
 
 
 
 
2  BIOLOGICAL  NEURON  STRUCTURE  AND  FUNCTIONS.  
 
A neuron, or nerve cell, is an electrically excitable cell that communicates with other cells via specialized  
connections called synapses. It is the main component of nervous tissue. Neurons are typically classified  
into t hree types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light  
that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons  
receive signals from the brain and spinal co rd to control everything from muscle contractions to glandular  
output.  Interneurons  connect  neurons  to other  neurons  within  the same  region  of the brain  or spinal  cord.  
A group  of connected  neurons  is called  a neural circuit.  
A typical neuron consists of a  cell body (soma), dendrites, and a single axon. The soma is usually compact.  
The axon and dendrites are filaments that extrude from it. Dendrites typically branch profusely and extend  
a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock,  
and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a  
constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can  
transmit a si gnal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term  
neurite  is used  to describe  either  a dendrite  or an axon,  particularly  when  the cell is undifferentiated.  
The soma is the body of the neuron. As it contains the nucleus, most protein synthesis occurs here. The  
nucleus  can range from  3 to 18 micrometers  in diameter.  
The dendrites of a neuron are cellular extensions with many branches. This overall shape and structure is  
referred to metaphorically as a dendritic tre e. This is where the majority of input to the neuron occurs via  
the dendritic  spine.  
The axon is a finer, cable -like projection that can extend tens, hundreds, or even tens of thousands of times  
the diameter of the soma in length. The axon primarily carrie s nerve signals away from the soma, and  
carries some types of information back to it. Many neurons have only one axon, but this axon may —and 
usually will —undergo extensive branching, enabling communication with many target cells. The part of the  
axon where  it emerges from the soma is called the axon hillock. Besides being an anatomical structure, the  
axon hillock also has the greatest density of voltage -dependent sodium channels. This makes it the most  
easily excited part of the neuron and the spike initiat ion zone for the axon. In electrophysiological terms, it  
has the most  negative threshold potential.  
While the axon and axon hillock are generally involved in information outflow, this region can also receive  
input  from  other  neurons.  
The axon terminal is f ound at the end of the axon farthest from the soma and contains synapses. Synaptic  
boutons are specialized structures where neurotransmitter chemicals are released to communicate with  
target  neurons.  In addition  to synaptic  boutons  at the axon  terminal,  a neuron  may  have  en passant  
boutons,  which are located  along  the length  of the axon.  
Most neurons receive signals via the dendrites and soma and send out signals down the axon. At the  
majority  of synapses,  signals  cross  from  the axon  of one neuron  to a dend rite of another.  However,  
synapses can connect an axon to another axon or a dendrite to another dendrite. The signaling process is  
partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage  
gradients across the ir membranes. If the voltage changes by a large amount over a short interval, the  
neuron  generates  an all-or-nothing  electrochemical  pulse  called  an action  potential.  This  potential  travels   
 rapidly along the axon, and activates synaptic connections as it reaches them. Synaptic signals may be  
excitatory  or inhibitory,  increasing  or reducing  the net voltage  that  reaches  the soma.  
In most  cases,  neurons  are generated  by neural  stem  cells  during  brain  development  and childhood.  
Neurogenesis  largely  ceases  during  adulthood  in most  areas  of the brain.  However,  strong  evidence  
supports generation  of substantial  numbers of  new  neurons  in the hippocampus  and olfactory  bulb.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page 3 of 24 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  4 of 24  
 STRUCTURE  AND  FUNCTIONS  OF ARTIFICIAL  NEURON.  
 
An artificial  neuron  is a mathematical  function  conceived  as a model  of biological  neurons,  a neural  
network.  Artificial  neurons  are elementary  units  in an artificial  neural  network.  The artificial  neuron  
receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic  
potentials at neural dendrites) and sums them to produce  an output (or activation, representing a neuron's  
action  potential  which is transmitted  along its axon).  Usually each input is  separately  weighted, and  the 
sum is passed through a non -linear function known as an activation function or transfer function. T he 
transfer functions usually  have a sigmoid shape, but  they may also  take the form of other  non -linear  
functions,  piecewise  linear  functions,  or step  functions.  They  are also  often  monotonically  increasing,  
continuous,  differentiable  and bounded.  The thresholding  function  has inspired  building  logic  gates  
referred  to as threshold  logic;  applicable  to building  logic  circuits  resembling  brain  processing.  For 
example,  new  devices  such  as memristors  have  been  extensively  used  to develop  such  logic  in recen t times.  
 
 
 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  5 of 24  
 STATE  THE  MAJOR  DIFFERENCES  BETWEEN  BIOLOGICAL  AND  ARTIFICIANEURAL  NETWORKS  
 
1. Size: Our brain contains about 86 billion neurons and more than a 100 synapses (connections). The  
number  of “neurons” in  artificial networks  is much less  than  that.  
2. Signal  transport  and  processing:  The human  brain  works  asynchronously,  ANNs  work  synchronously.  
 
3. Processing  speed:  Single  biological  neurons  are slow,  while  standard  neurons  in ANNs  are fast.  
 
4. Topology:  Biological neural  networks have complicated topologies, while  ANNs are often in a tree  
structure.  
5. Speed: certain biological neurons can fire around 200 times a second on average.  Signals travel at  
different speeds depending on the type of the nerve impulse, ranging from 0.61 m/s up to 119 m/s. Signal  
travel speeds also vary from person to person depending on their sex, age, height, temperature, medical  
condition, lack of sleep etc . Information in artificial neurons is carried over by the continuous, floating point  
number values of synaptic weights. There are no refractory periods for artificial neural networks (periods  
while it is impossible to send another action potential, due to  the sodium channels being lock shut) and  
artificial  neurons  do not experience  “fatigue”:  they  are functions  that  can be calculated  as many  times  and 
as fast  as the computer architecture  would  allow.  
6. Fault -tolerance:  biological  neuron  networks due  to their  topology  are also  fault -tolerant.  Artificial  
neural networks are not modeled for fault tolerance or self regeneration (similarly to fatigue, these ideas  
are not applicable to matrix operations), though recovery is possible by saving the current state (wei ght 
values)  of the  model and  continuing  the training  from  that  save  state.  
7. Power consumption: the brain consumes about 20% of all the human body’s energy — despite it’s large  
cut, an adult brain operates on about 20 watts (barely enough to dimly light a bulb) being extremely  
efficient.  Taking  into  account  how  humans  can still operate  for a while,  when  only  given  some  c-vitamin  
rich lemon juice and beef tallow, this is quite remarkable. For benchmark: a single Nvidia GeForce Titan X  
GPU runs on 250 watts a lone, and requires a power supply. Our machines are way less efficient than  
biological systems. Computers also generate a lot of heat when used, with consumer GPUs operating safely  
between  50–80°Celsius  instead  of 36.5 –37.5  °C. 
8. Learning: we still do not understand how brains learn, or how redundant connections store and recall  
information. By learning, we are building on information that is already stored in the brain. Our knowledge  
deepens by repetition and during sleep, and tasks that once required a fo cus can be executed automatically  
once mastered. Artificial neural networks in the other hand, have a predefined model, where no further  
neurons  or connections  can be added  or removed.  Only  the weights  of the connections  (and  biases  
representing thresholds) can change during training. The networks start with random weight values and  
will slowly  try to reach  a point  where  further  changes  in the weights  would  no longer  improve  
performance. Biological networks usually don't stop / start learning. ANN s have different fitting (train) and  
prediction  (evaluate) phases.  
9. Field of application: ANNs are specialized. They can perform one task. They might be perfect at playing  
chess,  but they  fail at playing  go (or vice  versa).  Biological  neural networks  can learn  completely  new  tasks.  
10. Training algorithm: ANNs use Gradient Descent for learning. Human brains use something different  
(but  we don't know  what).  Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  6 of 24  
 BRIEFLY  EXPLAIN  THE  BASIC  BUILDING  BLOCKS  OF ARTIFICIAL  NEURAL  NETWORKS.  
 
Processing  of ANN  depends  upon  the following  three  building  blocks:  
 
1. Network  Topology  
2. Adjustments of  Weights or  Learning  
3. Activation  Functions  
 
1. Network Topology: A network topology is the arrangement of a network along with its nodes and  
connecting  lines.  According  to the topology,  ANN can  be classified  as the following  kinds:  
 
A. Feed forward Network:  It is a non -recurrent network having processing units/nodes in layers and  
all the nodes in a layer are connected with the nodes of the previous layers. The connection has  
differ ent weights upon them. There is no feedback loop means the signal can only flow in one  
direction,  from  input to  output.  It may  be divided into  the following  two  types:  
 
 Single layer feed forward network: The concept is of feed forward ANN having only one  
weighted layer. In other words, we can say the input layer is fully connected to the output  
layer.  
 Multilayer feed forward network: The concept is of feed forward ANN having more than  
one weighted layer. As  this network has one or more  layers  between the i nput and the output  
layer,  it is called  hidden  layers.  
 
 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  7 of 24  
 B. Feedback Network: As the name suggests, a feedback network has feedback paths, which means the  
signal can flow in both directions using loops. This makes it a non -linear dynamic system , which  
changes continuously until it reaches a state of equilibrium. It may be divided into the following  
types:  
 
 Recurrent  networks:  They  are feedback  networks  with  closed  loops.  Following  are the two  types  
of recurrent  networks.  
 Fully  recurrent  network:  It is the simplest  neural  network  architecture  because  all nodes  are 
connected  to all other  nodes  and each  node works  as both input and  output.  
 
 
 Jordan  network  − It is a closed  loop  network  in which  the output  will go to the input  again  as 
feedback  as shown  in the following  diagram.  
 
2. Adjustments of Weights or Learning: Learning, in artificial neural network, is the method of modifying  
the weights of connections between the neurons of a specified network. Learning in ANN can be classified  
into  three  categories  namely  supervised  learning,  unsupervised  learning,  and reinforcement  learning.  
 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  8 of 24  
 Supervised Learning: As the name suggests, this type of learning is done under the supervision of a  
teacher. This learning process is dependent. Durin g the training of ANN under supervised learning, the  
input vector is presented to the network, which will give an output vector. This output vector is compared  
with the desired output vector. An error signal is generated, if there is a difference between t he actual  
output and the desired output vector. On the basis of this error signal, the weights are adjusted until the  
actual  output  is matched  with the desired  output.  
 
Unsupervised Learning: As the name suggests, this type of learning is done without the supervision of a  
teacher. This learning process is independent. During the training of ANN under unsupervised learning, the  
input vectors of similar type are combined to form clusters. When a new input pattern is applied, then the  
neural network gives an output response indicating the class to which the input pattern belongs. There is  
no feedback from the environment as to what should be the desired output and if it is correct or incorrect.  
Hence, in this type of learning, the network itself must discover the patterns and features from the input  
data,  and the relation  for the input data over the  output.  
 
Reinforcement Learning: As the name suggests, this type of learning is used to reinforce or strengthen the  
network over some critic information. This lear ning process is similar to supervised learning, however we  
might have very less information. During the training of network under reinforcement learning, the  
network receives some feedback from the environment. This makes it somewhat similar to supervised  
learning. However, the feedback obtained here is evaluative not instructive, which means there is no  
teacher as in supervised learning. After receiving the feedback, the network performs adjustments of the  
weights to get better  critic  information  in future . 
 
 
3. Activation Functions: An activation function is a mathematical equation that determines the output of  
each element (perceptron or neuron) in the neural network. It takes in the input from each neuron and  
transforms it into an output, usually between one and zero or between  -1 and one.  It may be defined as  
the extra force or effort applied over the input to obtain an exact output. In ANN, we can also apply  
activation functions over the input to get the exact output. Followings are some activation funct ions of  
interest:  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  9 of 24  
 i) Linear  Activation  Function:  It is also  called  the identity  function  as it performs  no input  editing.  It can be 
defined  as: F(x) =  x 
ii) Sigmoid  Activation  Function:  It is of two type  as follows  − 
 
 Binary sigmoidal function: This  activation function performs input editing between 0 and 1. It is  
positive in nature. It is always bounded, which means its output cannot be less than 0 and more  
than 1. It is also strictly increasing in nature, which means more the input higher would be the 
output.  It can be defined as  
 
F(x)=sigm(x)=11+exp(−x)F(x)=sigm(x)=11+exp(−x)  
 Bipolar sigmoidal function: This activation function performs input editing between -1 and 1. It  
can be positive or negative in nature. It is always bounded, which means its o utput cannot be less  
than -1 and more than 1. It is also strictly increasing in nature like sigmoid function. It can be  
defined  as 
 
F(x)=sigm(x)=21+exp(−x)−1=1−exp(x)1+exp(x)  
 
 
WHAT  IS A NEURAL  NETWORK  ACTIVATION  FUNCTION?  
In a neural  network,  inputs,  which  are typically  real  values,  are fed into  the neurons  in the network.  Each  
neuron  has a weight,  and the inputs  are multiplied  by the weight  and fed into  the activation  
function.  Each  neuron’s  output  is the input  of the neurons  in the next  layer  of the network,  and so the 
inputs  cascade  through  multiple  activation  functions  until  eventually,  the output  layer  generates  a 
prediction.  Neural  networks  rely  on nonlinear  activation  functions —the derivative  of the activation  
function  helps  the network  learn  through  the backpropagation  process.  
 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  10 of 24  
 SOME  COMMON  ACTIVATION  FUNCTIONS  INCLUDE  THE  FOLLOWING:  
 
1. The sigmoid function has a smooth gradient and outputs values between zero and one. For very  
high or low values of the input parameters, the netwo rk can be very slow to reach a prediction,  
called  the vanishing  gradient problem.  
2. The TanH function is zero -centered making it easier to model inputs that are strongly negative  
strongly  positive  or neutral.  
3. The ReLu function is highly computationally efficient but is not able to process inputs that  
approach  zero or negative.  
4. The Leaky ReLu function has a small positive slope in its negative area, enabling it to process zero  
or negative values.  
5. The Parametric ReLu function allows the negative slope to b e learned, performing  
backpropagation  to learn  the most  effective  slope  for zero  and negative  input  values.  
6. Softmax is a special activation function use for output neurons. It normalizes outputs for each class  
between  0 and 1, and returns  the probability  that the  input belongs to  a specific  class.  
7. Swish is a new activation function discovered by Google researchers. It performs better than ReLu  
with  a similar level of  computational efficiency.  
 
 
APPLICATIONS  OF ANN  
 
1. Data  Mining:  Discovery  of meaningful  patterns  (knowledge)  from  large  volumes  of data.  
2. Expert  Systems:  A computer  program  for decision  making  that  simulates  thought  process  of a 
human  expert.  
3. Fuzzy  Logic:  Theory  of approximate  reasoning.  
4. Artificial  Life:  Evolutionary  Computation,  Swarm  Intelligence.  
5. Artificial  Immune  System:  A computer  program  based  on the biological  immune  system.  
6. Medical:  At the moment,  the research  is mostly  on modelling  parts  of the human  body  and 
recognizing diseases from various scans (e.g. cardiograms, CAT scans, ultrasonic scans, etc.).Neural  
networks are ideal in recognizing diseases using scans since there is no need to provide a specific  
algorithm on how to identify the disease. Neural networks learn by example so the details of how to  
recognize the disease are  not needed. What is needed is a set of examples that are representative of  
all the variations of the disease. The quantity of examples is not as important as the 'quantity'. The  
examples  need  to be selected  very  carefully  if the system is  to perform  relia bly and efficiently.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  11 of 24  
 7. Computer Science: Researchers in quest of artificial intelligence have created spin offs like dynamic  
programming,  object  oriented  programming,  symbolic  programming,  intelligent  storage  
management  systems  and many  more  such  tools.  The primary  goal  of creating  an artificial  
intelligence still remains a distant dream but people are getting an idea of the ultimate path, which  
could  lead  to it. 
8. Aviation: Airlines use expert systems in planes to monitor atmospheric conditions  and system  
status.  The plane  can be put  on autopilot  once a  course is set for  the destination.  
9. Weather Forecast: Neural networks are used for predicting weather conditions. Previous data is fed  
to a neural  network,  which  learns  the pattern  and uses that  knowledge  to predict  weather  patterns.  
10. Neural  Networks  in business:  Business  is a diverted  field  with  several  general  areas  of 
specialization  such  as accounting  or financial  analysis.  Almost  any neural  network  application  
would  fit into  one business area or  financial  analysis.  
11. There  is some  potential  for using  neural  networks  for business  purposes,  including  resource  
allocation  and scheduling.  
12. There is also a strong potential for using neural networks for database mining, which is, searching  
for patterns implicit within the explicitly stored information in databases. Most of the funded work  
in this area is classified as proprietary. Thus, it is not possible to report  on the full extent of the  
work going on. Most work is applying neural network s, such as the Hopfield -Tank network for  
optimization  and scheduling.  
13. Marketing: There is a marketing application which has been integrated with a neural network  
system. The Airline Marketing Tactician (a trademark abbreviated as AMT) is a computer system  
made of various intelligent technologies including expert systems. A feed forward neural network is  
integrated with the AMT and was trained using back -propagation to assist the marketing control of  
airline  seat  allocations.  The adaptive  neural  approach  was  amenable  to rule  expression.  
Additionally,  the application's  environment  changed  rapidly  and constantly,  which  required  a 
continuously  adaptive solution.  
14. Credit Evaluation: The HNC company, founded by Robert Hecht -Nielsen, has developed several  
neural  network  applications.  One  of them  is the Credit  Scoring  system  which  increases  the 
profitability  of the existing  model  up to 27%.  The HNC  neural  systems  were  also  applied  to 
mortgage screening. A neural network automated mortgage insurance under writing sy stem was  
developed by the Nestor Company. This system was trained with 5048 applications of which 2597  
were  certified.  The data  related  to property  and borrower  qualifications.  In a conservative  mode  
the system agreed on the under writers on 97% of the cases. In the liberal model the system agreed  
84% of the cases. This is system run on an Apollo DN3000 and used 250K memory while processing  
a case file in  approximately  1 sec. 
 
ADVANTAGES  OF ANN  
 
1. Adaptive  learning:  An ability  to learn how  to do tasks base d on the data  given  for training  or initial  
experience.  
2. Self-Organisation:  An ANN  can create  its own  organisation  or representation  of the information  it 
receives during  learning  time.  
3. Real Time Operation: ANN computations may be carried out in parallel, and special hardware devices are  
being  designed and  manufactured  which take  advantage of  this capability.  
4. Pattern  recognition:  is a powerful  technique  for harnessing  the information  in the data  and generalizing  
about  it. Neural nets  learn  to recognize the  patterns  which exist  in the data  set. 
5. The system  is developed  through  learning  rather  than  programming..  Neural  nets  teach  themselves  the 
patterns in  the data  freeing  the analyst for  more interesting  work.  Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  12 of 24  
 6. Neural networks are flexible in a ch anging environment. Although neural networks may take some time  
to learn  a sudden  drastic  change they  are excellent  at adapting  to constantly  changing  information.  
7. Neural networks can  build informative models whenever conventional approaches fail. Because neural  
networks can handle very complex interactions they can easily model data which is too difficult to model  
with  traditional  approaches  such  as inferential  statistics  or programmi ng logic.  
8. Performance of neural networks is at least as good as classical statistical modelling, and better on most  
problems.  The neural  networks  build  models  that  are more  reflective  of the structure  of the data  in 
significantly  less time.  
 
LIMITATIONS  OF ANN  
 
In this technological era everything has Merits and some Demerits in others words there is a Limitation  
with every system which makes this ANN technology weak in some points. The various Limitations of ANN  
are:- 
1) ANN  is not  a daily  life general  purpos e problem solver.  
2) There  is no structured  methodology  available  in ANN.  
3) There  is no single  standardized  paradigm  for ANN  development.  
4) The Output  Quality  of an ANN  may  be unpredictable.  
5) Many  ANN  Systems  does  not describe  how  they  solve  problems.  
6) Black  box Nature  
7) Greater  computational  burden.  
8) Proneness  to over  fitting.  
9) Empirical  nature  of model  development.  
 
ARTIFICIAL  NEURAL  NETWORK  CONCEPTS/TERMINOLOGY  
Here  is a glossary  of basic  terms  you should  be familiar  with  before  learning  the details  of neural  networks.  
 
Inputs:  Source  data  fed into  the neural  network,  with  the goal  of making  a decision  or prediction  about  
the data.  Inputs  to a neural  network  are typically  a set of real values;  each  value  is fed into  one of the 
neurons  in the input  layer.  
Training  Set:  A set of inputs  for which  the correct  outputs  are known,  used  to train  the neural  network.  
Outputs  : Neural  networks  generate  their  predictions  in the form  of a set of real  values  or boolean  
decisions.  Each  output  value  is generated  by one of the neurons  in the output  layer.  
Neuron/perceptron:  The basic  unit  of the neural  network.  Accepts  an input  and generates  a prediction.  
Each  neuron  accepts  part  of the input  and passes  it through  the activation  function.  Common  activation  
functions  are sigmoid,  TanH  and ReLu.  Activation  functions  help  generate  output  values  within  an 
acceptable  range,  and their  non -linear  form  is crucial  for training  the network . 
 
Weight  Space:  Each  neuron  is given  a numeric  weight.  The weights,  together  with  the activation  
function,  define  each  neuron’s  output.  Neural  networks  are trained  by fine-tuning  weights,  to discover  
the optimal  set of weights  that  generates  the most  accurate  prediction.  
Forward  Pass:  The forward  pass  takes  the inputs,  passes  them  through  the network  and allows  each  
neuron  to react  to a fraction  of the input.  Neurons  generate  their  outputs  and pass  them  on to the next  
layer,  until  eventually  the network  generates  an output.  
Error  Function:  Defines  how  far the actual  output  of the current  model  is from  the correct  output.  When  
training  the model,  the objective  is to minimize  the error  function  and bring  output  as close  as possible  to 
the correct  value.  Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  13 of 24  
 Backpropagation:  In order  to discover  the optimal  weights  for the neurons,  we perform  a backward  
pass,  moving  back  from  the network’s  prediction  to the neurons  that  generated  that  prediction.  This  is 
called  backpropagation.  Backpropagation  tracks  the derivatives  of the activation  functions  in each  
successive  neuron,  to find  weights  that  bring  the loss  function  to a minimum,  which  will generate  the 
best  prediction.  This  is a mathematical  process  called  gradient  descent . 
Bias  and  Variance:  When  training  neural  networks,  like in other  machine  learning  techniques,  we try to 
balance  between  bias  and variance.  Bias  measures  how  well  the model  fits the training  set—able  to 
correctly  predict  the known  outputs  of the training  examples.  Variance  measures  how  well  the model  
works  with  unknown  inputs  that  were  not available  during  training.  Another  meaning  of bias  is a “ bias  
neuron ” which  is used  in every  layer  of the neural  network.  The bias  neuron  holds  the number  1, and 
makes  it possible  to move  the activation  function  up, down,  left and right  on the number  graph.  
Hyperparameters:  A hyper  parameter  is a setting  that  affects  the structure  or operation  of the neural  
network.  In real deep  learning  projects,  tuning  hyper  parameters  is the primary  way  to build  a network  
that  provides  accurate  predictions  for a certain  problem.  Common  hyper  parameters  include  the number  
of hidden  layers,  the activation  function,  and how  many  times  (epochs)  training  should  be repeated.  
 
MCCULLOGH -PITTS  MODEL  
 
In 1943  two  electrical  engineers,  Warren  McCullogh  and Walter  Pitts,  published  the first  paper  describing  
what  we would  call a neural network.  
 
 
It may be divided into 2 parts. The first part, g takes an input, performs an aggregation and based on the  
aggregated value the second part, f makes a decision. Let us suppose that I want to predict my own  
decision, whether to watch a random football game or not on TV. The inputs are all boolean i.e., {0,1} and  
my output variable  is also  boolean  {0: Will  watch  it, 1: Won’t  watch  it}. 
 
So, x1 could  be ‘is  Indian  Premier  League  On’ (I like Premier League  more)  
x2 could  be ‘is it a knockout  game  (I tend  to care  less about  the league  level  matches)  
x3 could  be ‘is Not  Home’  (Can’t  watch  it when  I’m in College.  Can I?) 
x4 could  be ‘is my favorite  team  playing’  and so on. 
These inputs can either be excitatory or inhibitory. Inhibitory inputs are those that have maximum effect  
on the decision making irrespective of other inputs i.e., if x3 is 1 (not home) then my output will always be  
0 i.e., the neuron will never fire, so x3 is an inhibitory input. Excitatory inputs are NOT the ones tha t will  
make the neuron fire on their own but they might fire it when combined together. Formally, this is what is  
going  on: 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  14 of 24  
 
 
 
We can see that g(x) is just doing a sum of the inputs — a simple aggregation. And theta here is called  
thresholdin g parameter. For example, if I always watch the game when the sum turns out to be 2 or more,  
the theta  is 2 here.  This  is called  the Thresholding  Logic.  
 
The McCulloch -Pitts neural model is also known as linear threshold gate. It is a neuron of a set of inputs I 1, 
I2,I3,…I m and one output ‘y’ . The linear threshold gate simply classifies the set of inputs into two different  
classes.  Thus  the output  y is binary.  Such  a function  can be described  mathematically  using  these  equations:  
 
 
Where,  are weight  values normalized  in the range  of either  or and 
associated  with  each  input  line, Sum  is the weighted  sum,  and is a threshold constant. The function  is a 
linear  step  function  at threshold  as shown in figure 2.3. The symbolic representation of the linear  
threshold  gate  is shown  in figure  below.  
Linear  Threshold  Function  
 
 
Symbolic  Illustration  of Linear  Threshold  Gate  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  15 of 24  
 BOOLEAN  FUNCTIONS  USING  MCCULLOGH -PITTS  NEURON  
 
In any Boolean  function,  all inputs  are Boolean  and the output  is also  Boolean.  So essentially,  the neuron  is 
just trying  to learn  a Boolean  function.  
 
 
This  representation  just denotes  that,  for the boolean  inputs x_1,  x_2 and x_3 if the g(x)  i.e., sum  ≥ theta,  the 
neuron  will fire otherwise, it  won’t.  
 
AND  Function  
 
 
 
An AND  function  neuron  would  only  fire when  ALL  the inputs  are ON i.e., g(x)  ≥ 3 here.  
 
OR Function  
 
For an OR function  neuron  would  fire if ANY  of the  inputs  is ON  i.e., g(x)  ≥ 1 here.  
 
NOR  Function  
For a NOR neuron to fire, we want ALL the inputs to be 0 so the thresholding parameter should also be 0  
and we take them  all as  inhibitory  input.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  16 of 24  
 NOT  Function  
 
For a NOT neuron, 1 outputs 0 and 0 outputs 1. So we take the input as an inhibitory input and set the  
thresholding  parameter to 0.  
 
We can summarize  these  rules  with  the McCullough -Pitts output  rule  as: 
The McCulloch -Pitts model of a neuron is simple yet has substantial computing potential. It also has a  
precise mathematical definition. However, this model is so simplistic that it only generates a binary output  
and also  the weight  and threshold  values  are fixed.  The neural  computing  algorithm  has diverse  features  
for various  applications.  Thus,  we need  to obtain  the neural  model  with  more  flexible  computational  
features.  
WHAT  ARE  THE  LEARNING  RULES  IN ANN?  
 
Learning rule is a method or a mathematical logic. It helps a Neural Network to learn from the existi ng 
conditions and improve its performance. Thus learning rules updates the weights and bias levels of a  
network when a network simulates in a specific data environment. Applying learning rule is an iterative  
process.  It helps a  neural  network  to learn  from  the existing  conditions and  improve  its performance.  
 
The different  learning  rules in  the Neural  network  are:  
 
1. Hebbian  learning  rule – It identifies,  how  to modify  the weights of  nodes  of a network.  
2. Perceptron  learning  rule – Network  starts its  learning  by assigning  a random value  to each  weight.  
3. Delta learning rule – Modification in sympatric weight of a node is equal to the multiplication of error  
and the input.  
4. Correlation  learning  rule  – The correlation  rule  is the supervised  learning.  
5. Outstar learning rule – We can use it when it assumes that nodes or neurons in a network arranged in a  
layer.  
 
1. Hebbian Learning Rule: The Hebbian rule was the first learning rule. In 1949 Donald Hebb developed it as  
learning algorithm of the unsupervised neural n etwork. We can use it to identify how to improve  the 
weights of nodes of a network. The Hebb learning rule assumes that – If two neighbor neurons activated  
and deactivated at the same time, then the weight connecting these neurons should increase. At the s tart,  
values of all weights are set to zero. This learning rule can be used for both soft - and hard -activation  
functions.  Since  desired  responses  of neurons  are not used  in the learning  procedure,  this is the 
unsupervised  learning  rule.  The absolute  values  of the weights  are usually  proportional  to the learning  
time,  which  is undesired.  
Mathematical  Formula  of Hebb  Learning  Rule.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  17 of 24  
 
2. Perceptron Learning Rule: Each connection in a neural network has an associated weight, which  
changes in the course of learning. According to it, an example of supervised learning, the network starts  
its learning by assigning a random value to each weight. Calculate the output value on the basis of a set  
of records for which we can know the expected output value.  This is the learning sample that indicates  
the entire  definition.  As a result,  it is called  a learning  sample.  The network  then  compares  the 
calculated output value with the expected value. Next calculates an error function ∈, which can be the  
sum  of squa res of the errors  occurring  for each  individual  in the learning  sample  which  can be 
computed  as: 
 
 
Mathematical  Formula  of Perceptron  Learning  Rule  
 
Perform the first summation on the individuals of the learning set, and perform the second summation on  
the output units. E ij and O ij are the expected and obtained values of the jth unit for the ith individual. The  
network then adjusts the weights of the different units, checking each time to see if the error function has  
increased or decreased. As in a convent ional regression, this is a matter of solving a problem of least  
squares.  Since  assigning  the weights of  nodes  according  to users,  it is an example  of supervised  learning.  
 
3. Delta Learning Rule: Developed by Widrow and Hoff, the delta rule, is one of the most common  
learning rules. It depends on supervised learning. This rule states that the modification in sympatric  
weight of a node is equal to the multiplication of error and the input. In Mathematical form the delta  
rule  is as follows:  
 
 
Mathematical  Formula  of Delta  Learning  Rule  
 
For a given input vector, compare the output vector is the correct answer. If the difference is zero, no  
learning takes place; otherwise, adjusts its weights to reduce this difference. The change in weight from ui  
to uj is: dwi j = r* ai * ej. where r is the learning rate, ai represents the activation of ui and ej is the difference  
between the expected output and the  actual  output of uj. If the set of input patterns form  an independent  
set then  learn  arbitrary  associations  using  the delta rule.  
It has  seen that for networks  with linear activation functions  and with no hidden units.  
The error squared vs. the weight graph is a paraboloid in n -space. Since the proportionality constant is  
negative,  the graph  of such  a function is  concave  upward and has  the least  value.  The vertex  of this 
paraboloid  represents  the point  where  it reduces  the error.  The weight  vector  corresponding  to this point  
is then the ideal weight vector. We can use the delta learning rule with both single output  unit and several  
output units. While applying the delta rule assume that the error can be directly measured. The aim of  
applying  the delta  rule  is to  reduce  the difference  between  the actual  and expected  output  that  is the error.  
 
4. Correlation  Learning  Rule:  The correlation  learning  rule  based  on a similar  principle  as the 
Hebbian learning rule. It assumes that weights between responding neurons should be more positive, and  
weights between neurons with opposite reaction should be more negative. Contrary to the  Hebbian rule,  
the correlation rule is the supervised learning, instead of an actual. The response, oj, the desired response,  
dj, uses  for the weight -change  calculation.  In Mathematical  form  the correlation  learning  rule  is as follows:  
Mathematical  Formula  of Correlation  Learning  Rule  
 
Where d j is the desired value of output signal. This training algorithm usually starts with the initialization of  
weights to zero. Since assigning the desired weight by users, the correlation learning rule is an exampl e of 
supervised  learning.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  18 of 24  
 
5. Out  Star  Learning  Rule:  We use the Out Star  Learning  Rule  when  we assume  that  nodes  or 
neurons in a network arranged in a layer. Here the weights connected to a certain node should be equal to  
the desired outputs for the neurons connected through those weights. The out start rule produces the  
desired response t for the layer of n nodes. Apply this type of learning for all nodes in a particular layer.  
Update the weights for nodes are as in Kohonen neural networks. I n Mathematical form, express the out  
star  learning  as follows:  
 
 
 
Mathematical Formula of Out Star Learning Rule  
This  is a supervised  training  procedure  because  desired  outputs  must  be known.  
BRIEFLY  EXPLAIN  THE  ADALINE  MODEL  OF ANN.  
 
ADALINE  (Adaptive Linear Neuron or later Adaptive  Linear Element) is an early single -layer artificial  
neural network and the name of the physical device that implemented this network. The network uses  
memistors. It was developed by Professor Bernard Widrow and his graduate student Ted Hoff at Stanford  
University  in 1960.  It is based  on the McCulloch –Pitts  neuron.  It consists  of a weight,  a bias  and a 
summation function. The difference between Adaline and the standard (McCulloch –Pitts) perceptron is that  
in the learning phase , the weights are adjusted according to the weighted sum of the inputs (the net). In the  
standard perceptron, the net is passed to the activation (transfer) function and the function's output is used  
for adjusting  the weights.  Some important  points  about  Adaline  are as  follows:  
 It uses  bipolar  activation  function.
 It uses delta rule for training to minimize the Mean -Squared Error (MSE) between the actual output  
and the desired/target output. 
 The weights  and the bias  are adjustable. 
 
Architecture  of ADALINE  network:  The basic  structure  of Adaline  is similar  to perceptron  having  an extra  
feedback loop with the help of which the actual output is compared with the desired/target output. After  
comparison  on the basis  of training  algorithm,  the weights and  bias will be updated.  
 
Architecture of ADALINE: The basic structure of Adaline is similar to perceptron having an extra feedback  
loop with the help of which the actual output is compared with the desired/target output. After comparison  
on the basis of training  algorithm,  the weights  and bias  will be updated.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  19 of 24  
 ƒ 𝑦i𝑛 = { Training  Algorithm  of ADALINE:  
 
Step  1 − Initialize  the following  to start  the training:  
 Weights
 Bias
 Learning  rate α
 
For easy  calculation  and simplicity,  weights  and bias  must  be set equal  to 0 and the learning  rate  must  be 
set equal  to 1. 
 
Step 2 − Continue step 3 -8 when the stopping condition is not true.  
Step  3 − Continue  step  4-6 for  every  bipolar  training  pair  s : t. 
Step  4 − Activate  each  input  unit  as follows:  
 
xi = si (i=1  to n) 
 
Step  5 − Obtain  the net input  with  the following  relation:  
𝑛 
𝑦i𝑛 = 𝑏 + ∑ 𝗑i wi 
i 
 
Here  ‘b’ is bias and  ‘n’ is the total  number  of input  neurons.  
 
Step  6 − Apply  the following  activation  function  to obtain  the final  output:  
 
( ) 1, iƒ 𝑦i𝑛 ≥ 0 
−1, iƒ 𝑦i𝑛 < 0 
 
Step  7 − Adjust  the weight  and bias as  follows  : 
 
Case  1 − if y ≠ t then,  wi(new)  = wi(old)+α(t−y in)xi 
b(new)  = b(old)+α(t−y in) 
Case  2 − if y = t then,  wi(new)  = wi(old)  
b(new)  = b(old)  
Here  ‘y’ is the  actual  output  and ‘t’ is the desired/target  output.  (t−y in) is the  computed  error.  
 
Step 8 − Test for the stopping condition, which will happen when there is no change in weight or the  
highest  weight  change  occurred  during  training  is smaller  than  the specified  tolerance.  Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  20 of 24  
 EXPLAIN  MULTIPLE  ADAPTIVE  LINEAR  NEURONS  (MADALINE) . 
 
Madaline which stands for Multiple Adaptive Linear Neuron, is a network which consists of many Adalines  
in parallel. It will have a single output unit. Three different  training algorithms for MADALINE networks  
called Rule I, Rule II and Rule III have been suggested, which cannot be learned using backpropagation. The  
first of these dates back to 1962 and cannot adapt the weights of the hidden -output connection.[10] The  
second training algorithm improved on Rule I and was described in 1988.[8] The third "Rule" applied to a  
modified network with sigmoid activations instead of signum; it was later found to be equivalent to  
backpropagation. The Rule II training algorithm is b ased on a principle called "minimal disturbance". It  
proceeds by  looping  over  training  examples, then  for each  example, it:  
 finds  the hidden  layer  unit  (ADALINE  classifier)  with  the lowest  confidence  in its prediction,  
tentatively  flips the sign  of the uni t,
 accepts  or rejects the  change  based  on whether  the network's error  is reduced, 
 stops  when  the error  is zero.
 
Some  important  points  about  Madaline  are as follows:  
 It is just  like a multilayer  perceptron,  where  Adaline  will act as a hidden  unit  between  the input  and 
the Madaline layer. 
 The weights and the bias between the input and Adaline layers, as in we see in the Adaline  
architecture,  are adjustable. 
 The Adaline  and Madaline  layers  have  fixed  weights  and bias  of 1.
 Training  can be done  with  the help  of Delta  rule.
 
BRIEFLY  EXPLAIN  THE  ARCHITECTURE  OF MADALINE  
 
MADALINE  (Many  ADALINE)  is a three -layer  (input,  hidden,  output),  fully  connected,  feed -forward  
artificial neural network architecture for classification that uses ADALINE units in its hidden and output  
layers,  i.e. its activation  function  is the sign  function.  The three -layer  network  uses  memistors.  The 
architecture of Madaline consists of “n” neurons of the input layer, “m” neurons of the Adaline layer, and 1  
neuron of the Madaline layer. The  Adaline layer can be considered as the hidden layer as it is between the  
input  layer and  the output  layer,  i.e. the  Madaline layer.  
 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  21 of 24  
 −1, i𝑓 𝗑 < 0 Training  Algorithm  of MADALINE  
 
By now  we know  that  only  the weights  and bias  between  the input  and the Adaline  layer  are to be 
adjusted,  and the weights  and bias  between  the Adaline and  the Madaline layer  are fixed.  
 
Step  1 − Initialize  the following  to start  the training:  
 Weights 
 Bias
 Learning  rate  α
For easy  calculation  and simplicity,  weights  and bias  must  be set equal  to 0 and the learning  rate  must  be 
set equal  to 1. 
 
Step 2 − Continue step 3 -8 when the stopping condition is not true.  
Step  3 − Continue  step  4-6 for every  bipolar  training  pair  s:t. 
Step  4 − Activate  each  input  unit  as follows:  
xi = si(I =1 to n) 
 
Step  5 − Obtain  the net input  at each  hidden  layer,  i.e. the Adaline  layer  with  the following  relation:  
𝑛 
Oi𝑛j = 𝑏j + ∑ 𝗑i wij j = 1 𝑡𝑜 𝑚 
i 
Here  ‘b’ is bias and  ‘n’ is the total  number  of input  neurons.  
 
Step  6 − Apply  the following  activation  function  to obtain  the final  output  at the Adaline  and the Madaline  
Layer:  𝑓(𝑦i𝑛 ) = { 1, i𝑓 𝗑 ≥ 0 
Output at the hidden Adaline unit  Qj=f(Q inj) 
Final  output  of the network  y = f(y in) 
i.e. 
𝑚 
𝑦i𝑛j  =  𝑏0 +  ∑ 𝑄j 𝑣j 
j=1  
Step 7 − Calculate the error and adjust the weights as follows – 
Case  1 − if y ≠ t and t = 1 then,  
wij(new)  = wij(old)+α(1−Q inj)xi 
bj(new)  = bj(old)+α(1−Q inj) 
In this case,  the weights would  be updated  on Qj where  the net input  is close  to 0 because  t = 1. 
 
Case 2  − if y ≠ t and t  = -1 then,  
wik(new) = w ik(old)+α(−1−Q ink)xi 
bk(new)  = bk(old)+α(−1−Q ink) 
 
In this case, the weights would be updated on Qk where the net input is positive because t = -1. 
Here  ‘y’ is the actual  output and  ‘t’ is the desired/target  output.  
 
Case  3 − if y = t, then  there  would  be no change in  weights.  
 
Step  8 − Test  for the stopping  condition,  which  will happen  when  there  is no change  in weight  or the 
highest  weight  change  occurred  during  training  is smaller  than  the specified  tolerance.  Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  22 of 24  
 
WHAT  IS A PERCEPTRON?  
 
A perceptron is a binary classification algorithm modeled after the functioning of the human brain —it 
was intended to emulate the neuron. The perceptron, while it has a simple structure, has the ability to  
learn  and solve  very  complex  problems.  
 
 
 
 
 
 
 
 
 
 
 
 
What  is Multilayer  Perceptron?  
 
A multilayer  perceptron  (MLP)  is a group  of perceptrons,  organized  in multiple  layers,  that  can 
accurately  answer  complex  questions.  Each  perceptron  in the first  layer  (on the left)  sends  signals  to all 
the perceptrons  in the second  layer,  and so on. An MLP  contains  an input  layer,  at least  one hidden  layer,  
and an output  layer.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
The  perceptron  learns  as follows:  
 
1. Takes the inputs which are fed into the perceptrons in the input layer, multiplies them by their  
weights,  and computes  the sum.  
2. Adds the number one, multiplied by a “bias weight”. This is a technical step that makes it possible to  
move the  output  function  of each  perceptron (the  activation function)  up, down,  left and right  on 
the number graph.  
3. Feeds the sum through the activation function —in a simple perceptron system, the activation  
function  is a step  function.  
4. The result  of the step  function  is the output.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  23 of 24  
 
A multilayer  perce ptron  is quite  similar  to a modern  neural  network.  By adding  a few ingredients,  the 
perceptron  architecture  becomes  a full-fledged  deep  learning  system:  
 Activation  functions  and  other hyperparameters : a full neural  network uses  a variety of  
activation functions which output real values, not boolean values like in the classic perceptron. It is  
more flexible in terms of other details of the learning p rocess, such as the number of training  
iterations (iterations and epochs), weight initialization schemes, regularization, and so on. All these  
can be tuned as  hyperparameters. 
 Backpropagation : a full neural network uses the backpropagation algorithm, to perform iterative  
backward passes which try to fin d the optimal values of perceptron weights, to generate the most  
accurate  prediction. 
 Advanced  architectures : full neural  networks  can have  a variety of  architectures  that  can help  
solve specific  problems.  A few  examples  are Recurrent  Neural  Networks  (RNN), Convolutional  
Neural  Networks  (CNN),  and Generative  Adversarial  Networks  (GAN).
 
WHAT  IS BACKPROPAGATION  AND  WHY  IS IT IMPORTANT?  
After  a neural  network  is defined  with  initial  weights,  and a forward  pass  is performed  to generate  the 
initial  prediction,  there  is an error  function  which  defines  how  far away  the model  is from  the true  
prediction.  There  are many  possible  algorithms  that  can minimize  the error  function —for example,  one 
could  do a brute  force  search  to find  the weights  that  generate  the smallest  error.  However,  for large  
neural  networks,  a training  algorithm  is needed  that  is very  computationally  efficient.  Backpropagation  is 
that  algorithm —it can discover  the optimal  weights  relatively  quickly,  even  for a network  with  millions  
of weights.  
 
HOW  BACKPROPAGATION  WORKS?  
1. Forward pass —weights are initialized and inputs from the training set are fed into the  
network.  The forward  pass  is carried  out and the model  generates  its initial  prediction.  
2. Error  function —the error  function  is computed  by checking  how  far away  the prediction  is 
from  the known  true  value.  
3. Backpropagation with gradient descent —the backpropagation algorithm calculates how  
much the output values are affected by each of the weights in the model. To do this, it  
calculates partial derivatives, going ba ck from the error function to a specific neuron and its  
weight.  This  provides  complete traceability  from  total  errors,  back  to a specific  weight  which  
contributed to that error. The result of backpropagation is a set of weights that minimize the  
error  function.  
4. Weight update —weights can be updated after every sample in the training set, but this is  
usually not practical. Typically, a batch of samples is run in one big forward pass, and then  
backpropagation performed on the aggregate result. The batch s ize and number of batches  
used in training, called iterations , are important hyperparameters  that are tuned to get the  
best  results. Running the entire training set through the backpropagation process is called  
an epoch . Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupti  for NNFL(15A02604)  – R15 - JNTUA  Page  24 of 24  
 Training  algorithm  of BPNN:  
 
1. Inputs  X, arrive  through  the pre connected  path  
2. Input  is modeled  using  real  weights  W. The weights  are usually  randomly  selected.  
3. Calculate  the output  for every  neuron  from  the input  layer,  to the hidden  layers,  to the 
output  layer.  
4. Calculate  the error  in the outputs  
 
  Error B= Actual  Output  – Desired Output   
 
5. Travel  back  from  the output  layer  to the hidden  layer  to adjust  the weights  such  that  the 
error  is decreased.  
 
Keep  repeating  the process until  the desired  output  is achieved  
 
Architecture  of back  propagation  network:  
 
As shown  in the diagram,  the architecture  of BPN  has three  interconnected  layers  having  weights  on them.  
The hidden layer as well as the output layer also has bias, whose weight is always 1, on them. As is clear  
from the diagram, the working of BPN is in two phases. One phase sends the signal from the input layer to  
the output  layer,  and the other  phase  back  propagates  the error  from the  output  layer  to the input  layer.  
 
 
1  LECTURE  NOTES  FOR  NEURAL  NETWORKS  AND  FUZZY  LOGIC  (15A02604)  
UNIT -3 
ANN APPLICATIONS TO ELECRICAL SYSTEMS  
WHAT  ARE THE  ANN  LEARNING PARADIGMS?  
Learning can refer to either acquiring or enhancing knowledge. As Herbert Simon says, Machine  
Learning denotes changes in the system that are adaptive in the sense that they enable the system  
to do the same task or tasks drawn from the same population more efficiently and more effectively  
the next  time.  
ANN learning paradigms can be classified as supervised, unsupervised and reinforcement learning.  
Supervised learning  model assumes the availability of a teacher or supervisor who classifies the  
training  examples  into  classes  and utilizes  the information  on the class  membership  of each  
training instance, whereas, Unsupervised learning  model identify the pattern class information  
heuristically  and Reinforcement  learning  learns  through  trial  and error  interactions  with  its 
environment  (reward/pen alty assignment).  
Though  these  models  address  learning  in different  ways,  learning  depends  on the space  of 
interconnection  neurons.  That  is, supervised  learning  learns  by adjusting  its inter  connection  
weight  combinations  with  the help  of error  signals  where  as unsupervised  learning  uses  
information associated with a group of neurons and reinforcement learning uses reinforcement  
function to modify local weight parameters. Thus, learning occurs in an ANN by adjusting the free  
parameters  of the network  that are adapted  where  the ANN  is embedded.  
 
BRIEFLY  EXPLAIN  SUPERVISED  LEARNING.  
 
Supervised learning is based on training a data sample from data source with correct classification  
already assigned. Such techniques are utilized in feed forward or Multi Layer Perceptron (MLP)  
models. These MLP has  three  distinctive  characteristics:  
1. One or more layers of hidden neurons that are not part of the input or output layers of the  
network  that  enable  the network  to learn  and solve any  complex  problems  
2. The nonlinearity  reflected  in the neuronal  activity  is differentiable  and,  
3. The interconnection  model  of the network  exhibits  a high  degree  of connectivity  These  
characteristics along with learning through training solve difficult and diverse problems. Learning  
through training in a supervised ANN model also called as error backpropagation algorithm. The  
error  correction -learning  algorithm  trains  the network  based  on the input -output  samples  and 
finds error signal, which is the difference of the output calculate d and the desired output and  
adjusts the synaptic weights of the neurons that is proportional to the product of the error signal  
and the input instance of the  synaptic weight. Based on this principle, error back  propagation  
learning  occurs in two  passes:  
 
Forward  Pass:  Here,  input  vector  is presented  to the network.  This  input  signal  propagates  
forward, neuron by neuron through the network and emerges at the output end of the network as  
output  signal:  
y(n)  = φ(v(n)), where  v(n)  is the induced  local  field  of a neuron  defined  by v(n)  =Σ w(n)y(n).  
The output that is calculated at the output layer o(n) is compared with the desired response d(n)  
and finds the error e(n) for that neuron. The synaptic weights of the network during this pass are  
remains  same.  2  Backward Pass:  The error signal that is originated at the output neuron of that layer is propagated  
backward through network. This calculates the local gradient for each neuron in each layer and  
allows the synaptic weights of the network to  undergo changes in accordance with the delta rule  
as: 
Δw(n)  = η * δ(n)  * y(n).  
This recursive computation is continued, with forward pass followed by the  backward pass for  
each  input  pattern  till the network  is converged.  Supervised  learning  paradigm  of an ANN  is 
efficient and finds solutions to several linear and non -linear problems such as classification, plant  
control,  forecasting,  prediction,  robotics etc.  
 
BRIEFLY  EXPLAIN  UNSUPERVISED  LEARNING.  
 
Self-Organizing neural  networks learn using unsupervised learning algorithm to identify hidden  
patterns in unlabeled input data. This unsupervised refers to the ability to learn and organize  
information  without  providing  an error  signal  to evaluate  the potential  solution.  The lack of 
direction for the learning algorithm in unsupervised learning can sometime be advantageous, since  
it lets the algorithm to look back for patterns that have not been previously considered. The main  
characteristics  of Self-Organizing  Maps (SOM) are:  
1. It transforms  an incoming  signal  pattern  of arbitrary  dimension  into  one or 2 dimensional  map  
and perform  this transformation adaptively  
2. The network  represents  feed  forward  structure  with  a single  computational  layer  consisting  of 
neurons  arranged  in rows and columns.  
3. At each  stage  of representation,  each  input  signal  is kept  in its proper  context  and,  
4. Neurons  dealing  with  closely  related  pieces  of information  are close  together  and they  
communicate  through synaptic  connections.  
The computational layer is also called as competitive layer since the neurons in the layer compete  
with each other to become active. Hence, this learning algorithm is called competitive algorithm.  
Unsupervised  algorithm  in SOM  works in  three phases:  
Competition phase:  for each input pattern x, presented to the network, inner product with synaptic  
weight w is calculated and the neurons in the competitive layer finds a discriminant function that  
induce competition among the neurons and the synaptic weight vector that is close to the input  
vector in the Euclidean distance is announced as winner in the competition. That neuron is called  
best  matching  neuron,  
i.e. x  = arg min  ║x - w║. 
Cooperative phase:  the winning neuron determines the center of a topological neighborhood h of  
cooperating  neurons.  This  is performed  by the lateral  interaction  d among  the cooperative  
neurons. This topological  neighborhood  reduces its  size over a  time  period.  
Adaptive  phas e: enables  the winning  neuron  and its neighborhood  neurons  to increase  their  
individual values of the discriminant function in relation to the input pattern  through suitable  
synaptic  weight  adjustments,  Δw = ηh(x)(x  – w). Upon  repeated  presentation  of the training  
patterns, the synaptic weight vectors tend to follow the distribution of the input patterns due to the  
neighborhood  updating  and thus ANN learns  without  supervisor.  
 
BRIEFLY  EXPLAIN  MULTI  LAYER  PERCEPTRON  MODEL.  
 
In the Multilayer perceptron, ther e can more than one linear layer (combinations of neurons). If we  
take the simple example the three -layer network, first layer will be the input layer and last will be  
output layer and middle layer will be called hidden layer. We feed our input data into t he input  
layer and take the output from the output layer. We can increase the number of the hidden layer as  
much  as we want,  to make  the model  more complex  according  to our task.  3   
 
Feed Forward Network, is the most typical neural network mod el. Its goal is to approximate some  
function f (). Given, for example, a classifier y = f ∗ (x) that maps an input x to an output class y, the  
MLP find the best  approximation  to that classifier by  defining a mapping,  y = f(x; θ) and learning  
the best parameters θ for it. The MLP networks are composed of many functions that are chained  
together. A network with three functions or layers would form f(x) = f (3)(f (2)(f (1)(x))). Each of  
these  layers  is composed of units  that  perform a  transformation  of a  linear sum of inputs. Each  
layer is represented as y = f(WxT + b). Where f is the activation function, W is the set of parameter,  
or weights, in the layer, x is the input vector, which can also be the output of the previous layer, b is  
the bias  vector  and T is the training  function.  The layers  of an MLP  consist  of several  fully  
connected layers because each unit in a layer is connected to all the units in the previous layer. In a  
fully connected layer, the parameters of each unit are independent of the rest  of the units in the  
layer,  that means each  unit  possess a unique  set of weights.  
 
Training  the Model  of MLP:  There  are basically  three  steps  in the training  of the model.  
1. Forward  pass  
2. Calculate  error  or loss 
3. Backward  pass  
1. Forward  pass:  In this step  of training  the model,  we just pass  the input  to model  and multiply  
with  weights  and add bias at  every  layer  and find  the calculated  output  of the model.  
 
4  2. Calculate error / loss: When we pass the data instance (or one example) we will get some output  
from  the model  that  is called  Predicted  output  (pred_out)  and we have  the label  with  the data  that  
is real output or expected output(Expect_out). Based upon these both we calculate the loss that we  
have to backpropagate(using Backpropagation algorithm). There is various Loss Function that we  
use based  on our  output  and requirement.  
3. Backward Pass: After calculating the loss, we back propagate the loss and update the weights of  
the model by using gradient. This is the main step in the training o f the model. In this step, weights  
will adjust  according  to the gradient flow  in that  direction.  
 
Applications  of MLP:  
1. MLPs  are useful  in research  for their  ability  to solve  problems  stochastically,  which  often  
allows  approximate  solutions  for extremely  complex  problems  like fitness  approximation.  
2. MLPs  are universal  function  approximators  and they  can be used  to create  mathematical  
models  by regression analysis.  
3. MLPs  are a popular  machine  learning  solution  in diverse  fields  such  as speech  recognition,  
image  recognition,  and machine translation  software.  
 
ELECTRIC  LOAD  FORECASTING  USING  ANN  
 
ANNs  were  first applied  to load  forecasting  in the late 1980’s.  ANNs  have  good performance in  data  
classification  and function  fitting.  Some  examples  of utilizing  ANN  in power  system  applications  
are: Load forecasting,  fault  classification,  power  system  assessment,  real  time  harmonic  evaluation,  
power factor correction, load scheduling, design of transmission lines, and power system planning.  
Load  forecast  has been  an attractive  research  topic  for many  decades  and in many  countries  all 
over  the world,  especially  in fast developing  countries  with  higher  load  growth  rate.  Load  forecast  
can be generally classified into four categories based on the forecasting time as detailed in the table  
below.  
 
Load  forecasting  Period  Importance  
Long  term  One  year  to 
ten Years   To calculate  and to allocate  the required  future  capacity.  
 To plan  for new  power  stations  to face  customer  requirements.  
 Plays  an essential  role  to determine  future  budget.  
Medium  term  One  week  to 
few months  Fuel  allocation  and maintenance  schedules.  
Short  term  One  hour  to a 
week   Accurate  for power  system  operation.  
 To evaluate  economic  dispatch,  hydrothermal  co-ordination,  
unit  commitment, transaction.  
 To analysis  system  security  among  other  mandatory  function.  
Very  short  term  One  minute  
to an hour  Energy  management  systems  (EMS).  
 
An ANN for load forecasting can  be trained on  a training set  of data that  consists of time -lagged  
load data and other non -load parameters such as weather data, time of day, day of week, month,  
and actual load data. Some ANNs are only trained against days with data similar to the forecast day.  
Once the network has been trained, it is tested by presenting it with predictor data inputs. The  
predictor data can be time -lagged load data and forecasted weather data (for the next 24 hours).  
The forecasted load output from the ANN is compared to the actual load to determine the forecast  
error.  Forecast  error  is sometimes  presented  in terms  of the root  mean  square  error  (RMSE)  but 5  more commonly in terms of the mean absolute percent error (MAPE). An ANN trained on a specific  
power system’s load and weather data will be system dependent. The ANN generated for that  
system  will most  likely  not perform  satisfactorily  on another  power  system  with  differing  
characteristics. It is possible the same ANN architecture may be reused on the new system, but  
retraining  will be required.  
 
Training  and  Testing  with  ANN  
 
The whole  data  set was  divided  into  two sets:  Training  set and Test  Set. Training  set consists  of 
80% of whole data and Test set contains the rest data. The training set was used to make a model  
which, therefore, predicts the load in the future. The model is made by a MATLAB app Neural Net  
Fitting.  The training  set has got inputs  which  are as follows:  
1. Temperature  (in o C) 
2. Humidity  (in %) 
3. Pressure  (in mBar)  
4. Time  (in hours)  
5. Global  Horizontal  (in W/m2)  
6. Previous  Day Same  Hour  Load  (in kW)  
7. Previous  Week  Same  Day Same  Hour  Load  (in kW)  
 
Data  Collected:  All the following  data  are collected  from  substation  of feeders:  
Maximum, Voltage Minimum, Voltage Maximum, Current Minimum,  present MWH consumption, &  
Temperature.  
Steps  for Implementation  of load  forecasting  using  ANN:  
• Gathering  and arranging  the data  in MS Excel  spreadsheet.  
• Tagging  the data  into  groups.  
• Analyse  the data.  
• SIMULINK  / MATLAB  simulation  of data  using  ANN.  
 
Procedure  of testing  load  forecasting  using  ANN:  
 
 ANN  was  created  for the user -defined  forecast day.  
 The data  was  performed  on the training  and forecast  predictor  datasets,  the number  of 
hidden  layers, or  neurons,  in the ANN  was  defined  to be  30 neurons.  
 The built -in MATLAB  Levenberg  - Marquardt  optimization  training  function  was  used  to 
perform  the backpropagation  training  of the feed -forward ANN.  
 This  process  iteratively  updated  the internal  weight  and bias  values  of the ANN  to obtain  a 
low error  output  when  utilizing  the training  predictor  dataset  and a target  dataset.  
 The target  dataset  consists  of the actual  load  values  for a given  predictor  dataset.  
 After testing, the ANN forecasted plot was plotted against test set data plot and MAPE was  
calculated. The results of this forecast were stored, and the entire ANN training, testing, and  
forecasting process was repeated a set number of times with the intention of reducing the  
forecast error. The Simulink Model was extracted from the net fitting toolbox is shown  
below.  6    
 
In comparing different models, the average percentage forecasting  error is used as a measure of the  
performance. This is defined  as: 
 
 
 
Result: A graph of forecasted load was plotted against the time (in hours) and comparison was  
made against the Actual Load (test data load). A part of this graph is shown  below. The graph  
shows a little deviation of the forecasted plot from the Test data load. The MAPE (mean absolute  
percentage  error) came  out to  be 5.1440  % which  is bearable.  
 
7  EXPLAIN  IN DETAIL  ABOUT  SYSTEM  IDENTIFICATION  USING  ANN:  
 
A system identific ation problem can be formulated as an optimization task where the objective is to  
find  a model  and a set of parameters  that  minimize  the prediction  error  between  the measured  
data and the model output. Recurrent neural network (RNN) based  adaptive algorithm, is now  
widely used in system identification due to its robustness and calculus simplicity. Based on the  
error signal, the filter’s coefficients are updated and corrected, in order to adapt, so the output  
signal  has the same values as  the reference  signal.  
System identification is the process of deriving a mathematical model of a system using  
observed data. In system modeling three main principles have to be considered such as separation,  
selection and parsimony. System Identification i s an essential requirement in areas such as control,  
communication, power system and instrumentation for obtaining a model of a system (plant) of  
interest or a new system to be developed. The identification task is to determine a suitable estimate  
of finit e dimensional parameters which completely characterize the plant. The selection of the  
estimate is based on comparison between the actual output sample and a predicted value on the  
basis  of input  data  up to that instant.  
Basic  flow  chart  of system  identification  using  neural  network  adaptive  algorithm  
 
Basic  System  Identification  Process  
8  EXPLAIN  IN DETAIL  ABOUT ANN  APPLICATION  IN CONTROL  SYSTEMS:  
 
The process industry implements many techniques with certain parameters in its operations to  
control the working of several actuators on field. Amongst these actuators, DC motor is a very  
common  machine.  The angular  position  of DC motor  can be controlled  to drive  many  processes  
such  as the arm  of a robot.  The most  famous  and well  known  controller  for such  applications is  PID 
controller.  It uses  proportional,  integral  and derivative  functions  to control  the input  signal  before  
sending  it to the plant  unit.  Neural  networks  model  human  neural  systems  through  computerized  
algorithm.  They  are capable  of parallel  computations  and distributive  storage  of information  like 
human  brain.  In recent  years,  they  have  been  widely  used  for optimum  calculations  and processes  
in industrial  controls, communications, chemistry  and petroleum.  
 
There  are various  types  of control  mechanisms  that  may  be applied  on the speed  and angular  
position  of a DC motor,  depending  upon the accuracy  required.  
 
 
 
Electric  circuit  of a DC motor  
The electric circuit of a DC motor is shown that governs its rotation for desired velocity or position.  
The dynamics  of a DC motor may  be explained by  the following  equations:  
 
 
Where, v is voltage applied across armature, R is Armature Resistance, i is Armature Current, L is Armature  
Inductance and e is back electromotive force (emf) produced across the armature terminals upon its  
rotation. In second equation, T is Torque, K is motor constant representing torque constant and back emf  
constant,  i is Armature  Current.  
 
The DC motor’s  torque  is also  represented by  the followin g relation:  
 
 
Where T is Torque, J is moment of inertia of motor and its load, θ is angular displacement of motor’s shaft  
and b is frictional constant of motor and its load. In order to control the velocity or position of a DC motor, a  
torque is applied a cross its armature with controlled parameters. This torque is controlled by a calculated  
voltage signal at the input. The most common control application for speed and position controls of DC  
motors with high  accuracy  is PID (Proportional -Integral -Derivati ve) control.  
9    
 
Artificial  Neural  Networks  are famous  learning  models  for their  ability  to  cope  with  the demands  of a 
changing environment.  This  network works  with  supervised  learning where  data  set is presented  to train  
the network  before  simulation  is run to get output  results.  The block  diagram  below  shows  the 
implementation  of ANN  control  for Robot Arm  (DC motor  position  control)  through  the available  
model  in MATLAB  analysis.  
 
In order to train the controller block of Artificial Neural Network controller, user is free to input the desired  
values as per the operational requirements before the start of controller’s training. In the initial step, the  
data is generated to train the controller. While data generation process, plant resp onse follows the reference  
model which is necessary for training’s data set to be valid. If the response is not accurate, the data set may  
be regenerated. If data set is acceptable, the controller may be trained through ‘Train Controller’ option. The  
train ing of Artificial Neural Network controller then starts according to the given parameters. However, it is  
done after ‘Plant Identification’ i.e. training the plant unit of  ANN  controller through the same procedure.  
The training of ANN controller may take s ignificant amount of time depending upon the given parameters  
and processing  speed.  
 
The simulation results in MATLAB showed that the output of plant in the examined ANN control follows the  
input reference signal with acceptable results in terms of time d elay factor and system dynamics. Since ANN  
control learns from experience as it is trained through data set in supervised learning, ANN control is more  
responsive than PID to unknown dynamics of the system which makes it even more suitable for industrial  
control  applications  having  uncertainties and  unknown  dynamics due to  environmental  noise.  
10  
BRIEFLY  EXPLAIN  PATTERN  RECOGNITION  USING  ANN.  
 
Pattern recognition is the automated recognition of patterns and regularities in data. Pattern recognition is  
closely related to artificial intelligence and machine learning, together with applications such as data mining  
and knowledge discovery in databases (KDD), and is often used interchangeably with these terms. However,  
these are distinguished:  machine learning is one approach to pattern recognition, while other approaches  
include hand -crafted (not learned) rules or heuristics; and pattern recognition is one approach to artificial  
intelligence,  while  other  approaches  include  symbolic artificial intelligence.  
 
A modern  definition  of pattern  recognition  is: The field  of pattern  recognition  is concerned  with  the 
automatic discovery of regularities in data through the use of computer algorithms and with the use of these  
regularities to take  actions s uch as classifying  the data into  different  categories.  
 
Pattern  recognition  is generally  categorized  according  to the type  of learning  procedure  used  to generate  
the output  value.  Supervised  learning  assumes  that  a set of training  data  (the  training  set)  has been  
provided,  consisting  of a set of instances that  have  been  properly  labeled  by hand  with  the correct  output.  
Unsupervised  learning,  on the other  hand,  assumes  training  data  that  has not been  hand -labeled,  and 
attempts  to find  inherent  patterns  in the data  that  can then  be used  to determine  the correct  output  value  
for new  data instances.  
Algorithms for pattern recognition based on statistical modelling of data. With statistical model in hand, one  
applies  probability  theory  and decision  theory  to get an algorithm.  This  is opposed  to using  
heuristics/”common sense” to design an algorithm. The following learning types are associated with pattern  
recognition  using  ANN.  
1. Supervised  learning  
2. Unsupervised  learning  
3. Generative  model  
4. Discriminative  model.  
Pattern recognition can be implemented by using a feed -forward neural network that has been trained  
accordingly. During training, the network is trained to associate outputs with input patterns. When the  
network is used, it identifies the input pattern and  tries to output the associated output pattern. The power  
of neural networks comes to life when a pattern that has no output associated with it, is given as an input. In  
this case, the network gives the output that corresponds to a taught input pattern tha t is least different from  
the given pattern. During training, the network is trained to associate outputs within put patterns. When the  
network is used, it identifies the input pattern and tries to output the associated output pattern. The power  
of neural network comes to life when a pattern that has no output associated with it, is give it as an input. In  
this case, the network gives the output that corresponds to a taught input pattern that is least different from  
the given  pattern.  
 
11  If we  representblacksquareswith0 and white squares with1 then the truth tables forth e 3neurons after  
generalization  are; 
 
In this case, it is obvious that the output should be  all blacks  since the input pattern is  almost the  same as  
the 'T'  pattern. Here also, it is obvious that the output should be all whites since the input pattern is almost  
the same as the 'H'pattern. Many common pattern recognition algorithms are probabilistic in nature, in that  
they  use statistical inference  to find  the best  label  for a  given  instance. Unlike other  algorithms, which  
simply output a "best" label, often probabilistic algorithms also output a probability of the instance being  
described by the given label. In addition, many probabilistic algorithms output a list of the N -best labels with  
associated probabilities, for some value of N, instead of simply a single best label. When the number of  
possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all  
possi ble labels  is output.  
Classification  of pattern  recognition  algorithms  (supervised  algorithms  predicting  categorical  labels)  
1. Parametric:  
 Linear  discriminant  analysis  
 Quadratic  discriminant  analysis  
 Maximum  entropy  classifier  
2. Nonparametric:  
 Decision  trees,  decision  lists  
 Kernel  estimation  and K-nearest -neighbor  algorithms  
 Naive  Bayes  classifier  
 Neural  networks  (multi -layer  perceptrons)  
 Perceptrons  
 Support  vector  machines  
 Gene  expression  programming  
12  3. Clustering  algorithms  (unsupervised  algorithms  predicting  categorical  labels)  
 Categorical  mixture  models  
 Hierarchical  clustering  (agglomerative  or divisive)  
 K-means  clustering  
 Correlation  clustering  
 Kernel  principal  component  analysis  (Kernel  PCA)  
4. Ensemble  learning  algorithms  (supervised  meta -algorithms  for combining  multiple  learning  
algorithms together)  
 Boosting  (meta -algorithm)  
 Bootstrap  aggregating  ("bagging")  
 Ensemble  averaging  
 Mixture  of experts,  hierarchical  mixture  of experts  
5. General  algorithms  for predicting  arbitrarily -structured  (sets  of) labels  
 Bayesian  networks  
 Markov  random  fields  
6. Multilinear  subspace  learning  algorithms  (predicting  labels  of multidimensional  data  using  tensor  
representations)  
Unsupervised:  
 Multilinear  principal  component  analysis  (MPCA)  
7. Real -valued sequence labeling algorithms (predicting sequences of real -valued labels)  
Supervised:  
 Kalman  filters  
 Particle  filters  
 Regression  algorithms  (predicting  real-valued  labels)  
8. Regression analysis  
Supervised:  
 Gaussian  process  regression  (kriging)  
 Linear  regression  and extensions  
 Neural  networks and  Deep  learning  methods  
 
Unsupervised:  
 Independent  component  analysis  (ICA)  
 Principal  components  analysis  (PCA)  
9. Sequence labeling algorithms (predicting sequences of categorical labels)  
Supervised:  
 Conditional  random  fields  (CRFs) 
 Hidden  Markov  models  (HMMs)  
 Maximum  entropy  Markov  models  (MEMMs)  
 Recurrent  neural  networks  (RNNs)  
Unsupervised:  
 Hidden  Markov  models (HMMs)  
 Dynamic  time  warping  (DTW)   
1  
LECTURE NOTES FOR NEURAL NETWORKS AND FUZZY LOGIC (15A02604)  
UNIT - 4 
FUZZY  LOGIC  
 
WHAT  IS FUZZY  LOGIC?  
Fuzzy logic is an extension of Boolean logic by Lotfi Zadeh in 1965 based on the mathematical theory of  
fuzzy  sets,  which  is a generalization  of the classical  set theory.  Fuzzy  logic  is a form  of many -valued  logic  
in which the truth values of variables may be any real number between 0 and 1 both inclusive. It is  
employed  to handle  the concept  of partial  truth,  where  the truth  value  may  range  between  completely  
true and completely false. Fuzzy logic is based on the observation that people make decisions based on  
imprecise  and non -numerical  information.  
 
One advantage of fuzzy logic in or der to formalize human reasoning is that the rules are set in natural  
language. For example, here are some rules of conduct that a driver follows, assuming that he does not  
want  to lose  his driver’s  licence:  
 
If the light is  red...  if my speed  is high...  and if the light is close...  then I brake  hard.  
If the light is red...  if my speed  is low...  and if the light is far... then I maintain  my speed.  
If the light is orange...  if my speed  is average...  and if the light is far... then I brake  gently.  
If the light is green...  if my speed  is low...  and if the light is close...  then I accelerate.  
 
BRIEFLY  EXPLAIN  CLASSICAL  / CRISP  SETS  
 
A classical set is a collection of objects in a given range with a sharp boundary. An object can either belong  
to the set or not belong  to the set. For example,  5, 10, 7, 6, 9 is a set of integers.  0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 
is the set of integers between 0 and 10. ’s’, ’d’, ’z’, ’a’ is a set of characters. ”Site”, ”of”, ”zero” is a set of  
words. We can also create sets of  functions, assumptions, definitions, sets of individuals (that is to say, a  
population), etc. and even sets of sets. Sets are often represented in graphic form, typically by circles, as  
figure below  illustrates.  
 
 
 
 
 
 
Graphical  representation  of the set {1, 5, 6, 7, 10} 
 
The concept of belonging is important in set theory: it refers to the fact that an element is part of a set or  
not. For example,  the integer  7 belongs  to the set 6, 7, 9. In contrast,  the integer  5 does  not belong  to the 
set 6, 7, 9. M embership is symbolized by the character in the non -membership and by the same symbol,  
but barred possible. Thus, we have 7 ∈ {6, 7, 9} and 5 ∈/ {6, 7, 9}. Below are few examples of set theory  
operations.  
 
Union  of two sets,  Intersection  of two  sets,  Representation  of the  sets  A = {6, 7, 9}  
denoted  A ∪ B denoted A ∩  B and B = {1, 5,  6, 7, 10}.  
 
It is clear that an element either belongs to a set or does not belong to that set in the classical set and its  
operation. There is a sharp boundary between different elements for different sets and these cannot be  
 
2  mixed  with  each  other. However, for the fuzzy  set, it  has different  laws.   
3  WHAT  ARE  FUZZY  SETS  / MODELS?  
 
A fuzzy set is a combination of the elements having a changing degree of membership in the set. Here  
“fuzzy”  means  vagueness,  in other  words,  the transition  among  various  degrees  of the membership  
complies that the limits of the fuzzy sets are vague and ambiguous. Therefore, the membersh ip of the  
elements from the universe in the set is measured against a function to identify the uncertainty and  
ambiguity. A fuzzy set is denoted by a text having tilde under strike. Now, a fuzzy set X would contain all  
the possible  outcome  from  interval  0 to 1. Suppose  a, is an element  in the universe  is a member  of fuzzy  
set X, the function gives the mapping by X (a) = [0, 1]. The notion convention used for fuzzy sets when the  
universe of discourse U (set of input values for the fuzzy set X) is discrete an d finite, for fuzzy set X is given  
by: 
The fuzzy set theory was initially proposed by a computer scientist Lotfi A. Zadeh in the year of 1965. After  
that lot of  theoretical development has  been done in a similar field. Previously the theory of crisp sets  
based in dual logic is used in the computing and formal reasoning which involves the solutions in either of  
two  form  such as  “yes  or no” and “true  or false”.  
 
Fuzzy logic: Unlike crisp logic, in fuzzy logic, approximate human reasoning capabilities are added in order  
to apply it to the knowledge -based systems. But, what was the need to develop such a theory? The fuzzy  
logic  theory  provides  a mathematical  method  to apprehend  the uncertainties  related  to the human  
cognitive process, for example, thinking a nd reasoning and it can also handle the issue of uncertainty and  
lexical  imprecision.  
 
Example: Let’s take an example to understand fuzzy logic. Suppose we need to find whether the colour of  
the object is blue or not. But the object can have any of the sha de of blue depending on the intensity of the  
primary colour. So, the answer would vary accordingly, such as royal blue, navy blue, sky blue, turquoise  
blue,  azure  blue,  and so on. We are assigning  the darkest  shade  of blue  a value  1 and 0 to the white  colour  
at the lowest end of the spectrum of values. Then the other shades will range in 0 to 1 according to  
intensities. Therefore, this kind of situation where any of the values can be accepted in a range of 0 to 1 is  
termed  as fuzzy.  
 
EXPLAIN  THE  PROPERTIES  OF FUZZY  SETS  
 
Fuzzy set have a number of properties. Here are definitions of the most important properties, but they are  
not necessary  for understanding  of the course.  If you want,  you can go now  directly  to the next  section.  Let 
X be a set and  A, a fuzzy subset of X and µA the membership function characterizing it. µA(x) is called the  
membership  degree of x  in A. 
 
Definition  1: Let X be a set. A fuzzy  subset  A of X is characterized  by a membership  function.  
fa : X → [0, 1]. (In theory, it is po ssible that the output is greater than 1, but in practice  
it is almost  never  used.)  
Note:  This  membership  function  is equivalent  to the identity  function  of a classical  set. 
 
Definition  2: The height  of A, denoted  h(A),  corresponds  to the upper  bound  of the codomain  of its 
membership  function:  h(A)  = sup{µA(x)  | x ∈ X}. 
 
Definition 3:  A is said to be normalized if and only if h(A) = 1. In practice,  it is extremely  rare to work on  
non -normalized  fuzzy  sets.  
 
4  Definition 4:  The support of A is the set of elements of X belonging to at least some A (i.e. the membership  
degree  of x is strictly  positive).  In other words,  the support  is the set  supp(A)  = {x ∈ X | µA(x)  > 0}.  
 
Definition  5: The kernel  of A is the set of elements  of X belonging  entirely  to A.  In other  words,  the kernel  
noy(A)  = {x ∈ X | µA(x)  = 1}. By  construction, noy(A)  ⊆ supp(A).  
 
Definition  6: An α-cut of A is the classical  subset  of elements  with  a membership  degree  greater  than  or 
equal  toα : α-cut(A)  = {x ∈ X | µA(x)  “ α}.  
 
Definition  7: Let V be a variable  (quality of  service,  tip amount,  etc.),  X the range  of values  of the variable  
and TV a finite or  infinite  set of fuzzy  sets.  A linguistic  variable  
corresponds  to the triplet  (V, X, TV ). 
 
Another membership function for an average tip through which we have included the above properties is  
presented  in Figure  
 
 
A membership  function  with  properties  displayed  
 
 
Linguistic  variable  ‘quality  of service’  
 
5  COMPARE  BETWEEN  CRISP/CLASSICAL  & FUZZY  SETS.  
Compared with a classical set, a fuzzy set allows members to have a smooth boundary. In other words, a  
fuzzy set allows a member to belong to a set to some partial degree. Previously the theory of crisp sets  
based in dual logic is used in the computing and  formal reasoning which involves the solutions in either of  
two form such as “yes or no” and “true or false”. For instance, still using the temperature as an example,  
the temperature can be divided into three categories: LOW (0 ~30°F), MEDIUM (30°F ~ 70°F)  and HIGH  
(70 ~ 120°F)  from the point  of view  of the  classical  set, which  is shown  in Figure  below.  
 
 
 
 
Graphical  representation  of a conventional  set and  a fuzzy  set 
 
In the classical set, any temperature can only be categorized into one subset, LOW, MEDIUM or HIGH, and  
the boundary is crystal clear. But in the fuzzy set such as shown in Figure (b), these boundaries become  
vague  or smooth.  One  temperature  can be categorized  into  two or maybe  even  three  subsets  
simultaneously.  For example,  the temperatu re 40°F  can be considered  to belong to  LOW  to a certain  
degree,  say 0.5 degree, but  at the same  time  it can belong  to MEDIUM  to about  0.7 degree.  Another  
interesting  thing  is the temperature  50°F,  which  can be considered  to belong  to LOW  and HIGH  to around  
degree and belong to MEDIUM to almost 1 degree. The dash -line in Figure b represents the classical set  
boundary. It is clear that a fuzzy set contains elements which have varying degrees of membership in the  
set, and this is contrasted with the classical or crisp sets because members of a classical set cannot be  
members unless their membership is full or complete in that set. A fuzzy set allows a member to have a  
partial degree of membership and this partial degree membership can be mapped into a function or a  
universe  of membership  values.  
Comparison  Chart  
 
BASIS FOR  
COMPARISON  FUZZY  SET  CRISP  SET  
Basic  Prescribed by vague or ambiguous  
properties.  Defined by precise and certain  
characteristics.  
Property  Elements are allowed to be  
partially  included  in the set. Element is either the member  
of a set or  not. 
Applications  Used  in fuzzy  controllers  Digital  design  
Logic  Infinite -valued  bi-valued  
 
6  EXPLAIN  FUZZY  LOGIC  SYSTEM:  
 
The implementation  of fuzzy  logic  technique  to a real  application,  requires  the following  three  steps:  
1. Fuzzification – convert classical data or crisp data into fuzzy data or  
Membership  Functions  (MFs)  
2. Fuzzy Inference Process – combine membership functions with the control rules  
to derive  the fuzzy  output  
3. Defuzzification – use different methods to calculate each associated output and put them  
into a table: the lookup table. Pick up the output from the lookup table  
based  on the  current  input during  an application  
As mentioned before, all machines can process  crisp or classical data such as either '0' or '1'. In order to  
enable machines to handle vague language input such as 'Somehow Satisfied', the crisp input and output  
must  be converted  to linguistic  variables  with  fuzzy  components.  For instance,  to control  an air 
conditioner system, the input temperature and the output control variables must be converted to the  
associated linguistic variables such as 'HIGH', 'MEDIUM', 'LOW' and 'FAST', 'MEDIUM' or 'SLOW'. The  
former is corresponding to the input temperature  and the latter is associated with the rotation speed of  
the operating motor. Besides those conversions, both the input and the output must also be converted  
from  crisp  data to fuzzy  data. All  of these jobs  are performed by  the first  step  –fuzzification.  
In the second step, to begin the fuzzy inference process, one need combine the Membership Functions  
with  the control rules  to derive the control  output,  and arrange those outputs into a table called the  
lookup  table. The control  rule is  the core of  the fuzz y inference  process, and those  rules are  directly  
related to a human being’s intuition and feeling. For example, still in the air conditioner control system, if  
the temperature is too high, the heater should be turned off, or the heat driving motor should be slowed  
down, which is a human being’s intuition or common sense. Different methods such as Center of Gravity  
(COG)  or Mean  of Maximum  (MOM)  are utilized to  calculate  the associated  control  output,  and each  
control  output  should  be arranged into  a table  called  lookup  table.  
During an actual application, a control output should be selected from the lookup table developed from  
the last step based on the current input. Furthermore, that control output should be converted from the  
linguistic variable back to the crisp variable and output to the control operator. This process is called  
defuzzification  or step  3. 
In most cases the input variables are more than one dimension for real applications, and one needs to  
perform  fuzzification  or develop  a Membership  Fun ction  for each  dimensional  variable  separately.  
Perform the  same operation  if the  system  has multiple output variables.  
Summarily, a fuzzy process is a process of crisp -fuzzy -crisp for a real system. The original input and the  
terminal output must be crisp  variables, but the intermediate process is a fuzzy inference process. The  
reason why one needs to change a crisp to a fuzzy variable is that, from the point of view of fuzzy control  
or a human  being’s  intuition,  no absolutely  crisp  variable  is existed  in our real  world.  
Any  physical  variable  may  contain  some  other  components.  For instance,  if someone  says:  the 
temperature  here  is high.  This  high  temperature  contains  some  middle  and even  low temperature  
components. From this point of view, fuzzy control use s universal or global components, not just a limited  
range  of components  as the classical  variables did.  
With  the rapid  development  of fuzzy  technologies,  different  fuzzy  control  strategies  have  been  
developed based on different classical control methods, such as PID -fuzzy control, sliding -mode fuzzy  
control, neural fuzzy control, adaptor fuzzy control and phase -plan mapping fuzzy control. More and more  
new fuzzy control strategies or combined crisp and fuzzy control techniques are being developed and will  
be applied to  many  areas  in our society  in the future.  
 
 
7  Fuzzy  Control  Rules:  
Fuzzy control rule can be considered as the knowledge of an expert in any related field of application. The  
fuzzy rule is represented by a sequence of the form IF- THEN, leading to algorithms describing what action  
or output should be taken in terms of the currently observed information, which includes both input and  
feedback if a closed -loop control system is applied. The law to design or build a set of fuzzy ru les is based  
on a human  being’s knowledge or  experience,  which  is dependent  on each  different  actual  application.  
A fuzzy IF -THEN rule associates a condition described using linguistic variables and fuzzy sets to an  
output or a conclusion. The IF part is mainly used to capture knowledge by using the elastic conditions,  
and the THEN part can be utilized to give the conclusion or output in linguistic variable form. This IF - 
THEN rule is widely used by the fuzzy inference system to compute the degree to which  the input data  
matches the condition of a rule. Figure below illustrates a way to calculate the degree between a fuzzy  
input T (temperature) and a fuzzy condition LOW. Here we still use the air conditioner system as an  
example.  
 
 
Matching a fuzzy input with a fuzzy condition  
This  calculation  can also  be represented  by the function  
M(T,  LOW)  = Support  min(PT(x), PLOW(x)) (2.16)  
Two  types  of fuzzy  control  rules  are widely  utilized  for most  real applications.  One  is fuzzy  mapping  
rules  and the other  is called fuzzy  implication  rules.  
 
Fuzzy  Mapping  Rules:  
Fuzzy mapping rules provide a functional mapping between the input and the output using linguistic  
variables. The foundation of a fuzzy mapping rule is a fuzzy graph, which describes the relationship  
between the fuzzy input and the fuzzy output. Sometimes, in real applications, it is very hard to derive a  
certain relationship between the input and the output, or the relationship between those inpu ts and  
outputs are very complicated even when that relationship is developed. Fuzzy mapping rules are a good  
solution  for those situations.  
 
Fuzzy mapping rules work in a similar way to human intuition or insight, and each fuzzy mapping rule  
only  approxima tes a limited  number  of elements  of the function,  so the entire  function  should  be 
approximated by a set of fuzzy mapping rules. Still using our air conditioner system as an example, a fuzzy  
mapping  rule can  be derived as  
 
IF the temperature is LOW, THEN the heater motor should be rotated FAST. For other input  
temperatures,  different  rules  should  be developed.  
 
For most  actual  applications,  the input  variables  are commonly  more  than  one 
dimension. For example, in our air conditioner system, the inputs incl ude both current temperature and  
the change rate of the temperature. The fuzzy control rules should also be extended to allow multiple  
inputs to be considered to derive the output. Table below is an example of fuzzy control rules applied in  
our air conditi oner  system.  
T 
1.0 
Fuzzy  Condition  
LOW  
0.4 
0 qF 
20 30 40 Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupati  for NNFL  (15A02604)  – R15 – JNTUA  Page  8 of 13  
 An example  of fuzzy  rules  
 
 
. T 
T LOW  MEDIUM  HIGH  
LOW  FAST  MEDIUM  MEDIUM  
MEDIUM  FAST  SLOW  SLOW  
HIGH  MEDIUM  SLOW  SLOW  
 
The rows  and columns  represent  two  inputs,  the temperature  input  and the change  rate  of the 
temperature input, and those inputs are related to IF parts in IF - THEN rules. The conclusion or control  
output can be considered as a third dimensional variable that is located at the cross point of each row  
(temperature) and each column (change rate of t he temperature), and that conclusion is associated with  
the THEN part in IF -THEN rules. For example, when the current temperature is LOW, and the current  
change rate of the temperature is also LOW, the heater motor’s speed should be FAST to increase the  
temperature  as soon  as possible.  This  can be represented  by the IF-THEN  rule  as 
 
IF the temperature is LOW, and the change rate of the temperature is LOW, THEN the conclusion or  
output (heater motor speed) should be FAST. All other rules follow a similar str ategy, which is very  
similar to a human being’s intuition. In this air conditioner example, a total of nine rules are developed.  
For those applications that need high control accuracy, the input and output should be divided into  
more  small segments, and  more fuzzy  rules  should  be applied.  
 
Fuzzy  Implication  Rules  
A fuzzy  implication  rule  describes  a generalized  logic  implication  relationship  between  inputs  and 
outputs. The foundation of a fuzzy implication rule is the narrow sense of fuzzy logic. Fuzzy implication  
rules are related to classical two -valued logic and multiple valued logic. Still using the air conditioner  
system as an example, the implication is IF the temperature is LOW, THEN the heater motor should be  
FAST. Based on this implication and a  fact: the temperature is HIGH. The result that the heater motor  
should  slow  down  or the SLOW  can be inferred.  
 
Defuzzification  and  the Lookup  Table  
The conclusion or control  output  derived from  the combination  of input,  output membership  functions  
and fuz zy rules is still a vague or fuzzy element, and this process in called fuzzy inference. To make that  
conclusion  or fuzzy  output  available  to real  applications,  a defuzzification  process  is needed.  The 
defuzzification process is meant to convert the fuzzy output back to the crisp or classical output to the  
control objective. Remember, the fuzzy conclusion or output is still a linguistic variable, and this linguistic  
variable needs to be converted to the crisp variable via the defuzzification process. Three defuzzification  
techniques are commonly used, which are: Mean of Maximum method, Center of Gravity method and the  
Height  method.  
 
Mean  of Maximum  (MOM)  Method  
The Mean of Maximum (MOM) defuzzification method computes the average of those fuzzy conclusions or 
outputs that have the highest degrees. For example, the fuzzy conclusion is: the heater motor x is rotated  
FAST.  By using  the MOM  method, this  defuzzification  can be expressed as  
 
 
where T is the  set of output  x that  has the  highest  degrees  in the set FAST.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupati  for NNFL  (15A02604)  – R15 – JNTUA  Page  9 of 13  
 A graphic representation of the MOM method is shown in Figure  below. A shortcoming of the  
MOM method is  that it does  not consider the entire shape of  the output membership function,  
and it only takes care of the points that have the highest degrees in that function. For those  
membership functions that have different shapes but the same highest degrees, this method will  
prod uce the same result.  
 
 
(a) MOM  method  example  (b) COG  method  example  
 
Graphic  representation  of defuzzification  techniques  
Center  of Gravity  (COG)  Method  
The Center of Gravity method (COG) is the most popular defuzzification technique and is widely utilized in  
actual  applications.  This method  is similar  to the formula  for calculating  the center  of gravity  in Physics.  
The weighted average of the membership function or the center of the gravity of the area bounded by the  
members hip function curve is computed to be the most crisp value of the fuzzy quantity. For example, for  
the conclusion:  the heater  motor  x is rotated FAST.  The COG  output can  be represented  as 
 
 
The  Lookup  Table  
The terminal product of defuzzification is the lookup table. Defuzzification needs to be performed for each  
subset of a membership function, both inputs and outputs. For instance, in the air conditioner system, one  
needs to perform defuzzification for each subset of temperature input such as LOW, MEDIU M and HIGH  
based  on the associated  fuzzy  rules.  The defuzzification  result  for each  subset needs to  be stored  
in the associated location in the lookup table according to the current temperature and temperature  
change  rate.  In the following  we use the air conditioner  system  as an example  to illustrate  the 
defuzzification  process  and the creation  of the  lookup table.  
 
To make  this illustration  simple,  we make  two  assumptions:  
I. assume that the membership function of the change rate of the temperature can be de scribed as in  
Figure  below;  
II. only  four  rules  are applied  to this air  conditioner  system,  which  are 
 
1) IF the temperature is LOW, and the change rate of the temperature is LOW, THEN the heater  
motor  speed  should  be FAST  
2) IF the temperature is MEDIUM, and the change rate of the temperature is MEDIUM, THEN the  
heater  motor  speed should  be SLOW  
3) IF the temperature is LOW, and the change rate of the temperature is MEDIUM, THEN the  
heater  motor  speed should  be FAST  
4) IF the temperature is MEDIUM, and the change rate o f the temperature is LOW, THEN the heater  
motor  speed  should  be MEDIUM  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupati  for NNFL  (15A02604)  – R15 – JNTUA  Page  10 of 13  
 INPUT  – Temperature  change  rate 
1.0 
MEDIUM  
LOW  HIGH  
F/Hour  
0 0 0.5 1 2 3 5 
T LOW  T S LOW  FAST  
0.6 
20 30 0 1 600 800 1000  
T MEDIUM  T MEDIUM  S SLOW  
0.8 0.8 
0.4 0.4 
30 55 0 1 2 100 300 500 
T S 
LOW  T MEDIUM  FAST  
0.5 0.5 
0.4 
0.4 
20 30 0 1 2 600 800 
T T 
MEDIUM  LOW  S 
1.0 MEDIUM  
0.8 0.8 
30 55 80 0 1 300 500 The  membership  function  of the change  rate  of temperature  
 
Based on the assumption made for the membership function and fuzzy rules, we can illustrate this  
defuzzification  process  using a  graph.  Four  fuzzy  rules can be  interpreted  as functional  diagrams,  as 
shown  in Figure.  As an exampl e, consider  the current  input  temperature  is 35 qF and the change  rate  of 
the temperature is 1 qF per hour. From Figure,  it can  be found  that  the points of  intersection  between  
the temperature  values  of 35 qF and the graph  in the first  column  (temperature  input  T) have  the 
membership functions of 0.6, 0.8, 0.5 and 0.8. Likewise, the second  column (temperatu re change rate  'T) 
shows  that  a temperature  change  rate  of 1 qF per hour  has the membership  functions  of 1.0, 0.4, 0.4 and 
The fuzzy output for the four rules is the intersection of the paired values obtained from the graph, 
or the AND  result  between  the temperature  input and  the temperature  change  rate input. 
According to  Equation, this operation result should be: min (0.6, 1.0), min (0.8, 0.4), min (0.5,  0.4) 
and min (0.8, 1.0),  which  produces  to 0.6, 0.4, 0.4  and 0.8, respectively.  
 
 
 
An illustration  of fuzzy  output  calculation  Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupati  for NNFL  (15A02604)  – R15 – JNTUA  Page  11 of 13  
 Off-line  and  On-line  Defuzzification  
 
As mentioned  , the defuzzification  process  is to derive  the desired  crisp  output  value  using  a 
defuzzification  technique  that  combines  the membership  functions  with  the fuzzy  rules.  The 
defuzzification process can be further divided into two categories: off -line defuzzification and on -line 
defuzzification.  
 
So-called  off-line defuzzification  means  that  all input  and output  membership  functions,  fuzzy  rules  
and t he lookup table should be developed based on the estimations of the real application prior to the  
implementation of the fuzzy logic technique to the actual application. This means that all input and output  
membership functions are developed based on the ac tual experience or the input and output parameter  
ranges of a specified application, and the lookup table is calculated in terms of those definitions of input  
and output membership functions. The advantage of this method is that most fuzzy inference relate d 
calculations are performed prior to the real implementation and therefore the fuzzy process is less time  
consuming. The disadvantage of this technique is that the fuzzy output is only based on the estimation of  
input  and output  parameters,  so the control  accuracy  is not as high  as that  of on-line method.  
 
The on -line method has real -time controllability. Both input and output membership functions are  
developed during the real -time processing of an actual application. Also the lookup table elements are  
calculated in real -time based on the current actual inputs and outputs. In this method, only fuzzy rules are  
developed prior to the real applications. The advantage of this method is that higher control accuracy can  
be obtained for a process and the fuzzy out put can be computed in real -time. The disadvantage of this  
method is that a longer processing time is needed and it is a somewhat time -consuming process. However,  
with  the development  of new  computer  technologies,  today  much  faster  CPUs  are available,  and 
processing  time is  no longer a  big deal  for this method.  
 
Architectures  of Fuzzy  Logic  Controls  
 
Combining the discussions we made in the previous sections, a structure or architecture of fuzzy logic  
control system  is given here.  As shown in Figure  below, which  is a typical  fuzzy closed -loop control  
system, the inputs are error and error rate, which are combined by block M to input to the  fuzzy inference  
system. The lookup table is derived based on the membership function of inputs, the output and the fuzzy  
control rules. A control gain factor G is used to tune the output of the lookup table to obtain different  
output  values.  The interpolation block S is used to smooth the output element of the lookup table. A  
feedback  signal  is obtained  from  the output of t he system.  
 
 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupati  for NNFL  (15A02604)  – R15 – JNTUA  Page  12 of 13  
 For a system that needs higher control accuracy, a multiple lookup tables fuzzy  control system is  
needed,  which is  shown  in Figure 2.13.  
Two  lookup  tables  are developed  in this control  system,  a coarse  and a fine table.  During  the 
application, the switch between the coarse and the fine table is under the control of the input error limit.  
This limit value can be defined by the user based on the real application. Two -set membership functions  
and control rules are utilized i n this system to satisfy the requirement of higher control accuracy. When  
the system needs quick responses or quick actions, the coarse table is used. When the system needs high  
control accuracy or small control errors, the fine lookup table is selected. T he sacrifice for this method is  
that more memory is needed to store both coarse and fine tables, and a little longer  time is needed to  
make  decision  in selecting  table  in terms  of the  input error  limit  value.  
 
 
EXPLAIN  THE  ARCHITECTURE  OF FUZZY  LOGIC  CONTROL  DESIGN  / CONTROLLER.  
A control system is an arrangement of physical components designed to alter  another physical system so  
that this system exhibits certain desired characteristics. Following are some reasons of using Fuzzy Logic in  
Control  System s: 
 While  applying  traditional  control,  one needs  to know  about  the model  and the objective  function  
formulated  in precise  terms. This makes  it very  difficult  to apply  in many  cases.
 By applying fuzzy logic for control we can utilize the human expertise and  experience for designing a  
controller. 
 The fuzzy  control  rules,  basically  the IF-THEN  rules,  can be best  utilized  in designing  a controller. 
 
Assumptions in Fuzzy Logic Control (FLC) Design: While designing fuzzy control system, the following  
six basic  assumptions  should  be made:  
 The plant  is observable  and controllable  − It must  be assumed  that  the input,  output  as well  as state  
variables are  available  for observation  and controlling  purpose. 
 Existence  of a knowledge  body −  It must  be assumed  that  there  exist  a knowledge  body  having  
linguistic rules and  a set of input -output  data  set from which  rules  can be extracted. 
 Existence  of solution  − It must  be assumed  that  there  exists  a solution. 
 ‘Good  enough’  solution  is enough −  The control  engineering  must  look  for ‘good  enough’  solution  
rather  than  an optimum  one.
 Range  of precision  − Fuzzy  logic  controller  must  be designed  within  an acceptable  range  of 
precision. 
 Issues  regarding  stability  and optimality −  The issues  of stability  and optimality  must  be open  in 
designing  Fuzzy  logic controller  rather  than  addressed explicitly. 
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupati  for NNFL  (15A02604)  – R15 – JNTUA  Page  13 of 13  
 Architecture  of Fuzzy  Logic  Control:  
The following  diagram  shows  the architecture  of Fuzzy  Logic  Control  (FLC).  
 
Major  Components  of FLC: Followings  are the major  components of  the FLC as shown  in the above  figure  − 
1. Fuzzifier  − The role  of fuzzifier  is to convert  the crisp  input  values  into  fuzzy  values.  
2. Fuzzy Knowledge Base − It stores the knowledge about all the input -output fuzzy relationships. It  
also has the membership function which defines the input variables to the fuzzy rule base and the  
output  variables  to the  plant  under  control.  
3. Fuzzy  Rule  Base  − It stores the  knowledge  about  the operation  of the process  of domain.  
4. Inference  Engine  − It acts  as a kernel  of any FLC.  Basically  it simulates  human  decisions  by 
performing  approximate  reasoning.  
5. Defuzzifier − The role of defuzzifier is to convert the fuzzy values into crisp values getting from fuzzy  
inference  engine.  
Steps  in Designing  FLC  
Following  are the steps involved  in designing  FLC:  
 Identification of variables − Here, the input, output and state variables must be identified of the plant  
which  is under  consideration. 
 Fuzzy  subset  configuration  − The universe  of information  is divided  into  number  of fuzzy  subsets  
and each subset is assigned a linguistic label. Always make sure that these fuzzy subsets include all  
the elements  of universe. 
 Obtaining membership function − Now obtain  the membership function for each  fuzzy subset that  
we get in  the above step.
 Fuzzy  rule  base  configuration  − Now  formulate  the fuzzy  rule  base  by assigning  relationship  
between  fuzzy  input and  output.
 Fuzzification  − the fuzzification  process  is initiated  in this step.
 Combining fuzzy outputs − By applying fuzzy approximate reasoning, locate the fuzzy output and  
merge them. 
 Defuzzification  − finally,  initiate  defuzzification  process  to form  a crisp  output.
State  the Advantages  of Fuzzy  Logic  Control:  
 Cheaper  − Developing  a FLC  is comparatively  cheaper  than  developing  model  based  or other  
controller  in terms  of performance.  
 Robust − FLCs are more robust than PID controllers because of their capability to cover a huge range  
of operating  conditions.  
 Customizable  − FLCs  are customizable.  
 Emulate human deductive thinking − Basica lly FLC is designed to emulate human deductive thinking,  
the process  people  use to  infer conclusion  from  what they  know.  
 Reliability  − FLC  is more  reliable  than  conventional  control  system.  
 Efficiency  − Fuzzy  logic  provides  more  efficiency  when  applied  in control  system.  
Lecture  notes  of Dr.R.Murugesan,  AITS -Tirupati  for NNFL  (15A02604)  – R15 – JNTUA  Page  14 of 13  
 State  the Disadvantages  of Fuzzy  Logic  Control:  
 
 Requires  lots of  data  − FLC needs lots of  data  to be applied.  
 Useful  in case  of moderate  historical  data  − FLC  is not useful  for programs  much  smaller  or larger  
than  historical data.  
 Needs  high  human  expertise  − This  is one drawback,  as the accuracy  of the system  depends  on the 
knowledge  and expertise  of human  beings.  
 Needs  regular  updating  of rules  − The rules must  be updated  with  time.  
 
LIST  THE  APPLICATIONS  OF FUZZY  LOGIC:  
 
Application in Aerospace: In aerospace, fuzzy logic is used in Altitude control of spacecraft, Satellite altitude  
control,  Flow  and mixture  regulation  in aircraft deicing  vehicles.  
 
Applications  in Automotive  Industry:  In automotive,  fuzzy  logic  is used  for idle speed  control,  Shift  
scheduling  method  for automatic  transmission,  Intelligent  highway  systems,  Traffic  control,  Improving  
efficiency  of automatic  transmissions  
 
Applications in Business: In business, fuzzy logic is used in Decision -making  support systems, Personnel  
evaluation  in a large  company.  
 
Applications  in Defense:  In defense,  fuzzy  logic  is used  in Underwater  target  recognition,  Automatic  target  
recognition of thermal infrared images, Naval decision support aids, Control of a hypervelocity interceptor  
Fuzzy  set modeling  of NATO  decision  making.  
 
Applications in Electronics: In electronics, fuzzy logic is used in Control of automatic exposure in video  
cameras,  to maintain  Humidity  in a clean  room,  Air conditioning  systems,  Washing  machine  timing,  
Microwave  ovens,  Vacuum  cleaners.  
 
Applications  in Finance:  In the finance  field,  fuzzy  logic  is used  in Banknote  transfer  control,  Fund  
management,  Stock  market predictions.  
 
Applications in Industrial Sector: In industrial, fuzzy logic is used in Cement kiln controls heat exchanger  
control,  Activated  sludge  wastewater  treatment  process  control,  Water  purification  plant  control,  
Quantitative pattern analysis for industrial quality assurance, Control of constraint satisfaction proble ms in  
structural  design,  Control  of water  purification  plants  
 
Applications  in Manufacturing:  In the manufacturing  industry,  fuzzy  logic  is used  in Optimization  of cheese  
production,  Optimization  of milk  production.  
 
Applications  in Marine:  In the marine  field,  fuzzy  logic  is used  in Autopilot  for ships,  optimal  route selection,  
Control  of autonomous  underwater  vehicles,  Ship  steering.  
 
Applications in Medical field: In the medical field, fuzzy logic is used in Medical diagnostic support system,  
Control  of arterial  pressure  during  anesthesia,  Multivariable  control  of anesthesia,  Modeling  of 
neuropathological  findings  in Alzheimer's  patients,  Radiology  diagnoses,  Fuzzy  inference  diagnosis  of 
diabetes and  prostate  cancer.  
 
Applications  in Transportation:  In transportation,  fuzzy  logic  is used  in Automatic  underground  train  
operation,  Train  schedule control,  Railway  acceleration,  Braking  and stopping.  
 
Applications in Pattern Recognition and Classification: In Pattern Recognition and Classification, fuzzy logic  
is used in Fuzzy logic based speech recognition, Fuzzy logic based, Handwriting recognition; Fuzzy logic  
based  facial  characteristic  analysis,  Command  analysis,  Fuzzy  image  search,  Criminal  investigation  and 
prevention  based on  fuzzy  logic  reasoning.  1  LECTURENOTESFORNEURALNETWORKSANDFUZZYLOGIC(15A02604) 
UNIT - 5 
FUZZYLOGICAPPLICATIONSTOELECTRICALSYSTEMSFUZZY 
LOGIC IMPLEMENTATION FOR INDUCTION MOTOR CONTROL  
Vector control, known as field -oriented control (FOC), is a variable -frequency drive (VFD) control scheme 
where the stator currents of a three -phase AC electric motor are acknowledged as two orthogonal 
components that can be visualized with a vector. One component defines the magn etic flux of the motor, the 
supplementary is the torque. The control system of the drive calculates from the flux and torque references 
specified by the drive's speed control the corresponding current component references. Typically 
proportional -integral ( PI) controllers are employed to keep the measured current components at their 
reference values. The pulse -width modulation of the variable -frequency drive describes the transistor 
switching according to the stator voltage references that are the output of the PI current controllers. FOC is 
used tomanage the AC synchronous andinductionmotors. It was originallydeveloped for high performance 
motor applications that are required to operate smoothly over the bursting speed range, generate fulltorque 
at zero spee d, and have high dynamic performance including fast acceleration and deceleration. However, it 
is becoming increasingly attractive for lower performance applications also due to FOC's motor size, cost 
and power consumption reduction dominance. It is expect ed that with increasing computational power of 
the microprocessors it will ultimately nearly universally relocate single -variable scalar volts -per- Hertz (V/f) 
control.  
 
FUZZYLOGIC CONTROLLER  
 
Fuzzy logic provides a sturdy framework for achieving robust and  simple solutions amid different 
approaches of intelligent computation. Fuzzy model is a collection of IF - THEN rules with indistinguishable 
predicates that use a fuzzy reasoning such as Sugeno and Mamdani models. Sugeno type systems can beused 
to model a ny inference system in which the output membership functions are either linear or constant 
whereasMamdanitypeproduceseitherlinearornonlinearoutput.Thefuzzylogiccontrollerconsistsoffour stages, 
fuzzification of inputs and derivation of rules, inference mech anism and de -fuzzification. Fuzzy logic systems 
are collective function approximations. In general, the goal of fuzzy logic system is to yield a set of outputs 
for given inputs in a non -linear system, lacking using any mathematical model but by usinglingui stic rules. A 
general block diagram of Fuzzy logic is shown Fig 1.  
 
SYSTEM MODEL  
 
The block model of the induction motor system with the controller be developed using the power system, 
power electronics, control system, signal processing toolboxes & from the fundamental functions availablein 
the Simulink libraryin Matlab / Simulink. In this paper, plots of voltage, torque, speed, load & flux, etc are 
plotted as functions of time with the controller and the waveforms are observed on the equivalent scopes 
after running the simulations. The entire system modelled in Simulink is a closed loop feedback control 
system consisting of the plants, controllers, samplers, comparators, feedback systems, the mux, de -mux, 
summers, adders, gain blocks, multipliers, clocks,  subsystems, integrators, state -space models, subsystems,  
2  the output sinks (scopes), the input sources, etc. The developedSimulink model for the control of 
variousparameters of the SCIM is shown  in the Fig 2. A set of 49 fuzzy rules are written and called in the 
form of afile in the developed Simulink model with the controller. While the simulation is run, the 2 fuzzy 
inputs are then given to the controller (Takagi -Sugeno -fuzzy), where the output  is obtained afterward. The 
responsecurves of flux, load, torque, terminal voltage, and speed & torques v/s time are observed on the 
respective FromthesimulationresultsshownintheFigure4below,itisobservedthatthestatorcurrentdoesnot 
exhibit any overshoots, u ndershoots, the response of the flux, torque, terminal voltage, speed & stator 
Currents,etc.takeslessertimetosettle&reachthedesiredvaluecomparedtotheresultsusingvector control. This 
shows the effectiveness of the developed controller. It is also observed t hat with the controller, the response 
characteristics curves take less time to settle & reach the final steady state value compared to that in. In the 
fig 5, it shows the graph of speed Vs time for vector control and fuzzy logic controller.  
 
 
 
3  FUZZYLOGICIMPLEMENTATIONFORSWITCHEDRELUCTANCEMOTOR CONTROL  
 
The SRMs overall performance can be improved by two main ways. The first way, by improving the 
mechanical design and second one by thecontrol techniques. There are different type s of controlstrategy 
canbeappliedonSRMsuchasspeed/positioncontrol,currentcontrol,anddirect/indirecttorquecontrol. The 
torque ripples consider the main challenge of the SRM in many applications particularly in EVs applications, 
this problem is very complica ted and affected by many factors and it is not easy to solve.  
Differentstrategiesofcontrolareusedtoovercomeofthisproblemliketraditionalcontrolstrategy,torque 
distribution strategy, linearization control, intelligent control, and other control methods. So, by selecting 
the suitable control strategy for each application, the torque ripple can be efficiently decreased. The SRMs 
can be run in current or voltage control mode. The voltage controlled of SRM is high sensitive to voltage 
ripple on the supply side an d its bandwidth control is lesser, so it is essential to use current control when 
SRM performance is desired to an accurate torque control.  
 
 
 
FUZZYLOGICCONTROL APPROACH:  
 
The fuzzy logic approach offers a simpler, quicker and more reliable solution than conventional techniques. 
TheFLChasthreemainblocks.Thefirstoneis Fuzzificationblock,whichmodifiescrispinputs (inputvalues from 
real world) into linguistic variables to enable the input physical signal to use the rule -base through 
membership functions. The s econd block is Rule -base, where fuzzy inputs are compared, and the controller 
makes the decision based on the membership functions of each input. The last one is Defuzzification block, 
which converts back the fuzzy outputs of the rule -base to crisp ones an d selects membership functions for 
the different control outputs from the rule -base.  
 
4  FLCSpeedControl Algorithm:  
 
Step1:TheSwitchedreluctancemotorspeedsignalis sampled. Step 2: 
Calculate speed error and change in speed error.  
Step 3: Dete rmine fuzzy sets for speed error. 
Step4:Determinemembershipforspeederror.  
Step 5: Determine fuzzy sets for change in speed error. 
Step6:Determinemembershipforchangeinspeederror.  
Step7:FindingcontrolactionasperfuzzyruleandCalculate error.  
Step8:Sendingcontr olcommandtothesystemaftercalculationof error.  
 
Inputandoutputvariablesoffuzzymembershipfunctionareselectedasfollows, PB -
Positive Big  
PM-PositiveMedium 
PS-Positive Small 
NB-Negative Big  
NM-NegativeMedium 
NS-Negative Small  
Z-Zero  
ThetriangularshapedfunctionischosenasmembershipfunctionsBecauseofthebestcontrol performance and 
simplicity.  
 
Fuzzification: This is considered the first step to be programmed, the FLC uses linguistic variables insteadof 
numerical variables. So, the error  input signals can be assigned as Negative Very Big (NVB), Negative Big 
(NB),NegativeMedium(NM),NegativeSmall(NS),Zero(ZE),PositiveSmall(PS),PositiveMedium(PM), Positive 
Big (PB), Positive Very Big (PVB). The triangular membership functionis used for fuzzi fication asshown in 
Figure. The process of fuzzification convert numerical variable (real number) to a linguistic 
variable(fuzzyset).Thechangeoferror,whichusedasthesecondinputforfuzzysystemalsoconverted from 
numerical value to a linguistic variable accordi ng to the triangular membership.  
Defuzzification: The fuzzylogic rules generate the demanded output ina linguistic variable;these variables 
must be transformed to crisp output (real number). This step is the defuzzification; the membership 
functions used i n this study for defuzzification are shows in Fig. 7. The output signal can be assigned as: 
Extremely Low (EL), Very Low (VL), Low (L), Under Medium (UM), Medium (M), Above Medium (AM), High 
(H), Very High (VH), Extremely High (EH), which represent the mod ulation index (m), where (0 ≤ m ≤ 1). 
There are three different methods can be used for membership defuzzification, Center of Area(COA), 
Bisector, or Middle of Maximum (MOM). The center area (COA) is considered the most popular method, so it 
is used for de fuzzification in this study, which is presented in Equation,  
 
By adopting the FLC, the SRM current track the reference signal with minimum values of current ripples 
comparing with traditional current control techniques, and hence, the torque ripples durin g the conduction 
period of each phase of the motor were reduced. The controller was tested at different load conditions and 
with different turn on angles.  
5  BRIEFLYEXPLAINFUZZYBASEDEXCITATIONCONTROLFOR AVR.  
 
Generating Unit Controlis a compl ete closed -loop system and in the past a lot of effort has been dedicatedto 
improve the performance of the controllers. The main problem for example with excitation control is that 
the control law is based on a linearized machine model and the control para meters are tuned to some 
nominal operating conditions. In case of a large disturbance, the system conditions will change in a highly 
non -linear manner and the controller parameters are no longer valid. In this case the controller may even 
add a destabilizi ng effect to the disturbance by for example adding negative damping. Fuzzy logic concept 
incorporates an alternative way which allows one to design a controller using a higher level of abstraction 
without knowing the plant model. Conventional modeling and control approaches based on differential 
equations are often insufficient, mainly due to the lack of precise formal knowledge about the process to be 
controlled. Unlike the conventional control, if the mathematical model of the process is unknown we can 
design fuzzy controllers in a manner that guarantees certain key performance criteria. Fuzzy logic control 
(FLC) reduces the time and complexity in analyzing the differential equations involved in the conventional 
control and hence in the overall design deve lopment cycle as depicted below.  
 
ConventionalDesign Methodology  
 
 
Understandphysicalsystem and 
control requirements  
DevelopaLinearModelof Plant,  
Sensorsand Actuators  
Determineasimplified controller  
fromcontrol theory  
Developanalgorithmfor the 
controller  
Simulate,Debug,and Implement  
theDesign  6  
 
 
 
e  
 
Fuzzifier  
 
 
Inference 
Engine   
Defuzzifier  Rule Base  FuzzybasedDesign Methodology  
 
 
Fuzzy Logic Controller design is a three -stage process. It comprises of fuzzification, inference mechanismand 
defuzzification stages. To design the controller, firstly, membership functions for the input variables (error, 
and integral of error must be specified). Seco ndly, the fuzzy inference system must be defined which 
consists of a series of “If…..then…..” linguistic rules. Then finally, the membership functions for the output 
must be selected. A structure of a fuzzy logic controller is shown in figure below.  
 
Fuzzy controller architecture  
 
 
 
 
Ref.signal  
 
 
 
 
 
 
 
Thefuzzylogiccontrollerhasthreemain components:  
1. Theinferencemechanismincorporatestherulebase whichholdstheknowledgeintheformofaset of 
rules of how best to control the system, and evaluates which of the control rules are relevant at the 
current time and then decides what the input to the plant should be.  
2. Thefuzzificationinterfacethatsimplymodifiest heinputssothattheycanbeinterpretedand 
compared to the rules in the rule base.  
3. Thedefuzzificationinterfacethatconvertstheconclusionreachedbytheinterferencemechanism into 
the inputs to the plant.  
ForthefuzzylogicAVRcontrol,theerrorbetweenthereferencevoltageV refandtheterminalvoltageVti.e. Ve and 
the integral of the error V1 which is the difference between the immediate and previous voltage error 
values are considered as the inputs to the fuzzy controller while the output is the specified output voltage of 
the alternator Vt. In this controller, eleven fuzzy subsets are chosen.  Understandphysical system  
andcontrol requirements  
Designthe controller  
usingfuzzy rules  
Simulate,Debugand implement  
thedesign  7  These are: 
“NegativeVerylarge”(NV),“NegativeLarge”(NL),“NegativeBig”(NB),“NegativeMedium”(NM),“Negative Small” 
(NS), “Zero” (Z), “Positive Small” (PS), “Positive Medium” ( PM), “Positive Big” (PB), “Positive Large” 
(PL),“PositiveVerylarge”(PV),Theseinputvariablesareassignednumericalvaluesas( -1,-0.8,-0.6,-0.4,- 0.2,0, 0.2, 
0.4, 0.6, 0.8 and 1) which stands for Negative Very large, Negative Large, Negative Big, Negative Medium , 
Negative Small, Zero, Positive Small, Positive Medium, Positive Big, Positive Large and Positive Very large 
respectively. For Fuzzy -AVR control, triangular membership function is suitable and therefore used.  
Membershipfunctionsforthe error  
 
Withthetwoinp utsforthisFLC,an(11x11)decisiontableisconstructedasshowninTablebelow.Every entity in the 
table represents a rule. The antecedent of each rule conjuncts Ve and V 1 fuzzy set values.  
Ve 
 
 
 
 
 
 
 
Ve 
 
 
 
 
 
Defuzzification operates on the implied fuzzysets produced bythe inference mechanism and combinestheir 
effects to provide the most certain controller output which is the output of the plant. Widely, center of area 
method (centroid) is used for defuzzification according to the membership function of the output. The 
arrangement of the structure results in a computationa lly less intensive control algorithm. Another 
significant advantage of fuzzy logic is that it can easily accommodate additional input signals. Fuzzy Logic 
Control provides a convenient means to develop the controller which can accommodate the non -linear 
nature of the exciter -generator system. Fuzzy logic on the other hand is not without any set back, the set 
back is observed on the steady -state error which can be cleared by tuning the gains.  
 
 NV NL NB NM NS  
 PS PM PB PL PV 
NV NV NV NV NV NV NV NL NB NM NS  
 
NL NV NV NV NV NV NL NB NM NS  
 PS 
NB NV NV NV NV NL NB NM NS  
 PS PM 
NM NV NV NV NL NB NM NS  
 PS PM PB 
NS NV NV NL NB NM NS  
 PS PM PB PL 
 
 NV NL NB NM NS  
 PS PM PB PL PV 
PS NL NB NM NS  
 PS PM PB PL PV PV 
PM NB NM NS  
 PS PM PB PL PV PV PV 
PB NM NS  
 PS PM PB PL PV PV PV PV 
PL NS  
 PS PM PB PL PV PV PV PV PV 
PV  
 PS PM PB PL PV PV PV PV PV PV 
 
8  EXPLAIN IN DETAIL, THE DESIGN OF FUZZY LOGIC CON TROLLER FOR A 18 / INFINITE BUS 
BAR SYSTEM:  
It is possible that as constrained by the operationalrequirements the single machine infinite bus bar system 
is frequently subjected to sudden real and reactive power loading and also as a result of sudden inclusi on or 
exclusion of sources and loads connected to the grid, it may exhibit low frequency Power Oscillations. 
Though the traditional Proportional integral (PI) controller can handle this problem the associated non 
linearity and the lack of precise mathemati cal models of sub systems it warrants that some intelligentcontrol 
techniques are necessary so that the system offers seamless stable operation. The adaptive Fuzzy inference 
system is an effective alternate, since it can handle problems associated with sys tems for whichthe exact 
mathematical model is either not available or what is available is approximate or too complex.  
UPFCcontrolarchitectureforan18/infinitebus system  
 
 
 
The FLC being a control technique to deal with approximate data to arrive at an agreeable result it can beput 
under service where there is an expectation with a certain range or degree of acceptability. In the 
management of UPFC, the FLC can offer a stable operation tracking the set points and keeping the actual 
values within close vi cinity of the desired set points. The fuzzy logic system in its basic form has the 
following steps.  
 Fuzzyfication,  
 InferenceandDecisionmakingusingtherule base  
 Defuzzification.  
Out of these three basic steps of the FLC the Fuzzification process and the Inference and decision making 
processdependlargelyontheperceptionandexperienceof thedesigner.ThustheperformanceoftheFLCis a 
function of the experience of the operator and hence the entire control system is at the disposal of the 
designer. Further performan ce improvement is possible by integrating the FLC with an artificial neural 
network system (ANN) toform a adaptive neuro fuzzy inference system(ANFIS). In ANFIS, a neural network 
is framed to do the job of learning and gaining experience. Once the learning  process is over the Neural 
Network can be set to decide on the ranges for various linguistic variables and their overlap. In ANFISsystem 
the ANN is used to coin the rules based on the experimental data supplied to the ANN. Thus if the capability 
of an ANN  is incorporated with the FLC the drawbacks of the conventional FLC of being reliant on the 
designer is alleviated. A. Training datacollection In the present design four FLCs were firstdesigned and the 
system was put under service. There were a setof four error -error rate outputs. The individual data sets of 
these four data sets were individually used to train an ANFIS unit and thus four units of ANFIS were 
developed.Therelatedscreenshots astheyappearstepafterinMATLABSIMULINKenvironmentareshown in 
figures b elow.  
9  ANFIS structure  
 
 
 
 
Algorithmfor ANFIS:  
Step1:Definethelinguisticvariablesandterms(initialization) 
Step 2: Construct the membership functions (initialization) 
Step 3: Construct the rule base (initialization)  
Step4:Convertcrispinpu tdatatofuzzyvaluesusingthe membershipfunctions(fuzzification) Step 5: 
Evaluate the rules in the rule base (inference)  
Step6:Combinetheresultsofeachrule (inference)  
Step7:Converttheoutputdatatonon -fuzzyvalues (defuzzification)  
10  Linguistic variables  
 Quantificationvariables(All,Many, None)  
 Usabilityvariables(Sometimes,Frequently, Always)  
 Likelihoodvariables(Possible,Likely, Certain)  
 
Thetwomostimportantconceptswithin FuzzyLogic(FL)areLinguisticvariableandfuzzyif -then rule. The 
widely used fuzzy if-then rule is:  
1. IF(loadismorethantoo -high)THENcommandis fault.  
2. IF(loadistoo -high)THENcommandisReducethe  Load.  
3. IF(loadisNormal)THENcommandisno -change.  
 
 
Since by using FL method we can detect the location of fault exactly it saves the time of the operator and the 
fault can be quickly rectified and the power can be supplied to the affected area immediately from the area 
where there is excess of power available by means of resource sharing. As fuzzy logic is simple and quick in 
detecting the power fault in the power system it is widely used. Normally each and every power grid 
substation is connected with Load Control Centre. Load Control Centre will be monitoring those of 
Substations every second. If fault occurs in one Substa tion that will be obtained by Load control Centre 
through SCADA (Supervisory control and Data Acquisition) and informed to healthy substations which  
are connected with the faulty Substation, then only power will be shared to faulty substation. In which Dat a 
Traffic is a major drawback because at the same time many substations may fail when communication delay 
will occur and so the resource recovery and load balancing will be delayed, people will be affected without 
power, cost loss. The ANFIS will overcome the present status of power sharing and fault diagnosis power 
system method which eliminates the need for Load Control Centre.  


Lecture Notes in Artificial Intelligence 
Subseries of Lecture Notes in Computer Science 
Edited by J. Siekmann 
Lecture Notes in Computer Science 
Edited by G. Goos and J. Hartmanis 
UBR UBR UBR UBR UBR 
069008343590 F. Schmalhofer G. Strube 
Th. Wetter (Eds.) 
Contemporary Knowledge 
Engineering and Cognition 
First Joint Workshop 
Kaiserslautern, Germany, February 21-22, 1991 
Proceedings 
Springer-Verlag 
Berlin Heidelberg New York 
London Paris Tokyo 
Hong Kong Barcelona 
Budapest Series Editor 
Jorg Siekmann 
University of Saarland 
German Research Center for Artificial Intelligence (DFKI) 
Stuhlsatzenhausweg 3, W-6600 Saarbriicken 11, FRG 
Volume Editors 
Franz Schmalhofer 
Deutsches Forschungszentrum fur Kiinstliche Intelligenz GmbH 
Erwin-Schrodinger-StraBe, Postfach 2080, W-6750 Kaiserslautern, FRG 
Gerhard Strube 
Universitat Freiburg, Institut fur Informatik und Gesellschaft 
Friedrichstr. 50, W-7800 Freiburg, FRG 
Thomas Wetter 
IBM Deutschland GmbH, Wissenschaftliches Zentrum IWBS 
Wilckensstr. la, W-6900 Heidelberg, FRG 
g 570 040 
CR Subject Classification (1991): 1.2.0,1.2.4,1.2.6,1.2.8 
ISBN 3-540-55711-3 Springer-Verlag Berlin Heidelberg New York 
ISBN 0-387-55711-3 Springer-Verlag New York Berlin Heidelberg 
This work is subject to copyright. All rights are reserved, whether the whole or part of 
the material is concerned, specifically the rights of translation, reprinting, re-use of 
illustrations, recitation, broadcasting, reproduction on microfilms or in any other way, 
and storage in data banks. Duplication of this publication or parts thereof is permitted 
only under the provisions of the German Copyright Law of September 9, 1965, in its 
current version, and permission for use must always be obtained from Springer-Verlag. 
Violations are liable for prosecution under the German Copyright Law. 
© Springer-Verlag Berlin Heidelberg 1992 
Printed in Germany 
Typesetting: Camera ready by author/editor 
Printing and binding: Druckhaus Beltz, Hemsbach/Bergstr. 
45/3140-543210 - Printed on acid-free paper Foreword 
The roots of this book can be traced back to a conversation I had with Gerhard 
Strube at the German Workshop on Artificial Intelligence (GWAI) in September 
1989. As spokespersons of the Special Interest Groups (SIG) Cognition and Knowl­
edge Engineering of the German Society for Informatics (GI) Gerhard and myself 
were wondering whether any knowledge engineering tools could be applied or analy­
zed in cognition research and what insights and methods of cognitive science might 
be relevant for knowledge engineers. To answer these and related questions we de­
cided to have a common workshop organized by the two SIGs. At the next SIG 
meeting on knowledge engineering in April 1990 at Berlin, I asked Franz Schmalho-
fer and Thomas Wetter to organize such a workshop together with Gerhard. This 
joint workshop was then held February 21-22 at Kaiserslautern. 
At the workshop, the first thing I learned was that the relationship between 
our two disciplines is not a simple import/export business. For instance I was 
told that repertory grids, the best automated knowledge elicitation technique of 
all, are not very popular with scientifically oriented psychologists. And imagine, 
knowledge engineers imported it blue-eyed! On the other hand, I would never bore 
and consequently nerve an expert with a repertory grid technique, even if some 
psychologist told us that enraged experts tend to answer more to the point. 
But how should knowledge engineers, being too busy to become a semi-expert for 
each new application, keep up-to-date with cognitive science as well? Nor could we 
require cognitive scientists to become knowledge engineers! Well, we have to keep 
ourselves mutually informed about the hot spots, will say, problems, approaches, 
trends, or shifts of paradigm in each discipline. This is exactly what we did at our 
workshop. 
• For instance, the last few years have witnessed a shift of paradigm in knowledge 
engineering. It was recognized that expertise cannot be simply extracted from 
the human expert and his books and mapped onto the machine. Neither is an 
expert's head a container of thousands or millions of rules. Second-generation 
knowledge engineering, as we might call it, is viewed as a creative process that 
engages knowledge engineers and experts in (re-)constructing knowledge and 
problem solving methods so that they can be made to run on a computer, re­
sulting in an expert support system rather than an expert replacement system. 
While first-generation knowledge engineers might have been able to simply im­
port methods from other disciplines to extract the knowledge, cognitive science 
is now becoming more important in the new paradigm. This subject came up 
quite a number of times. 
• A more specific issue concerned the generic problem solving methods which are 
being adopted by more and more knowledge engineers. Are experts actually 
in command of such generic procedures which they suitably instantiate for 
their respective tasks? Or don't they distinguish domain-specific and generic 
knowledge at all? Another question addressed to cognitive scientists inquired 
their opinion on multimedia representations. • As a second type of cooperation it was suggested that cognitive scientists 
could take the knowledge engineer's methods, tools, or expert systems back 
into their laboratories in order to experimentally determine their cognitive 
adequacy, whatever is meant with this term. 
• A subject where both disciplines were already cooperating is that of cases, 
both as they arise during knowledge acquisition and as they are used for case-
based reasoning. Questions tackled were: How do humans proceed from cases 
to more general rule-like knowledge? When do they reason by cases or by 
analogies, when do they use heuristics or first principles? How does case-based 
reasoning work, and how is it related to learning? 
The workshop benefitted from international contributions from Canada, England, 
France, Switzerland, and the USA, demonstrating how knowledge engineering and 
cognitive science are interwoven between those countries. But to be quite honest 
with you, the (potential) reader of this book, I was not the only attendant of the 
workshop who was surprised by the wide gap between our two disciplines. 
Then why did we write this book? Because by now we understand much better 
which questions we should ask and which we should better forget. And although 
Franz, Gerhard, and Thomas put lots of work and pain into organizing the workshop 
and editing the book (and this foreword), it still does not answer all the questions 
we raised. Reading this book will consequently not give you any final answers, but 
hopefully provide you with intriguing stimulations for producing your own answers. 
Those of you who are only interested in a quick import/export affair, need not go on 
reading. Our book is intended for persons who are really interested in the cognitive 
science aspects of knowledge engineering. But be warned: the articles reflect their 
authors' different backgrounds. And they assume a certain familiarity with central 
notions. For instance, you should have heard about KADS or MOPS. 
The book is structured into three parts: The first one contrasts work in knowl­
edge engineering with approaches from the side of the "soft sciences". The second 
part deals with case-based approaches in expert systems. Cognition research and 
the cognitive adequacy of expert systems are discussed in the third part. 
My personal route through this book, which I do not want to conceal from 
you, deviates from this structure and is more oriented towards the course of the 
workshop: 
Franz Schmalhofer sets off to explain the paradigmatic shift leading to a second 
generation of knowledge engineering approaches. He argues that the import/export 
attitude which sometimes emerged during the workshop must be replaced by inter­
disciplinary approaches. 
How he personally experienced the shift of paradigm in his knowledge acquistion 
project is reported by Marc Linster. He sees the new task of cognitive scientists 
in helping to find an adequate modelling terminology and later in evaluating the 
resulting expert systems. Gerhard Strubc picks up a panel discussion which, according to the opinion of many 
participants, was the highlight of the workshop. It centered around the fuzzy notion 
of cognitively adequate expert systems. Everybody claims to build such systems -
just like everybody claims to follow a model-based approach - but Gerhard elab­
orates at least three different readings of that notion. He argues why we should 
strive at building "strong cognitively adequate" systems, and thus imposes certain 
requirements on knowledge engineering paired with concrete advice on the first steps 
to be taken. 
Four articles present different methodological views on knowledge engineering. Al­
though I would not call them completely incompatible, they demonstrate how far 
the field is still from having a consistent view of itself. 
• In their very detailed survey on psychological literature, Brian Woodward, 
Mildred Shaw, and Brian Gaines stress the cognitive processes going on while 
knowledge engineering. 
• Beate Schlenker and Thomas Wetter view knowledge acquisition as an itera­
tive process of scientific theory formation driven by falsification. They try to 
reformulate a scientific paradigm in order to make it applicable for knowledge 
engineering. 
• Dieter Fensel argues that knowledge acquisition and qualitative social science 
have common goals, and suggests how to adopt techniques developed by the 
latter for knowledge engineering. 
• Rolf Pfeifer, Thomas Rothenfluh, Markus Siolze, and Felix Steiner present the 
most concrete approach. They suggest how to match think-aloud protocols 
with generic problem solving models. Thus they partially answer one of the 
questions I raised above. 
The next three articles report on experiences with actually employed knowledge 
acquisition systems. The tools developed by the three groups are candidates to be 
taken back to the laboratories of cognitive scientists. 
• Their work on knowledge acquisition front-ends that are to completely replace 
the knowledge engineer drives Frank Puppe and Uie Gappa to pose two urgent 
questions to cognitive scientists, namely the ones I already mentioned before: 
How cognitively adequate are "canned" problem solving methods, and what 
about graphical knowledge representations? 
• Nigel Shadbolt presents problems that arose in designing an integrated knowl­
edge acquisition workbench in the ACKnowledge project. He discusses differ­
ent types of users whose different needs have to be taken into account. 
• Geoffroy Dallemagne, Georg Klinker, David Marques, John McDermott, and 
David Tung describe Spark, Burn, Firefighter, a knowledge-based software 
engineering tool. It helps application programmers with workplace analysis, 
selecting pieces to be automated and configuring these programs from available 
mechanisms. The last group of articles is about cases, as they arise during knowledge acquisition 
and in case-based reasoning. 
• Klaus-Dieier Althoff establishes the terminology and gives a survey of case-
based approaches as compared to machine learning. His article should help to 
classify the following ones. 
• In a short survey, Sonja Branskat gives the flavour of a tool she developed to 
support the knowledge engineer in gradually transforming cases as they appear 
in the real world, laden with context, to the formal and often decontextualized 
representations used by case-based reasoners. 
• Peter Reimann and Thomas Schult report on experiments they conducted to 
find out how humans learn from examples in physics text books. In particular, 
they deal with the basic mechanisms involved in learning from cases in complex 
domains. Their results should carry over to knowledge engineers who typically 
are confronted with such situations. 
• Franz Schmalhofer, Chrisioph Globig, and J org Thoben describe how they built 
a system implementing the generic problem solving method of skeletal plan 
refinement. They elicited cases to acquire the skeletal plans employed by their 
system. Their system is situated in the sense that new skeletal plans can be 
acquired during normal operation. They relied on the expert's experience, 
perception, and selected attention which enable him to identify the right cases 
as a basis for refinement. 
• Ralph Bergmann goes on to present the explanation-based learning method 
used to automatically abstract cases into skeletal plans. They are partially 
based on common sense knowledge. 
• Michel Manago and Noel Conruyt describe their extension of the ID3 induction 
algorithms to a frame-based knowledge representation language. They show 
that mechanical learning techniques can be considerably enhanced when the 
knowledge engineer imposes a suitable structure on the representation of cases. 
Their paper includes a one-page comparison between learning and case-based 
reasoning. 
• From their cognitive science perspective, Dietmar Janetzko and Gerhard Strube 
compare case-based reasoning approaches with those using generic problem 
solving methods, coming up with suggestions of how to integrate both. By 
transferring ideas from cognitive science into the knowledge engineering ter­
minology of the KADS methodology, their article builds a bridge between the 
two disciplines. 
In his concluding remarks, Thomas Wetter does a tremendous job in bringing to­
gether many controversial arguments we encountered at the workshop and presents, 
if not a final word, a comparative assessment. Now you are asked! What is your opinion about this book, and more impor­
tantly, about the questions it raises, and the tentative answers it proposes? Please 
let us know, possibly using the forum of our two special interest groups in the GI. 
Hopefully, we thus get loaded with a lot of dynamite for a successor workshop. 
St. Augustin, May 1992 Angi Voss Table of Contents 
Foreword V 
A. Vofi 
Part 1: Knowledge Engineering and Cognition in Comparison 
Relations Between Knowledge Engineering and Cognition 3 
F. Schmalhofer 
Making Application Programming More Worthwhile 6 
G. Dallemagne, G. Klinker, D. Marques, J. McDermott, andD. Tung 
Using Information Technology to Solve Real World Problems 23 
M. Manago andN. Conruyt 
Facts, Fantasies and Frameworks: 
The Design of a Knowledge Acquisition Workbench 39 
N. Shadbolt 
Mapping Expert Behavior onto Task-Level Frameworks: 
The Need for "Eco-Pragmatic" Approaches to Knowledge Engineering ... 59 
R. Pfeifer, T. Rothenfluh, M. Stolze, and F. Steiner 
Knowledge Acquisition and the Interpretative Paradigm 78 
D.Fensel 
Part 2: Case-Based Approaches to the Development of Expert Systems 
Case-Based Reasoning and Model-Based Knowledge Acquisition .... 99 
D. Janetzko and G. Strube 
The Refitting of Plans by a Human Expert 115 
F. Schmalhofer, Ch. Globig, and J. Thoben 
Knowledge Acquisition by Generating Skeletal Plans from Real World Cases . 125 
/?. Bergmann 
Knowledge Acquisition from Cases 134 
5. Branskat 
Transforming Examples into Cases 139 
P. Reimann and TJ. Shult 
Case-Based Reasoning and Expert System Development 146 
K.-D.Althoffand S.WeJi Part 3: Cognitive Adequacy of Expert Systems 
The Role of Cognitive Science in Knowledge Engineering 161 
G. Strube 
Knowledge Acquisition as an Empirically Based Modelling Activity . . . .175 
B. Schlenker and Th. Wetter 
Shifting Positions: Moving from a Cognitive Science Point of View to a 
Knowledge Engineering Stance 183 
M. Linster 
Two Questions from Expert System Developers to Cognitive Scientists . . .190 
F. Puppe and U. Gappa 
The Cognitive Basis of Knowledge Engineering 194 
J.B. Woodward, M.L.G. Shaw, andB.R. Gaines 
Concluding Remarks 
A Comparative Assessment of Selected Approaches in the 
Focal Area of Knowledge Engineering and Cognition 225 
Th. Wetter 
About the Authors 253 Part 1: 
Knowledge Engineering and Cognition 
in Comparison Relations between Knowledge Engineering and 
Cognitive Science: From Import/Export to a Truly 
Interdisciplinary Knowledge Acquisition Enterprise 
Franz Schmalhofer 
German Research Center for Artificial Intelligence 
University Bldg 57 
Erwin-Schroedinger Str. 
W-6750 Kaiserslautern 
email: schmalho@informatik.uni-kl.de 
1. Introduction 
Knowledge Engineering is generally known as the field that is responsible for the analysis and 
design of expert systems and is thus concerned with representing and implementing the 
expertise of a chosen application domain in a computer system. Research on cognition or 
cognitive science, on the other hand, is performed as a basic science, mostly within the 
disciplines of artificial intelligence, psychology and linguistics. It investigates the mental states 
and processes of humans by modelling them with a computer system and combining analytic 
and empirical viewpoints. 
Early on, knowledge acquisition was known as the activity of making explicit the human 
knowledge that is relevant for performing a task, so that it can be represented and become 
operational in an expert system. Knowledge acquisition and the field of knowledge engineering 
are consequently closely related to human cognition, which is studied in cognitive science. The 
specific relationship between knowledge engineering and cognitive science has changed over 
the years and therefore needs to be reconsidered in future expert system developments. 
Although knowledge acquisition activities are at most twenty years old, there is already a 
respectable history with noticeable successes and some initially disappointing failures to be 
looked back upon. Actually, more progress was made by the analysis of the failures than with 
the short term successes. 
2. Early Knowledge Acquisition 
Early knowledge acquisition was supported by knowledge acquisition systems such as 
TEIRESIAS (Davis, 1978) which were designed as front-ends for existing expert systems (i.e. 
MYCIN) and knowledge engineers viewed knowledge acquisition as the process of transferring 
knowledge from a human expert to a program. 
After it was observed that humans can hardly express their knowledge in distinct chunks, so 
that each chunk can somehow be transformed into a rule (or some other syntactically defined 
structure), which would then do "the right thing" in combination with an already existing expert 
system shell (e.g. EMYCIN), knowledge acquisition became recognized as "a bottleneck in the 
construction of expert systems" (Buchanan et al., 1983, p. 129): Not the development of the 
original expert system (shell), but the acquisition of the domain specific rules for that shell 
turned out to be the tough part in building a fully functional system. Since some of an expert's relevant knowledge is tacit or implicit (Schachter, 1987), experts 
cannot directly verbalize all relevant rules. Knowledge engineers therefore concluded that some 
special psychological method would be necessary in order to acquire the desired knowledge in 
the form that was needed for rule-based (or other) systems. 
For mining the expert's deeply hidden knowledge, various data collection and data analysis 
methods were subsequently imported from psychology into knowledge engineering (Hoffman, 
1989) and respective computer tools were built (Boose & Gaines, 1989). Some of these tools 
were quite successful in constructing rules for relatively small application domains. 
This early knowledge acquisition period was determined by the knowledge engineers who 
emphasized the full implementation of small scale knowledge acquisition tools over a common 
and cognitively founded design rationale for the expert system and its knowledge acquisition 
components. 
Knowledge engineering and cognitive science followed two separate research agendas during 
this period and those slots of the research agenda which were difficult to fill from inside the 
field of knowledge engineering were assigned to the field of cognition (e.g. supplying the rules 
for some rule interpreter). The cooperation of the two disciplines thus consisted of quickly 
importing selected research items (vague ideas, theoretical frameworks or methods) from the 
other discipline in a relatively unreflected way. The use of repertory grids (Kelley, 1955) in 
knowledge acquisition is probably a good example of such a type of import/export relation 
between knowledge engineering and psychology which is one of the disciplines contributing to 
cognitive science. 
While the problem of transferring human expertise into computer programs was (at least 
partially) solved, it was discovered that the knowledge acquisition problem had been incorrecdy 
stated, right from the beginning. One piece of evidence for that is: Even after the successful 
construction of an operational rule-base, the meaning of the individual rules remained a mystery 
(Clancey, 1983; p. 241). The maintenance of larger expert systems was consequently 
impossible. Since such systems were found to have several other disturbing deficiencies (e.g. 
brittleness), the definition of knowledge acquisition needed to be changed. 
3. Knowledge Acquisition as a Truly Interdisciplinary Task 
One of the necessary changes in the definition of knowledge acquisition is already well 
established: Knowledge acquisition is now understood as a modelling activity where models of 
the conventional expertise in an application domain and models of the target expert system are 
to be developed (Breuker & Wielinga, 1989). Unfortunately, the cognitive science issues which 
have become important for successful knowledge engineering are hardly discussed in this 
context. The nature of different types of models and their relationship to one another needs to 
be determined: How should the models of existing or future artifacts (e.g. expert systems) be 
related to models of natural systems (e. g. human cognition)? Can they be structurally similar or 
even identical or do they need to be quite different? Since knowledge engineering deals with 
such artifacts and cognitive science with the modelling of human cognition, the two fields need 
to intensively cooperate to successfully address the question of the relation between the models. 
Newell's (1990) assertion of describing human intelligence as a symbol system is equally 
important for this discussion as Searle's (1981) views about intrinsic intentionality and human 
commitment. 
Another question, where the expertise of cognitive scientists needs to be respected by 
knowledge engineers, is: What kind of mental models (Norman, 1983) do humans develop 
about expert systems? How are the mental models of a domain expert, of a knowledge engineer 
and of the future users of some target system related to one another? What kind of mental models are users capable of and willing to maintain and how can the mental models about 
different systems be related to one another? How can expert systems play the role of fancy 
representations, which allow the communication of knowledge between the domain expert and 
knowledge engineer on the one side and the users of the system on the other side? 
Knowledge engineers must finally learn to appreciate that expert systems have to function in the 
real world in order to become a success in business. Unlike the microworlds, in which 
knowledge engineers liked to test their rapid prototypes, the real world refuses to be (correctly) 
represented once and for all time by some formal specification. The future application 
requirements can consequently only be partially predicted. This basic fact is often ignored. 
Expert systems must be developed so that new types of inputs can be processed at the time 
when the system is applied (Schmalhofer & Thoben, 1992). In other words, expert systems 
must allow for situated applications (Clancey, 1991) and that means that they must be end-user 
modifiable (Fischer & Girgensohn, 1990). These challenging demands can only be 
successfully met, when the engineering science and the cognitive and social sciences cooperate 
with the mutual respect for one another, which is required to make an interdisciplinary 
enterprise a success. 
References 
Boose, J.H. & Gaines, B.R. (1989). Knowledge Acquisition of Knowledge-Based Systems: 
Notes on the State-of-the-Art. Machine Learning. 4, 377-394. 
Breuker, J. & Wielinga, B. (1989). Models of expertise in knowledge acquisition. In Guida, G. 
& Tasso, C. (Eds.), Topics in expert system design, methodologies and tools (pp. 265 -
295). Amsterdam, Netherlands: North Holland. 
Buchanan, B.G., Barstow, D., Bechtal, R., Bennett, J., Clancey, W., Kulikowski, C, 
Mitchell, T. & Waterman, D.A. (1983). Constructing an Expert System, in: Hayes-Roth, 
F., Waterman, D. & Lenat, D.B. (eds.) Building Expert Systems. Reading 
Massachussetts: Addison-Wesiey Publishing Company, Inc. pp. 127-167. 
Clancey, W.J. (1983). The Epistemology of a Rule-Based Expert System - a Framework for 
Explanation. Artificial Intelligence. 20. pp. 215-251. 
Clancey, W.J. (1991). Situated Cognition: Stepping out of Representational Flatland. AL 
Communications. 4. 2/3, 109-112. 
Davis, R. (1978) Knowledge Acquisition in rule-based systems - knowledge about 
representations as a basis for system construction and maintenance. In: Waterman, D.A. 
& Hayes-Roth, F.(eds) Pattern-Directed Inference Systems. Academic Press, New 
York. 
Fischer, G. & Girgensohn, A. (1990). End-User Modifiability in Design Environments, Human 
Factors in Computing Systems, CH1'90. Conference Proceedings (Seattle, WA) ACM, 
New York (April 1990), pp. 183-191. 
Hoffman,R. A Survey of Methods for Eliciting Knowledge of Experts. In: SIGART Newsletter 
108,19-21, 1989. 
Kelley, G.A. (1955). The Psychology of Personal Constructs. New York: Norton. 
Newell, A. (1990). Unified Theories of Cognition. Cambridge, Massachusets: Harvard 
University Press. 
Searle, J.R. (1981). Minds, Brains and Programs. In Haugeland, J. (Ed) Mind Design. 
Cambridge Massachussets: London, 282-306. 
Schachter, D.L. (1987) Implicit memory: history and current status. Journal of Experimental 
Psychology: Learning. Memory and Cognition. 13. 501-518. 
Schmalhofer, F. & Thoben, J. (1992). The model-based construction of a case oriented expert 
system. Al-Communications. 5. 1,3-18. Making Application Programming More Worthwhile 
Geoffroy Dallemagnc, Georg Klinker, David Marques, 
John McDermott, David Tung 
Digital Equipment Corporation 
111 Locke Drive 
Marlboro, MA. 01752 
e-mail:dallemagne@airg.dec.com 
Abstract. We are designing and implementing an integrated programming frame­
work to assist application program developers with the automation of a broad range 
of tasks. Our framework encourages the following activities: 
• analyzing the situation in which automation is going to be introduced, 
• capturing the results of the analysis as a model, 
• building a workflow application program to manage all of the activities, 
• configuring small collections of reuseable mechanisms to perform or assist with 
some of the activities, 
• customizing the configured mechanisms thus generating one or more application 
programs, 
• refining the resulting application programs on the basis of user reactions to them. 
1 Introduction 
Our research problem is how to make application programming more worthwhile. Our initial 
explorations focused on making it easier — ie, making it more worthwhile by allowing less ex­
perienced people to create programs more quickly [see Klinker 90 and McDermott 90]. A 
number of researchers have focused on the issue of how to make application programming eas­
ier [see Krueger 89]. The efforts most closely related to our own include [Bennett 85, Birming­
ham 88, Breuker 89, Chandra 83, Clancey 83, Davis 79, Eshelman 88, Klinker 88, Marcus 88, 
Musen 91 and Yost 89]. Each of these research efforts has identified one or more problem-
solving methods and shown how the methods can be exploited in the development of applica­
tion programs that use them. 
Recently it has become clear to us that our goal of making application programming easier was 
under-constrained and needed to be married to the goal of making application programming 
more effective. There is substantial evidence that many application programs that are devel­
oped are not used anywhere nearly as extensively as their developers anticipated [see, for ex-ample, Leonard-Barton 87]. One significant factor in this under-utilization appears to be the 
mismatch that often exists between the functionality provided by the application program and 
the functionality that would actually be useful in its intended situation. Insight into why these 
mismatches are so pervasive and ideas for reducing their magnitude are provided by research in 
situated action [Suchman 87, Wenger 90]. 
This paper describes a framework for identifying homes for effective application programs and 
for making the construction of those programs easier. A high level overview of our framework 
is presented in section 2. Sections 3 through 7 provide more details. The framework includes a 
place for workplace analysis (section 3), using the results of the analysis to model the 
workplace (section 4), generating a workflow management application program (section 5), se-
\ lecting and configuring reusable mechanisms that can assist with some of the work (section 6), 
customizing the mechanisms for the workplace thus generating one or more application pro-
i grams (section 7). 
2 An Application Programming Framework 
Our framework (graphically depicted in Figure 1) assumes that some group wants computer as­
sistance with one or more of the tasks they perform. The framework is structured around a 
small set of methods and tools ~ a few of which we have developed and the rest borrowed from 
others. Our tools, which we call Spark, Bum, and FireFighter, help with the design, implemen­
tation, and refinement of application programs. They take leverage from a workplace analysis 
methodology and a workflow manager; for the purposes of this paper, we will use BAM-2 
I (Business Architecture Methodology) to show the role a workplace analysis methodology 
; plays1 and we will use EDGE (an event driven workflow controler) to show the role a 
I workflow manager plays, but other workplace analysis methods and other workflow managers 
I could serve just as well within our framework. 
I The framework is designed to help a group of people who want automated assistance with 
I some task. We refer to a group with shared responsibility for a task as a community of prac-
l tice. Our framework guides the community through a series of steps that will result in their 
more easily creating effective application programs. The first step in our framework is 
I workplace analysis. A facilitator, proficient in some workplace analysis methodology (eg, 
) BAJM-2), helps the group identify the processes that are actually at play in their workplace, the 
activities that comprise those processes, the agents who perform the activities, the resources 
[ consumed by the activities, and the products produced by the activities (see 1 in Figure 1). The 
i second step in our framework is capturing the results of the workplace analysis in a model. 
I Burn is used by one or more of the people in the group to record these results. Bum provides a 
j knowledge acquisition tool designed to capture knowledge of the sort needed by a workflow 
\ manager — ie, knowledge of what work is done under what circumstances, of the processes that 
\ support that work, of the agents who perform the work, and of the pieces of work that flow 
I among the workers (see 2 in Figure 1). Once these two tasks (the analysis task and the model­
s' ing task) have been performed, Bum uses the model to generate a workflow management appli-
\ * The creator of the BAM-2 methodology is Jane Roy; Digital Equipment Corporation sells BAMming as a serv-
\ ice. cation program (see 3 in Figure 1) which helps the workers manually perform their work by 
tracking who is to do what and when and by making resources available to the appropriate 
worker at the appropriate time (see 7 in Figure 1). 
Another step in our framework supports is identifying opportunities for automation. Spark is 
used by one or more of the people in the group (the community of practice using our frame­
work) to do this identification. Spark has a library of mechanisms (reusable software struc­
tures) that can be used in composing application programs; associated with each mechanism are 
descriptions of the kinds of situations in which that mechanism can provide assistance. The 
trick is to make contact between the activity descriptions that Spark uses to index into its 
mechanisms and the activity descriptions in the workplace model. Spark tries to help its users 
make contact between these two sets of descriptions (see 4 in Figure 1). If an automation pos­
sibility is identified and if Spark has an appropriate set of mechanisms in its library, it passes 
Bum a configuration of mechanisms that it believes, on the basis of its interactions with the 
users, will allow an effective application program to be generated (see 5 in Figure 1). Bum is 
used, as it was in the second task described above, to elicit relevant details about the workplace. 
In this case, it provides a knowledge acquisition tool for each of the mechanisms selected by 
Spark; each knowledge acquisition tool interacts with some of the people in the group who per­
form the activities to be automated and elicits from them the domain knowledge that its associ­
ated mechanism requires. The outcome of Burn's efforts is a program integrated into the 
workflow management application program generated previously that automates a previously 
manual activity (see 6 in Figure 1). 
The remaining step in our framework is that of refining and maintaining the application pro­
grams that have been generated. As a consequence of using the framework described in this 
paper, the group of workers will have created a workflow management application program 
which helps them keep track of their work, and they will have created activity-specific applica­
tion programs within the context defined by that workflow application. FireFighter's role is to 
assist with the refinement and maintenance of these application programs (including the 
workflow management application) to insure that the programs are and remain effective (ie, are 
modified to reflect any changes in the work the group does or in the processes supporting the 
work). Since both Spark and Bum deliberately use simple heuristics to configure and 
instantiate the mechanisms, it is to be expected that several versions of each application pro­
gram will have to be generated before getting to one that satisfies the users. There are several 
reasons why a mismatch between an application program and a task is likely: (1) Bum may 
not elicit enough knowledge, (2) the mechanism configuration may not be appropriate for the 
activity, (3) the task analysis may have been incorrect. FireFighter assists the users in deter­
mining and resolving these problems. It then re-invokes Burn to acquire more knowledge or 
re-invokes Spark to modify the mechanism configuration (see 8 in Figure 1). 
The following sections provide an example of the use of our framework in its current state. To 
understand the sections, you need the following information about the task that our examples 
focus on: A research group inside a large company is in charge of sponsoring research efforts 
in various universities; this sponsoring is done through grants and involves several activities 
including the selection of what researchers to sponsor and a fairly elaborate process, adminis­
tered by Tiera Washington, for actually presenting the grants. The research group decided to 
see whether automation could smooth the process and thus free up time and the minds of its 
researchers. Task & Knowledge Application program 
Mechanism Elicitation 
Identification 
Figure 1 An Application Programming Framework 3 A Methodology That Allows Workers to Describe Their Work 
The task of processing a grant is comprised of many activities. The problems with automating 
this task are first, understanding what work is actually done and what ought to be done, and 
second, since not all of the activities comprising the task would benefit from or are suitable for 
automation, determining what to automate and how to package the automation. As the work is 
done in the real world (with all its incoherence) within a real organization by real people trying 
to work together, understanding what activities comprise the work is at least a big task. But in 
addition to being big, it is slippery in the sense that no collection of characterizations ever quite 
do the activities or their interrelationships justice. 
Our framework enjoins the use of some kind of workplace analysis methodology to initiate the 
application programming enterprise. The methodology that we use in our own application pro­
gramming experiments is called BAM-2. The following characteristics make BAM particu­
larly attractive as a practical methodology: 
• its result is a description of work produced by the collection of people (a community of prac­
tice) actually performing the work; 
• rather than being an essentially unconstrained fabrication, the description produced is con­
strained to take into account most, if not all, of what is consumed and produced while the task 
is being performed; 
• a complete analysis of a task takes only a few days to do. 
Figure 2 depicts the seven steps of the BAM methodology. The methodology assumes that the 
people involved in performing a task or set of tasks have gathered together in the same room 
with a BAM facilitator. In the first step (see Figure 2.1), the facilitator asks the workers to use 
verb-noun phrases to begin to identify the work they do. The participants are encouraged not to 
edit or filter what they say, but rather to be as spontaneous as possible; the idea is to break 
away from preconceptions and vague abstractions as much as possible. When out example task 
of sponsoring external research was BAMmed, the result of the first step was 66 verb-noun 
phrases. 
The next two steps group the pieces of work identified in the first step; pieces of work diat deal 
with the same kind of thing are grouped (see Figure 2.2 and Figure 2.3). The purpose of these 
steps is to provide the participants with a basis for describing activities in terms of what they 
consume and produce. In our example, the work involved in sponsoring external research was 
grouped into four subtasks: define a research program, issue a grant, finance the grant, and fol­
low the relationship. Defining a research program deals with whom to support and how much 
support to provide; issuing a grant deals with creating a grant letter, getting it approved, and 
getting a check to send the researcher; financing the grant deals with handling the financial 
transactions required to get the check generated; and following the relationship deals with 
monitoring the research, making suggestions, and figuring out how to exploit the results. This 
grouping process forces participants to argue through each other's concepts and thus results in 
a common understanding of the task. 
At this point BAM focuses on what each subtask consumes and produces. The facilitator asks 
the participants to identify all of the products produced by each subtask and then to identify ail 
of the resources consumed (see Figure 2.4). This step draws attention to the objects manipu­
lated within a subtask (eg, grant letter, check voucher) and also draws attention to the stages each of those objects goes through (eg, a grant letter template, an unsigned grant letter, a signed 
grant letter). Then the participants are asked to identify, for each subtask, the customer for 
each product and the producer of each resource. A subtask typically consumes some of its own 
products (eg, issue grant consumes an unsigned grant letter to produce a signed one), it typi­
cally produces products for and consumes products of other subtasks within the broader task 
(eg, issue grant produces a check voucher request which is consumed by the finance grant 
subtask), and it typically produces products for and consumes products of activities outside the 
task being BAMmed (see Figure 2.5). Part of the role of this step is to uncover inadequacies in 
the emerging task description. A product that goes nowhere is unneeded work; a resource that 
comes from nowhere is a bottleneck. The participants collectively decide what is wrong with 
their emerging picture. 
The sixth step in die BAM process creates an event diagram for each subtask (see Figure 2.6). 
The facilitator helps the participants interleave die products and resources by asking them to 
identify the initial triggering event (the resource that starts it all) and the end product (the prod­
uct that justifies the existence of the subtask). Then the participants work back from the end 
product, insuring that each product is preceded by the resources consumed in its creation, and 
forward from the triggering resource, insuring that each resource is succeeded by the products 
produced by its consumption. If not all of the products and resources of a subtask show up in 
the main stream of the event diagram, one or more additional interleavings are created. These 
secondary interleavings either correspond to a support function or to the discovery of an unno­
ticed subtask in which case the BAM facilitator returns to step three (see Figure 2.3) and helps 
the participants further refine their picture. 
When one or more event diagrams have been created for each subtask, each is converted to a 
process diagram by noticing the product/resource boundaries in the event diagram. The idea is 
that in the workplace being BAMmed, these boundaries demark units of work that "make 
sense" in that workplace because each of these pieces of work (each function) has been defined 
in terms of the way the resources in that workplace present themselves (see Figure 2.7). 
4 Capturing the Results of the Workplace Analysis 
Given tiiat a method like BAM can assist in creating a grounded understanding of some task in 
some workplace, our next issues are how to capture that understanding and then exploit it to 
provide useful automation. To capture the understanding, we provide in Burn a knowledge ac­
quisition tool that allows all the information uncovered during the BAMming to be stored as a 
coherent collection of non-ephemeral declarative structures (ie, the different functions the work 
decomposes into, the community of practice and its agents, the resources consumed and the 
products produced are modeled). It is important to notice that, at this point, no information 
about how any piece of work is performed is available. Thus the knowledge acquisition tool is 
prepared to be told only about the "what" "who" "when" and "why" of the task, leaving the 
"how" for later. As Figure 3 shows, the tool presents three interfaces to the user: Func­
tion/Activity, Organization/Agent, and Data; (a fourth interface, one which will allow users to 
enter information about the duration of activities, is not yet implemented). We will illustrate 
the use of this tool for the issue grant subtask. 1) Spout the activities 
• • •• 
• • 
• • • 
• • 2) Identify similarities 
ID 
I • • 
• • 
• • 3) Group activities 
4) Identify products and resources 
o c: z> c Z> 
CZ CZ cz CZ CZl CZI r-~i i i Hi CZ CZ CZ 
• l=I CZI CZI i i cm r"~i HB CZ cz 
• CZ cz CZJ cz r~i i i CZ CZ CZ 
CZ CZ CZ CZI CZl CZI r~~i i i CZ CZ HB 
cz cz CZ CZI cz CZI B Si CZ CZ cz EIH 
cz cz CZ CZI CZI E—1 m ilia 
CZ cz HB cz 
CZ 5) Producers and consumers 
6) Build an event diagram 
r's p's 
r~i 1 I 
r~—i rr^i 7) Extract activity diagram 
E23 E23 E22I CZZI mm BB 
P r k p p p r~~\/ \ r~ 
B^i^i^BI K^Z^^^] [^^&&] H^^H^^l l^^^^^Z^ 
r : p r p p 
~A/ r> P 
V 
Z3HH rosai 
r p p 
Figure 2 The BAM Methodology With the Function/Activity interface, the user creates a function view for each subtask identi­
fied in step 3 of the BAMming. Each view is constructed with a simple graphical editor that 
presents basic building blocks on the left and a drawing area on the right. For the Func­
tion/Activity interface in Figure 3.1, the building blocks are: Activity (a circle), Flow (an arrow 
) and Subtask Port (a diamond); subtask ports allow the current view to connect, via resource 
links, to other subtasks. The user describes a subtask by selecting a building block and creating 
one or more instances of it in the drawing area; the user can describe the functions that com­
prise a subtask (identified in step 7), can identify other subtasks (identified in steps 5 and 6) 
that either produce products for or consume products of the subtask being described, and can 
indicate which functions are directly connected to one another or to functions in other subtasks 
(also identified in steps 5 and 6). 
Widi the Organization/Agent interface, the user creates a view for each organization or com­
munity of practice; the building blocks for these views are Agent, Group, Solid Line, and Dot­
ted Line (see Figure 3.2). Here the user inputs the knowledge about the players (identified in 
steps 5 and 7), the user can also identify the fonnal relationships among the players. To associ­
ate players with functions, the user selects an agent in the organization view and then selects, in 
the function view, whatever functions that player is involved with. 
With the Data interface, the user creates a view that defines the products and resources pro­
duced and consumed by the subtask; the building blocks of this view are Object, Attribute, and 
Has-a (see Figure 3.3). The first thing the user does when creating a data view is to create one 
instance of each resource and product type (identified in step 4). The user can then indicate 
what objects are produced and consumed by what functions by selecting an object in the data 
view and then selecting, in the function view, whatever flows that object rides. 
The model that is being created using this knowledge acquisition tool contains all of the infor­
mation required to generate a workflow management application program that will run on 
EDGE. To automatically generate such a program, the user goes to the function view and indi­
cates which functions are to be included; one (and perhaps the only) reason for excluding a 
function or a subtask from the purview of the workflow manager is if the people responsible for 
some of the functions do not have access to workstations connected to an EDGE server. Before 
generating the application program, the knowledge acquisition tool checks the model to insure 
that no essential information is missing (eg, to insure that an agent has been associated with 
each function). The knowledge acquisition tool also generates a workflow management win­
dow for the workstations of each of the people involved. 
5 Workflow Management Functionality 
Together, the three views discussed in the previous section allow a user to enter information 
that is required for any workflow management assistance. If someone enters information of the 
sort we described, then based on that infonnation, Bum's workflow knowledge acquisition tool 
generates a program that provides workflow assistance from a function/activity perspective. In 
other words, the program keeps each person in the group apprised of what activities are cur­
rently in his or her work queue (see Figure 4 for an example of the kind of information each 
person is provided with). Each time one of the members of the community of practice selects a Figure 3 A Knowledge Acquisition Tool for the BAM Methodology 
piece of work and tells the workflow manager that the work has been completed (by clicking 
on the "Work Done" button), a message is sent to the person responsible for the next activity. 
For example, if Tiera selects 
Issue Grant Martin_03 Create VCR 04/01/91 
and then clicks on "Work Done", the following message would appear on Jake's screen (since 
the Approve VCR activity is always done after the Create VCR activity and Jake is responsible 
for doing Approve VCR). 
Issue Grant Martin_03 Approve VCR 04/03/91 Assistance from a function/activity perspective is confined to reminders. That is, the people 
who will actually perform the work are informed when an activity instance is in their queue, 
but they are not given any assistance beyond that. However, substantial additional assistance 
can be provided if assistance from a data perspective is coupled with assistance from a func­
tion/activity perspective. If the workflow manager can deal with data instances as well as with 
activity instances, then not only can people be informed of what work is in their queue, but they 
can also be provided with pointers to the data that they will operate on. For example, with re­
spect to the Approve VCR activity, Jake could be told where to find the VCR for Martin. 
Finally, assistance from the organization/agent perspective can be coupled with the assistance 
from the other two perspectives. Agents are the repositories of the knowledge needed to per-
fonn the activities. If an agent's understanding of how to perform some activity is put into an 
application program, then that program becomes an instance of the agent. This allows the 
workflow manager to direct some of the work to programs instead of to people. For example, 
with respect to the Create VCR activity, Tiera, instead of having to display the specification for 
some grant and then invoking her favorite editor on the template she instantiates when prepar­
ing a VCR, could instead rely on the workflow manager to display the grant specification and 
invoke her editor for her on a blank VCR form. This is detailed in the following sections. 
[EH] Personal Workflow Manager 
Control Customize Sort Help 
User Window 
tiera's work 
Task 
Issue Grant 
Issue Grant 
Issue Grant 
•t^t Issue Grant 
Issue Grant 
Issue Grant 
Issue Grant _ljris]t8Ltic e Ac ti yity Due 
Santoro_02 Create Letter 12/19/90 
Willow 01 Create Letter 01/21/91 
MaldenUl Ask Approval 02/01/91 
Ne>vton_02 Get Approval 02/20/91 
Solo_03 Offer Grant Q3/t)5/9l 
Martin_u3 Create VCR 04/t)l/91 
Right 02 Create VCR 04/01/91 Timeframe 
.8 1 
<0C 3=iO 
Down { ^kdc^WorkjJ Cjjfl^^l Work Done j 
Reload Quit 
• «VVVVVYVVVV*4XVVVVVVV*VVAiAYY%VYVYA*YA\^ 
Figure 4 User interface for each Agent 6 Identifying Automation Opportunities 
As indicated in the previous section, the task activities described in the workflow diagram of 
Figure 3 represent possible opportunities for automation. If the user decides to explore the fea­
sibility of automating a task activity, he or she mouses on that activity and selects the "auto­
mate" option from the popup menu. In our example, the user chooses to automate the "Create 
VCR" activity by clicking on that activity. This invokes Sparks browsing capability which as­
sists in identifying whether a given task activity has been previously automated in the context 
of another task analysis. That is, Spark assists the user in identifying activities in its library that 
are similar to the current one. The idea is that to the extent two task activities are similar, the 
mechanisms implementing them are the same and can be reused for the task at hand. 
Two activities are considered similar if they consume the same type of resources, produce the 
same type of products, accomplish the same objective, and are performed by people with simi­
lar competencies. Two resources, products, objectives or agents are similar if their labels are 
synonyms. Figure 5 identifies the resource, product, agent, and objective of the "Create VCR" 
activity. The user must now relate these new tenns to the terms in Spark's vocabulary. That 
vocabulary was defined in the process of previously perfomied task analyses (whenever a new 
term is defined, it is added to Spark's vocabulary). In Figure 5 the user indicates that he wants 
to identify Spark's term for "VCR". 
I Spark 4171 
View Edit Format Help 
Please find a synonym for each of the following terms: 
Grant Letter (resource) 
VCR (product) 
Tiera Washington (agent) 
Create VCR (objective) 
I OK I I Resell lAbort 1 
Figure 5 Terms defining the "Create VCR" activity 
Spark uses simple heuristics to suggest a subset of its vocabulary to the user as candidate syno­
nyms. First, Spark's vocabulary is divided into diree classes: resource/product, agent, and ob­
jective. Only the terms from the relevant class are displayed. In our example, resource/product 
is the relevant class. In reducing the set of relevant terms further, Spark takes into account the 
tenns that are already identified for the activity in question. For example, Spark's synonym for 
"Tiera Washington" is "Administrative Secretary". This reduces the set of candidate synonyms 
for "VCR" to the terms denoting a resource consumed by an activity performed by an "Admin­
istrative Secretary". A third heuristic for reducing the set of relevant terms is to consider the 
terms defined for the subtask. In our case, the "Issue Grant" subtask produces a "Congratula­
tion Letter" defined as a "Letter". This heuristic reduces the set of candidate synonyms to the 
ones denoting the products of activities that — among other things — produce letters. Figure 6 demonstrates the basic vocabulary that Spark considers relevant for the "VCR" re­
source after applying the above heuristics. 
If the user does not see a synonym in the set of tenns provided by Spark, Spark can help the 
user by 
• demonstrating a candidate term's use within the context of other task analyses, 
• displaying more general related terms, 
• displaying more specific related terms, 
• displaying all synoyms. 
1 Spark 
View Edit Format Help 
Please click on the term or terms closest in meaning to VCR 
Technical Article Review Message 
Letter Book 
Resume Form 
QK 1 1 Reset! lAbort Where used... 
Other terms... 
More detail... 
All synonyms. 
Use term 
Figure 6 Spark's suggested synonyms for " VCR" The user identifies "Check Voucher" as a synonym of "VCR". This is shown in Figure 7. He 
then identifies synonyms for the other tenns in Figure 5. 
I Spark ?~ 
View Edit Format Help 
Please click on the term or terms closest in meaning to VCR 
Authorization Form Itinerary IPR 
Timecard EBCF Workorder 
Expense Voucher Check Voucher Where used... 
Other terms... 
More detail... 
OK I I Resell lAhortl All synonyms... 
Use term All synonyms... 
Use term 
Figure 7 Shared Resource Vocabulary detailing "Form" 
Once all terms from the new task are identified, Spark checks whether a mechanism configura­
tion has been associated with those tenns (by the software engineer who created the mecha­
nism). In our example, other activities similar to "Create VCR" have been automated before 
and their mechanisms can be reused. Three potentially relevant mechanism configurations are 
shown in Figure 8. 
1 Spark W7I 
View Edit Format Help 
Please choose one of the following programs: 
invoke-SCRIBE 
invoke-DECwrite 
invoke-Document 
I QK I I Resell lAbort I 
Figure 8 Relevant Mechanisms 
Each of the configurations can assist an "Administrative Secretary" in accomplishing the "Cre­
ate Form" objective. The resources are a "Fonn Specification" and a "Form Template" and the product is a filled out "Form". Spark now asks the user to select one of the three configura­
tions. The user indicates the "invoke-DECwrite" configuration since DECwrite is Tiera's fa­
vorite editor. If Spark had not found a mechanism, that would have indicated that the automa­
tion opportunity might not be much of an opportunity; in order for the activity to be automated, 
a software engineer would need to design one or more new mechanisms from the specifications 
provided by the task model. 
7 Tailoring Application Programs to the Workplace 
; When the user selects "invoke-DECwrite", Spark infonns Bum that it is time for the user to 
I interact with the knowledge acquisition tool associated with it. Bum activates the knowledge 
I acquisition tool which asks the user to type a template for "VCR"; that is, it asks the user, in 
{ this case Tiera Washington, to type in the text that will be common to all VCRs (see Figure 9). 
I For the "Create VCR" activity in our example situation, this is all the knowledge that needs to 
| be supplied in advance of the task. Now when the workflow manager window informs Tiera 
I that she should create a VCR, Tiera clicks the "Do" button, and two windows open before her 
\ eyes: one displays the specification for the grant she is preparing (ie, recipient, amount of grant, 
etc) and the other is a DECwrite window containing a copy of the template she had previously 
created. She then fills in that blank form and saves it; that instance of the VCR flows with the 
! subtask instance from that point on. 
m DECwrite: $1 Sdual :[dallemagne. workflow, demolvcr_template.doc; |fU]|al 
File Edit Search Type Elements Style Draw Links Customize Help % Style: General TexUilock Page: 
Font: Helvetica Size: 14.00 1 
u , , . ...... I*. . . IBNBIMMMIEMMfllllSMIflll $ 
$i 
k/V! IT 1 
$ VOUCHER CHECK REQUEST 1 
TT 
$ Date: $ 
To: '/Vv 
i$ Address: \ ZIP code: : : ?\'V 
Am ount: /V. 
$: - DESCRIPTION: 
1 >yv. 
:$ 
C.C. MANAGER SIGN. FINANCE SIGN. 
Badge No.: Badge No.: •$ 
ST 
$ 
Figure 9 VCR template The mechanisms that we have used to date in the context of a workflow manager are few and 
simple. However, we are now focused on integrating work we have done on more interesting 
mechanisms and knowledge acquisition tools [see Marques 91] with our work on workflow 
managers. 
8 Conclusion 
To explore the value of our proposed application programming framework, we used the BAM-
2 methodology to analyze a small, but real, task in our own workplace. We then used Spark 
and Bum, two tools we are developing, to introduce additional automation into that workplace. 
We interacted with one of Burn's knowledge acquisition tools to create a model of our task, 
given the information that came out of the BAMming. Bum then generated a workflow man­
agement application program for our task. We interacted with Spark to select a mechanism that 
could provide assistance with one of the activities in the task — the activity of creating a VCR. 
And finally, we interacted with another of Bum's knowledge acquisition tools, the tool associ­
ated with the mechanism Spark selected, to provide the knowledge that mechanism would need 
to perform the activity. Bum then generated an application program to assist with the Create 
VCR activity and made that program available from within the workflow management applica­
tion. 
Thus the work reported in this paper marshals a small amount of data to support the following 
claims: 
• The potential usefulness of application programs is significantly increased if they are viewed, 
not as isolated pieces of automation, but as agents that need to be carefully situated within a 
community of practice. 
• A workplace analysis methodology that looks for structure in tasks by focusing on the re­
sources the task consumes and products the task produces can make it straight-forward to con­
struct an application program to manage workflow. 
• A workflow management application program provides a context that allows other application 
programs to be integrated within a community of practice. 
We are now in the process of gathering more data by using our approch for several other tasks. 
We are analyzing the BAM-2 methodology; we are exploring the use of other workflow man­
agers — based on different paradigms; and we are analyzing and extending our library of 
mechanisms and associated knowledge-acquisition tools. 
Acknowledgments 
We would be remiss if we did not mention co-workers in the project: Jane Roy is the architect 
of the BAM-2 methodology and Steve Kennedy is the architect of the EDGE workflow control­
ler. Glee Cameron, Patrice Gautier, Therese Mersereau, Charlie Reed and Tina Whitney are 
members of our research group and made significant contributions. References 
[Bennett 85] Bennett, J. ROGET: A Knowledge-Based System for Acquiring the Conceptual 
Structure of a Diagnostic Expert System. Journal of Automated Reasoning, 1,1, 1985. 
[Birmingham 88] Birmingham, W. Automated Knowledge Acquisition for a Computer Hard­
ware Synthesis System. Proceedings of the 3rd Knowledge Acquisition for Knowledge-based 
Systems Workshop. Banff, Canada, 1988. 
[Breuker 89] Breuker, J., B. Wielinga, M. van Someren, R. de Hoog, G. Schreiber, P. de Graf, 
B. Bredeweg, J. Wielemaker, and J. P. Billault. Model-Driven Knowledge Acquisition: Inter­
pretation Models. Deliverable task Al, Esprit Project 1098, Memo 87, VF Project Knowledge 
Acquisition in Formal Domains, Amsterdam 1989. 
[Chandra 83] Chandrasekaran, B. Towards a Taxonomy of Problem Solving Types. Al Maga­
zine, 4, 1,1983. 
[Clancey 83] Clancey, W.J. The Epistemoiogy of a Rule-Based Expert System — a Framework 
for Explanation. Artificial Intelligence, 20, 3, 1983. 
[Davis 79] Davis, R. Interactive Transfer of Expertise: Acquisition of New Inference Rules. 
Artificial Intelligence, 12, 2,1979. 
[Eshelman 88] Eshelman, L. MOLE: A Knowledge-Acquisition Tool for Cover-and-
Differentiate Sytems. In S. Marcus (ed), Automating Knowledge Acquisition for Expert Sys­
tems. Kluwer, 1988. 
[Klinker 88] Klinker, G., C. Boyd, D. Dong, J. Maiman, J. McDermott, and R. Schnelbach. 
Building Expert Systems with KNAGKT. Knowledge Acquisition, 1, 3, (299-320), 1989. 
[Klinker 90] Klinker, G., C. Bhola, G. Dallemagne, D. Marques, and J. McDermott. Usable 
and Reusable Programming Constructs. Proceedings of the fifth Knowledge-Acquisition for 
Knowledge-Based Systems Workshop, Banff, Canada, November 1990. 
[Krueger 89] Krueger, C. Models of Reuse in Software Engineering. Technical Report CMU-
CS-89-188, Department of Computer Science, Carnegie Mellon University, 1989. 
[Leonard-Barton 87] Leaonard-Barton, D. The Case for Integrative Innovation: An Expert 
System at Digital. Sloan Management Review, Fall 1987. 
[Marcus 88] Marcus, S. SALT: A Knowledge-Acquisition Tool for Propose-and-Revise Sys­
tems. In S. Marcus (ed), Automating Knowledge Acquisition for Expert Systems. Kluwer, 1988 
[Marques 91] Marques, D., G. Dallmagne, P. Gautier, G. Klinker, J. McDermott, D. Tung. 
Some Data on the Effectiveness of Software Reuse, Submitted for publication. 
[McDermott 90] McDermott, J., G. Dallemagne, G. Klinker, D. Marques, and D. Tung. Ex­
plorations in How to Make Application Programming Easier. Japanese Knowledge Acquisition 
Workshop, Osaka, Japan, 1990. 
[Musen 91] Musen, M., and S. Tu. A Model of Skeletal-Plan Refinement to Generate Task-
Specific Knowledge-Acquisition Tools. Report KSL-91-05, Knowledge Systems Laboratory, 
Stanford University, 1991. 
[Newell 81] Newell, A. The Knowledge Level. Al Magazine, 2,1, 1981. [Suchman 87] Suchman, L. Plans and Situated Actions. Cambridge University Press,1987. 
[Wenger 90] Wenger, E. Toward a Theory of Cultural Transparency. PhD Dissertation, De­
partment of Information and Computer Science, University of California, Irvine, 1990. 
[Yost 89] Yost, G. A Problem-Space Approach to Expert-System Specification. Proceedings of 
the Eleventh International Joint Conference on Al, Detroit, Michigan, 1989. Using information technology to solve real world 
problems 
Michel MANAGO, Noel CONRUYT 
ACKNOWLEDGE, 16 Passage Foubert, 75013, Paris, France. 
Abstract. We present an induction algorithm, KATE, whose learning strategy is similar to the ID3 
algorithm but which can handle examples described by several object, relations between objects, and use 
background domain knowledge to constrain the search space. The efficient numeric learning techniques 
used in ID3 have been combined with a rich symbolic knowledge representation language (frames) which 
allows using known induction techniques for a broader range of applications. 
1 Induction 
Since the early 1980's, induction tools have been used to generate knowledge based 
systems from databases. From a set of training cases, an induction system automatically 
builds a knowledge base in the form of a decision tree or a set of production rules. For 
instance, from a database of patients whose diseases are known, the induction engine 
learns diagnostic rules that are then used to identify the disease of new incoming patients. 
The ID3 algorithm [Quinlan, 1983] is such an induction system. Its descendants have 
been used for building numerous real-life applications [Michie, 1989]. Nevertheless, all 
the potential applications cannot be tackled by ID3. Its knowledge representation 
capabilities are too limited to cope with the training data when it is made out of complex 
entities and it lacks the ability to handle objects and relations. Due to the increasing 
sophistication of Database Management Systems (relational and objet-oriented DBMS), 
there is a clear gap between what can be achieved with ID3 and the needs to fulfil. This 
was our motivation for developing an induction tool based on the ID3 algorithm but with 
more powerful representation language. In the next section, we analyze why ID3 cannot 
be used for an application in tomato plant diseases [INSTIL 88]. We show that the 
problem with 1D3 does not arise from its induction algorithm but from its knowledge 
representation formalism which fails to capture the true meaning of the information 
contained in the data. 
2 The Notion of Object 
A diseased tomato plant can be affected by several different symptoms. Each symptom 
is a complex entity, or object, whose description depends on both its type and on its 
location. For example, a symptom of type spot is not described the same way as a 
symptom of type hole since a spot has a color, and a hole does not. Likewise, a symptom 
on a leaf is not described the same way as a symptom on a fruit since the position of the 
symptom with respect to the nerves of the leaf is relevant for describing a symptom on a leaf but not for describing a symptom on a fruit. For instance, the symptom tache-ou-
plage-sur-folioles (spot-on-leaves) is described by the following 17 features: 
REPARTITION-SUR-PLANTE, REPARTITION-SUR-FEUILLE, NOMBRE, COULEUR, ZONATIONS, MESURE, LIMITES, 
LOCALISATION-SUR-FOLIOLE, PROPORTION-SUR-FOLIOLE, REPARTITION-SUR-FOLIOLE, JAUNISSEMENT-
POURTOUR, MYCELIUM-FRUCTIFICATIONS, ASPECT-DU-MYCELIUM, TOUCHER, FORME, RELIEF * 
Another symptom, chartere-exterieur-collet (canker-on-the-outside-of-the-stem), is 
described by the 14 features: 
ZONATIONS, TOUCHER, MESURE, FORME, LIMITES, RELIEF, LIEN-SYMPTOME-INTERNE, DEGRE-D-ATTAQUE, 
MYCELIUM-FRUCTIFICATIONS, REPARTITION-SUR-PLANTE, LOCALISATION-SUR-EXTERIEUR-TIGE, 
REPARTITION-SUR-EXTERIEUR-TIGE, NOMBRE. COULEUR** 
These two complex objects (symptoms) share some common features such as COULEUR 
(color) and FORME (shape). They also have some unshared features: JAUNISSEMENT-POURTOUR 
(yellowing-around-the-edges) for the spot on leaves, LOCALISATION-SUR-EXTERIEUR-TIGE, 
(location-on-the-outside-of-the-stem) for the canker on stem etc. In the tomato 
application, there are 147 different kinds of symptoms. On the average, each kind of 
symptom is described by 9 features. Furthermore, more than one symptom (out of these 
147) can appear in the description of a training example (there are up to 6 symptoms for 
the same example). How can we represent such an example with a vector of attributes? 
A possibility is to introduce an attribute for each type of symptom (ex: exists-spot-on-
leaves) and an attribute for each one of its feature (ex: color-of-spot-on-leaves). 
Unfortunately, since there are 147 different symptoms with an average of 9 features per 
symptom, this leads to a total of 147 * 9 = 1323 attributes. Such a large number of 
attributes generates so much computations that ID3 fails to find a solution (combinatorial 
explosion). At the root node, ID3 makes 1323 information gain computations, then for 
each son 1322 computations and so on. In this application, the search space is thus very 
large because each example is very complex to describe and not because there are too 
many examples (there are less than 400). Unfortunately, ID3 is good at handling 
databases with a large number of examples, but the examples are usually described by 
less than 100 attributes. Since the mathematical complexity of ID3 is in O(n) with the 
number of examples, and in O(n^) with the number of bits of information needed to 
represent an example [Manago, 1988], it is clear that having a very large number of 
attributes is a source of problems. In addition, with this choice of representation, it is 
impossible to have two or more symptoms of the same type in the same example. For 
instance, one cannot adequatly represent and example where there are two symptoms of 
type spots-on-leaves with different colors and shapes. 
* distribution-on--plant, distribuuon-on-leaf, number, color, spherical-zones, measure, limits, 
localization-on-folioles, proportion-on-folioles, distribution-on-folioles, yellowing-of-edges, 
mycelium-fructifications, aspect-of-mycelium, feel, form, scraps 
** spherical-zones, feel, measure, limits, scraps, link-to-internal-symptom, state-of-damages, mycelium-
fructifications, distribution-on-plant, localization-on-the-outside-of-the-stem, distribution-on-the-
outside-of-the-stem, number, color There is a second solution which seems to be a little more economical in terms of the 
total number of attributes required to describe an example. In the first solution, when a 
feature is shared by two different symptoms, two attributes are generated anyway. For 
instance, canker-on-the-outside-of-the-stem and spot-on-leaves share the color feature but, with 
the above choice of representation, the two attributes color-of-spot-on-leaves and color-of-
canker-on-stem are generated. We could thus take advantage of the fact that the symptoms 
have some features in common in order to reduce the number of attributes. A special 
attribute called "type-of-symptom", which has one of 147 values (the 147 kinds of 
symptoms), is introduced. We then take the union of all the features for all the attributes 
and associate an attribute to each one. This yields a total number of 80 attributes per 
symptom. Thus, since there is an average of 9 feature per symptom, 71 attributes are, on 
the average, marked as irrelevant. For example, color is not relevant for describing a 
hole-in-leaves but it must still appear in the description of this symptom since it is useful 
for describing some other symptom such as a spot-on-leaves. Since the same tomato plant 
(i.e. an example) can have up to 6 different symptoms, these figures are multiplied by 6 
which yields a total number of 480 attributes for describing all the symptoms in an 
example, 429 of which are irrelevant. 
As we have seen, in both cases we must introduce a special value to mark that an 
attribute is not meaningful in a certain context. For example, if we choose the first 
representation, we must introduce a special value for the attributes of the symptoms 
which do not exist (i.e. what is the value of color-of-spot-on-leaves in an example where 
exists-spot-on-leaves = no?). If we choose the second representation, we must mark that a 
feature is irrelevant for describing a certain kind of symptom (i.e. what is the value of 
color when the symptom is hole-in-leaves?). This value, which we will call "irrelevant", is 
processed by the induction engine in a special way so that it does not disrupt the 
information gain computation.: the training examples with value "irrelevant" are 
propagated in each branch of the node. This increases the complexity of the induction 
algorithm which is no longer linear with the number of examples. Let us assume we have 
chosen the second solution for representing the data. Consider a symptom : 
TYPE-SYMPTOME COULEUR JAUNISSEMENT-
POURTOUR REPARTITION 
SUR-EXTERIEUR-TIGE 
EX 109 TACHE-OU-PLAGE-SUR-FOLIOLES BRUN OUI IRRELEVANT 
EX206 CHANCRE-EXTERIEUR-COLLET GRIS-BEIGE IRRELEVANT GENERALISEE 
Fig.l An attribute-vector representation of an object of type "spot-on-leaves" 
The set of attributes which describes the symptom is the union of all the features for all 
the possible symptoms (80 attributes). On the average, there are 71 attributes with value 
"irrelevant".Therefore, during induction, as soon as the type of the symptom is known, 
ID3 makes 71 useless computations of information gain at each subsequent node of 
the decision tree. Since there can be up to 6 symptoms per plant, there are 6 times as 
much attributes and ID3 make 6*71=429 useless information gain computations at each 
node of the tree. Furthermore, the set of possible values for these features is also increased. For 
instance, the set of possible colors for a symptom of type TACHE-OU-PLAGE-SUR-FOUOLES 
is different from the one of a symptom of type CHANCRE-EXTERIEUR-COLLET. Using H^'s 
attribute-value representation, the set of legal values for each feature is the union of the set 
of values possible for the feature when it is attached to any symptom. The legal values for 
the color attribute include the ones for a symptom of type TACHE-OU-PLAGE-SUR-FOLIOLES 
and for a symptom of type CHANCRE-EXTERIEUR-COLLET. In the tomato domain 
application, this yields an average number of 30 different possible values for each 
attribute. Since in its computation of information gain, ID3 evaluate the entropy at each 
branch (i.e. each possible value for the attribute), ID3 makes 429 * 30 = 12870 useless 
computations of entropy for irrelevant features plus computations for irrelevant 
feature values for the remaining relevant features at each node of the decision tree. 
As we have seen, due to the fact that ID3 lacks the ability to use common sense 
knowledge about the entities which describe the examples, it performs massive amounts 
of totally useless computations. While these computations turn out to be marginal for 
some applications, they cannot be afforded when the training data is complex since the 
induction algorithm is in 0(n2) with the number of attributes (combinatorial explosion). 
3 Beyond propositional calculus 
In the tomato application, there is more than merely constraining the search space 
during induction. The fact that there are several objects in the training examples may drive 
ED3 into learning incorrect rules. In order to represent that there are up to 6 symptoms per 
example with an attribute-value vector, we introduce 6 attributes called exists-symptoml, 
exists-symptom6 in the description of the examples. There are other attributes which 
describe each symptom in more details (such as color-symptorn 1,..., color-symptom6, 
etc.). Note that the choice of label "symptomr'...Msymptom6" for a given symptom on a 
tomato plant is purely random. For instance, there are no objective reasons to decide that 
a particular spot-on-folioles is to be called "symptoml" instead of "symptom2". There is 
no ordering of the symptoms that forces a type of symptom to always have the same label 
since any 6 out of the 147 symptoms may appear in an example. Unfortunately, the 
choice of label has a strong impact on what can be learned. It can even drive the induction 
engine into learning incorrect rules. Consider what would happen if in every examples of 
a disease "A" there is a spot-on-folioles called symptoml. 
Ex1 (A): <exist-symptom1 = yes> <symptom1 s spot-on-folioles> <color-symptom1 = yellow> ... 
<symptom1 = canker-on-stem> <exist-symptom2 = yes> <color-symptom2 = white>... 
Ex2 (A): <exist-symptom1 = yesxsymptoml = spot-on-folioles> <color-symptom1 = brown> ... 
<exist-symptom2 = yes> <symptom2 = hole-in-folioles> <color-symptom2 = irrelevant... 
Ex3 (B): <exist-symptom1 = yes> <symptom1 = mold-on-fruit> <color-symptom1 = white> ... <exist-
symptom2 = yes> <symptom2 = spot-on-folioles> <color-symptom2 = brown> 
ID3 might possibly learn the rule "IF <symptoml = spot-on-folioles> THEN disease 
A". However, if there is a "spot-on-folioles" that is called "symptom2" in an example of 
disease "B", the rule is syntactically consistent with the data but it is semantically incorrect. This is due to the fact that ED3 consider the two attributes exist-symptoml and 
exist-symptom2 as two different unrelated entities. The fact that both these attributes give 
some information about the existence of a symptom is ignored. In fact, as far as ID3 is 
concerned, these two attributes could just as well have been called X and Y since it only 
cares about their numerical information gain and ignore the true symbolic meaning of the 
information that they convey. What is needed here is a way to express a test of the form 
"Is there any symptom of type spots-on-folioles?" instead of the tests "is symptoml of 
type spots-on-folioles?", "is symptom2 of type spots-in-folioles?" etc. The induction 
engine will then compute information gain of this more generic test. In other word, this 
application requires a more powerful knowledge representation language and a more 
powerful pattern matching algorithm. The pattern matcher must handle variables and it 
must be based on some form of first order logic instead of propositional calculus. 
There are other reasons why ID3 cannot be used for this application. For example, it 
fails to represent adequatly relations between objects. However, as we have shown, ID3 
its induction mechanism is not inadequate but its knowledge representation formalism is 
insufficient. Having noticed this fact, the obvious idea is to build an induction tool with 
the same learning mechanism as ID3 (hill-climbing search strategy, heuristic preference 
criterion based on entropy) but with a more powerful knowledge representation language. 
4 Constraining Search During Induction 
4.1 Representing Background Knowledge with Frames 
About twenty years ago, researchers in knowledge representation came up with the 
notion of frames [Minsky, 1975]. Rapidly, frame based languages invaded most of the 
Artficial Intelligence tools: expert system shells, blackboard systems, image recognition 
systems, natural language parsers, planing systems etc. The practical interest of these 
languages has been well demonstrated: they are powerful, efficient, the knowledge is 
made of modular reusable entities, the formalism is natural and it enables object-oriented 
programming. 
A frame can be viewed as a data structure. It represents a set of object (a class) or of 
the objects themselves (the instances). Properties and relations are attached to the frame 
and are called slots. Facets are attached to the slots. Facets enable representing different 
kinds of information about the slots. For example, there are which state the actual value 
of a slot, the type (or legal range of values) of a slot, procedures which deduce features 
from other features in backward or forward chaining etc. Consider the following frame: 
[TACHE-OU-PLAGE-SUR-FOLIOLES 
own slots: 
SUPERCLASSES {(VALUE (TACHE-OU-PLAGE SYMPTOME-SUR-FOLIOLES))}> 
member slots: 
<LOCALISATION-SUR-FOLIOLE 
{(TYPE (FACE-SUP FACE-INF NERVURES LIMBE) 
(CARDINAL 1 2)}> 
<MESURE 
{(TYPE (REAL 0.2 60))}> etc.] Frames may be organized in a hierarchy of generality. The most general concepts 
appear toward the roots of the hierarchy. The value facet of slot "SUPERCLASSES" indicates 
that the concept of TACHE-OU-PLAGE-SUR-FOLIOLE (spot on leaves) inherits slots from its 
two superconcepts TACHE-OU-PLAGES (spots) and SYMPTOME-SUR-FOLIOLE (symptom on 
leaves). For instance, slot LOCALISATION-SUR-FOLIOLE has been inherited from 
SYMPTOME-SUR-FOLIOLE and MESURE from TACHE-OU-PLAGES. This hierarchy of concepts 
is entered manually and is part of the background knowledge. The hierarchy of symptom 
can be organized both according to the location of the symptom or according to the kind 
of symptoms (KATE handles multiple inheritance). 
The frame representation is modular and economical: once the superconcepts have 
been declared, a new lower level concepts, such as FLETRISSEMENT-SUR-FOLIOLE, is 
declared by simply stating that it inherits from the two superconcept FLETRISSEMENTS and 
SYMPTOME-SUR-FOLIOLE which has already been defined for TACHE-OU-PLAGE-SUR-
FOLIOLE. All the slot declarations attached to the two superconcepts are transmitted to 
FLETRISSEMENT-SUR-FOLIOLE. For complex problems such as the one we are describing, 
having such a flexible, modular and economical representation is very important. 
4.2 Using frames to constrain candidate nodes during induction 
The notion of "slot" bears some similarities with the notion of "attribute" in ID3. The 
difference lies in the fact that a slot is attached to an object. The slot LOCALISATION-SUR-
FOLIOLE of the object TACHE-OU-PLAGE-SUR-FOLIOLES is a different attribute (i.e. it has a 
different range of allowed values and a different cardinal) than the slot LOCALISATION-SUR-
FOLIOLE of the object FLETRISSEMENT-SUR-FOLIOLE. The induction engine will behave 
differently when it computes information gain for that slot depending on wether it is 
attached to one object or the other. For instance, it will generate different branches at that 
test node since the range of allowed slot values is different. 
Since a list of all the features which are relevant to describe an object in a given context 
is found in the appropriate frame, KATE never considers irrelevant features for the 
information gain computation. For example, we may have a class frame called MEN with 
boolean slot BEARDED and a class frame called WOMEN with boolean slot PREGNANT. Since 
INSTIL and KATE test for information gain of a slot attached, they never test the state of 
pregnancy of an object who is known to be a man since PREGNANT is not a slot of the MEN 
frame. In other word, they compute information gain of PREGNANT(WOMEN) and 
BEARDED(MEN) instead of PREGNANT or BEARDED in general. 
KATE uses common sense domain knowledge, contained in the frames, when 
building the decision tree in order to to constrain the set of features considered for the 
information gain computation. In the tomato domain, considering every features generates 
such a large number of computations that the induction algorithm fails to complete its 
task. If at the first root of the decision tree, the most informative feature is the SIZE slot of 
a symptom of type HOLE-IN-A-LEAF, at subsequent nodes in the tree only the slots of frame 
HOLE-lN-A-LEAF are considered. Features such as COLOR, TEXTURE etc. which are irrelevant for this concept are discarded. Thus, at different nodes of the decision tree, different 
features are considered. 
4.3 Constraining the Set of Branches for a Decision Node 
Information gain is computed for a slot that is attached to a certain object type.KATE 
computes information gain of COLOR(SYMPTOM-ON-LEAVES) and not information gain of 
the COLOR attribute by itself as ID3 does. Two different frames may have different "type" 
facets for the same slot when the range of legal slot values is different for two different 
objects. The induction engine then uses this knowledge to constrain the set of branches 
generated for the current candidate node in the decision tree. 
Consider two kinds of MEN called PUNK-ROCKERS and BUSINESMEN (defined as 
subclasses of MEN ). Let us assume that the frames have been defined the following way: 
[PUNK-ROCKERS <HAIR-COLOR {(TYPE (PURPLE BLUE GREEN BLOND BROWN RED))}>] 
[BUSINESMEN <HAIR-COLOR {(TYPE (BLOND BROWN RED WHITE))}>] 
When the induction engine computes information gain of HAIR-COLOR(PUNK-ROCKERS) 
it does not consider the WHITE branch since there are no old PUNK-ROCKERS with white 
hairs. Likewise, when it computes information gain of HAIR-COLOR(BUSINESMEN), it does 
not compute the entropy of branches PURPLE BLUE and GREEN since there are no 
BUSINESMEN with hairs dyed in funny colors. Background domain knowledge is used to 
constrain the set of branches considered at a candidate node. 
To summarize, the frame-based language is used to represent common sense domain 
knowledge about the entities which describe the training examples. This knowledge is 
used during induction to constrain the set of features that are relevant in a given context 
(i.e. reduce the number of candidate nodes considered for the information gain 
computation), and to constrain the set of values for these features (i.e. reduce the number 
of branches for the candidate nodes). Note that our frame based language is, from a 
theoretical perspective, a form of first order logic as shown in [Nilsson, 1980]. In 
addition, any database which can be represented using ID3's attribute-value vectors can 
also be trivially represented by frames: there is a single frame whose slots correspond to 
the ID3 attributes. The induction engine then behaves exactly like ID3. 
4.4 Handling Examples Described by Different Objects 
As we have shown, the tests considered for the information gain computation at 
different nodes in the decision tree are different. The procedure which dynamically 
generates candidate nodes for the information gain computation is described below. 
• When all the examples at the current branch of the decision tree contain an object 
of the same type, information gain of the non relational slots of this object is 
computed (i.e. nominals, ordered nominals, integers and reals). This can either 
introduce a new object in the current path of the decision tree, or specialize an object that already appears above the current node. Numerical tests are binarized 
so as to maximize information gain. 
• When some of the examples at the current test node do not contain an object of 
the same type, information gain of the special test "Existst(type-of-the-object)" is 
computed. 
• Information gain of a relational slots is computed when the relation links two 
objects that already appear in the current path (i.e. above the current node) or 
when it introduce one new object. Relations that introduce two new objects are 
not considered in order to constrain the search space. 
The boolean test "exists(object-type)" which appears in the decision tree presented 
below means "is there an object of that type in the example?". Each object that appears in 
the tree can be indexed further down. The object's identifier is a variable which is bound 
to the appropriate object in a training example. The induction engine can then index the 
slots of that object at subsequent decision nodes. 
File Edit Learn Window Graph 
5 
2 / me3ure(tache-ou-plage-3ur-f oliole) < 0.15 
. e>dst3(lache-ou-plage-3ur-foliole) = yes { couleu^tache-oi 
couleur(tache-oi 
couleur(tache-oi 
/ zon«tions(tach 
„ 2onalion3(lach 
x me3ure(tache-ou-plage-3ur-foliole) >= O.ISf 
Fig. 2 When the training examples contain different objects, the special test "Exists(object-type)" is 
considered when building the tree 
This procedure for generating candidate nodes dynamically enables dealing with 
examples described by different objects. Once information gain has been computed for all 
candidate nodes, the one with highest information gain is retained as the current decision 
node. The search strategy (hill-climbing) and preference criterion (maximize information 
gain) remain the same as in ID3. The difference lies in the set of candidate nodes 
generated by the algorithm: ID3 computes information gain for all the attributes which do 
not already appear in the decision nodes above the current node, while KATE does more 
work to dynamically generate the set of candidate nodes which are considered. 5 Experimental Results 
5.1 Diagnosis in Plant Pathology 
In the tomato application, there is a small number of examples per concept (there are 
less than 400 examples for 60 diseases). There are even some diseases for which there is 
no examples since, according to the expert, it takes about 7 years to see all the diseases. It 
is therefore meaningless to use a training set and a test set to measure the accuracy of the 
learnt rules and extrapolate results which are not statistically significant. The rules have 
been evaluated empirically by the human expert. One of his conclusions was that the rules 
describing features of existing objects seemed to be meaningful. In fact, the first time the 
system ran on the data, it found the exact rule he had given for the disease of "Moelle 
noire". However, he did not like negative rules such as the one shown below: 
IF exists(tache-ou-plage-sur-foliole) = no & exists(anomalie-de-la-forme-ou-de-la-taille-sur-foliole) = no & 
exists(jaunissement-sur-foliole) - no & exists(autres-anomalies-sur-foliole) = no & 
exists(jaunissement/dessechement-sur-foliole) = no & exists(dessechement/tache-ou-plage-sur-foliole) 
= no & exists(fletrissement/jaunissement-sur-foliole) = no & exists(ravageurs-sur-foliole) = no & 
exists(tache-ou-plage/autres-anomalies-de-coloration-sur-foliole) = no & exists(jaunissement/tache-ou-
plage-sur-foliole) = no & exists(fletrissement/tache-ou-plage-sur-foliole) = no & exists(fletrissement-sur-
foliole) = no & exists(dessechement-sur-foliole) « no & exists(autres-anomalies-de-coloration-sur-foliole) 
= no THEN Oidium (0.20), Pvy (0.80) 
This rule says "if there are none of these 16 symptoms, the disease is "Oidium" with a 
probability of 0.2 or "Pvy" with a probability of 0.8. In this application domain, 
considering that each year there are new diseases or variations in existing diseases 
(mutation, changes in the climate etc.), the expert simply refuses to view his application 
domain as a closed world. Although a purely negative rule is consistent with the training 
data, it is totally meaningless for him. The final conclusion is that, for this application, 
induction is not so useful for knowledge acquisition considering how difficult it is to 
obtain training examples. However, this project showed that information technology can 
be extremely interesting for the purpose of maintaining a diagnostic system. In this 
domain, it is not unusual that a new disease appears or that an existing disease mutate and 
presents some unseen features. Thus, maintaining rapidly the knowledge base from one 
crop season to the other is of vital importance. As a consequence, the decision tree that is 
generated by KATE is not used for interactive consultation as it is usually done. Instead, 
the on-line interactive questionnaire, that is normally used to collect the description of a 
training example, is used for entering the full description of the case. The user then 
chooses to record the description as a training example (the expert provides his diagnosis 
in a second stage) or to consult the decision tree through KATE's auto-consultation 
mode. The expert system is then maintained by regenerating the decision tree as new 
examples are integrated in the database. 
5.2 Credit assesments 
The french "Societe d'Informatique et de Systeme (SIS)" has tested KATE on an 
application delivered to one of France's major bank. The expert system's task is to evaluate financial risks when the bank covers credit assessed by a supplier to one of its 
customer. SIS is a large french servicing company with 20 years of experience in 
business data processing and 6 years of experience in expert system technology. They 
have performed a full comparison of knowledge acquisition with KATE, knowledge 
elicitation by interviewing the human experts and knowledge acquisition using statistical 
analysis (scoring techniques) for this application. 
The training set presented to KATE contains 700 examples described by one object 
with 31 slots: 12 slots with integer values, 2 slots with real value, 17 slots with nominal 
values (with an average of 5 values per slot). There are 7 concepts to identify. The 
problem is complicated due to the fact that, on the average, 80% of an example's 
description is unknown. Considering the massive amount of unknown values, one cannot 
associate to the value a probability that is computed on the rest of the training set as it is 
described in [Quinlan, 1989]. When building the tree, KATE propagates examples with 
unknown values in each branch of a decision node. Furthermore, the customer has to 
give a quick answer for a large number of cases and the information available is mostly 
qualitative. The decision taken has to be explained and justified to the customer. The rule 
that is extracted from the decision tree provides this explanation. The following table 
summarize the results of the comparison between the panel of experts, KATE, statistical 
analysis, and knowledge elicitation (interview of experts). 
board of experts KATE knowledge elicitation statistical analysis 
perfect (%) 91 70.3 73.2 48.6 
good (%) 8.1 21.7 12.3 26.8 
errors (%) 0.9 8 14.5 24.6 
time required 14 60 25 
men power 11 45 15 
analyst 8 7 9 
experts 1 8 2 
specialist 2 30 4 
Perfect means the exact answer that was expected, good means that the correct answer 
was suggested with a probability above 0.5, errors means that the system answered 
something incorrect, failed to answer or suggested the correct response with a probability 
that was too low, time means the total time it took to build the system, men power means 
the total men power that was required to build the application. The men power has been 
broken in between the specialist of the method (knowledge engineers for knowledge 
elicitation, specialists of the method for induction and statistical analysis), human experts 
who have been interviewed or who have evaluated the learnt knowledge base and 
computer analysts. The figures have been produced and published by the customer in a 
more detailed comparative evaluation of the techniques for the purpose of building 
financial applications [Perray, 1990]. 
14 men days were required to build the system from scratch using KATE (from an 
existing database). KATE was ran three times and the background knowledge was 
slightly modified in between each run. For instance, in between the first two runs, the 
expert added some useful ratios such as the total revenue of the company divided by the number of employees. The resulting tree was tested using KATE's auto-consultation 
mode on a test set of 400 unseen examples. KATE made 70,3% perfect predictions (same 
response as the expert) and an additional 21,7% correct predictions (the right answer was 
suggested with the highest probability). 5% of the examples in the training set were 
classified with uncertainty through auto-consultation which demonstrates that the human 
expert has access to additional information or that there is some uncertainty in the domain. 
Note that, for this application, the frame structure of KATE is not useful. The data is 
represented by a single object whose slots correspond to ID3's attributes. This proves our 
claim that any applications that can be tackled using ID3 can also be tackled using an 
induction tool with a more elaborated representation language such as KATE's. 
5.3 Military Applications 
KATE has also been used for a military decision support system (this last application 
is classified). Let us simply state that, like for the tomato application, the training data is 
represented by complex objects and that a flat attribute-based representation would not 
have adequatly captured the information contained in the training data for this last 
application. A second application for helping a human expert to identify objects in a 2-D 
pictures has been being built. The database consists of 134 examples which belong to 14 
different classes. There are currently 12 types of objects and an average number of 58 
slots (8 relations, 9 integers, 41 nominals). The description of an example varies 
depending on the type of the objects involved. 
5.4 Failure analysis in a telecommunication network 
We have also used the method for an application which analyzes failures in a 
telecommunication network. The database of examples contains 160 examples with 7 
different faults described by a dozain features (mostly numerical). The fault analysis 
system was tested on 8 files with 200 unsen cases each. The 8th fault for which there was 
no examples in the training set, 75% saturated network, was in between two known 
faults (network is 50% saturated, and network is fully saturated). KATE outperformed an 
existing system by over 20%. Some faults, such as general overload, were identified with 
99% accuracy. 
6 From induction to case-based reasoning 
Consider the following database of examples: 
EXAMPLE DISEASE SYMPTOM YELLOWING(SPOT) SIYE(SPOT) 
Exl BOTRYTIS SPOT YES 18 
Ex2 OIDIUM SPOT NO 16 
Ex3 ALTERNARIOSE SPOT YES 2 KATE works in two stages: it first learns the decision tree, such as the one shown 
below, from the examples and then uses the tree (and not the examples) to predict the 
disease of a new plant. Consider what happens if the user does not know whether the 
symptom is yellowing or not. 
exist(spot) =yes 
no yes 
yellow[ng(spot)= ??? 
yesy no 
size(spot)=2 | frotryirs: exiH 
> ro/\. < 10 
oidium: exl | alternar lose; exaTH 
Figure 3: A consultation of the tree 
When consulting the expert system, he is first asked wether there is a spot. He 
answers "yes". He is then asked wether the spot is yellowing. He answers "unknown". 
KATE proceeds by following both the "yes" and "no" branches and combines the 
answers at the leaf nodes. In the "no" branch, it reaches the botrytis leaf node. In the 
"yes" branch, we reach a test node and the user is queried for the value of the size of the 
spot. He answers "2". The system follows the "<10" branch and reach the "alternariose" 
leaf node. It then combines the leaf nodes and concludes that it is "botrytis" with a 
probability of 0.5 and "alternariose" with a probability of 0.5. However, if we consider 
the example at the "botrytis" leaf node (ex2), we note that its size is 16 (which is greater 
than 10). Therefore, the correct conclusion should have been alternariose with a 
probability of 1 since the current case is much closer to ex3 than to ex2. Unfortunately, 
the information about the size of ex2 was generalized away during the induction phase 
and it is lost. 
The problem described above is not a consequence of a flaw in the induction algorithm 
nor is it consequence of a flaw in the decision tree formalism (we could have obtained the 
same conclusion if we had used production rules instead). It is a consequence of the fact 
that we are reasoning using some abstract knowledge (an abstraction of the information 
contained in the examples) instead of reasoning directly about the information contained 
in the training cases. The reasoning system uses general knowledge and not the actual 
examples. Note that this tree, or rules, could have been entered by hand instead of being 
derived from the examples by induction. It is therefore a flaw of the knowledge-based 
system approach to problem solving (reasoning about a problem using general 
knowledge). We argue that in order to provide a general solution to this problem, we 
need to use a case-based reasoning system instead of a knowledge based system. 
One might object that if we had the same configuration of unknown values in the 
training cases (for instance, a fourth example with unknown size), the conclusion would have been correct. Unfortunately, this is not realistic for many practical applications. 
Consider an application where we try to assist a user in identifying an object on a photo. 
We are dealing with three-dimensional objects from which we can only see one part (ex: 
the right side). Furthermore, parts of the characteristics of the object may be hidden by 
other objects which are on the first plan. What would be the size of the database training 
example if we wanted to enter all the configurations of unknown values which could 
possibly occur on such a picture? We were faced to this problem in practice. In the 
application we had to deal with, an object was described by an average of 58 
characteristics. There were 91 different classes of objects to identify. Each characteristic 
of the object could be hidden in any combinations. Therefore, there are: 
Cf = 3306 
combinations of unknown values per class (i.e. a characteristic is known versus it is 
unknown). In order to provide an exhaustive database of examples we must enter at least 
3306 for each class of object to identify which comes to a total of 3306*91= 300846 
examples. This is clearly not very practical and it is much more efficient, in this 
application, to enter only 91 prototypical cases (reconstructed from several photos) and to 
index them dynamically in order to retrieve the cases which best match the current picture. 
As pointed out in [Bareiss, 1989] "case-based reasoning is the usual name given to 
problem solving methods which make use of specific past experiences rather than a 
corpus of general knowledge. The key distinction between case-based methods and other 
forms of automated reasoning is that in the former, a new problem is solved by 
recognizing its similarities to a specific known problem then transferring the solution of 
the known problem to the new one (...) In contrast, other methods of problem solving 
derive a solution either from a general characterization of a group of problems or by 
search through a still more general body of knowledge". We believe that this key 
distinction is essential to understand the fundamental differences between induction and 
case-based reasoning. The two technologies are often confused and some induction 
products appearing on the market are presented with the label "case-based reasoning tool" 
(for example, the REMIND™ product of Cognitive Systems inc.). In addition, in some 
implementations, the distinction is fuzzy: some researchers working on induction have 
developed systems which remember the training cases in order to be incremental such as 
ID5 (Utgoff 1988), and some researchers working on case-based reasoning have 
developed tools which build indexes into the case library in order to be more efficient 
(ex: Leibowicz's UNIMEM). Thus, the distinction is not technical but lies in how the 
technology is used. 
In order to provide a solution to problems such as photo-interpretation, we have made 
some extensions to KATE which allows it to do some form of case-based reasoning. 
These extensions are part of the CAS-SYSTEM tool. The distinction between the two 
products is not the underlying technology (both use information metrics to find the most 
discriminant characteristic) but how it is used. KATE builds a static decision tree and 
uses the tree for consultation. CAS-SYSTEM reasons from the case library and dynamically builds a path which corresponds to the current case. At the root node, it 
computes information gain for all the features. It then ask the user for the value of the 
most discriminant one. If the user answers, it develop the corresponding branch. 
However, if the answer is unknown, then the next best test is tried until the user is able to 
answer.There are some links between the two technologies. CAS-SYSTEM (case-based 
reasoning) performs better than KATE when there are unknown values during 
consultation. It is incremental and new cases can be added without having to reconstruct a 
decision tree like in KATE. On the other hand, KATE is able to extract knowledge from 
the case library. It can detect inconsistencies in the case library (a leaf node covers two 
different classes with some probabilities attached to the classes), it can be used by the 
expert to notice that the case representation should be modified (add a feature to an object) 
and so on. In addition, KATE computes the tree once and for all and the consultation is 
much faster than with CAS-SYSTEM. Thus, KATE and CAS-SYSTEM are not 
redundant but complement each other. For some applications it is better to use induction, 
for some application it is better to use case-based reasoning. 
7 Conclusions 
KATE is an extension of known induction techniques which works on databases 
modelled by complex entities. The technical extensions that were brought into ID3 
(dynamic generation of candidate nodes when building the tree, treating differently a 
ground property and a relation for the information gain computation, testing for the 
existence of an object etc.) are independent of the chosen knowledge representation 
language (frames). With predicate calculus, the same technical problems and more would 
have to be overcomed. The frame structure allows for efficient retrieval of all the relevant 
features of an object in a given context and the legal range of values for these features. 
This knowledge is used during induction to dynamically generate candidate decision 
nodes and their branches before computing information gain. 
The induction system can handle large databases of examples, handle complex data 
described by several objects with relations, learn generalizations of all the concepts in a 
single learning cycle and is resilient to noise in the training data. It has been validated for 
building complex, large scale, real world applications in agronomy (diagnostic system), 
banking (risk evaluation), military (decision support systems) and telecommunications 
(failure analysis). However, the induction methology ("generalize and forget") was found 
to be too restrictive in some applications. We then developped a case-based reasoning 
tool, CAS-SYSTEM, which uses the same basic technology but reasons directly on the 
database of cases instead of reasoning on the decision tree which was induced from the 
database. Our case-based reasoning approach is currently being tested at the Museum of 
Natural History in a system which identifies marine sponges. 
Acknowledgements 
We would like to thank ACKNOWLEDGE for allowing us to publish this paper. Michel MANAGO 
would also like to thank the European Economic Community which supported its PhD research (ESPRIT 
contract P1063, the INSTIL project with the General Electric Company, CNRS and Cognitech). ACKNOWLEDGE would like to thank all the organisms which supports its current research: ANVAR, 
MRT, ANRT and DRRT. KATE is a trademark of Michel MANAGO. 
References 
Bareis R. (1989). Exemplar-Based Knowledge Acquisition: A Unified Approach to 
Concept Representation, Classification, and Learning. Academic Press. 
INSTIL (1989). Project Summary. ESPRIT Deliverable. Brussel, Belgium: Commission 
of Economic Communities. 
Manago, M. (1986). Object-oriented Generalization: A Tool for Improving Knowledge 
Based Systems. Proceedings of the International Meeting on Advances in Learning, Les 
Arcs, France. 
Manago, M. & Kodratoff, Y. (1987). Noise and Knowledge Acquisition, Proceedings of 
the Tenth. International Joint Conference on Artificial Intelligence (pp. 348-354). Milan, 
Italy .-.Morgan Kaufmann. 
Manago, M. (1988). Integration de Techniques Numeriques et Symboliques en 
Apprentissage Automatique. PhD dissertation, University of Orsay, France. 
Manago, M. & Kodratoff, Y. (1990). Induction of Decision Trees from from Complex 
Structured Data. In G. Piateski-Shapiro & W. Frawley (Eds.), Knowledge Discovery in 
Databases. Detroit, Mi: AAAI Press. 
Michie, D. (1989). New Commercial Opportunities Using Information Technology. 
Proceedings of the third. International Gi-Kongress (pp. 64-71). Munich, West 
Germany: Springer Verlag. 
Minsky, M. (1975). "A framework for representing knowledge," in The Psychology of 
Computer Vision, Winston P. H. ed, Mc Graw-Hill, New York 1975. 
Nilsson, N. (1980). Principles of Artificial Intelligence. San Matteo, CA: Morgan 
Kaufmann. 
Perray, M. (1990). Etude Comparative Entre Trois Techniques d Acquisition des 
Connaissances: Interview, Induction et Analyse Statistique pour Construire une Meme 
Base de Connaissances. Proceedings, of the Journies Informatique et Intelligence 
Artificielle. Paris, France. 
Quinlan, J. R. (1983). Learning efficient classification procedures and their application to 
chess end games. In R. S. Michalski, J. G. Carbonell & T. M. Mitchell (Eds), Machine 
Learning: An Artificial Intelligence Approach (Vol. 1). San Matteo, CA: Morgan 
Kaufmann. Quinlan, J. R. (1986). Simplifying decision trees. Proceedings, of the first AAAI 
Workshop on Knowledge Acquisition for Knowledge Based Systems (pp. 36.0 -
36.15). Banff, Canada. 
Quinlan, J. R., (1987). Generating Production Rules from Decision Trees, Proceedings 
of the Tenth. International Joint Conference on Artificial Intelligence (pp. 304-307). 
Milan, Italy:.Morgan Kaufmann. 
Quinlan, J. R. (1989). Unknown Attribute Values in Induction. Proceedings, of the Sixtl 
International Workshop on Machine Learning (pp. 164-168). Ithaca, NY: Morgan-
Kaufmann. 
Utgoff, P. (1988). ID5 : An Incremental ID3. In Proceedings of the fifth International 
Conference on Machine Learning. Irvine, CA: Morgan Kaufmann. Facts, fantasies and frameworks: the design 
of a knowledge acquisition workbench 
Nigel Shadbolt 
Artificial Intelligence Group 
Department of Psychology 
University of Nottingham 
Nottingham NG7 2RD 
Abstract This paper presents a number of core issues that are seen as fundamental to 
the success and well-being of the knowledge engineering enterprise. In particular, it examines 
the problems that dominate the current state of the art in knowledge acquisition (KA). These 
include: the development of KA methodologies, the construction of software support tools for 
KA, the integration of knowledge acquired from various sources, the problems of verification 
and validation of the acquired knowledge, and the elimination of bias from expert knowledge. 
Progress in our subject is likely to depend on our ability to advance our own knowledge and 
understanding in these key areas. The paper describes work which we have been conducting in 
the context of an ESPRIT project (P2576 ACKnowledge), and whose aim is to ameliorate some 
of the problems identified. 
Section 1 Introduction 
Our ability to build effective knowledge based systems is less than we would wish. Our 
understanding of many aspects of our subject is incomplete and doubtful. This does not prevent 
some people from claiming too much — distinguishing fact and fantasy in knowledge engineering 
can sometimes be a tricky business. The structure of this paper is based around a set assertions 
which I believe to be currently true of knowledge acquisition, and which I shall now enumerate. 
1. Knowledge acquisition is difficult 
2. We have only the beginnings of a KA methodology 
3. There is little reusability of our knowledge in experts systems 
4. Few integrated knowledge acquisition environments exist 
5. There is little synergy between acquisition techniques 
6. It is difficult to translate between the results of different KA tools 
7. It is hard to integrate the results of different acquisition sessions 
8. It is difficult to verify and validate the results of acquisition 
9. The acquisition techniques are sometimes inappropriately applied 
10. Experts and their cognitive processes are poorly understood 
I do not pretend that these are the only pressing issues that are relevant to knowledge 
acquisition. However, I consider progress on these issues vital if we are to develop knowledge 
acquisition into a stable component of an engineering discipline. 
The outline of this paper will therefore be as follows. In section 2 I will examine the stale 
of the art with regard to methodologies in KA. I will also discuss the emergence of model based 
approaches to the KA process. In section 3 and throughout I will describe our own attempts 
to produce an integrated workbench to support the knowledge engineer in the process of KA. Separate sections have been given over to the discussion of knowledge transformation between 
tools, section 4, and the problems of the integration of results from disparate sources, section 5. 
Section 6 addresses the problem of evaluating and validating the results of KA. Finally sections 
7 and 8 consider the problems of acquisition inherent in our techniques and our subjects. 
Throughout I will refer to our own experience in building knowledge acquisition workbenches 
as part of a large ESPRIT project ACKnowIcdgc1. The philosophy of die ACKnowlcdgc approach 
is discussed in Shadboll and Wiclinga (1990). In particular, discussion will focus around a system 
ProioKEW2, which wc have developed at the University of Nottingham and which was built 
originally as one of a number of concept demonstrators for the ACKnowlcdgc project. 
Section 2 Knowledge Acquisition: Methodologies, 
models and frameworks 
Many of the problems enumerated in section 1 can be traced back to the lack of robust, 
comprehensive methodologies for the KA process. Whilst there arc a growing number of articles 
and books available on 'how to do knowledge elicitation', these often contain advice of the most 
general kind, and emphasise the pragmatic considerations of expert system development (cf for 
example. Wclbank, 1983; Hoffman. 1987; Kidd, 1987 and Hart, 1986). 
One of die most thorough attempts to provide a comprehensive KA framework is provided by 
KADS - Knowledge Acquisition and Domain Structuring (sec Breuker and Wiclinga, 1987 for an 
overview). KADS embodies various principles for the acquisition of knowledge and construction 
of expert systems. These include two central tenets: 
The analysis should be model-driven as early as possible. 
The content of the model should be expressed at die cpisiemological level. 
The first of these requires that one should bring to bear a model of how the knowledge is 
structured early on in the KA process, and use it to interpret subsequent data. The notion of 
model wc arc using is some abstraction of expert competence or performance. Indeed the view 
now predominant is that KA is itself a modelling activity. KA is best regarded as a constructive 
process — data about the expert's (or experts') behaviour arc used by the knowledge engineer, 
along with other information, to construct a model, or an increasingly complex scries of models 
embodying the desired behaviour of die system. These models can later be reused to provide 
expectations regarding the general structure of problem solving systems. 
The second insists that knowledge is formulated in terms of its role and function in problem 
solving. This formulation should be independent of particular implementation issues. Moreover, 
existing models of problem solving should be used, wherever possible, to organise the expertise. 
The KADS use of models relics on distinguishing four knowledge layers which should be 
discriminated in any expert system - human or otherwise. The first of these is domain knowledge 
and describes the domain concepts, elements and their relations to one another. A second type 
is task knowledge, which has to do with how goals and sub-goals, tasks and sub-tasks should be 
performed in any problem solving. A third sort of knowledge is referred to as strategic. This is 
used to monitor and control problem solving. Finally, inference layer knowledge is distinguished. 
1 The ACKnowlcdgc consortium comprises: Cap Scsa Innovation, Marconi Command and Control Systems, GEC-Marconi 
Research Centre, Telefonica, Compuias Expert Systems, Veritas Research, the University of Amsterdam, Sintcf, and the University 
of Nottingham. 
2 Elsewhere wc have described prototype versions of our knowledge engineering workbench (Rcichgclt and Shadboll, 1991, In 
Press). This work was carried out by the author in conjunction with Han Rcichgclt and Nigel Major. Thanks arc also due to GEC 
MRC, the University of Amsterdam and UK University of Aberdeen who made available tools which served to inspire some of the 
components of ProioKEW. This has to do with how the components of problem solving and expertise are to be organised 
and used in the overall system. Within each KADS layer various structures or models can exist 
that generate expectations about die nature and form of die knowledge instantiating that layer. 
The use of knowledge level models to inform and direct the construction of KBSs is central. 
In fact, a number of methodologies use models although die emphases are somewhat distinct 
(Steels, 1990; Karbach at al 1990). Nevertheless, common to all is the attempt to abstract aspects 
of expertise. 
The KADS approach itself makes particular use of models of problem solving - or models 
at the inference layer (Breuker and Wielinga, 1989). An alternative approach is to build task 
layer representations (Bylander and Chandrasekaran, 1988). Here the aim is to use general, 
invariant features of a task to guide KA. A number of researchers exploit the fact that many 
KBS applications are built in similar, if not nearly identical domains (Eshelman 1989; Marcus, 
1988). In this case one attempts to build reusable domain layer descriptions. Ultimately models 
and structures at all of these levels will be important in any informed and directed knowledge 
acquisition approach. 
The use of mediating models to direct acquisition promises an additional advantage. It offers 
die prospect of reusability (Neches at al 1991). The aim would be to standardise on libraries 
of generic components — components derived in part from the models developed in KA. It is 
argued that in this way declarative knowledge, problem solving techniques and reasoning could 
all be shared among systems. As Ncche et al point out mere are currently a number of critical 
impediments to reuse: heterogeneous representations, dialects within language families, lack of 
communication facilities, and model mismatches at the knowledge level. Nevertheless, initiatives 
are underway to ameliorate these problems and produce effective reuse. 
Section 3 Knowledge engineering workbenches 
A variety of systems now provide computer support for KA. There are software products that 
implement specific acquisition techniques. An example is the implementation of the repertory grid 
technique (cf, for example, Boose et al, 1989) which we will discuss later in this paper. It is time 
consuming and labourious to apply manually, but die underlying algorithms arc straightforward. 
Many other individual KA techniques have also been implemented. As implementations of 
individual techniques any one of these products is limited in scope to the elicitation of certain 
types of knowledge. 
A second type of system, exploits die fact that common features can sometimes be found 
both within and between domains. OPAL (Musen et al, 1987) is an expectation driven acquisition 
system that elicits knowledge from experts in die domain of oncology treatment. The system 
comprises templates, or knowledge frames, which the user fills with appropriate knowledge. Such 
frames might contain, for example, the details of a particular drug treatment. These details can 
be expected to instantiate slots in frames which have a similar structure across treatment regimes. 
The MORE system (Kahn et al, 1986) is an abstraction from an original system MUD which 
diagnoses oil drilling fluid problems. As such it incorporates templates about expected domain 
components as well as the task structure and problems solving strategies that are general to all 
particular instances of such applications. In both OPAL and MORE an attempt has been made 
to reuse models of expert knowledge. 
The disadvantage of these tools is die large amount of effort that needs to be spent building 
customised acquisition tools for generic application domains. However, it is an important point to discover whether associate domains really share the amounts of common structure required to 
make the customised tools approach viable. 
A further class of support tools provide a set of acquisition functions within a single system. 
Examples of such software include Shelley (Anjcwicrden et al, 1990) and KEATS (Motta, Rajan, 
Dominguc and Eiscnstadt, 1990). These combine a number of documentation and browsing aids 
to help knowledge engineers find their way around acquisition material. They also include a few 
particular clicitntion methods. What they lack, is any significant degree of integration between 
the components, and any strong view as to the type of knowledge and knowledge level structures 
that might underlie an application. 
Wc can sec that each of these various types of support system is restricted in scope. The 
aim of the ACKnowlcdgc project is to achieve integration between a wide range of acquisition 
techniques, and to combine die best features of current support tools into a knowledge engineering 
workbench (KEW). KEW will implement a range of elicitation techniques, incorporate machine 
learning methods, be applicable across domains, and embody a principled or knowledgablc 
approach to the entire acquisition process. To build such a workbench requires that we construct, 
in effect, a knowledge-based system for knowledge acquisition (Shadbolt & Wiclinga, 1990). 
To help us design KEW wc have built a number of concept demonstrators. One such concept 
demonstrator, ProtoKEW, comprises: a number of KE and ML acquisition techniques, methods of 
translating results from tool dependent formats into a common knowledge representation language, 
techniques to support die integration and evaluation of disparate knowledge, and model directed 
KA. 
The best way to appreciate ProtoKEW is to consider its architecture as shown in Figure 
1 below. Wc can sec that a number of distinct KE tools arc incorporated. These include: 
laddering, concept sorting, and repertory grids3. Elsewhere wc describe these techniques in detail 
(Shadbolt and Burton, 1990) and their implementation in ProtoKEW is described in Rcichgclt and 
Shadbolt (1991, In Press). In addition to KE methods a number of machine learning algorithms 
arc supported: AQ11. ID and CN2. All these machine learning methods arc similarity based 
induction techniques, and provide for the automatic construction of classifier rules. 
Figure 1 The architecture of ProtoKEW 
laddering cud tort 
trauform inuufcrm 
truttformed knowledge bate* 
:oKp»*d knowledge batei 
3 More conventional KH techniques such as interviewing and protocol analysis have not been included in this demonstrator 
although they will be found in the final ACKnowlcdgc workbench. Individual KA tool results are transformed into distinct knowledge base "partitions" within 
what is termed the Common Knowledge Base (CKB). The representation within the CKB is full 
first order predicate calculus (FOPC). The CKB has an attendant theorem prover (die interpreter) 
for die logic. The various transformed knowledge bases can be integrated within our FOPC 
formalism to provide what we term "integrated knowledge bases". This process of integration 
can proceed as far as the knowledge engineer wishes. The various CKB partitions (integrated or 
not) can be viewed as logical theories capturing aspects of the domain of application. A theorem 
proving capability allows us to evaluate components of die emerging knowledge base. Of course, 
a FOPC inference engine is unlikely to be the final run-time implementation. However, this 
logic based formulation can be used to produce a high level specification of the ultimate system. 
As such it allows us to investigate the scope, coverage and consistency of the knowledge being 
acquired. 
Finally, in Figure 1 die Knowledge Engineering Knowledge Base (KEKB) is distinguished. 
This embodies the knowledge about knowledge acquisition which is needed if our workbench 
is to be an expert system for acquisition. Any software system that is going to fulfill this goal 
will need to be informed by a variety of types of knowledge. A first type of knowledge has to 
do with knowledge about the knowledge acquisition process itself. This would be advice and 
guidance about what to do when. Knowledge about die knowledge acquisition tools themselves 
constitutes a second type of knowledge. Thus, tools make assumptions about data, how it is 
represented and analysed. The system and knowledge engineer should have explicit command 
and use of this knowledge. A further type of knowledge is an account of how to integrate 
results acquired from different tools into a consistent evolving application knowledge base. The 
emerging application KB needs to be evaluated. Our system needs to know something about 
how this might be achieved. And, of course, we should attempt to incorporate knowledge about 
the components of expertise. 
In order to provide a context for the discussion of die general KA issues highlighted by 
ProtoKEW exemplars of a KE and ML method within ProtoKEW will be described — the 
laddered grid and CN2. 
Our implementation of the laddered grid method (Major and Reichgelt, 1990) supports die 
expert and the knowledge engineer in the construction of a graphical representation of application 
knowledge in terms of the relations between knowledge elements. The result is a two-dimensional 
graph (or conceptual map) whose nodes are connected by labelled arcs. A number of settings 
on die tool allow for different interaction protocols which expand and explore the graph in ways 
that have been discovered to be natural and productive (Burton et al, 1987, 1988, Shadbolt and 
Burton 1989). 
Figure 2 shows part of a laddering session from the domain of respiratory tract diseases. 
In ProtoKEW the tool uses a simple object-oriented language called CommonSloop (Reichgelt, 
Major and Jackson, 1990) as its tool-specific knowledge representation language. Each node in die 
laddered hierarchy is represented as an object in CommonSloop. An object can have user-defined 
slots which are properties of the item in the node hierarchy. CommonSloop supports multiple 
inheritance. Inheritance also follows a default principle; attributes associated with higher-level 
objects are inherited by their children but can be overridden lower in the hierarchy. Figure 2 Laddering in the respiratory tract diseases domain 
[ Optt»»« ") 
)T#nTt*«^%a^TrTov*^3^o^ecta^r 
I Do yOu t ,nl 13 (J'«V«nUU« b«lv«»n 11 »o"y lococCl I _po«v/»on l • tod pnauaococcal_pnau»onta (V/M) n 
ta d'f'a*-ariti»ta b*ti*«*n atapr-y locoeca I. pr>«<j»on t a and laQlonnalrea.dlaeaa* (Y/N) n 
O d • Macecit >a ta battwaan p«av*»>ococca 1_pr»«\j»©r> la »n<j l*jiomtirti.dlitiu (V/N) n 
9 »«f <f we can attribute* 'or cact«r<t>.pn»v»oni«9. 
il.pntuioniii ntnar than It* auO-c !•»»«» • '•lancet? (V/N) n 
. |M-tutM«.'» 
ALLCJGY H4y_rr/f» 
\ 
•\> COM) lEGNWN*IRCS_OISEASf. 
PNeiW0C0CCAL_PNCl*«NU 
—» G« AM-HE GAUVt. PNEUMONIA 
\ ST4PMyi0C0CCAUPNtlN(WIA 
PNflXOCYSTIS.PHClHONlA 
grun-nso«l 1 ve.pnauftonI• : 
in partition rtda 
cougn.present - yvs 
culture.taat - pa*uoa*onaa_aeruglnoaa 
attoc.fever - high 
tiiiml : unotf 
fro»: rtdt 
claae - claet 
self i gram-mgat i ve_pneui>onia 
super i bacterial.pneuaomas 
The most suitable internal representations for a laddering tool arc structured object languages. 
In such representations wc aim to bring together, under a simple indexing method, die associated 
properties of objects. The use of inheritance allows us to exploit taxonomic relations in an 
efficient way. Specialising objects by overwriting or adding attributes and their values provides 
for a means of discriminating between objects. These features of structured object representations 
clearly map onto the characteristics of die laddering technique itself. In this way different 
representations afford different possibilities for die various types of KA tool. 
Our second exemplar technique, die CN algorithm (Clark and Niblctt, 1989) was developed 
to address certain problems inherent in the widely known AQ algorithm (Michalski, 1969). Both 
algorithms operate by generating what is termed a complex. This is a conjunct of attribute tests 
— for example temperature = high Sc cough = present. The complex covers, or is satisfied by, 
a subset of the training examples. This complex forms die condition part of a production rule 
"if condition then predict class", where class is the most common class in the training examples 
which satisfy the condition. 
In brief, the search method in CN proceeds by repeatedly specialising candidate examples 
until one is located which covers a large number of examples of single class, and a few other 
rules which cover a few of the other classes. CN is more robust than AQ since it can tolerate 
noise in the domain — this can arise where an example has been misclassificd or perhaps an 
atuibutc has no known value or else an incorrect one. Figure 3 A rule set induced by CN for respiratory diseases 
| UN-ORDERED RULE LIST | 
IF breathlessness = discontinuous 
AND heart-state = normal 
THEN diagnosis = asthma [IS 0 0 0] 
IF braathlessness = continuous 
AND Hps-shape = normal 
THEN diagnosis = coad [0 12 0 0] 
IF heart-state = enlarged 
THEN diagnosis = coad [0 9 0 0] 
IF Hps-shape = pursed 
THEN diagnosis = emphysema [0 0 3 0] 
I (DEFAULT) diagnosis = asthma [IS 15 3 0] 
1 CN> Execute 
EVAL> all 
Executing rules... 
PREDICTED 
ACTUAL asthma coad emphyse cor.pul Accuracy 
asthma 15 0 0 0 100.0 X 
coad 0 15 0 0 100.0 X 
emphysem 0 0 3 0 100.0 I 
cor.pulm 0 0 0 0 — 
jOverall accuracy: 100.0 X 
j Default accuracy: 45.5 X 
Figure 3 shows a run of CN on a set of cases from ihe respiratory diseases domain. The 
examples are described in terms of attribute value pairs with one attribute regarded as the assumed 
outcome class. In die situation above we have determined that we are interested in generating 
rules which discriminate cases in terms of their diagnostic category. The algorithm is run and a 
set of rules produced. Figure 3 also shows die performance of these rules on the training set. 
As a stand alone tool CN has limited utility for the knowledge engineer engaged in extensive 
KA. However, by combuiing die technique with other simple KA methods we obtain synergy 
— combinations of methods provide greater benefits than the tools used in isolation. Illustrative 
is the example given in Shadbolt (1991) in which preprocessing of cases is carried out using 
a sorting method — the expert decides which attributes in a case description are relevant for 
a particular outcome class. Rapid re-presentation of rules back to an expert for critiquing has 
proved to be very useful. This evaluation can suggest ways in which die case description is 
deficient. This may leading, in turn, to the use of techniques such as repertory grids to obtain 
more discriminating case descriptions. 
The interaction of KA tools in this way will provide the foundations for effective synergy 
between components of KA workbenches. However, to date relatively hide work has focused 
on exploring the range of possible synergies between tools. 
Section 4 Knowledge Interchange and Transformation 
In order to construct expert systems out of die partial results of individual KA tools we need 
a means to integrate the results together. Before this can proceed, however, we must have some 
way of pooling the knowledge accrued within an acquisition workbench. There are two distinct 
approaches that could be taken to this problem. These arc indirect and direct transformations. 
Indirect transformation achieves communication through a common knowledge representation 
language (CKRL). Each KA tool, represented as a circle in Figure 4, has a translator into and out 
of the CKRL. The direct method allows for pairwisc translation between all KA tools. Whilst 
direct translation is viable for small number of KA tools, beyond four or so the direct approach leads to an explosion of translation possibilities. For each additional tool added using the indirect 
method only two additional translations arc required. 
In ProtoKEW wc have adopted the indirect transformation approach. As stated our CKRL is 
the first order predicate calculus (FOPC). In addition to expressive power such a logic has a modci-
thcorctic semantics which allows one to determine the correctness (soundness) of the interpreter. 
Moreover, in the present context, the use of logic as die CKRL has die added advantage that 
it makes the problem of transformation and integration scmantically tractable, allowing us to 
address the issue of whether the knowledge remains valid under die effects of transformation. 
The process of transformation itself wc have come to see as an important additional stage 
in die acquisition and refinement of knowledge. The tools wc build to support transformation 
arc interactive in nature. Consider the knowledge base produced by die laddering tool in Figure 
2. The tool specific knowledge base will contain a number of frames with different slots. Each 
frame cither stands for a single object in die domain, or a class of such objects. What is the 
logical translation of such a frame structure? It is fairly clear that concepts such as COAD or 
PNEUMONIAS arc classes and as such arc translated into FOPC as one-place predicate. But arc 
the leaf-nodes in such structures intended to be an individual object or a class of objects? The 
user is asked for each leaf node what the intended interpretation is. In the case that die intended 
interpretation is that the leaf nodes arc individuals then they arc translated into constants. Class 
leaf nodes arc assumed to be non-empty, and wc add axioms to the effect that there exists some 
object with the property denoted by die class-frame name. 
Some parts of the translation can be automatic. The class superclass relation between two 
"predicate" frames p and y is translated as ((Vr )(/;(*) — q{x))). Slots can normally be translated 
automatically: a slot corresponds to a two-place predicate. The exact translation of a slot and its 
filler depends on the type of filler. In most cases translation is straightforward4. A slot s with value 
v is translated as (s(a. v)) in the case of a "constant" frame, and as ((Vx)(p(x) —* s(x, v)))in 
the case of a predicate frame p. 
In default inheritance the translation algorithm first determines whether any of the 
descendants of the frame have an incompatible slot value. If so, these are ex­
plicitly stored as exceptions in the antecedent of a rule. Thus the expression 
((Vx)((bactcrialpnc union ia(x)k,-^lcy ionnai res ({i$casc(x)) — gram stain(x, positive))) 
This is not the end of the translation process. Additional interaction with the user can 
serve to further elucidate the intended semantics of Figure 2. Consider die class VIRUS in 
Figure 2, is it intended that the three children of this concept form an exhaustive list? The 
4 Full details of this and oilier translations can be found in Rcichgclt and Shadbolt (In Press). Figure A Knowledge transformation methods 
INDlRfiCT DIRECT answer, of course, very much depends on die context of acquisition, the scope of the elicitation 
sessions, and the coverage of the intended system. In any event if the answer is affirmative, 
our translation algorithm adds an axiom to die effect that any entity that is an instance of 
die parent frame must be an instance of at least one of the children — ((Vx)(viru$(x) —• 
(influenza(x) V laryngiiis(x) V common — cold(x)))). 
During transformadon implicit assumptions often inherent in the context of the local knowl­
edge acquisition task have to be made explicit. Establishing the wider context in which the 
knowledge is to be used is an important stage in acquiring and validating knowledge. 
The declarative translation of a structure such as that shown in Figure 2, along with all 
its slot and filler information is very substantial. Such a translation into FOPC affords certain 
advantages — not least of which is that die notion of semantic consistency can be examined. 
However, the original frame notation facilitates other sorts of computation. For example, it is 
very easy within the structured object representation to determine if objects can be discriminated 
in terms of their attributes and values, whether the attributes on an object have been specified to 
a sufficient level of abstraction in the hierarchy. The lesson is that a variety of representations 
enrich and extend the acquisition possibilities. 
The issue of knowledge traiisformation and interchange within and between acquisition 
workbenches is, of course, a large and complex one. Ginsberg (1991) has argued that knowledge 
interchange between complex formalisms is fraught with dangers. One of which is the production 
of semantically opaque translation programs. His own proposals are that we effect interchange 
via a minimalist "first order predicate calculus, with specific notationai conventions to handle 
variables, quantification, and so on". These ideas are close to our own. He also argues that there 
remains an open research question about whether substantial knowledge sharing is possible at all. 
Section 5 Knowledge Integration 
Once any KA tool has produced its results and they have been subject to either direct or 
indirect transformation there remains the issue of knowledge integration. How are wc to assemble 
fragments of expertise into a consistent and evolving knowledge base? 
Within ProtoKEW we have a very simple but appealing notion of integration — the merging 
of logical theories. In ProtoKEW, the CKB contains a partitioning mechanism which allows one 
to divide logical axioms into different partitions. In logical terms, each partition corresponds 
to a different theory. While the system attempts to ensure consistency within a single partition, 
it does not ensure consistency between partitions. The partitioning mechanism therefore allows 
users to explore different theories simultaneously. 
Integration of two partitions is achieved by copying one partition into a new partition, and 
then incrementally trying to add the propositions from die other original partition. While adding 
these new propositions two tests can be performed. We can ask the system to determine if the 
new propositions are non-redundant and consistent. The redundancy test determines whether a 
proposition is already entailed by die existing propositions in die partition. If a proposition fails 
this test, then it is not added to the knowledge base. The consistency test involves determining 
whether the proposition is consistent with die other propositions stored in the partition. Can the 
system prove its negation from the information already in die knowledge base? If a proposition 
fails this test, then a reason maintenance system (McAllester, 1980) is called on to ensure consistency. If both tests arc performed successfully, then the user is guaranteed that die set 
of propositions is minimal and consistent, at least within the limitations of die theorem provcr5. 
In Figure 5 below wc can sec that two partitions in the respiratory diseases domain arc being 
integrated. One partition called cases contains simple facts about a patient: the fact that he is 
currently diagnosed as suffering from cor pulmonale, and the fact that his heart state is normal. 
In a second partition diseases wc sec knowledge to the effect that all individuals who suffer from 
cor pulmonale would have an enlarged heart state, and that a heart state cannot both be normal 
and enlarged. Attempting to integrate these two partitions has resulted in the RMS being invoked 
with the inconsistent set of propositions. 
Figure 5 Integration of knowledge in the respiratory diseases domain 
[TJ (Acquit' How taola Tp tK«OMledfl« bast top I, »)) ( Htla ~~) t Quit ~1 
[ 9»M 1 
•^rrxxj oarl'lion to i"t*yi'.a d-i»a«a*a 
I fremiti (r>« InW^ritvd carltl'on Da Ctll»<P «ay 
ESSES 
[l in M""U»I] [ Shm. C»T") [ihfmt aartuiaw) [Show ffMUtli ] [ihtm f»r 1111 m/prnmn) [ Quit ) 
• i«ti' lf.»i'.v>: r'tdiut Tha fallovtaa araaaatttana 1 
((•r.p.lMiiili klll.utilh) with •••part ttatu* m 
((•II B) (•> (h»e«M_,tata * normal) (- (•••rt.tlal* * tnlargt*)))) with Miaaart •l«tn» 
(haart.atata a»H_a»Mh aaraal) with tuaaart itataa In 
(lall al li> <c«r.»a laxMta la a) (naart.atata • aalarga*))) wit a aupfort atatat in 
t " 1 
i (*.» :-:v.pu>or.» 1# 
i < i-> C-.car'. *ta'.t x ^u»"^al 
:cct ilil.i: •» 
ltt'lcit'0": pram I fca rt_»'.j»a « « 
i-1 HiU - Wa'jaoi))) 
The expert and knowledge engineer decide what piccc(s) of knowledge is suspect. In this 
case wc sec that in Figure 6 the general rule that any sufferer of cor pulmonale has an enlarged 
heart has been retracted. Notice, that in our implementation wc have acquired new information in 
this process. Wc now believe dial at least some patients with cor pulmonale will not have enlarged 
hearts. Moreover, wc have a justification of why this piece of knowledge is now believed — 
namely that a counter example has been discovered. 
5 It is in general impossible to determine automatically whether a set of propositions in classical first-order predicate calculus is 
consistent. Any theorem provcr needs to contain hcunsucs to hall the search for a proof. Because these heuristics may terminate the 
search for a proof loo early wc can never guarantee consistency for any set of propositions in KB. Figure 6 Maintaining consistency in the respiratory diseases domain 
[list eartltlentj [Shaw CM"") [Show nrtll1on| [Show preal cite] (Shew p«^miow/»rt<tcTtiQ f 
(hurLstiU bUI.Mlth normal) 
support-statue: in 
Juatif Icatlon: premise 
(cor.pu)mona1« bM1_j»»(th) 
support-status: In 
Justification: promise 
(~ ((all x) (=> (cor.pulownale x) (heart.atate M enlarged)))) 
aupport-atatua: In 
justification: (((heert.state b !»_*»< th normal) ((all K) (=> (heart_state i 
C (heart_»tatt H enlarged)))) (cor.pulaiona le bi1 l.srntlh))) 
((all «) (=> (heart_stat« M norma 1) (- (heert.stete * enlarged)))) 
support-itatus: In 
JuattfIcatlon: premise 
I 
This type of integration ensures logical integrity. However, integration can mean a great 
deal more than this. In the type of integration described above we have no idea whether our 
integration provides better or more complete coverage of the domain application. There is no 
concept of knowledge integration at the knowledge level — the role and function of knowledge 
in problem solving. For a richer notion of integration we need to appeal to the idea of evaluating 
die consequences of integration with respect to a model of problem solving. 
Section 6 Evaluating and validating the results of acquisition 
One obvious way of evaluating a knowledge base is by presenting different propositions in 
die knowledge base to the expert. While this may in general allow one to detect certain false 
propositions, it cannot guarantee that we find all errors. Moreover, wc are not guaranteed to find 
missing pieces of knowledge. An alternative is to check die knowledge base against a number 
of test cases, and to see whether it provides appropriate solutions for these cases. Adoption of 
this approach leads to two further issues. First, one has to generate a comprehensive set of test 
cases. Generating a satisfactory set of test cases is itself a non-trivial knowledge acquisition 
task. Second, given inappropriate performance on a test case, the knowledge base needs to be 
refined. If the system is able to derive a false conclusion, tools must be provided for retracting 
whatever propositions were responsible for the conclusion, and, conversely, if the system fails 
to derive a true conclusion, tools must be provided to add the relevant knowledge. We have 
seen that ProtoKEW provides limited support for refinement via retraction of propositions using 
RMS capabilities. Refinement by addition of missing knowledge is largely supported in our 
ACKnowledge workbenches by appeal to die notion of model based validation. 
As discussed in section 3 we arc aiming to build an active and directive knowledge 
acquisition workbench. The evolution of such a system is a gradual process. Initially we 
have used die knowledge engineer as the main controller of KA activity. Ultimately we hope 
to encapsulate more and more knowledge about knowledge acquisition into our knowledge 
engineering workbenches. CurrcnUy in ProtoKEW we have a mixed initiative system with control 
very much in the hands of the knowledge engineer. The directive knowledge encapsulated in 
Iteiv.-Eihliofhek ProtoKEW is restricted to a number of compiled knowledge structures which can be used to 
direct acquisition activity. 
The type of structure that is available is illustrated in Figure 7 . 
Figure 7 Directive models in ProtoKEW 
The directive model under consideration is the interpretation model for heuristic classification 
(Clancey, 1985). In the current version of ProtoKEW wc assume that selection of this object 
has been made by the knowledge engineer from a library of such structures contained in the 
workbench. It serves as a candidate for die type of problem solving observed in the application 
domain6. The point to note about selection of this structure is dial it sets the context for subsequent 
acquisition. 
The directive model contains knowledge which can be used to inform acquisition. It shows 
us both the inputs and outputs (representing as rectangles) of various processes (representing as 
ellipses) that make up a generic type of problem solving. If we formulate our respiratory diseases 
example in terms of this model wc would note four kinds of I/O classes: obscrvablcs, findings, 
abstract solutions, and solutions. In our domain examples of knowledge that plays these roles 
in problem solving might be 
obscrvablcs — die patient has a respiratory rate of 25 
obscrvablcs — the patient lias a blue hue to the tongue 
findings — die patient has tachypnea 
findings — die patient has central cyanosis 
abstract solutions — die patient has COAD 
abstract solutions — die patient has pneumonia 
solutions — the patient has emphysema 
solutions — die patient has staphylococcal pneumonia 
In Shadbolt and Wiclinga (1990) wc describe how this selection might ultimately be automated. The processes that operate on these metaclasses are: abstract, match and refine. Each of these 
processes is in turn associated with a set of methods for effecting the change from input to 
output. The process abstract is defined as abstracting observables into findings; a method which 
can effect this is qualitative abstraction. Once again in our domain examples of knowledge that 
plays these roles might be: 
abstract — if the patient has a respiratory rate > 20 the patient has tachypnea 
abstract — haemoptysis is the presence of blood in die sputum 
match — if the patient has creamy yellow sputum then the patient may have staphylococcal 
pneumonia 
match — if the patient has cyanosis then the patient may have emphysema 
refine — emphysema is a kind of COAD 
refine — staphylococcal pneumonia is a kind of pneumonia 
Expectations about the types of knowledge that may be implicated in problem solving is 
precisely the sort of knowledge that can generate acquisition goals. In Figure 7 the abstract 
solution metaclass has been selected and this has activated knowledge in ProtoKEW. This in turn 
displays a knowledge acquisition goal tree in die window to the right of the directive model. 
This goal tree indicates how knowledge might be acquired about this part of the expertise space. 
The goal tree indicates that in this context we can use either laddering or a card sort to explore 
die structure of the solution space. It is this knowledge that might lead the knowledge engineer 
to select a particular acquisition tool for the task at hand. Knowledge acquired will be stored in 
partitions which reflect the structure shown in die directive model. 
As well as directing acquisition the directive model can serve as a template for the adequacy 
of the knowledge accrued. For example, we might well discover that we have knowledge sufficient 
to generate a wide class of data abstractions: 
FEV1/FVC ratio < 70% is obstructive 
peak expiratory flow rate = 500L/min is normal 
central cyanosis is a blue hue to the tongue 
But then discover that many of the findings are not used in die matching process — this may 
well suggest incomplete coverage in this part of the problem solving model. In this sense models 
could be used constandy in the refinement of knowledge bases. 
An obvious area of development in ProtoKEW is to increase the range and use of directive 
models. This has been done in die latest version of die KEW. Moreover, we have begun to 
operationalise the directive models. We are implementing a language which describes the structure 
of die directive models and serves as a means of invoking various KA tools to acquire certain 
types of knowledge, and organise partitions of knowledge to reflect the process of problem 
solving embodied in die model. 
This has obvious implications for validation. It enables the knowledge engineer to determine 
for parts of the overall problem solving process whether there is knowledge sufficient to solve 
the role assigned to it. Validation and evaluation can take place in terms of local components 
of expertise and reasoning. It is hoped that die extra principles of structure which the models 
impose will be useful in dealing with die complexity of any knowledge base. 
Section 7 Understanding acquisition techniques 
Although a reasonable number of acquisition techniques are now available there are a number 
of reasons to be worried about both the uptake and use of the techniques. To illustrate this point I will examine the use of the repertory grid method in KA. The repertory grid is widely used 
and has formed die basis of some influential KA environments. 
In this techniques an expert is given, or is asked to generate, a set of elements. They arc 
then usually presented with three elements, and asked to describe some way in which two arc 
similar or different from the third. Thus the expert might be presented with the disease elements, 
TB, lung cancer and asthma. The expert might judge that TB and lung cancer are similar in 
that they arc often associated with haemoptysis (blood in the sputum). The dimension by which 
these elements have been separated is called the construct. The ends of the construct arc called 
poles and the construct is taken to be measurable, that is wc assume that elements can be rated 
or ranked along die construct. 
This process is continued with different triads of elements until the expert can dunk of no 
further discriminating constructs. The result is a matrix relating elements and constructs. The 
matrix is analysed using a statistical technique called cluster analysis. This reveals clusters of 
elements and constructs which represent meaningful concepts. These concepts may not have 
been articulated in the original elicitation session. We are also able to subject the results of 
cluster analysis to die technique of entailment analysis (Gaines and Shaw, 1986) which generates 
implications between constructs. For example, wc might find that die pole smoking related of die 
construct smoking implicated implies dyspnea onset chronic on the construct onset of dyspnea. 
Figure 8 shows our repertory grid tool. 
Figure 8 The repertory grid tool in ProtoKEW 
_pneu*otnorei IA . m 0 5 0 
rent/dyspnea pereiele>nt 
i- a»ti ittrva* 
hrootc Orooth .t^aii/no wneere 
-nocturnal dyepnee/dyspnes anytime. 
_ dttvaeee/dleoroer 
-purulent spulm assoc/purulent sputum unessoc 
— dyspnea occupation rela ted/dyspnea occupation > 
_ dyspnea acute onset/dyspnea enrome onset 
""L sacking related/smofcinq unrelated 
naamnotyete acsoe/hiiaaootysis urvjeaoc 
Mil 
jijJJ 
* 3! ! 
i\ ij ;{ s 3j 2| lj & jKXtumal dysonatayoyaonea a"yl«ae 
_dy»P«e* acute on««t/Uy«un«a chronic and 
j»y*pnea recurrenVdyaooe* persistent 
_3y*»nea occuostlon relaUd/dyapnea occupation unrelated 
_->hea*e/r>o 
apwtua eeeoc/puruieAt eputue unassoc 
its assoc/^se»optysis unassoc 
e'*ted/«p«o* '"a unrelated 
oneufiot^gra* 
lung cancer 
pulmonary ant 
Whilst the method can reveal very interesting patterns and results there are a number of areas 
where bias and misinterpretation can occur. These include the selection of elements, constructs, 
and measures. Thus elements need to be of die same cpistcmological type. Ideally they should be 
representative for the purposes of elicitation. Wc have to take care lest die clement set chosen be 
skewed or unrepresentative in some way. This is a difficult problem and considerable knowledge 
of the domain may itself be required to select a suitable set. Constructs elicitation are no less unproblematic. Yorke (1978) for example, has found differences depending on whether triadic 
(sets of three elements) or dyadic (sets of two elements) are presented to the user in order to elicit 
a construct. The labels that are ascribed to constructs and their poles are also problematic. The 
knowledge engineer should be aware that these labels are likely to be highly polysemous. That 
is they convey for the expert all sorts of assumptions, associations, implications, caveats and the 
like. It is important that these meanings arc not lost or overlooked in the course of elicitation. 
There are also problems inherent in the notion of measuring semantic distance. Consider our 
example in Figure 8 above. Here we have a construct symptom onset — its poles are acute to 
chronic. In fact in the domain this can range in time from the symptom appearing over the past 
few hours to it having been present over months or years. What are we to do if an element has a 
wide range of values? Moreover, not all constructs are well suited to a continuous interpretation. 
Certain constructs have categorical values. These are more difficult to represent in continuous 
dimensions. Finally, when the grids are subject to cluster analysis the numerical techniques 
makes strong assumptions about the nature of semantic space. 
A fuller description of the problems inherent in this technique can be found in Rugg and 
Shadbolt (1991). We are not decrying these techniques. However, it is important to be aware of 
the limits of any single method. This is one reason why we believe in combining techniques. It 
might also be appropriate to consider reevaluating KA tools now that they are becoming widely 
deployed. The aims of such research ought to be to produce an understanding sensitive of the 
strengths and weaknesses of KA tools, techniques and methods in a variety of application contexts. 
Section 8 Understanding experts and expertise 
We have discussed a number of important issues central to KA. What of the experts 
themselves? Work in the US by Arthur D Little, and in die UK at the University of Nottingham 
has indicated the importance of expert differences. Kindle et al (1989) distinguish between: 
"academics", "drones" and "samurai". Although in any particular situation one is likely to find 
a mix of these types in any one individual expert. 
To see how these categories can be important we will consider an application context. 
Imagine we are building a system to assist in respiratory disease diagnosis. Our expertise will 
come from a local teaching hospital, in such contexts a hierarchy of "experts" is often evident. 
One can discern the "academic" who in this case might be the Professor of a Department of 
Respiratory Medicine. The "drone" might correspond to the "houseman" or doctor who is on a 
rotation around wards in a hospital. Both of these sorts of expert have qualified via traditional 
medical training - the Professor will have made the subject his speciality. Finally, it is a medical 
technician who will perform and initially analyse many of the routine tests — this is our "samurai". 
The trairdng of the technician is not likely to be as long or as comprehensive as the other two types 
of expert. However, they spend much of their working day rjerforming the same kind of task. 
How are we to distinguish between these types of expert in terms of the expertise they might 
embody? Certainly one important difference is in the desired outcome of any piece of problem 
solving. The characteristic of the "academic" is the pursuit of the "true" solution - a conclusion 
that follows from the systematic application of domain principles. For the "drone" the concern 
is much more pragmatic - they require a solution that works given the resource constraints and 
context within which they have to work. For the "samurai" the issue of an outcome is simply 
performance - doing what they are expected to do, and to a level acceptable to the system within 
which they operate. In our example the characterisation given to a test, for example, a chest x-ray, is fundamen­
tally determined by the desired outcome. The medical technician may classify it as "abnormal" 
and forward the case for further investigation. The houseman may look at the "abnormal" case 
and decide that it is nevertheless not serious enough to warrant further testing because of the cost 
associated with die tests that would determine the diagnosis completely. The Professor looking 
at this same x-ray might decide that it represents an "interesting abnormality", it may present a 
number of features not normally seen in this type of case. 
The nature of die outcome is just one of a number of dimensions which distinguish die expert 
types. Others include the problem solving environment in which they find themselves and die 
types of problem they arc likely to encounter. The "academic" may have relatively large amounts 
of lime to devote to a few pathological cases - seldom spending long on common or mundane 
problems. The "drone" spends a lot of their time in a problem solving environment where there is 
loo much information, the cases they deal with tend to reflect the mainstream. The "samurai" may 
spend almost all of their time making judgements about a large number of cases. The judgements 
they arc expected to make, however, arc between a relatively small number of categories. They 
arc not called upon to provide detailed explanations and justifications for the decisions made. 
The nature of the training between these various types of expert is very different. The 
"academic" and "drone" are likely to have been taught by "academics" - die former going on 
to specialise exclusively in one problem solving domain, whilst the latter may have received 
additional training in a "journeyman" fashion. The "samurai" may have received some formal 
instruction but by and large they lcam their problem solving from other "samurai". 
All of the factors mentioned above obviously have an effect on die form in which die 
knowledge is represented - both internally and in die way in which the expert can externally 
communicate their knowledge. In the case of die "academic" die theory predominates. The 
"academic" is likely to be fluent and articulate, able to talk about their problem solving - though 
very often only in terms of die theory. For the "drone" heuristics dominate - the knowledge 
may be systematic although it is likely that understanding of the deep principles of die domain 
is patchy. Surface structure knowledge tends to predominate. For the "samurai" die knowledge 
may be highly "automated" or "compiled". Consequently competence is implicit and the expert 
may have great difficulty in articulating the basis of their expertise. 
There is a crucial point in this for die knowledge engineer - depending on die type of expert 
wc arc dealing with then die kinds of KA techniques that are most effective are likely to vary. 
As will die form and content of die knowledge used by the cxpcrtt and in each case die context 
of problem solving is all important. 
A further depressing feature of much expert systems development is die neglect shown by 
knowledge engineers of die principles of human cognition. Despite a number of researchers 
writing explicitly on this material (Slater, 1987; Hoffman, 1987; Chi et al, 1988; Meyer and 
Booker, 1991) little attention seems to be paid to the cognitive issues wc shall discuss below. 
This is potentially disastrous. The problems indicated can be such as to vitiate any confidence 
wc might have in die validity and robustness of the knowledge wc elicit. 
One of the striking features of expertise is the amount of information which experts process 
while developing their expertise, and die speed with which it can be recalled. A skilled radiologist, 
for example, may have seen literally hundreds of thousands of X rays (Lcsgold et al, 1988); a 
chess master will probably have seen a similar number of chess positions in various games (Chase 
& Simon, 1973). The speed of recall is similarly impressive - in many domains it appears to be 
virtually instantaneous, and die accuracy of recall can be equally high. The impressive aspects of expert memory should not, however, blind us to the considerable 
literature on limitations and bias in human memory. Memory is not a passive process like tape 
recording; it is an acdve process, with many facets of selectivity and large scope for unconscious 
bias. 
Features such as primacy and recency effects, making die earliest and latest instances more 
easily memorable than die intermediate ones, have been accepted as basic concepts in psychology 
for decades. At a more sophisticated level, it has been known for over half a century that memory 
of events or stories is subject to considerable distortion, to the point of reversing sequences of 
events (cf Bartlctt 1958). 
The selectivity of memory, its reconstructive nature and so on, mean that die knowledge 
engineer should produce materials and approaches that can detect and correct these biases (cf for 
example Hoffman 1987, Meyer et al 1990) 
Not only is human memory subject to error, but also die way in which the information is 
used. Humans are prone to systematic patterns of error across a number of apparendy simple 
operations (Rips and Marcus 1977). For example, Modus Tollens states that if A implies B is 
true, and not B is true then not A must be true. However, people, whether expert in a domain 
or not, make errors on this rule. This is in part due to an inability to reason with contrapositive 
statements. Also in part it depends on what A and B actually represent. In other words, we 
arc affected by the content. 
There have been many explanations for this sort of data but what is interesting is that 
virtually all the research in this area shows dial at a fundamental level the basic laws of logic 
and associated proof strategies are difficult for people to apprehend and follow (Wason 1961; 
Johnson-Laird 1983). 
This means dial one cannot rely on die veracity of expert reasoning. In assembling acquisition 
material these kinds of experiment indicate die need to keep chains of implication simple when 
asking die expert for knowledge or asking die expert to review or critique existing material. 
There is also a considerable literature on human handling of uncertainty, most notably that 
by Kahncman and Tversky (e.g. Kahncman, Slovic & Tversky, 1982). This body of research has 
shown unequivocally dial in many situations people arc remarkably poor judges of probability, 
and that experienced experts may be demonstrably inferior in performance to even the crudest 
linear models. Problems arise when experts are asked to provide certainty values, judgements 
of probability and weights of evidence. People arc known to undervalue prior probabilities, 
to use die ends and middle of the probability scale rather than the full range, and to anchor 
their responses around an initial guess (Kahneman et al, 1982). Cleaves (1987) and Hink and 
Woods(1987) both review these sorts of problems and make suggestions about how these biases 
might be ameliorated in die context of KA. The KA community must remain cogniscent of the 
important findings emanating from psychology. 
Section 9 Concluding remarks 
In this paper I have tried to detail a number of key issues that confront die knowledge 
engineering enterprise. I have concentrated, in particular, on die problems inherent in providing 
computational support for die KA process. I have explored these in die context of our own 
experience in building acquisition workbenches. 
There is a large and difficult research agenda ahead of us. Nevertheless, in a number of areas 
wc can sec the emergence of ideas that will, I believe, assume a central role in the future. These 
include: die deployment of model based acquisition methods, reuse of knowledge structures within and between application domains, and the development of knowledgeable knowledge 
acquisition tools. 
Finally, we should not forget that the formative influences on knowledge acquisition have 
been eclectic and interdisciplinary. In the attempt to secure a sound basis for knowledge 
engineering we must retain this broad based view of our subject. 
Section 10 References 
1. Anjewierden, A., Wielemaker, J. & Toussaint, C. (1990) Shelley — Computer Aided 
Knowledge Engineering. In B. Wielinga, J. Boose, B. Gaines, G. Schreiber & M. van 
Someren, (ed), Current Trends in Knowledge Acquisition, pages 313-338. Amsterdam: IOS 
Press. 
2. Boose, J.H., Shema, D.B and Bradsi, J.M. (1989) Recent progress in AQUINAS: a knowledge 
acquisition workbench, Knowledge Acquisition, volume 1(2), Academic Press, London. 
3. Breuker. J. &. Wielinga, B. (1987) Use of models in die interpretation of verbal data. 
In AJL. Kidd, (ed), Knowledge Acquisition for Expert Systems, a practical handbook, New 
York: Plenum Press. 
4. Breuker. J. &. Wielinga, B. (1989) Model driven knowledge acquisition. In P. Guida and 
G. Tasso, (eds), Topics in the design of expert systems, Amsterdam:. North Holland. 
5. Burton, A., Shadbolt, N., Hedgecock, A. & Rugg, G. (1987) A formal evaluation of 
knowledge elicitation techniques for expert systems. In D Moralee, (ed), Research and 
development in expert systems, IV, 136-145. 
6. Burton, A., Shadbolt, N., Rugg, G. & Hedgecock, A. (1988) Knowledge elicitation techniques 
in classification domains. ECAI-88, 85-90 
7. Bylander, T. & Chandrasekaran, B. (1988) Generic tasks in knowledge-based reasoning: 
The 'right' level of abstraction for knowledge acquisition. In B. Gaines and J.Boose, (eds), 
Knowledge Acquisition for Knowledge Based Systems, volume 1, pages 65-77. Academic 
Press, London, 1988. 
8. Chase, W.G. & Simon, H.A. (1973) Perception in Chess Cognitive Psychology, 4, 55-81 
9. Chi, M., Claser, R. and Farr, M. (Eds) (1988) The Nature of Expertise. New Jersey: LEA. 
10. Clancey, W. (1985) Heuristic classification. Artificial Intelligence, 27:289-350. 
11. Clark, P. & Niblett, T. (1989) The CN2 induction algorithm. Machine Learning Journal, 
3(4), pages 261-283. 
12. Cleaves, D. A. (1987) Cognitive biases and corrective techniques: Proposals for improvin-
gelicitation procedures for knowledge-based systems. International Journal of Man-Machine 
Studies, 27,155-166. 
13. Eshelman, L. (1989) MOLE: A knowledge-acquisition tool for cover-and-differentiate sys­
tems. In S. Marcus, (ed), Automating Knowledge Acquisition for Expert Systems, pages 
37-80. Dordrecht: Kluwer Academic Publishers. 
14. Gaines, B. R. & Shaw, M. L. G. (1986) Induction of inference rules for expert systems. 
Fuzzy Sets and Systems, 8 (3), 315-328. 
15. Ginsberg, M. L. (1991) Knowledge Interchange Format: The KIF of Death. Al Magazine, 
12(3), pp. 57-63. 
16. Hart, A. (1986) Knowledge Acquisition for Expert Systems. London: Kogan Page. 
17. Hink, R. F. and Woods, D. L. (1987) How humans process uncertain knowledge. Al 
Magazine, 8, 41-53. 18. Hoffman, R. R. (1987) The Problem of Extracting the Knowledge of Experts from die 
Perspective of Experimental Psychology. Al Magazine, 8, pp. 53-66. 
19. Johnson-Laird, P. N. (1983) Mental models. Cambridge. Cambridge University Press. 
20. Kahn, G„ Nowlan, S. & McDermott, J. (1986) Strategies for Knowledge Acquisition. PAMI 
Special Issue on Knowledge Representation. 
21. Kahncman, D., Slovic, P. and Tversky, A. (Eds) (1982) Judgement under uncertainty: 
Heuristics and biases. New York: Cambridge University Press. 
22. Karbach, W., Linstcr, M. & Voss, A. (1990) Model-Based Approaches : one label —one 
idea? In B. Wielinga, J. Boose, B. Gaines, G. Schreiber & M. van Someren, (ed), Current 
Trends in Knowledge Acquisition, pages 313-338. Amsterdam: IOS Press. 
23. Kidd, A. L. (Ed.) (1987) Knowledge Acquisition for Expert Systems: A Practical Handbook. 
New York: Plenum Press. 
24. Kindle, K. W., Cann, R. S., Craig, M. R. and Martin, T. J. (1989) PFPS: Personal Financial 
Planning System. In H. Schorr and A. Rappaport, (eds), Innovative Applications of Artificial 
Intelligence MIT Press: Cambridge, Mass. 
25. Lesgold, A., Rubinson, H., Feltovich, P., Glascr, R., Klopfer, D. and Wang, Y. (1988) 
Expertise in a complex skill: diagnosing X-ray pictures. In Chi, M.T.H., Glaser, R. & Farr, 
M.J. (eds) The Nature of Expertise Lawrence Erlbaum; London. 
26. McAllcster, D. (1980) An oudook on truth maintenance. Technical report, MIT Al LAB. 
27. Major, N. & Reichgelt, H. (1990) Alto: An automated laddering tool. In B. Wiclinga, 
J. Boose, B. Gaines, G. Schreiber & M. van Someren, (ed),Current Trends in Knowledge 
Acquisition, pages 222-236. Amsterdam: IOS Press. 
28. Marcus, S. (1988) Automatic knowledge acquisition for expert systems. New York: Kluwer. 
29. Meyer, M. A., Booker, J. M. and Bradshaw, J. M. (1990) A flexible six-step program for 
defining and handling bias in knowledge elicitation. In B. Wielinga, J. Boose, B. Gaines, 
G. Schreiber, M. van Someren, (eds), Current trends in knowledge acquisition. Amsterdam: 
IOS Press. 
30. Meyer, M and Booker, J. (1991) Eliciting and analysing expert judgement: a proactical 
guide. Knowledge Based Systems Vol 5, Academic Press, London. 
31. Michalski, R.S. (1969) On die quasi-minimal solution of hte general covering problem. 
Proceedings of the 5th International Symposium on Information Processing (FCIP 69), Vol. 
A3 (Switching Circuits), Bled, Yugoslavia, pages 125-128. 
32. Motta, E., Rajan, T., Domingue, J. & Eisenstadt, M. (1990) Methodological foundations 
of KEATS, die knowledge engineer's assistant. In B. Wielinga, J. Boose, B. Gaines, G. 
Schreiber & M. van Someren, (cd),Currcnt Trends in Knowledge Acquisition, pages 257-
275. Amsterdam: IOS Press. 
33. Musen, M. A., Fagin, L.M., Combs, D.M. & Shortliffc, E.H. (1987) Use of a domain model to 
drive an interactive knowledge-editing tool. International Journal of Man-Machine Studies, 
26, pages 105-121. 
34. Ncchcs, R., Fikcs, R., Finin, T., Gruber, T., Paul, R., Senator, T. and Swartout, W. (1991) 
Enabling Technology for Knowledge Sharing. Al Magazine, 12(3), pp. 36-56. 
35. Reichgelt, H., Major, N. & Jackson, P. (1990) Commonsloop: The manual. Technical report, 
Al Group, Dcpt Psychology, University of Nottingham. 
36. Reichgelt, H. and Shadbolt, N.R. (1991a). Knowledgeable knowledge acquisition. In L. 
Steels and B. Smith, Eds. AISB91 Springer-Verlag 
37. Rcichgclt, H. and Shadbolt, N.R. (1991b). ProtoKEW: A knowledge-based system for 
knowledge acquisition. In D, Slceman and N. Bemscn, Eds. Research directions in cognitive 
science volume 5: Artificial Intelligence. Lawrence Erlbaum. 38. Rips, L. J. and Marcus, S. L. (1977) Supposition and the analysis of conditional sentences. 
In Just, M. A. and Carpenter, P. A. (eds), Cognitive processes in comprehension. Hillsdale, 
NJ: Erlbaum. 
39. Rugg, G & Shadboll, N. (1991) On die limitations of the repertory grid technique. Technical 
report, Al Group, Dept Psychology, University of Nottingham. 
40. Shadbolt, N. & Burton, M. (1989) The empirical study of knowledge elicitation techniques. 
SIG ART Newsletter, 108, April 1989, ACM Press. 
41. Shadbolt, N. & Burton, M. (1990) Knowledge elicitation. In J. Wilson and N. Corlett, (eds), 
Evaluation of Human Work: A Practical Ergonomics Methodology, pages 321-346. Taylor 
and Francis. 
42. Shadbolt, N. & Wielinga, B. (1990) Knowledge based knowledge acquisition: die next 
generation of support tools. In B. Wiclinga, J. Boose, B. Gaines, G. Schreiber & M. van 
Someren, (ed), Current Trends in Knowledge Acquisition, pages 313-338. Amsterdam: IOS 
Press. 
43. Slaucr, P. E. (1987) Building expert systems: cognitive emulation. Chichester: Ellis 
Horwood. 
44. Steels, L. (1990) Components of expertise. The Al Magazine, 11: 30-62. 
45. Wason, P. C. (1961) Response to affirmative and negative binary statements. British Journal 
of Psychology, 52 ,273-81. 
46. Welbank, M. A. (1983) A Review of Knowledge Acquisition Techniques for Expert Systems. 
British Telecom Research, Martlcsham Heath. 
47. Yorkc, D.M. (1978) Repertory grids in educational research: some methodological consid­
erations. British Journal of Social and Clinical Psychology, 9, 108-21. Mapping Expert Behavior onto Task-Level Frameworks: 
The need for "Eco-Pragmatic" Approaches 
to Knowledge Engineering 
Rolf Pfeifer, Thomas Rothenfluh*, Marlcus Stolze & Felix Steiner 
Al Lab, Computer Science Department 
University of Zurich, Winterthurerstrasse 190 
CH-8057 Zurich, Switzerland 
Abstract 
The main goal of this paper is to explore the possibilities of exploiting 
psychological methods for the purpose of knowledge engineering. Hypotheses are 
presented why both the pure "psychological" and the pure "engineering" positions 
are not viable for building expert systems. A "middle-out" strategy is proposed that 
preserves the best of both worlds while minimizing the problems of each. This 
"middle-out" strategy consists of the application of so-called "task-level 
frameworks". However, these frameworks do not sufficiently support one of the 
most crucial tasks in the knowledge engineering process, namely the mapping of 
the actual expert behavior onto conceptual models. In this paper, a new method 
which makes this process easier and more reliable is described and a standardized 
several-step procedure for mapping expertise-in-action protocols onto a task-level 
framework is illustrated with a case study. It is concluded (a) that protocol analysis 
is a good starting point for developing tools to support the knowledge engineering 
process—if appropriate methods are available, and (b) that methods are only 
appropriate if they are ecological on the one hand and pragmatic on the other. 
Introduction 
For more than a decade a sometimes heated debate has been going on over the problem of 
knowledge acquisition in building expert systems. Acquiring knowledge from a domain expert 
was recognized early on as a major task in developing an expert system. In fact it was 
considered the most time-consuming one and thus the term "knowledge acquisition bottleneck" 
was coined. 
To cope with this bottleneck, a large variety of methods have been suggested—some 
involving principles from psychology, others mostly inspired by computer science and 
software engineering. The former typically assume that a human expert has a central role in the 
knowledge engineering process. The main idea underlying this paradigm is that a successful 
expert system will have to include as much expert knowledge as possible, and that this expert 
knowledge has to be extracted from a human expert. Proposals along these lines typically deal 
with (a) interviewing techniques, (b) various knowledge elicitation techniques—from 
observation to thinking-aloud methods, to video-recordings, to expert-guided novice problem 
* Currently: CIS/LAIR, Ohio State University, USA solving (e.g., Diaper, 1989), (c) methods for knowledge modeling, and (d) methods for 
protocol analysis. 
Among the engineering-inspired approaches are the following: (a) Machine-learning 
methods, for example rule-induction from a set of examples, (b) model-based reasoning 
techniques (first principles approach, e.g., Davis, 1984, de Kleer, 1984), or (c) approaches 
which focus on the aspect of decision support (e.g., Sol, Takkenberg & DeVries, 1987), rather 
than on modeling expertise. 
Proponents of the psychological perspective argue that human beings are the best problem 
solvers known and it is thus best to model a problem solving system after humans. Moreover, 
users of expert systems are—since they are human beings—most comfortable with a system 
behaving like a human problem solver. If the system behaves like a human expert the 
explanations will be more natural and thus more comprehensible to the user. Moreover, if the 
knowledge-base is to be tested, psychological methods can be applied, that is, the system's 
solution and solution path can be compared those of experts in natural ways. We will situate 
proponents of this approach in the expert camp. 
Opponents of this "expert centered" view have put forward a number of arguments. First, 
so the argument, it is not sensible to model the problem solving behavior of human experts 
since different experts may differ widely in how they solve a particular problem. Second, the 
knowledge-base developed in this way will always be incomplete—it is hard to determine the 
boundaries of a system. Third, what the user of a system needs is not human expertise in the 
first place, but rather a system which supports his or her work. Fourth, it is very difficult for 
experts to express and verbalize their knowledge and thus it is also very difficult for the 
knowledge engineers to elicit this knowledge. And, last but not least, even experts may make 
errors or are unable to correctly recall all their behaviors and reasoning steps—in short, they are 
subject to the same psychological limitations as all other human beings. Proponents of this 
view advocate the use of systems and software engineering for the development of expert 
systems and are thus to be placed in the engineering camp. 
While there are hard-liners in each camp there is—as is so often the case—a "middle-out" 
strategy. It consists in the application of so-called task-level frameworks. This strategy will be 
outlined below. 
We will proceed as follows. First it is argued that neither a pure psychological nor a pure 
engineering approach will work, but that a "middle-out" strategy has to be applied which allows 
for both a more psychologically sound but still application-oriented knowledge engineering 
process. One example of such a task-level framework, the componential framework, is then 
introduced as a possible means to bridge this gap. Next, problems with task-level frameworks 
are listed and possible approaches to deal with them are outlined. This is followed by a 
discussion of a major problem which is not given adequate treatment in the literature, the one of 
mapping expertise onto a task-level framework. How this process can be supported is 
discussed in general. More specifically it is shown how protocol analysis can be exploited to 
develop appropriate methods and tools to support the knowledge engineer in this mapping 
activity. A concrete method for protocol analysis is then proposed and conclusions are drawn. 
Three basic hypotheses concerning expert systems development 
Earlier we stated that there are—in essence—two camps, the one who wants to include expert 
knowledge (the expert camp) and the one who does not (the engineering camp). Both claim that their approach to building expert systems is the best one. We claim that neither camp is 
correct and that a third strategy should be used. The following three statements of the 
hypotheses will be used as a starting point for our discussion. They are also intended to clarify 
the reasons why there is a large gap between them and include a proposal on how to get the best 
of both worlds. 
Hypothesis 1: Psychological methods, in particular the ones from cognitive 
psychology, typically yield results which cannot be directly exploited for expert 
system construction because they serve a different purpose, and are frequently too 
specific and low level. 
Hypothesis 2: The pure engineering approach will not work because of the 
fundamental epistemological problems one needs to cope with in the domains 
requiring intelligent behavior. 
Hypothesis 3: If appropriate psychological methods were available they would be 
used by knowledge engineers. What is needed are methods which are at the same 
time ecological and pragmatic, that is, we need an eco-pragmatic approach. 
Hypothesis 1: As is well-known the methods of cognitive psychology have been criticized 
as being too narrow and thus producing only irrelevant results (e.g., Abelson, 1981; Neisser, 
1982; Norman, 1988). Abelson and Neisser—although they are involved in very different 
sorts of research—argue both in a sense for a direction of psychology that is more ecological, 
that is, a kind of psychology which focuses on natural settings and avoids laboratory situations 
which are too artificial. Although this "ecological view" of psychology is getting to be more 
and more important there is still much skepticism if not scorn of this paradigm. 
If we peruse the large body of literature of psychological studies of expert knowledge or 
problem solving in general, we can draw a number of conclusions (for reviews of the field of 
psychological expertise research, see Holyoak, 1990, or Waldmann & Weinert, 1990): 
(a) The studies mostly pursue the scientific goal of understanding (rather than 
engineering systems) and largely leave open what the results imply for the 
construction of problem solving systems. Examples are the investigations by 
Newell & Simon (1972), and Dorner (1976) on problem solving, or the studies on 
novice/expert comparisons (e.g., Chi, Feltovitch & Glaser, 1981, Elstein, 
Shulman & Sprafka, 1978, Patel & Groen, 1986), to mention but a few. 
(b) The studies are frequently not sufficiently ecological to be of relevance to expert 
system construction. For instance, they frequently focus on tasks which are too 
specific or are taken from domains which only remotely resemble the sorts of tasks 
that are of interest for expert system development, such as chess, memory, writing, 
motor behavior, algebra, high school physics, geometry, sports, etc., and which do 
not easily generalize to other domains. Moreover, many of them are still confined 
to controlled laboratory-like situations. 
(c) Although some studies have resulted in methods for actually building systems, 
they are at times motivated and used in a psychologically somewhat naive way 
(e.g., Repertory Grids or Laddering techniques). 
(d) There is a lot of research and there are many methods in psychology that try to 
capture real-world problem-solving behavior. Protocol analysis is used here as a 
pertinent example and will be discussed in more detail later on. We would like to stress that from our discussion it does not follow that psychological studies of 
expertise are uninteresting, quite the contrary. In fact, the review of Holyoak (1990) is highly 
informative since—among other things—it dismantles a lot of folklore about expertise (e.g., 
that experts always solve problems with more ease than novices, that performance increases 
continuously with practice, etc.). We are only saying that psychological studies have not 
contributed much to improve knowledge engineering practice because of the methodology 
applied. We also want to mention that since psychological studies of expertise add to our 
understanding this might eventually lead to building better systems. 
Hypothesis 2: Because of the difficulties just outlined, a pure engineering approach to 
building expert systems has been proposed. "Engineering" in this context means that principles 
from systems and software engineering, and computer science in general are to be used. The 
resulting system consists of a collection of algorithms (mainly search, optimization, Bayesian 
classifiers) that are linked to data bases which are, perhaps, augmented by a number of 
production rules to support the interaction between user and data base. Another kind of "pure" 
engineering approach is model-based reasoning. Model-based reasoning tries to model the 
problem domain from "first principles" (Davis, 1984; de Kleer, 1984), i.e. without regard to 
the ways in which this model is to be used. Of course, since these principles are typically given 
by the laws of physics, for example of electronic circuits, and since the way a device works is 
described in the technical manuals, no human expert is needed. At the most, the human expert 
might be asked about technical details of a device, but his expertise of problem solving is not of 
interest. 
The reason why these pure engineering approaches do not work well outside the research 
arena is that they do not give adequate consideration to the epistemological limitations that 
humans and computers have (Steels, 1990; see also Simon's concept of "bounded 
rationality"—Simon, 1969). Very briefly, these limitations concern time and space, 
observation, and theory formation. Time and space are always limited: there is only so much 
time that can be allocated to make decisions, and the amount of information which can be stored 
is restricted (resource limitation). Thus a search process which would have to explore too 
many alternatives, or computations which are too time consuming cannot be used. Observation 
must be done through interaction with the real world. Sensory organs are prone to error and— 
always, without exception—only partial information is available. Moreover, observations take 
time and effort and only a limited number can be performed. Since theories—again—have to 
be developed by interacting with the environment (physical, social) using limited resources, 
they will, by necessity, always be restricted. 
A pure engineering approach will only work under highly unrealistic assumptions. First, the 
assumption must be made that all measurements and observations are correct and that all the 
measurements which are necessary for a particular type of model can be made within the time 
and cost limitations allocated to the problem solving process—an assumption which only holds 
in the most trivial cases. Second, the use of such models typically leads to combinatorial 
problems which are ignored in this approach. And third, it is assumed that indeed a good 
domain theory and thus precise models are available. This assumption is also only true in a few 
restricted, predominantly technical domains. 
Hypothesis 3: Given that neither the "cognitive psychology solution" nor the "pure 
engineering solution" seem to work as methods for building real life expert systems, an 
alternative is needed. The assumption is that if appropriate methods were available they would be used. The requirements for these methods have to be (a) ecological, that is they must be 
applicable with reasonable effort to realistic natural settings in which complex problem solving 
takes place, and they (b) must be directed toward building working expert systems, that is, 
they must be pragmatic. In other words, what we propose is an approach we will call "eco~ 
pragmatic". 
If behavior is complex and subject to large variations, such as expert problem solving, it is 
difficult to devise tightly controlled experiments without severe interference with the process to 
be investigated. Reducing the problem to some highly constrained subtasks such as 
recognizing specific patterns (e.g., in sound perception, or patterns of pieces of LISP code) is 
likely not to provide the data needed to design a system for the complete problem solving 
process. 
Alternatively, methods are used which are not so much directed at studying expert behavior 
but rather at eliciting certain parts—typically the declarative ones—of expert knowledge. 
Examples are card sorting and grid laddering techniques for knowledge acquisition. These 
techniques are a step in the right direction: First, they can be applied to real expert knowledge 
with reasonable effort (the ecological aspect), even though only a small part—static concepts 
and structures—of the expert knowledge is accessible in this way. Second, this knowledge can 
be used—more or less directly—to actually build a system (the pragmatic aspect). On the 
negative side, they ignore the vital aspects of problem solving knowledge, pf control, and of 
strategies and will not be further studied in this paper. 
Steps toward an "eco-pragmatic" approach: Task-level frameworks 
Overview: We believe that knowledge engineering is paradigmatic for an interdisciplinary eco-
pragmatic field: it is neither pure engineering (as the term engineering might suggest), nor is it 
pure psychology. There are psychological theories of problem solving (e.g., Newell & Simon, 
1972, Dorner, 1976) but they are usually difficult to apply to a concrete problem with 
reasonable effort. However, out of the expert systems community, a number of proposals have 
been made for so-called task-level frameworks. They_comprise computational theories or 
skeletons of theories of problem solving which are thus directly geared towards system 
development. Examples of such frameworks are the theory of heuristic classification 
(Clancey, 1985), generic tasks (Chandrasekaran, 1986), problem solving methods 
(McDermott, 1988), KADS (Breuker & Wielinga, 1989), and the componential framework 
(Steels, 1990). Although the main purpose of these frameworks is the development of 
systems, they are based partly on psychological evidence. But they are not meant to be 
psychological theories of problem solving. Rather they are theories of problem solving which 
are potentially applicable to humans or to machines. As such they provide appropriate means 
for performing knowledge acquisition. Let us briefly illustrate this point. 
An example - the "componential framework": In the "componential framework" (Steels, 
1990), so-called tasks are mapped onto problem solving methods and actions. Actions are 
methods which are executable without further decomposition. If an action can be found to 
perform a specific task, it is executed. A simple example of an action, i.e. an executable 
method, is "ask user" which performs data acquisition. This method can be applied whenever 
data are needed which cannot be deduced or whose deduction would be too costly. If no 
executable method is available the problem is recursively further decomposed into subtasks 
until executable methods are available. The task "diagnosis", for example, is decomposed into 
the subtasks "gather initial data" (inquire about the symptoms), "restrict context" (using general information about the patient's age, medical history etc. and the initial symptoms, narrow 
down context), "select" (select a particular diagnostic class, i.e. a disease). Now, for the 
selection step, many methods are potentially applicable. Examples are "linear search", 
"hierarchical classification", "weighted evidence combination", "Bayesian classification", 
"discrimination" and "association". Which one of these methods is selected by the expert 
depends on the pragmatic features of the task. Pragmatic features are concerned with the 
amount, the quality, the availability and the cost of the input data, and the way in which the 
desired output is defined (e.g., as concepts with necessary and sufficient conditions, or as 
prototypes). The idea is to ask the expert about the sorts of problem solving methods he knows 
about (e.g., Vanwelkenhuysen, 1989). At each decision point he typically has a choice of 
several methods one of which he has to select. He can be asked about the reasons for selecting 
one method over others. The important part here is that these reasons do not have to 
correspond to the "real" reasons for choosing a particular method. Even post-hoc 
rationalizations (e.g., Nisbett & Wilson, 1977) are fine as long as they are plausible and 
consistent. In other words, the reasons do not have to be psychologically valid. Plausible 
means that a potential user should be able to follow the rationale, and consistent means that 
there should be no contradictions between the selection criteria in different situations. In this 
way such a task-level framework can be used for knowledge acquisition purposes, that is, a 
problem solving program may be developed through the interaction with a domain expert 
without developing a psychological model of his problem solving behavior. Nevertheless, we 
are applying, in a sense, a psychological method. 
In addition to providing a structure at the conceptual level some task-level frameworks 
include methods for mapping—at least parts of—the task-level analysis onto a computational 
framework (Vanwelkenhuysen & Rademakers, 1990). But this aspect is of less concern for 
the current arguments and will therefore not be further discussed. 
Problems with task-level frameworks and what can be done about it 
Problems 
A number of problems with task-level frameworks are discussed by Steels (1990). They 
include the fact that some are at too high a level of abstraction and thus leave many important 
aspects unspecified (e.g., Clancey's inference structures leave the nature of the "heuristic 
match" entirely unspecified) and others make too many assumptions about the problem solving 
process (e.g., Chandrasekaran's "generic task approach"). Another problem—highly relevant 
to our argument—concerns the lack of consensus between the different frameworks. It is 
generally agreed that an abstraction level above the one of programming is needed, a proposal 
which was originally made by Newell (1982) in his "knowledge level" hypothesis. But there is 
only little agreement on the details (see below). A last problem we would like to mention—and 
we consider it to be a major one—is the difficulty of applying task-level frameworks to a 
specific problem at hand, or stated differently, mapping expert behavior onto the frameworks. 
Experience with such frameworks and careful analysis of the literature shows that — 
although such frameworks are a major step forward—important parts are still left open. In 
trying to reconstruct the cognitive processes of the knowledge engineers there are major gaps to 
be filled in. For example, in the description of the MOLE system (Eshelman, 1988) it is not 
specified how a diagnostic problem is mapped onto the problem solving method "cover-and-
differentiate". It seems that this is largely done by "magic". What may be obvious to a good knowledge engineer who has worked with a specific framework for a long time, remains 
obscure to an outsider. 
Approaches 
There are essentially two ways in which these problems can be approached, namely (a) to 
extend or refine task-level frameworks, or (b) to study the process of mapping expertise onto 
task-level frameworks and develop ways of supporting this activity. 
(a) Extending and refining task-level frameworks: It is clear that the final word on task-level 
frameworks is not out and that extensions and refinements in several ways are still possible, 
and to some extent needed. However, it is an interesting observation that, even though the 
debate about the details of the various frameworks has been going on now for more than half a 
decade, there is still a striking lack of consensus, as pointed out before. The main reason 
seems to be that the knowledge level, which corresponds directly to Dennett's "intentional 
stance" (Dennett, 1971), is highly underconstrained and no agreement can be achieved on the 
precise nature of the constraints. There is an intrinsic arbitrariness at the knowledge level 
which will not go away, no matter how long one discusses the problems. So, one is lead to the 
conclusion that, perhaps, the extending and refining the frameworks and to search for the 
"right" one to eventually reach consensus, may not be the most appropriate goal. Experience 
has shown that most task level frameworks can lead to usable practical systems. The main 
difficulty which remains in all frameworks as a matter of principle (not merely because they are 
insufficiently developed) is mapping the human expertise onto the framework. If the 
framework is highly abstract the mapping process is severely underconstrained and thus the 
application to a particular problem is very difficult. In order to provide a high degree of detail, 
a great many models will have to be developed and some of them will always be missing or 
incomplete. Even if many such detailed models were available, the selection of the appropriate 
structures would become another major problem for the knowledge engineer. But irrespective 
of the level of detail, the mapping problem remains. And it seems to hold for all frameworks 
that if you are clever, you can successfully apply the framework, if not, you fail. As far as we 
know this mapping problem has not or only marginally been addressed in the literature. And 
this leads us to the second line along which improvements are possible, which in our view, is a 
promising one. 
(b) Study and support of the mapping process: Rather than trying to refine the framework by 
adding many types of models, we propose the following strategy. Start with a framework from 
the literature: which one is not very essential. Study the ways in which experienced knowledge 
engineers go about applying it to a particular problem. It is exactly in the analysis phase where 
psychological methods will be highly useful. Since the complexity of this task is so 
overwhelming, the construction of an automated system is out of the question. It should be 
pointed out that the goal in this case is not to develop a computational model of knowledge 
engineering expertise but to better understand the process in order to support it by appropriate 
computerized tools. Since task-level frameworks have been developed for modeling problem 
solving behavior, we will apply one of them, the componential framework, to this analysis. 
This is a kind of "recursive" analysis of a task-level framework, similar to what has been 
suggested by Bouchet, Brunet & Anjewierden (1989). So far nothing has been specified about 
the particular method by which the experts, in our case the knowledge engineers, should be 
studied. A more or less unbiased registration of the problem solving behavior of experts can be 
obtained with expertise-in-action protocols, that is, think-and-talk-aloud protocols of experts produced during their problem solving. We will use such protocols as empirical material for 
mapping expert behavior onto the structures required by the componential framework. 
A note on protocol analysis 
Protocol analysis does not have a high reputation for being useful in the knowledge engineering 
process. So we need to argue why we suggest to use it all the same. First we will point out 
why it is not popular in expert systems circles and second we will show why we still think it 
should be used. Perhaps it is worth mentioning that there are different ways to perform 
protocol analysis. Two main approaches are content analytic methods and theory-based ones. 
The former essentially perform statistical or interpretative analyses on the text (e.g., Holsti, 
1969; Mayring, 1990), while the latter provide theoretical frameworks (such as our task-level 
frameworks) onto which the protocol needs to be mapped. The remarks in the sequel only refer 
to the latter type. Moreover, the kinds of protocols we are interested in most are "expertise-in­
action" ones. 
Why protocol analysis is unpopular: There are a number of reasons why protocol analysis is 
not popular among knowledge engineers. First, to most knowledge engineers the benefit from 
protocol analysis does not justify the effort required (transcription, a lot of detailed work, see 
e.g., Ericsson & Simon, 1984). A factor contributing to the low cost-benefit ratio is the simple 
experience that protocol analysis is hard. The main reasons why protocol analysis is hard are: 
(a) There are many omissions in the text. They have to be completed somehow to arrive at a 
coherent "causal story" (see our example below) even though there is often insufficient support 
for a particular assumption, (b) Assignments of instances to abstract variables (concepts, 
categories) of the framework is initially to a large extent arbitrary, but gets successively more 
constrained. Information appearing later in a protocol may invalidate previous assumptions 
which may entail significant revisions of the initial assignments (truth maintenance), (c) Many 
things have to be kept track of in parallel. This means that there is a considerable "cognitive 
load" on the protocol analyst. In particular revisions are cognitively costly and tend to be 
avoided. 
A second reason why protocol analysis is unpopular has to do with the large variations in 
expert behavior. In other words, since protocols from different experts on the same task tend 
to be very different, the respective analyses will also differ widely and it is not clear what 
should be included in a system. A third reason is the incompleteness of the knowledge gained 
from protocols. A protocol will only yield a very small part of the knowledge needed to build a 
system (e.g., Shadbolt & Burton, 1989). A fourth reason concerns methodological problems 
related to the nature of expertise, that is, it is questionable whether the method of protocol 
analysis can be used to assess real expertise since it always starts from verbal accounts (Nisbett 
& Wilson, 1977). A fifth one is that, given a particular protocol and a task-level framework, 
different knowledge engineers would come up with different interpretations. 
Why protocol analysis can be beneficially applied: In spite of these difficulties we argue that 
protocol analysis can applied with great benefit. But we don't argue that it should be used 
exclusively. It is useful for certain purposes like getting an initial idea of expert behavior in a 
domain, in particular where control knowledge (when to do what), and basic difficulties are 
concerned. It can also be helpful in trying to determine where the expert or the potential user 
could be most effectively supported by tools. However, it can, for the reasons given before, 
by no means replace the use of other methods. Let us now briefly go over the caveats against protocol analysis and suggest remedies where 
possible. The reasons given for why protocol analysis is hard can mostly be traced back to 
insufficient methodology and insufficient tools. For example, there are currently no guidelines 
as to how omissions are to be dealt with. We will suggest one way to deal with this problem. 
Second there are only limited tools available to manage revisions, but this is a practical problem 
rather than a fundamental one and no major difficulties seem to prevent the development of 
appropriate tools. The fact that many things have to be kept in mind in parallel could perhaps 
be supported by graphical means, but more investigation is needed on this point. Some tools 
for supporting protocol analysis already exist (e.g.. the "KOD Station™"—Vogel, 1990; tools 
within KADS or Shelley—Breuker & Wielinga, 1989, Bouchet, Brunet, & Anjewierden, 
1989). They provide powerful sets of tools but little guidance on how to map protocol 
elements to predefined conceptual elements. Our own view is expert centered rather than tool 
centered, that is, in our view tool support should start from an analysis of the user (in this case 
the knowledge engineer) rather than from a catalog of things compiled by a knowledge engineer 
that he or she thinks might be useful to have. 
The problem of variability in expert behavior cannot be readily solved. In any case this is a 
problem for knowledge acquisition in general and not only for protocol analysis, although in 
expertise-in-action protocols individual differences tend to show up most strongly. There are 
methods for integrating knowledge from different experts (e.g., Dym & Mittal, 1985) but they 
are not without problems (e.g., Shaw & Gaines, 1989). The completeness of the knowledge 
base can be assured by using additional methods. In any case, the goal of protocol analysis is 
not to cover all the knowledge needed to build a system. The methodological point, that it is 
questionable whether protocols do indeed capture expertise, is a fundamental one and will never 
completely disappear, but it seems to apply least (compared to other forms of protocols) to 
expertise-in-action protocols. The last point, the disagreement between different protocol 
interpreters, can be taken into account by an appropriate methodology (see below). 
In conclusion, for most problems with protocol analysis it appears that there are practical 
solutions. But there is one highly significant reason why protocol analysis will—in spite of 
difficulties—remain an important method: that is its highly ecological nature. Once experts 
have been trained in thinking-aloud, the protocols can be taken in real-life settings; this is one of 
the major reasons for using them. Moreover, they provide very rich sources of information and 
experience has shown that the careful analysis of protocols can yield highly valuable insights 
(Ericsson &Simon, 1984). 
Protocol analysis for knowledge engineering support 
Goals and preliminary remarks 
The knowledge engineer can be supported in several ways in the task of mapping expert 
behavior onto abstract models. The most extreme case would be an automatic system in which 
the knowledge engineer would only have to provide the input (i.e., in our case the expertise-in­
action-protocols) to the system which would then do all the work, but this is clearly beyond 
what is doable today (it is also highly questionable whether this would constitute a sensible goal 
in the first place). A second kind of support is in the form of a number of computerized tools 
including guidelines on how to use them. A third form is a set of verbal instructions. Our 
approach is to start with an operational set of verbal instructions and use them—after careful 
evaluation and validation—as starting points to build computerized tools. The proposal is based on the following assumptions: 
(a) The goal of protocol analysis is to produce a knowledge-level representation of 
the problem solving process in terms of a task-level framework. 
(b) A structured procedure based on the theoretical framework can be defined to 
ensure more reliable encoding. 
(c) If systematic studies (e.g., comparisons among different knowledge engineers, 
among different frameworks) are to be performed, a method is needed to achieve 
continuous consensus. 
The results reported in this paper have been derived from a number of empirical studies. 
However, they are still of a preliminary nature and a more systematic investigation is needed. 
The main purpose is to stimulate the discussion and get feedback for a next "iteration cycle". 
The theoretical points are illustrated with results of an empirical study 
An example 
In order to determine the precise nature of the difficulties with protocol analysis that were 
described theoretically above, a number of experiments were conducted. In particular we had 
experienced knowledge engineers analyze expertise-in-action protocols. We then analyzed the 
protocols of these sessions in terms of the componential framework to gain a better 
understanding of the process by which knowledge engineers analyze protocols and derived a 
methodology that tries to overcome the observed difficulties. 
The example that will serve as an illustration has been taken from an interview with an expert 
repairing old-fashioned record players. Figure 1 gives an idea of the sort of device we are 
discussing about. A segment out of the protocol which the knowledge engineers had to analyze 
is given in Figure 2. Figure 3 shows an excerpt from a thinking-aloud protocol while 
performing this protocol analysis task. A number of difficulties can immediately be identified. 
Figure 1: Example of a record player Thinking aloud protocol: Record player diagnosis ("Expert se-in-action") 
(E = Expert; I = Interviewer/Customer) 
E: You said "it sometimes happens", "it jumps". Now does it jump towards 
the periphery or towards the center? 
I: I think it exclusively jumps towards the periphery, no, towards the center, 
excuse me. 
E: Towards the center. Well before it was jumping the other way—the same 
part of a piece was played over and over. 
I: Yes but then it jumps ... 
E: It is playing from the periphery towards the center, so when it jumps 
toward the periphery it repeats. 
I: You are right, of course, it jumps toward the periphery. 
E: Now there has been the suspicion that it would not stand horizontally, but 
it does not have that much weight, it would have to be really tilted. And 
now there is a first conjecture perhaps, that the bearing is somehow sticky 
and does not turn appropriately. But as far as I can tell this does not... 
I: ... seem to be the case? 
E: Well, it is difficult to say because the forces are really small and it does 
not seem really sticky. 
I: What is your reason for saying this? 
E: If it lies on this ring for holding the arm then it is stuck and if you move it 
you can see its small uneven movements. This is because it does not rest 
on the bearing but on this little bolt. 
And therefore I lift it slightly and then I move back and forth sideways and I 
feel that there is no real lateral resistance. 
Figure 2: Transcript of a protocol (segmented) 
The statement in line 1 of Figure 3 is typical for the non-monotonicity of the task. 
Statement 2 relates to the fact that more and more information has to be added into the task 
structure as the analysis proceeds. It is a clear indicator of missing editing facilities. 
Statement 3 refers to the way the knowledge engineer proceeds. He is using a list of generic 
problem solving methods (memorized or on a sheet of paper) which he can potentially use in 
the task structure. So he not only uses the information in the protocol, but consults the 
available methods in a sort of interactive process. Statement 4 represents an apparently 
underconstrained situation. There are several plausible possibilities and the knowledge 
engineer seems to postpone the decision (well, we'll see). It is not clear whether at this point 
he actually writes something down or just makes a mental "note" (which he might be likely to 
forget). In 5 the expert is trying to identify the domain model but he is uncertain. Statement 6 
refers to his confusion and to the experimental situation itself (the "others" are the other 
participants in the experiments). Thinking aloud protocol: Protocol analysis ("Expertise-in-action") 
1. Well I might as well put it here, but perhaps I will have to change it again, 
later. 
2. Well, this paper is going to be too small and I am not really sure what's 
generic or domain specific. Well, we'll see as we go along. 
3. Now, "get-symptoms" decomposes into "ask-user" and something like 
"generate-new-observations". 
4. Is this a subtask of "get-symptoms" or perhaps of something like "test-
hypothesis" or doesn't he have a hypothesis yet? Well, we'll see. 
5. Well, but that thing with the weight, that's more like causal reasoning, so I 
should put here two alternatives, well ... 
6. space is getting tight, I am loosing track of things, I hope that the others 
are no better... 
Figure 3: Transcript of a protocol of a knowledge engineer analyzing a protocol 
(the lines are numbered for reference purposes only). 
From this informal analysis there are a number of conclusions which can be drawn. But 
first let us analyze the "nature" of the task of protocol analysis itself by applying the 
componential framework to it. Protocol analysis is a task with inputs (the protocol) and outputs 
(the task structure) and thus lends itself to such an analysis. In order to determine the kind of 
support that can be provided for this type of task, we must look at its pragmatic features. Let 
us look at the protocol, that is, the input of the knowledge engineer's task: there is much 
irrelevant data (text which is not relevant to the task), the quality is limited due to the abstraction 
inherent in linguistic descriptions and due to practical problems (quality of recording and 
transcript), a lot of potentially relevant information is missing (due to the method of thinking-
aloud), and additional information may be hard or impossible to get. The output, the task 
structure, is highly underconstrained and many interpretations are possible. Problem solving 
methods are very unstructured and a lot of common sense knowledge is involved. From this 
description it follows immediately that the protocol analysis task cannot be automated in the 
near future, since for most of the identified problems entailed by the pragmatic features there are 
no implementable methods available. What is needed is a way to filter and to complete the 
original data (in our case the raw text) in such a way that the mapping is more straightforward. 
An obvious conclusion from the example shown in Figure 3 is that there is too much 
"cognitive load" on the knowledge engineer: there are things that have to be traced in parallel, 
there is the interpretation of the plain text on the one hand and the search of appropriate 
elements from the framework and the task-taxonomy on the other. This problem can be 
alleviated by computerized tools. This could also solve to some extent the non-monotonicity 
issue. Such tools must allow for: 
• easy modification (including consistency tracing) of the task structure 
• ways of graphically represent the results to provide the knowledge engineer with 
an overview of the task structure. 
• ways of browsing through the taxonomies (tasks and problem solving methods) 
to support top-down processing (thus enabling the knowledge engineer to do 
interactive processing, i.e. bottom-up (from the protocol text) and top down 
(from the concepts in the framework). 
• ways of manipulating the protocol itself (marking, extracting, connecting, 
editing, etc.). 
Some of this is already available in some tools or tool-kits such as KADS, Shelley or the KOD-
station. While tools are certainly an essential part of any system for protocol analysis, more is 
needed. It seems that even with the availability of tools that essentially remove the large 
cognitive load, the basic problem of the mapping of the raw text to the framework largely 
remains. 
A method of protocol analysis 
Our proposal for a method of protocol analysis is based on a number of preliminary studies. 
The method is designed for protocols in which there is relatively little reflection on the part of 
the domain expert, as is the case in most expertise-in-action protocols. This implies that most 
of the expert's actual behavior can be reconstructed from the protocol. 
The basic idea of the method follows a three-step procedure: Phase I performs a 
[K]nowledge-[G]uided [B]ehavior analysis in the form of a "re-narration" (KGB-phase); 
Phase II then tries to achieve [C]onsensus [I]n the [A]nalysis among different knowledge 
engineers (CIA-phase); Phase III [B]uilds [N]ew [D]escriptions in terms of the problem 
solving language of the task-level framework (BND-phase). 
The "re-narration" phase produces from the raw text an "edited" text. A basic problem of 
behavior data is that on the one hand there is too much, and on the other there is not the "right" 
data available. These are characteristic pragmatic features of the input to the analysis task, as 
discussed above. Thus not only do we have the problem of data re-duction but also of data 
pro-duction, or data completion. The output of the first phase, that is the "edited" text, is a 
causally coherent story of the behavior of the expert, of what he does and thinks. The 
instructions which encode our method are as follows: 
Instructions Phase I: "Re-narration" (rephrasing) 
Try to "re-narrate" what is happening in the protocol using the following 
instructions. The purpose is to arrive at a coherent description of what 
the expert does, says and thinks that contains as little interpretation of 
his actions and thoughts as possible. 
1. Eliminate "filler" sentences and obviously extraneous text. 
2. Complete the text. In order to get a plausible story, actions may have to be 
inferred. Using common sense reasoning turn the text into a coherent causal 
story. It is a good strategy to visualize the situation. 3. If possible, use only a vocabulary which is close to behavior and to the 
domain terminology. Choose whenever possible terms from the vocabulary 
given (list of terms to be supplied). Avoid terminology from the taxonomy of 
the task-level framework. Unless it is explicitly mentioned in the protocol 
avoid terms like "hypothesis" or "goal". 
4. The segmentation should naturally be determined by the behavioral level of 
this description. 
Figure 4 shows a sample output of phase I. A comparison of "edited" protocols (after 
phase I) of the same protocol segment by different knowledge engineers showed that there are 
a number of differences. This is undesirable if the protocols are to be compared and if 
conclusions are to be drawn about expert behavior. So whenever the protocols are analyzed by 
several knowledge engineers there must be a so-called "consensus session" after the first phase. 
In other words the knowledge engineers sit together, discuss their solutions and try to find a 
consensus on one solution (phase II). 
It is surprising how quickly—at least in our experiments—agreement could be achieved. And 
agreement was virtually always unanimous, not by majority. Why this is the case, we do not 
know. It just seems that communication is conducive to discovering plausible solutions. Thus, 
preferably there should be at least two people doing the analysis. After agreement has been 
achieved among the raters, phase in can be tackled according to the following instructions: 
"Edited" Thinking aloud protocol (after Phase 1) 
Domain: Fault diagnosis of a record player 
(E = Expert; 1 = Interviewer) 
"Raw Text" "Re-narration" 
E: So the arm sometimes jumps Restates initial complaint: 
towards the periphery arm jumps toward periphery 
now there is a possibility that the Considers possibility: sticky 
bearing 
bearing might somehow be sticky causes arm to jump 
but as far as 1 can tell this does not moves arm back and forth 
sideways 
1: ... seem to be the case? 
E: Well, it is difficult to say because the finds lateral resistance 
normal 
forces are extremely small and it 
does not really seem sticky finds that this manual test is 
if 1 move it back and forth sideways too insensitive 
1 feel not real lateral resistance 
Figure 4: Output of Phase I: Re-narration Instructions Phase III: Problem solving analysis phase 
(mapping the "edited" text onto the framework) 
Try to map the "edited" protocol onto the task-level framework. There 
will be no need to go back to the original text at any point (except to 
revise the whole analysis). The goal of Phase HI is a task-structure 
which describes the concrete task at hand. 
0. It is a good strategy to do two tasks in parallel: (a) inserting the problem solving 
terminology into the protocol, and (b) developing the graphical task structure1. 
1. Try to identify the top-level task and decompose it into subtask as indicated by the 
"components" framework. 
2. Add the generic labels to the subtasks and to the problem solving methods. 
Whenever possible, use labels from the taxonomy that is provided with the 
framework. 
3. Whenever there is no appropriate label in the taxonomy, invent a new meaningful 
name. 
4. Organize each problem solving method with a control regime. 
5. For each problem solving method, identify the domain models, that is, the 
knowledge needed to perform the task. Again, in labeling use the predefined terms 
whenever possible. 
6. For each task work out the epistemological problems and its associated pragmatic 
features and try to integrate them into the selection criteria of the identified problem 
solving methods. 
7. Try to account for as many statements in the "edited" protocol as possible. 
The output of phase III (see Figures 5 & 6) thus achieved is subject to less variation 
according to our preliminary experiments if this method of protocol analysis is applied. But 
more systematic tests are needed. The next step will be to follow the second phase up with a 
second consensus session. It would be of interest to see if consensus can also be achieved as 
easily as after the first phase. Moreover, the whole procedure would have to be done with 
knowledge engineers from different theoretical backgrounds (KADS, Componential 
Framework, Generic Tasks, Problem Solving Methods etc.). If it turns out that the inter-group 
reliability is high, this method would provide a good vehicle to be used in expert system 
construction. 
Since methods of protocol analysis vary with the nature of the protocol (e.g., expertise-in­
action, structured or a focussed interview, expert-guided problem solving, unstructured 
conversation), the protocol needs to be classified first. Our method will only work for 
expertise-in-action or behavior observation protocols. The construction of a causal story 
largely relies on this fact. It also restricts the applicability of the method. However, expertise-
in-action is among the most valuable data for gaining more insights into the ways humans deal 
with the pragmatic features of a task. 
1 Some of the steps and products mentioned in the instructions are of course not required for 
the use of task-level frameworks, but serve purposes of documentation and enable discussion 
with other knoweldge engineers and experts. Output of Phase III: adding problem solving terminology (protocol) 
Domain: Fault diagnosis of a record player 
Restates initial complaint: arm jumps get-initial-symptoms 
toward periphery 
Has suspicion: sticky bearing cover-with-hypothesis: 
causes arm to jump sticky bearing 
moves arm back and forth test-hypothesis: sticky 
bearing 
sideways 
finds lateral resistance normal evaluate-test-result: 
negative, 
but test insufficient 
finds that this manual test is too hypothesis not rejected but 
insensitive given less weight 
Figure 5: Output of Phase III: Problem solving analysis phase 
diagnose-and-repair 
diagnose 
get-symptoms 
ask-user generate-
new-
observation find-
covering-
hypo theses 
find-by-
associadon 
{association} repair 
sequential-
manipulation-
and-observation 
{primitve actions} test-
hypotheses 
{functional 
model} 
find-by-
consultadons-
of-causal-model 
{causal-model} find-
explanations 
Figure 6: Task structure (including the problem solving process) of the fault diagnosis task Validation 
It is clear that we have not said much about validity yet. In our case validity means to what 
extent the analysis reflects the actual thinking processes of the expert. Validity could be 
assessed by showing the final analysis back to the expert and getting his reactions. The expert 
could also contribute to the consensus sessions. However, it has to be kept in mind that it is 
not our goal to adequately describe the expert's reasoning, but that we aim at building expert 
systems, which is a very different story. For the present purposes we are therefore not worried 
too much about validity. What is more important is that the method is ecological (i.e. 
applicable to real-life problem solving) and pragmatic (i.e. leads to systems). 
Summary and conclusions 
We first introduced a distinction between two extreme positions, the "engineering" and the 
"expert" one. We argued that both suffer from significant problems and suggested that the use 
of task-level frameworks, if applied appropriately, provides a means for getting the best of both 
worlds. We identified a largely neglected problem, namely that of mapping expertise onto task-
level frameworks. The goal of our current efforts is to support precisely this process. As a 
starting point we studied how expertise-in-action protocols can be mapped onto task-level 
frameworks. We developed a methodology to make protocol analysis more effective and more 
suitable for supporting this mapping activity. Given the high complexity of this activity, 
support will consist of a set of guidelines, mainly in the form of verbal instructions, plus a set 
of computerized tools. There will be little in terms of automated systems. 
It is concluded (a) that protocol analysis is a good starting point for developing tools to 
support the entire knowledge engineering process — //appropriate methods are available, and 
(b) that methods are only appropriate if they are both ecological and pragmatic. 
The methods developed on the basis of the approach outlined in this paper (using protocol 
analysis) should not be viewed as final or complete. Rather they serve as a starting point for 
further development: knowledge engineers must be studied carefully over extended periods of 
time in how they use the tools. What parts do they use most, what parts not at all, and—very 
importantly—how does the nature of their task change as they are using the tool. So, protocol 
analysis is only a starting point, but because of its ecological nature we expect that the tools 
developed on this basis can be naturally extended. 
Acknowledgement 
This work was partly supported by the University of Zurich, Tecan AG Hombrechtikon, and 
the "Swift Al Chair" of the Free University of Brussels. 
References 
Abelson, R.P. (1981). Whatever became of consistency theory. Proceedings of the 3rd 
International Conference of the Cognitive Science Society. 
Bouchet, C, Brunet, E., & Anjewierden, A. (1989). Shelley: An integrated workbench for 
KBS development. Proceedings of the 9th International Workshop on Expert Systems 
and Their Applications, Avignon, 303-315. Breuker, J.A., & Wielinga, B.J. (1989). Models of expertise in knowledge acquisition. In 
G. Guida & C. Tasso (Eds.). Topics in expert system design, 265-295. Amsterdam: 
Elsevier. 
Chandrasekaran, B. (1986). Generic tasks in knowledge based reasoning: High level building 
blocks for expert system design. IEEE Expert, Fall, 1986, 23-30. 
Chi, M.T.H., Feltovich, P.J., & Glaser, R. (1981). Categorization and representation of 
physics problems by experts and novices. Cognitive Science, 5, 121-152. 
Clancey, W. (1985). Heuristic classification. Artificial Intelligence, 27, 298-350. 
Davis, R. (1984). Diagnostic reasoning based on structure and behavior. Artificial 
Intelligence, 24 347-411. 
de Kleer, J. (1984). How circuits work. Artificial Intelligence, 24, 205-281. 
Dennett, D. (1971). Intentional systems. The Journal of Philosophy, 68, 87-106. (reprinted in 
J. Haugeland (Ed.) (1981). Mind design (pp.220-242). Montgomery, VT: Bradford 
Books.) 
Diaper, D. (Ed.) (1989). Knowledge elicitation. Chichester: Ellis Horwood. 
Dorner, D. (1976). Problemlosen als Informationsverarbeitung (Problem solving as 
information processing). Stuttgart: Kohlhammer. 
Dym, C.L. & Mittal, S. (1985). Knowledge acquisition from multiple experts. Al Magazine, 
6(2). 
Elstein, A.S., Shulman, L.S., & Sprafka, S.A. (1978). Medical problem solving: An 
analysis of clinical reasoning. Cambridge, MA: Harvard University Press. 
Ericsson, A., & Simon, H.A. (1984). Protocol analysis: Verbal reports as data. Cambridge, 
MA: MIT Press. 
Holsti, O.R. (1969). Content analysis for the social sciences and humanities. Menlo Park, 
CA: Addison-Wesley. 
Holyoak, K. (1990). Symbolic connectionism: toward third-generation theories of 
expertise. Techreport UCLA-CSRO-90-14. (to appear in I.A. Ericsson & J. Smith (Eds.). 
Toward a general theory of expertise: prospects and limits. Cambridge, MA: Cambridge 
University Press.) 
Mayring, P. (1990). Qualitative Inhaltsanalyse: Grundlagen und Techniken (2., durchges. 
Auflage). (Qualitative content analysis: Foundations and techniques). Weinheim: 
Deutscher Studien Verlag. 
McDermott, J. (1988). Preliminary steps toward a taxonomy of problem-solving methods. In 
S. Marcus (Ed.). Automating knowledge acquisition for expert systems (pp. 225-256). 
Boston, MA: Kluwer. 
Neisser, U. (1982). Memory observed. San Francisco, CA: Freeman. 
Newell, A. (1982). The knowledge level. Artificial Intelligence, 18, 87-127. 
Newell, A., & Simon, H.A. (1972). Human problem solving. Englewood Cliffs, NJ: Prentice 
Hall. 
Nisbett, R.E., & Wilson, T.D. (1977). Telling more than we can know: Verbal reports on 
mental data. Psychological Review, 84. 231-259. 
Norman, D.A. (1988). The psychology of everyday things. New York: Basic Books. Patel, V.F., & Groen, GJ. (1986). Knowledge-based solution strategies in medical reasoning. 
Cognitive Science, 10, 91-116. 
Shadbolt, N., & Burton, A.M. (1989). The empirical study of knowledge elicitation 
techniques. SIG ART Newsletter, 108, 15-18. 
Shaw, M.L.G., & Gaines, B. (1989). Knowledge acquisition: Some foundations, manual 
methods, and future trends. Proceedings ofEKAW-89, Paris, France. 3-19. 
Simon, H.A. (1969). The sciences of the artificial. Cambridge, MA: MIT Press. 
Sol, H.G., Takkenberg, CA., & DeVries Robb€, P.F. (Eds.) (1987). Expert systems and 
artificial intelligence in decision support systems. Dordrecht: Reidel. 
Steels, L. (1990). Components of expertise. Al Magazine, 11(2), 28-49. 
Vanwelkenhuysen, J. (1989). TroTelc: An expert system troubleshooting printed ciruit 
boards. VUB Al Memo 89-17. Free University of Brussels, Belgium. 
Vanwelkenhuysen, J., & Rademakers, P. (1990). Mapping a knowledge level analysis onto a 
computational framework. Proceedings ECAI (pp. 661-666). Stockholm, Sweden. 
Vogel, C. (1990). KOD: A method for knowledge acquisition and modeling. Tutorial at the 
Tenth International Workshop on Expert Systems and Their Applications. Avignon, 
France. 
Waldmann, M.R., & Weinert, F.E. (1990). Intelligenz und Denken (Intelligence and 
thinking). Gottingen: Hogrefe. Knowledge Acquisition and the Interpretative 
Paradigm 
Dieter Fensel 
Institut fur Angewandte Informatik und Formale Beschreibungsverfahren 
University of Karlsruhe, P.O. Box 6980, 7500 Karlsruhe, Germany 
e-mail: Fensel@aifb.uni-karlsruhe.de 
Abstract. The problems arising during the early steps of knowledge acquisition 
are similar to problems in social research based on the interpretative paradigm. 
Therefore the article shows the transfer of principles, methods, and techniques from 
social science to knowledge acquisition. This transfer offers a methodological 
foundation for the gathering and interpretation of knowledge and a framework 
which guides the application of the different techniques for the different tasks of 
incremental and model-based knowledge acquisition. 
1. Introduction "Knowledge acquisition, however, is not only the »transfer of expertise«. 
Knowledge acquisition is a creative process in which the systems builder 
constructs qualitative models of human behaviours." [Musen] 
Knowledge acquisition is the process which gathers and models knowledge for an expert 
system. The problems [Ber87] arising in its course are well known: 
- There is an insufficient understanding of experts' abilities to solve problems in their 
field. 
- Expertise is based on skill and implicit knowledge. Therefore it cannot directly be asked 
for. 
- Experts tend to justify their behavior like any other person. They try to construct good 
reasons for their actions even if they do not know them. 
- Experts are experts in solving problems, not in explaining their solutions. 
The literature about knowledge acquisition offers some techniques to overcome these problems 
but there is still no clarity "when and where to use these techniques" [NPB91]. This lack of 
methodological foundation and framework which connects the different techniques with the 
different tasks of the whole knowledge acquisition process is called the "mismatch problem" 
[NPB91]. 
The difficulties described above relate knowledge acquisition to sciences such as psychology or 
sociology. Both disciplines have a long tradition of questioning and observing people. As a 
consequence they are familiar with problems similar to those described above. This article 
attempts to show how knowledge acquisition can benefit from the principles, methods, and 
techniques of these fields. Social research based on qualitative methods only will be subject to discussion here as two major assumptions of social science working with quantitative methods, 
and oriented towards the normative paradigm, do not fit the described problems. 
- Social research based on quantitative methods checks given hypotheses by 
conducting surveys and observations. 
- It works with a representative sample and tries to get general statements by applying 
statistical methods. 
Knowledge Acquisition is not mainly a process of checking already given hypotheses, and 
normally, there is no representative sample of data. 
The following section presents the main ideas of social science working with qualitative 
methods. An introduction to model-based and incremental knowledge acquisition is given in the 
third section. The last section presents the application of principles, methods and techniques of 
social science in the early phase of knowledge acquisition. 
2. The main ideas of social science working with qualitative 
methods1 
If mathematical theorems are related to reality they are not sure and if they are 
sure they are not related to reality. [Einstein] 
The main idea of the interpretative paradigm [Blu66, Wil71, Blu73, May90a] is that human 
behaviour is not specified by fixed social or cultural rules and norms as is asserted by the 
normative paradigm. Every social activity or reaction of a person is based on a specific 
interpretative process. People give situations a special meaning, and their activities are based on 
this meaning. Therefore, in order to understand their activities, the researcher must understand 
their interpretation. This paradigm has some consequences on the goals of social research as 
described below. 
w hypothesis 
1 subject ) [ object J v / refutation V / / ~4 
Figure 1. Separation of subject and object. 
Overcoming the separation of subject and object: The main principle of the normative 
paradigm is the separation of the researcher (subject) and the research object. This strict 
separation seems to be necessary to get impartial results. Every subjective influence of the 
*See [Att74, Bot75, Bru66, Den70, Den89, Dou73, Dou85, Ras79, and Wil71] as 
introductions to principles and methods of qualitatively oriented social science written in 
English. researcher is regarded as a disturbance which must be minimized by standardization of the 
elicitation process, for example. Based on this separation the coherence of the research process 
is achieved by hypotheses which are the premisses of the process. The presumed hypotheses 
are the means of connecting the two disjunct parts of the research process. The research activity 
attempts to refute them. No answer is given where the hypotheses are coming from. 
However, this principle of the classical natural sciences is not applicable to social sciences for 
two reasons: 
1. The examined object is a subject itself. The reduction of a human to a stimulus-
response-object ignores the fact that human reactions are conveyed by meaning. In other 
words, the behavior is not direct, and humans do not only react. The behavior is a result 
of an interpretation process. In addition, humans are active and also create meaning. 
2. The researcher's subjectivity is an important tool for the research process. As opposed 
to natural science, the object of the investigation is not an extraneous object, but also a 
subject which is a member of the same or a similar social community as the researcher is. 
This common basis is tji£ premise for understanding the object of the research. A 
substantial basis of the research process will be lost by the elimination of the researcher's 
subjectivity. 
common understanding based on "common sense" 
Figure 2. Overcoming the separation of subject and object 
"Understanding" as the goal of research: The task of the classical natural science is to 
explain natural phenoma by causal reasoning. Asking for the meaning does not make sense. 
This situation changes when humans become the objects of research. They interpret reality and 
produce meaning by their activity. Examining a person means: try to understand him or her. 
This may create some new problems for the research process: 
- The researcher and his object do not speak the same language; 
- They speak the same language but interpret it differently; 
- Often there is a difference between the articulated meaning and the real meaning, e.g. 
somebody is lying; 
- Often there is a difference between conscious and tacit meaning. 
The researcher consequently needs a "second socialization" that teaches him to think like 
the person he wants to examine. Theory production vs. theory revision: An important principle of qualitatively oriented 
social science is the openness of the examination. I want to show this principle by contrasting 
it with quantitatively oriented social science. The task of elicitation and interpretation of data is 
to refute presupposed hypotheses. The researcher can only perceive features of the object of 
research which are within the range of his hypotheses, their operationalizations, and 
measurements. The only influence the subject of research can have is on refuting some 
hypotheses. In qualitatively oriented research, however no special theory or hypothesis is used 
as premise of the research process. Based on some knowledge about the object of research 
(prejudices), which must be kept open and flexible, the researcher must try to become familiar 
with the new field. Hypotheses can be created during or after the examination. This has two 
implications: 
- The goal is not to verify given hypotheses but to create new ones. 
- It is possible to gain new points of view about the object. More than only those features 
may be found which become prominent due to the presumed hypothesis of the researcher 
and his community. 
Research as communication and context sensitivity of its results: Quantitatively 
oriented social science postulates a qualitative difference between common sense understanding 
and scientific explanation. It therefore works with standardized interviews, for example, in 
order to reduce the subjective influence of the researcher as much as possible. Qualitatively 
oriented social science claims on the contrary that this interaction is the main tool for 
understanding the object of research and is not an error to be eliminated. No standardized 
interview but an intensive and open talk enables the researcher to learn to think in the same way 
as his research object does. Therefore scientific understanding is not a qualitatively different 
way of understanding but an extented and methodologically well-defined common sense 
understanding (cf. [Lam88]). 
A similar methodological problem of the stimulus-response approach is that it ignores the 
context sensitivity of its results. For example, thorough examinations of surveys show the 
context sensitivity of peoples' opinions. They depend on the communication partner and on the 
situation the interaction takes place in. This is not a problem of opportunism, but it shows 
human complexity and ability to learn. A research method which produces an abnormal context 
creates abnormal results. Reducing the influence of the researcher and the context of the 
examination to an error to be minimized produces an uncontrolled influence on the object of 
research. Therefore qualitatively oriented social science assumes that data do not exist 
independently of their creation but as a result of a specific interaction between the researcher 
and the object of his research. They are no literals but indexical expressions [Wil71]. The 
influence of the researcher cannot be reduced but must be documented as an integrated part of 
the result. 
The research activity is an iterative process: Understanding is an iterative, 
approximative and faulty process. A first judgement is the foundation for the first perception 
and interpretation of the object of research. This interpretation creates a new understanding. 
This procedure must be repeated several times. If the process works well it leads to an 
increasing understanding. Because of the different types of socializations this process is 
infinite. The process can be illustrated graphically by a spiral called hermeneutic circle. The single-case approach: The research methods of the normatively oriented social science 
are based on a representative sample of the examined domain. It establishes the general features 
of this domain with the application of statistical procedures. Therefore it needs a large and 
representative sample to reduce the statistical error of its results. This requires a standardization 
of the elicitation process and data reduction as an interpretation technique, for example multiple-
choice tests. The collecting and interpreting of representative samples is not suitable for 
understanding because it is a very time-consuming process. Qualitatively oriented social science j 
uses the single-case approach. The researcher does not collect a representative sample but 
tries to find some typical or interesting members of the examined group. Not reduction to 
standardized attributes but the explication of the whole case with all its relevant features guides 
the research process. 
This shows the similarity of qualitative social science and the expert system building process: 
"A single proband is not regarded as an insignificant and exchangeable member of a sample and 
as a set of attributes defined by the researcher ... but the individuum is regarded as an expert j 
of understanding and interpreting everyday life." [Lam89, p. 6] j 
3. Incremental and model-based knowledge acquisition 
The knowledge engineering literature shows the contrast of rapid prototyping and life-cycle 
based approaches similar to earlier discussions in the field of software engineering. Rapid 
prototyping has the advantage that it quickly leads to a running system which can be used to 
evaluate the gathered knowledge. But it also leads to unstructured solutions, thus impeding 
understandability and maintenance of larger systems. It also needs to simultaneously consider 
knowledge aspects and their implementation in the used knowledge representation formalism. 
Therefore the view on the knowledge is determined by the chosen implementation language. 
Contrary to rapid prototyping, the application of the life-cycle paradigm leads to a well 
structured development process and result. But this result is often not the one wished by the 
client. Only the last phase produces a running system which can be used to evaluate the 
requirements and their realization. Subsequently, both approaches are discussed, and a method j 
of development which combines both approaches is described (cf [AFS90, ALF91, FAL91]); 
this method tries to combine their advantages by avoiding their disanvantages. j 
Model-based knowledge acquisition has two characteristical features. First, it 
distinguishes between the analysis and design/implementation phases. The expertise is analysed 
at the knowledge level independently of any details dealing with the subsequent technical 
realisation. The efficient realisation of the expertise at the symbol level is the task of a 
separate phase. The result of the analysis phase - the knowledge acquisition - is a model of the 
expertise. Secondly, knowledge acquisition is further subdivided into two different steps. The j 
task of the first phase is to get the so-called process knowledge [Mus89a, Mus89b], which j 
corresponds to the problem-solving method used by the expert. The result of this phase is a \ 
model of the problem-solving methods and their interactions. This is a very critical situation 
because the knowledge engineer must become familiar with a new field and task, and, besides, 
a great deal of the problem-solving capacity of the expert is based on tacit knowledge or skill. j 
This model serves as guidance for the acquisition of field-specific content knowledge 
[Mus89a, Mus89b] in a second step. The domain knowledge is gathered as needed for the problem-solving process. The result of the analysis phase is a model of the problem-solving 
method expressed in terms which are specific of the field. This model is called a conceptual 
model [Mus89a, Mus89b]. 
[Mor87] describes knowledge acquisition as an iterative, approximative and faulty 
process and therefore calls it sloppy modeling. There is no complete and perfect model of 
the expertise in the expert's head, but the expertise is based on experience, vague heuristics and 
tacit knowledge. Therefore a model is created as a result of the knowledge acquisition process. 
Knowledge acquisition shares these features with any other modelling process. Methods and 
tools for knowledge acquisition must therefore meet the following requirements: 
1. They must allow iterative modelling. 
2. They must be suitable for detecting errors. 
3. They must support error corrections. 
The incremental development of a prototype, its evaluation and modification meet the above 
given features. The disadvantages of rapid prototyping can be avoided if the prototype is not 
meant to become the real final system, but is only used for analysing the expertise in the way 
suggested by explorative prototyping (cf. [Flo84]).2 
In order to integrate the model-based and the prototyping point of view, an model-based and 
incremental knowledge engineering process consisting of three phases is suggested. 
Like in a life-cycle oriented approach three different phases are distinguished: analysis, design, 
and implementation. Specifying the result of the analysis phase, the conceptual model, in an 
operational language allows to integrate explorative and throw-away prototyping during the 
analysis phase. 
In the following the first phase, the analysis phase, is discussed in more detail. It is called 
model-based and incremental knowledge acquisition. Before discussing the details of 
it, the research paradigm to be followed by the knowledge acquisition process must be 
determined. This is connected to the question: is knowledge acquisition related to natural 
sciences or to the arts? The normative paradigm and its separation of subject and object as 
practiced in natural science is unsuitable for the understanding process as it is conceived in 
human sciences. The interpretative paradigm which deals with understanding is unsuitable for 
causal explanations. [Bro89] proposes a complementary application of both paradigms due to 
the ambiguity of the research topic, the human expertise. The content of the expertise is 
"knowledge" of natural phenomena, like knowledge of mechanical faults or illnesses. This 
knowledge exists as a human quality and hence as an object of cognitive sciences. The general 
justification for the qualitative methods is the fact that, beside the model of a natural field, a 
model of the problem-solving process in the human being is needed. 
This general statement must be restricted by the differentiation of knowledge acquisition in 
several steps. There are the three steps: theory production, theory instantiation, and 
theory evaluation. Only for the first step an analogy to social sciences (oriented on the 
interpretative paradigm) can be drawn. The other two steps violate the central principles of 
2She distinguishes three main kinds of prototyping: throw-away prototyping during the 
analysis phase, throw-away prototyping during the design/implementation phase and the 
stepwise development of a prototype to the target system. She calls the different kinds 
explorative, experimental, and evolutionary prototyping [Flo84]. openness and theory production. In the following section the first step, which is called 
knowledge elicitation according to [Add87]3, will be discussed in detail. 
Knowledge elicitation 
r^- Elicitation 
Interpretation 
Theory = model of the 
problem-solving method Interview, 
group discussion, 
observation 
content analysis 
(Tool: hypertext) 
Knowledge collection 
Instantiated theory 
Knowledge validation 
used method 
used technique 
data flow — Interactive 
techniques 
(Tool: e.g. MOLE) Automatical 
techniques 
(Tool: e.g. ID3) 
Executable 
language (tool: e.g. KARL) 
Figure 3. Incremental and model-based knowledge acquisition 
The second step, the collection of content knowledge, is a theory-guided instantiation of a 
formal theory with domain-specific terms. The model of the problem-solving method is 
reformulated with structures, concepts, relations, and constraints of the specific domain (cf. 
3"The terms elicitation and acquisition tend to be used interchangeably. However, there is a 
clear distinction between elicitation and acquisition in the creation of a model. Elicitation is the 
process of developing a model by interviewing the experts and observing their environment; it 
is theory formation or model design. Acquisition is the process of collecting the detailed 
information (facts) which fit into the framework defined by the model." [Add87] [BWS87]). This violates the principle of openness and therefore prohibits the application 
of qualitative methods or techniques. This is no disadvantage. The guidance by a theory can be 
exploited for building powerful acquisition tools. For example, the model of the problem-
solving method guides the collection of domain knowledge by interactive tools like e.g. MOLE, 
MORE, SALT [Mar88], and PROTEGE [Mus89a, Mus89b]. Automatical procedures like 
conceptual clustering (cf. [MiS84]) can generalize examples to taxonomies of concepts and 
relations. 
The domain-specific theory is checked for correctness and completeness during the third step. It 
tries to refute a given theory and therefore does not fit the interpretative paradigm either. There 
are less operational criteria which can be applied for this process. This is comparable to 
traditional software engineering where no effective and efficient procedures exists to show the 
correctness of a program. A pragmatical answer to this problem is prototyping. An 
implemented program is used to check whether the specification satisfies the intentions of the 
clients. Therefore, an operational language for the conceptual model is necessary to allow 
explorative prototyping. Currently some research is done for a language (cf. [AFL91, AHS90, 
FAL91, Lin90, KVS91, Wet90]) which enables the formulation of an operational conceptual 
model based on the different knowledge types proposed by KADS (cf. [BRW89]). 
4. Knowledge Elicitation 
The goal of knowledge elicitation is a model of the problem-solving method 4 The human 
expertise which is partially based on skill, experience, and tacit knowledge must be transformed 
into a formal and well understood theory. In the following the application of principles, 
methods, and techniques of qualitatively oriented social science will be discussed. A similar 
idea and examples for its succesful application can be found in [BeH89]. Bell and Hardiman, 
however, do not separate the different phases of knowledge acquisition because they do not use 
the model-based approach. An analogous suggestion is made by [Kor87], which refers to the 
distinction of "formal" and "narrative thinking". 
4.1. Principles for Knowledge Elicitation 
Knowledge elicitation is not only concerned with "what" the expert is doing, but "why" 
he is doing something, and what meaning it has for him. The goal is to find general patterns of 
his behavior, the so-called problem-solving methods. They are to be elicited, systematized, and 
generalized. The elicitation of the so-called process knowledge also shows the main difference 
to traditional software engineering. Not only "what" the system should do but also "how" it is 
done is a topic in knowledge elicitation. The "how" cannot be solved simply by an algorithmic 
solution which is independent of a task and a domain. One needs specific knowledge, like 
heuristics, on the part of the expert in order to build an efficient solution (cf. [Par86, AFS90]). 
The principles of knowledge elicitation are: 
Understanding: the task of the knowledge engineer can be described as a "second 
socialization". He must learn new words and new meanings for old words. Based on his 
4This is similar to the idea of a "grounded formal theory" in social science (cf. [Lam88]). common understanding and experience he must overcome his own way of thinking to learn to 
think like the expert. 
Openness and theory production: Process knowledge is knowledge implicitly 
encoded in human problem-solving abbility. It is based on skill and implicit knowledge. 
Therefore, a formalized model of the problem-solving process can only be achieved as a result 
of an open eliciting and modeling process. Any pre-judgement must be handed very carefully, 
because a wrongly chosen problem-solving method will strongly decrease the effectivity of the 
subsequent build system. In addition, this problem-solving method leads the further collecting 
of the content knowledge. 
Searching for implicit meaning: An important part of the expert's problem-solving 
capability is based on implicit knowledge. Therefore, not only his intentions have to be 
considered, but also his unconscious motives. Understanding the expert means in particular 
understanding unconscious parts of his expertise. 
Understanding by communication: The influence of the knowledge engineer on the 
expert is not a distortion which must be reduced. Explaining, structuring, and generalizing the 
expertise during the knowledge elicitation process extends its reliability, validity, and 
applicability. The learning process of the expert can improve the elicited knowledge. Sometimes 
he learns so much about his own skills that an expert system is not needed anymore. 
Process orientation: The modelling of expertise is an iterative, approximative, and faulty 
process. Therefore the activities of knowledge elicitation, knowledge collection, and knowledge 
evaluation have to be repeated several times. As modelling a problem-solving method is an 
infinite process, knowledge elicitation by itself is an iteration of the two activities elicitation 
and interpretation. 
Single case approach: Knowledge elicitation is usually based on the investigation of a 
single expert, and not on that of a representative sample of experts. For this reason, the 
information has to be interpreted and not reduced. 
4.2. Methods and Techniques 
This section does not give a complete survey of all methods and techniques of knowledge 
acquisition (cf. [BRW84, DaH88, COR89, KaL90, Kid87, McH89, 01R87]) but shows 
potential applications of methods developed by social scientists. Especially content analysis 
techniques - "Qualitative Inhaltsanalyse" and "Objektive Hermeneutik" - are new possible 
means of improving the interpretation step during knowledge acquisition. Because of the space 
limitation the article only touches on the methods of group discussion and observation (cf. 
[Gir84, Lam89, Man73]). 
4.2.1. Interview 
The standardized interview is an unnatural way of communication [Sch62], it is a means of 
trying to get context-free and objective information about the examined object. Like in an 
experiment, the central requirement is to obtain the same result under the same conditions. Consequently the questions are determined before the interview and the interviewer provides 
the stimulus the subject has to react to. Often possible answers are supplied, and the 
interviewee has to choose among them. The ideal is a procedure which is free from all inter-
subjective influences. This kind of interview violates central principles of qualitatively oriented 
social research and is therefore not one of its methods. 
The narrative interview [Sch76] is the opposite type of interview. The person being 
interviewed is invited to tell a story about a given topic. He has the active part during the 
interview. The idea is that this active role of the person being questioned allows the researcher 
to detect the implications which the story has for this person. The interviewed person decides 
on which part of the topic he wants to discuss and stress. Everybody who tells a story wants to 
make the meaning which the story has for him plausible. The selection of the parts and their 
emphasis can help to comprehend the narrator's understanding. In addition, one can postulate 
general patterns for the structure of every story. Every deviation of this scheme indicates hidden 
meanings which can be recognized. The general patterns of a story produce some pressure on 
the narrator - e.g. he has to choose between figures, to decide which details should be told, 
what should be shortened - therefore the interviewer can restrict his interaction to stimulation. 
Compiurisoa ©if A$ different interview typ&z 
closed 
interview + 
theory 
revision 
open 
interview + 
theory 
production ] standardized interview 
depth interview 
focussed int. 
problem-centred interview 
narrative interview 
understanding 
Figure 4. Comparision of the different interview types 
The main purpose of the problem-centred interview [Wit82, Wit89] also is the production 
of hypotheses. On the other hand a suggested hypothesis is a premise which becomes modified 
during the interview. The suggested hypothesis is a guideline for the interview, providing it 
with structure and direction. The main difference to the narrative interview lies in the more 
active role of the interviewer. He tries to guide the flow of the narration by using examples of 
stories and other techniques. The individual steps of the problem-centred interview are [Spo89]: 
1. Starting the discussion. 
2. General survey. 
3. Specific survey: 
•Reflection: the researcher formulates his own understanding and accepts 
possible corrections of it. - Questions: he asks questions about contradictory and incomplete parts of the 
interview. 
- Confrontation: the interviewee is confronted with his contradictions. 
4. Ad hoc questions: Asking about points of the interview guide which have not been 
covered yet. 
Further interview types are the focussed interview [Lam89, Spd89], which contains 
qualitative and quantitative elements, the deep or intensive interview [Lam89], the 
receptive interview [Lam89], the intensive interview with two researchers [Lam89] 
and the ethnographic interview [Sp689]. 
Application of the different interview types 
The degree to which an interview type conforms to the principle of qualitative social science or 
knowledge elicitation can be used to construct a framework which guides its application during 
different activities of knowledge elicitation. For example, the effort and closedness of the 
techniques leads to the following order of their application. This order also shows how 
focussed the research topic must be to allow the application of a interview type: narrative 
interview, problem-centred interview and focussed interview. This contributes to overcoming 
the mismatch problem as complained by [NPB91]. 
4.2.2. Group discussion and observation 
There are mainly three arguments for applying group discussion as an elicitation technique. 
First, it is possible to get a great amount of data in short time. Secondly, the members of a 
group discussion stimulate each other, thus hidden meanings are more easily articulated than 
during an interview. Thirdly, the creation of meaning and the ability of understanding it is the 
result of social processes, they are results of interactions of many people. Group discussion 
offers a better simulation for these social processes than, for example, an interview. Interviews 
always produce an artificial context. There exists a lot of different kinds of discussion 
techniques. [Lam90, p. 142] proposes the following features to catalogue them: 
1. Criteria applied to select the group members: 
- homogeneous vs. inhomogeneous groups 
- artificial vs. natural groups 
- groups with related and with unrelated members, e.g. randomly chosen samples 
vs. a family. 
2. Discussion style: 
- thematical structuring vs. openness 
- formal structuring vs. openness 
- neutral or involved discussion leader 
- directly or indirectly guided discussion 
On one hand observation is certainly the best technique to get familiar with a domain. On the 
other hand it is the most time-consuming technique. In the literature about knowledge 
acquisition this technique is mainly discussed as protocol analysis. The normatively oriented 
observation technique, which tries to create a context from an experiment in natural sciences is discussed in [K6n73]. The unstructured and structured participative observation 
oriented towards the interpretative paradigm is discussed in [Lam90, Gir84]. 
4.2.3. Content analysis 
The topic of the content analysis is the analysis of past communications like texts, tapes, or 
videos. Research guided by the normative paradigm looks for directly visible features of 
interaction whereas research oriented towards the interpretative paradigm searches for the 
meaning intended in the document. Therefore, in the first case, the content analysis helps to get 
data, whereas in the second case, it is applied to interpret data which are gathered by 
interviews, observation, or group discussion. 
Normatively oriented content analysis is a research technique for the objective, 
systematical, and quantitative description of the directly visible contents of interactions 
[May90b, Sil62]. Frequency analysis counts the relative portion of square centimetres in a 
newspaper dealing with the topic of research, for example. The valence and intensity 
techniques extend analysis by measuring the point of view of the communication act, e.g. 
whether it is pro or contra death penalty. Contingency analysis measures correlations of 
different communication acts, e.g. the attitude towards abortion and foreigners. 
The "Qualitative Inhaltsanalyse" of Mayring is a systematical analysis of past 
communication acts based on theory and rules [May89]. It is used for interpreting results of 
open interviews. The purpose is to recognise the intentions of the interviewee, that is the 
meanings which he or she is conscious of. Figure 5 shows the life cycle of a complete 
interpretation. The definition of the analysis units - phase 7 - determines the code unit, the 
context unit and the interpretation unit. The code unit defines the minimal size of the 
parts of a document which can be a member of a category, e.g. sentences, paragraphes, etc. 
The context unit determines the maximal size of the document's parts which can be member of 
the category, and the interpretation unit determines the temporal succession for interpreting 
defined parts of the documents [May90b]. Step eight is the vital phase of text analysis. 
[May90b] proposes four techniques for its execution. 
- Summary: The goal is to reduce the material while exposing its main structure and 
topics. Mayring discusses the macro operators generalization, construction, 
integration, and selection. 
- Explication: The goal is to extend the given parts of the documents by related parts 
which are necessary to understand the intention. 
-Structuring: The formally oriented structuring classifies the material formally, 
e.g. it defines the introduction of the story, the different chapters, and the conclusion. 
The content-oriented structuring classifies the material into different topics or 
aspects which are discussed in the different parts. The type-oriented structuring tries 
to organize the material by vital parts of it. It determines extreme, freuquent or 
theoretically interesting parts of it. The scale-oriented structuring maps parts of the 
documents as to values of variables like in valence or intensity techniques. All these four 
techniques need a definition of the category used for the structuring and an 
operationalisation by rules and examples. Preparation of the material 
1. Select the material 
2. Analyze the situation the text is a result of 
3. Describe the material formally 
T Z 
Formulate the goal of the research 
4. Determine the direction of the analysis 
i 
5. Give a theoretical differentiation of the research topic 
T 
Interpretation of the single case 
6. Determine the applied analysis technique and its detail 
life-cycle (these different detail life cycles are not shown in this 
paper). 
7. Define the analysis units 
4 ~ 
8. Real text interpretation by applying the used analysis 
techniques 
* 
9. Check the results by comparing them with the theory and 
further material 
Generalization and evaluation 
10. Generalize and create types 
11. Apply the evaluation criteria 
Figure 5. A life-cycle of the "Qualitative Inhaltsanalyse" of Mayring [May90b, p. 50]. 
The goal of the "Objektive Hermeneutik" by Oevermann is to discuss the implicit 
meanings of a communication act which are called objective meanings [Lam88, Lam89, 
OAK83, Sch89]. Understanding implicit meanings is a common sense-based ability, the 
"Objektive Hermeneutik" tries to systematize this ability. It proposes a life-cycle and rules for 
such an approach. It tries to carry out content analyses in a way like an engineering discipline. 
A second difference to common sense understanding is the intensity of the approach. In 
everyday life, some economic techniques are used which allow a rapid understanding to enable 
quick reactions. These techniques are given by socialization. The "Objektive Hermeneutik" does not use such economic techniques. It consists of four phases. Its third phase, the 
sequential detail analysis , is presented in figure 6 in detail. 
0. Explicate the context of the interaction: Show the alternatives which the 
person thinks he has. 
1. Paraphrase the impartial content based on "common sense". 
2. Explicate the intention of die person. Look only for those meanings the 
person would agree with you as being relevant. It is assumed that the person 
is not lying. 
3. Explicate the impartial meaning and its impartial consequences. Use the 
context of the text, the context of the subject, and theoretical knowledge. 
4. Explicate the function of the interaction by considering the different 
interaction roles of its members. 
5. Characterize the linguistic and grammatical features of the interaction. 
6. Compare the interpretation with the interpretation of other text parts. Look 
for general patterns. 
7. Explicate general relations, create types and patterns. 
8. Compare the results with the theory and the results of other parts. 
[Lam90, AOK83] 
Figure 6. The sequential detail analysis of the "Objektive Hermeneutik" 
Further approaches: The "Hermeneutik", the science of understanding and interpreting 
texts has a long tradition in arts and in law (cf. [Lam88, Lam89, May90b]). The approach of 
Banner tries to differentiate several steps involved in such an analysis (cf. [May90b]). The 
"Strukturelle Beschreibung" by Hermanns is a technique for interpreting the results of 
narrative interviews [Lam89]. See [Lam88] for the procedure of Barton und Larsfeld, 
[Lam88, May90a] for the procedure of building the so-called grounded formal theories of 
Glaser and Strauss, and [Hei87] for the "Sozialwissenschaftlich-hermeneutische 
Paraphrase" by Heinze und Klusemann. For more procedures also see in [Wit82]. 
Application of the different interpretation types 
The "Qualitative Inhaltsanalyse" of Mayring can be used to look for meanings. The "Objektive 
Hermeneutik" can be used to get implicit meanings. Because this is a very time-consuming 
procedure it can be used only for some very important sections. The "Hermeneutik" by Danner 
and the "structural description" by Hermanns can be used as a general framework for the 
interpretation of texts. 4.3. Tools for knowledge elicitation - Is it possible to shift knowledge 
elicitation to a theory-guided procedure? 
Experience in software engineering shows that there is a need for combining bottom-up and 
top-down procedures. This article also suggests this for the knowledge acquisition process. It 
offers a mainly bottom-up approach for knowledge elicitation which has to transform the 
narrative gestalt oriented thinking [Kor87] of the expert into a sound formal model and a 
mainly top-down approach for the collection of domain knowledge, see also [MRD91]. The 
main facility for bottom-up modeling is hypertext (cf. [MRE90] or ACQUIST [MRD91]). 
Especially the context connections done by the text analysis require a non-linear organization of 
the texts. Theory-guided tools for knowledge collection are e.g. MOLE, MORE, SALT 
[Mar88], and PROTEGE [Mus89a, Mus89b]. 
As an unguided bottom-up development is a very time-consuming process it is necessary to 
think about integrating top-down elements not only into the knowledge collection but also into 
the knowledge elicitation process. The KADS group proposes a library of so-called 
interpretation models (i-models) which are generic problem-solving methods (cf. 
[BWS87, HJK89]). This library can contribute to elicitating the expert's process knowledge. 
But until now there have been some problems with this approach. First, there is no clear list of 
criteria for selecting the optimal i-model. Second, the i-models are only vaguely described and 
still lack a clear semantics. Third, there are no good i-models for synthetical problem solving. 
Fourth, real-life applications often require combining different i-models. Finally, if the 
knowledge engineer wants to select a well suited i-model which can be used for building a 
conceptual model, he must be familiar with the problem. Therefore he needs some open 
techniques, even if he can reuse an existing i-model or a shell with a fixed problem-solving 
method. These open techniques, especially the narrative interview or the "Objektive 
Hermeneutik", are very time-consuming procedures. But the problem of knowledge elicitation 
is also a very ill-structured, and therefore complex, problem. 
5. Conclusion 
Knowledge elicitation, which is one step in a model-based and incremental knowledge 
acquisition, and qualitatively oriented social science show great similarities. Therefore 
principles, methods, and techniques of qualitatively oriented social science can be applied to 
knowledge elicitation. These similar features are created by the required openness in eliciting 
knowledge which is involved in the problem-solving ability of humans. As shown, this can be 
used for formulating principles for knowledge elicitation and obtaining categories to construct a 
framework which helps to improve the application of the various elicitation and interpretation 
techniques. The literature about knowledge acquisition offers a variety of techniques, but the 
question of when and for what purpose a technique should be chosen still remains to be dealt 
with in a more exhaustive manner. In addition, the article introduces interpretation techniques 
from the field of qualitatively oriented social science which can also be used for knowledge 
elicitation. The purpose is to contribute to shifting knowledge elicitation from an art to an 
engineering discipline. Acknowledgement 
I thank Jurgen Angele, Dieter Landes, Andrea Schneider, Rudi Studer, and especially Angi 
VoG for helpful comments, Christiane Rest and Gabi Rudnich for their support in correcting my 
manuscript. 
References 
[Add87] Addis, T.R.: A framework for knowledge acquisition. In Proceedings of the first 
European Workshop on Knowledge Acquisition for Knowledge-based Systems (EKAW'87), 
Reading University, September 2-3, 1987. 
[AFL90] Angele, J.; Fensel, D.; Landes, D.; Neubert, S.; and Studer, R: Knowledge 
Engineering in the Context of Related Fields of Research. In O. Herzog et.al. (eds.), Text 
Understanding in LILOG, Springer-Verlag, Lecture-Notes in Artificial Intelligence no. 546, 
Berlin, 1991, pp. 490-500. 
[AFL91] Angele, J.; Fensel, D.; Landes, D.; and Studer, R: KARL: An executable language 
for the conceptual model. In Proceedings of the 6th Banff Knowledge Acquisition for 
Knowledge-Based Systems Workshop, vol. I, Banff, October 6-11, 1991. 
[AFS90] Angele, J.; Fensel, D.; and Studer, R.: Applying software engineering methods and 
techniques to knowledge engineering. In D. Ehrenberg, D. et.al. (eds.), Wissensbasierte 
Systeme in der Betriebswirtschaft, Reihe betriebliche Informations- und 
Kommunikationssysteme, no. 15, Erich Schmidt Verlag, Berlin, 1990. 
[AHS90] Ackermann, H.; van Harmelen, F.; Schreiber, G.; and Wielinga, B.: A formalisation 
of knowledge-level models for knowledge acquisition. In International Journal of Intelligent 
Systems, 1990, Forthcoming. 
[Att74] Attewell, P.: Ethnomethodology since Garfinkel. In Theory and Society, 1, 1974, p. 
179-210. 
[BeH89] Bell, J.; and Hardiman, R.J.: The third role - the naturalistic knowledge engineer. In 
D. Diapper (ed.), Knowledge Elicitation - principles, techniques and applications, Ellis 
Horwood Series in Expert Systems, Chichester, 1989. 
[Ber87] Berry, D.C.: The problem of implicit knowledge. In Expert Systems, vol. 4, no. 3, 
August 1987. 
[Blu73] Blumer, H.: Der methodologische Standpunkt des symbolischen Interaktionismus. In 
Arbeitsgruppe Bielefelder Soziologen (eds.), Alltagswissen, Interaktion und gesellschaftliche 
Wirklichkeit, Bd. I: Symbolischer Interaktionismus und Ethnomethodologie, Rowohlt, 
Reinbeck bei Hamburg, 1973, pp. 80-188. 
[BoT75] Bogdan, R.; and Taylor, S. J.: Introduction to qualitative research methods, John 
Wiley & Sons, New York, 1975. 
[Bro89] Bromme, R.: Aufgaben- und Problemanalyse bei der Untersuchung problemlosenden 
Denkens. In G. Juttemann (ed.), Qualitative Forschung in der Psychologie, Roland Asanger 
Verlag, Heidelberg, 1989. 
[Bru66] Bruyn, S. T.: The Human Perspective in Sociology, Prentice-Hall, New Jersey, 1966. 
[BrW84] Breuker, J.A.; and Wielanga, B.J.: Techniques for Knowledge Elicitation and 
Analysis. Report 1.5 Esprit Project 12, University of Amsterdam, Department of Social 
Science Informatics) and Laboratory for Experimental Psychology, July 1984. 
[BrW89] Breuker, J.; and Wielinga, B.: Models of Expertise in Knowledge Acquisition. In G. 
Guida et.al. (eds.), Topics in Expert Systems Design, Elsevier Science Publisher B. V., North-
Holland, 1989. 
[BWS87] Breuker, J.; Wielinga, B.; Someren, M.v.; de Hoog, R.; Schreiber, G.; de Greef, 
P.; Bredeweg, B.; Wielemaker, J.; and Billault, J.-P.: Model-Driven Knowledge Acquisition: 
Interpretation Models. Esprit Project P1098, University of Amsterdam, 1987. [COR89] Knowledge elicitation techniques for knowledge-based systems. In D. Diapper (ed.), 
Knowledge Elicitation - principles, techniques and applications, Ellis Horwood Series in Expert 
Systems, Chichester, 1989. 
[DaH88] Davis, M.; and Hakiel, S.: Knowledge harvesting: A practical guide to interviewing. 
In Expert Systems, vo. 5, no. 1, February 1988. 
[Den70] Denzig, N. K.: The Research Act, Aldine Publishing Company, Chicago, 1970. 
[Den89] Denzin, N. K.: Interpretative Interactionism, Sage Publications, Newbury Park, CA, 
1989. 
[Dou73] Douglas, J.D. (ed.): Introduction to sociology, The Free Press, New York, Collier 
Macmillan Publisher, London, 1973. 
[FAL91] Fensel, D.; Angele, J.; and Landes, D.: KARL: A Knowledge Acquisition and 
Representation Language. In Proceedings of expert systems and their applications, 11th 
International Workshop, Conference "Tools, Techniques & Methods", 27-31 Mai, Avignon, 
1991. 
[Dou85] Douglas, J.D.: Creative Interviewing, Sage Publications, Beverly Hills, CA, 1985 
[Flo84] Floyd, C: A systematic look at prototyping. In R. Budee et. al. (eds.), Approaches to 
Prototyping, Springer-Verlag, Berlin, 1984. 
[Gir84] Girtler, R.: Methoden der qualitativen Sozialforschung, Hermann Bohlaus Nachf. 
Gesellschaft mbH, Graz, 1984. 
[Hei87] Heinze, T.: Qualitative Sozialforschung, Westdeutscher Verlag, Opladen, 1987. 
[HJK89] Hickman, F.R.; Killin, J.L.; Land, L.; Mulhall, T.; Porter, D.; and Taylor, R.M., 
Analysis for Knowledge-Based Systems: a practical guide to the KADS methodology, Ellis 
Horwood Limited, Chichester, GB, 1989. 
[Hof89] Hoff, E.-H.: Datenerhebung als Kommunikation: Intensivbefragung mit zwei 
Interviewern. In G. Juttemann (ed.), Qualitative Forschung in der Psychologic Roland 
Asanger Verlag, Heidelberg, 1989. 
[KaL90] Karbach, W.; and Linster, M.: Wissensakquisition filr Expertensysteme. Techniken, 
Modelle und Softwarewerkzeuge, Carl Hanser Verlag, Miinchen, 1990. 
[Kid87] Kidd, A.L. (ed.): Knowledge Acquisition for Expert Systems. A Pratical Handbook, 
Plenum Press, New York, 1987. 
[K6n73] Konig, R.: Die Beobachtung. In R. Konig (ed.), Handbuch der empirischen 
Sozialforschung, vol. 2, Grundlegende Methoden und Techniken, part 1, Deutscher 
Taschenbuchverlag, 3. Aufl., Stuttgart, 1973. 
[Kor87] Kornell, J.: Formal thought and narrative thought in knowledge acquisition. In Int. J. 
Man-Machine Studies, vol. 26, no. 2, 1987, pp. 203-212. 
[KVS91] Karbach, W.; VoB, A.; Schuckey, R.; and Drouven, U.: MODEL-K: prototyping at 
the knowledge level. In Proceedings of expert systems and their applications, 11th International 
Workshop, Conference "Tools, Techniques & Methods", 27-31 Mai, Avignon, 1991. 
[Lam88] Lamnek, S.: Qualitative Sozialforschung, Band 1, Methodologie, Psychologie 
Verlags Union, Miinchen, 1988. 
[Lam89] Lamnek, S.: Qualitative Sozialforschung, Band 2, Methoden und Techniken, 
Psychologie Verlags Union, Miinchen, 1989. 
[Lin90] Linster, M.: Declarative problem-solving procedures as a basis for knowledge-
acquisition: a first proposal. In Arbeitspapiere der Gesellschaft fiir Mathematik und 
Datenverarbeitung (GMD), no. 448, June 1990. 
[Mar88] Marcus, S (ed.): Automating Knowledge Acquisition for Experts Systems, Kluwer 
Academic Publisher, Boston, 1988. 
[Man73] Mangold, W.: Gruppendiskussion. In R. Konig, Handbuch der empirischen 
Sozialforschung, vol. 2, Grundlegende Methoden und Techniken, part 1, Deutscher 
Taschenbuch Verlag, 3. Aufl., Stuttgart, 1973. 
[May89] Mayring, P.: Qualitative Inhaltsanalyse. In Qualitative Forschung in der Psychologie, 
G. Juttemann (ed.), Roland Asanger Verlag, Heidelberg, 1989. 
[May90a] Mayring, P.: Einfuhrung in die qualitative Sozialforschung, Psychologie Verlags 
Union, Miinchen, 1990. [May90b] Mayring, P.: Qualitative Inhaltsanalyse, Deutscher Studien Verlag, Weinheim, 2. 
Aufl., 1990. 
[McH89] McGraw, K.L; and Harbison-Briggs, K.: Knowledge Acquisition. Principles and 
Guidelines, Prentice-Hall International, Inc., Englewood Cliffs, NJ, 1989. 
[MiS84] Michalski, R.S.; and Stepp, R.E.: Learning from Observation: Conceptual Clustering. 
In R.S. Michalski et.al. (eds.), Machine Learning. An Artificial Intelligence Approach, 
Springer Verlag, Berlin, 1984, pp. 331-364. 
[Mor87] Morik, K.: Sloppy modeling. In K. Morik (ed.), Knowledge Representation and 
Organisation in Machine Learning, Springer-Verlag, Berlin, 1987. 
[MRD91] Motta, E., Rajan, T., Domingue, J., and Eisenstadt, M: Methodological foundations 
of KEATS, the Knowledge Engineer's Assistant. In Knowledge Acquisition, vol. 3, no. 1, 
March 1991, pp. 21-47. 
[MRE90] Motta, E.; Rajan, T.; and Eisenstadt, M.: Knowledge acquisition as a process of 
model refinement. In Knowledge Acquisition, vol. 2, no. 1, March 1990, pp. 21-49. 
[Mus89a] Musen, M.: Conceptual models of interactive knowledge acquisition tools. In 
Knowledge Acquisition, vol. 1, nr. 1, March 1989, pp. 73-98. 
[Mus89b] Musen, M.: Automated Generation of Model-Based Knowledge-Acquisition Tools, 
Morgan Kaufmann Publisher, San Mateo, CA, 1989. 
[NPB91] Nwana, H. S.; Paton, R. C; Bench-Capon, T. J. M.; and Shave, J. R.: Facilitating 
the Development of Knowledge Based Systems. In Al Communications, vol. 4, no. 2/3, 
June/Sept. 1991, pp. 60-73. 
[OAK83] Oevermann, U.; Allert, T.; Knau, E.; and Krambeck, J.: Die Methode einer 
"objektiven Hermeneutik". In P. Zedler et.al. (eds.), Aspekte qualitativer Sozialforschung, 
Leske Verlag + Budrich GmbH, Opladen, 1983. 
[01R87] Olson, J.R.; and Rueter, H.H.: Extracting expertise form experts: methods for 
knowledge acquisition. In Expert Systems, vo. 4, no. 3, 1987. 
[Par86] Partridge, D.: Aritificial Intelligence. Application In The Future Of Software 
Engineering, Ellis Horwood Limited, Great Brtain, 1986. 
[RaS79] Rabinow, P.; and Sulliva, W.M. (eds.): Interpretative Social Science. A Reader, 
University of California Press, Berkeley, 1979. 
[Sch62] Scheuch, E., K.: Das Interview in der Sozialforschung. In R. Konig (ed.), Handbuch 
der empirischen Sozialforschung, I. Band, Ferdinand Enke Verlag , Stuttgart, 1962. 
[Sch76] Schutze, F.: Zur Hervorlockung und Analyse von Erzahlungen thematisch relevanter 
Geschichten im Rahmen soziologischer Feldforschung - dargestellt an einem Projekt zur 
Erforschung von kommunalen Machtstrukturen. In Arbeitsgruppe Bielefelder Soziologen 
(eds.), Alltagswissen, Interaktion und gesellschaftliche Wirklichkeit, Bd. 2: Ethnotheorie und 
Ethnographie des Sprechens, Rowohlt, Reinbeck bei Hamburg, 1976. 
[Sch89] Schneider, G.: Strukturkonzept und Interpretationspraxis der objektiven Hermeneutik. 
In G. Juttemann (ed.), Qualitative Forschung in der Psychologie, Roland Asanger Verlag, 
Heidelberg, 2. Aufl., 1989. 
[Sil62] Silbermann, A.: Systematische Inhaltsanalyse. In R. Konig (ed.), Handbuch der 
empirischen Sozialforschung, Bd. 4: Komplexe Forschungsansatze, Deutscher Taschenbuch 
Verlag, Stuttgart, 1962. 
[Sp689] Sporing, W: Qualitative Sozialforschung, B.G. Teubner-Studienskripten, Stuttgart, 
1989. 
[Wet90] Wetter, T,: First Order Logic foundation of the KADS Conceptual Model. In Current 
Trends in Knowledge Acquisition, B. Wielinga et. al. (eds.), IOS Press, Amsterdam, 1990. 
[Wil71] Wilson, T.P.: Normative and interpretative paradigms in sociology. In J.D. Douglas 
(ed.), Understanding everyday life, Routledge & Kegan Paul, London, 1971. 
[Wit82] Witzel, A.: Verfahren der qualitativen Sozialforschung, Campus Verlag, Frankfurt a. 
M., 1982. 
[Wit89] Witzel, A.: Das problemzentrierte Interview. In G. Juttemann (ed.), Qualitative 
Forschung in der Psychologie, Roland Asanger Verlag, Heidelberg, 1989. Part 2: 
Case-based Approaches to the 
Development of Expert Systems Case-based Reasoning and 
Model-based Knowledge-Acquisition 
Dietmar Janetzko & Gerhard Strube 
University of Freiburg 
Dept. of Cognitive Science 
This chapter outlines two different yet complementary approaches to enhance cognitive adequacy 
in the process of knowledge engineering: model-based knowledge acquisition and case-
based reasoning. Although both differ with respect to methods, goals and scientific background, 
arguments are advanced that a linkage of both approaches will result in a significant contribution 
to the methodology of knowledge acquisition for expert systems. To combine case-based reasoning 
techniques with conventional rule-based approaches poses the problem of when to use which 
technique. A conceptual framework for turn taking in problem solving is outlined that involves both 
heuristics of turn taking and architectural options for knowledge-based systems that impose 
constraints on turn taking. 
Introduction 
Concerning knowledge-acquisition the development of the first artificial intelligence (Al) systems 
started with a misunderstanding. Neither has elictation of knowledge taken place in early examples 
of expert systems, as the domain expert and the system programmer usually were the same person, 
nor has there been an encoding of knowledge, as the knowledge was just programmed in (Riesbeck, 
1988). Thus, the problems linked to the crucial steps of what has later been coined knowledge 
acquisition have not been realized in their full extent. 
When Al systems left the laboratories and set out to conquer real world domains the difficulties of 
knowledge acquisition became obvious. As a first reaction, primary attention has been given to 
increase the efficiency of knowledge elicitation. Methods to speed up knowledge elicitation 
(Hoffman, 1987), or tools that enable the domain expert to state his knowledge without help from 
a knowledge engineer (e.g. Boose & Bradshaw, 1987) are among the most prominent research 
objectives that are undertaken in that spirit. The attention given to knowledge elicitation contrasts 
sharply with the fact that the model of problem solving underlying most expert systems remained 
untouched. This line of research has been referred to as the mining view of knowledge engineering 
(Breuker & Wielinga, 1989). Breuker & Wielinga advocate an alternative approach, which they call 
the modeling view. Building a knowledge-based system is no longer understood as filling a shell with knowledge. According to the modeling view, a specification of expertise is built that defines 
a mapping between the real world of expertise and the artificial world of computer systems. 
There is general agreement that the modeling view is currently one of the most promising 
approaches to knowledge engineering (Schmalhofer & Bergmann, 1990; Ueberreiter & VoB, 1991). 
Nevertheless, there is a substantial body of work directed at various aspects of knowledge 
acquisition not yet covered by studies inspired by the modeling view. Some of these take up the 
controversial issues whether techniques for knowledge acquisition might eliminate the knowledge 
engineer. There is a debate on whether knowledge engineering is just a variety of software 
engineering, or an approach in its own right based on its own methodology and equipped with 
methods that give it a clear-cut profile (Becker, 1991). Finally, the question how cognitive science 
can contribute to the development of theories and tools of knowledge engineering has been 
discussed at length. 
This article attempts to outline relationships between cognitive science research and approaches to 
knowledge engineering based on the modeling view. Our focus lies in the area of case-based 
reasoning, which is rooted both in cognitive science and computer science. Special attention is given 
to turn taking, i.e. change of the mode of problem solving (case-based, rule-based etc.). The paper 
is organized as follows: The first part gives a brief account of case-based reasoning and sketches 
basic notions of KADS, the most advanced example of the modeling view. Then, case-based 
reasoning is specified anew in terms of the modeling view. Since turn taking is of special importance 
for linking case-based reasoning with model-based approaches to knowledge engineering, we review 
results of cognitive science that refer to this issue, and discuss the conditions (heuristics and 
architectural options) under which case-based reasoning or rule-based reasoning is (or should be) 
used to solve a given problem. 
The Modeling View of Knowledge Engineering 
The acquisition of knowledge and the maintenance of knowledge-based systems are the two basic 
tasks of knowledge engineering. Knowledge acquisition, in turn, is usually subdivided in knowledge 
elicitation and knowledge encoding (cf. Christaller, Gusgen, Hertzberg, Linster, VoB & VoB, 1988). 
The elicitation and encoding of knowledge results in a knowledge-base which is at the heart of 
knowledge-based systems. The two-phase model of knowledge acquisition gives the impression of 
being straightforward and simple. However, there is a number of difficulties related to this approach 
that have instigated research efforts directed at a methodology for knowledge acquisition. Among 
the problems that necessitate further research is the task-modeUmismatch (Kurbel, 1989), which 
occurs when a knowledge-based system cannot solve the problems it is intended to solve because 
the model of expertise realized in the knowledge-base of the system is fundamentally inadequate. 
Often, this happens if elicitation and encoding of knowledge has been fitted to some particular 
expert-system shell. In this case, even slight changes of the knowledge base or the inference rules 
of the system can only be carried out at the price of time-consuming efforts. Partly in response to 
this problem, Breuker & Wielinga (1987) proposed a distinction between an analysis and design 
phase. The analysis phase leads to a description of expertise at the 'knowledge level' (Newell, 
1980), called the conceptual model. The conceptual model is a functional specification of all kinds of knowledge (concepts, inferences, tasks, goals) and their relations being used for building a model 
of expertise. The conceptual model is a means to assess the completeness and consistency of the 
model of expertise that is realized in the knowledge-based system. Only after a conceptual model, 
at least a preliminary one, has been specified, work may advance into the design phase. Here, the 
conceptual model is translated into a design model, which provides the base for implementation. 
To design a detailed specification first, is in accord with the conventions of software engineering 
(Pressman, 1987). At the same time, the specification of a conceptual model means to step away 
from common knowledge-engineering approaches like rapid prototyping that usually lack a 
conceptual level (Christaller et al., 1988, Kurbel, 1989). According to Karbach, Linster & Voss 
(1989), the use of a conceptual level entails a number of advantages with regard to knowledge 
acquisition and the maintenance of knowledge-based systems: 
• Precise specification of the type of problem-solving 
• Support of systematic elicitation of the required knowledge 
• Opportunity of specification-guided implementation 
• Better documentation of the implemented system 
The Four-Layer Model of Expertise 
Among the first to advocate and develop a detailed methodology of a model-based approach to 
knowledge engineering were Joost Breuker and Bob Wielinga (e.g. Wielinga & Breuker, 1984). 
Their approach, called KADS (Knowledge Acquisition and Documentation Structuring), is intended 
to provide epistemological primitives for the description of expertise at the knowledge level. Thus, 
KADS allows for the development of conceptual models of expertise independent of particular im-
plementational constraints. Though partially inspired by cognitive science (e.g. Norman, 1983), 
KADS models do not strive for genuine cognitive modeling of expertise. The reconstructive 
description of expertise used in KADS comprises four layers of knowledge (four layer model of 
expertise): 
Domain Layer: The knowledge at the domain layer covers concepts, relations and structures. In a 
domain like toxicology, for example, alcohol, vomiting, and gastric lavage are concepts. There are 
various classes of relations (subsumption relations, causal relations, empirical associations), e.g., 
causal relations between toxins and symptoms. Concepts and relations can be represented in an 
inheritance hierarchy (Voss, Karbach, Drouven, Lorek, & Schukey, 1990). Relations build up 
structures, i.e., networks of relations. Domain layer knowledge is static and task-independent, 
because procedural knowledge about a particular task is not represented at the domain layer. 
Inference Layer: At the inference layer, knowledge is classified according to its functions in the 
model of expertise. The two basic objects on the inference layer are meta-classes and knowledge 
sources. Meta-classes constrain the range of potential roles concepts of the domain layer can take during problem solving. In the domain of toxicology, for example, toxins, symptoms and therapies 
are meta-classes. It is possible to build a hierachy of meta-classes; concepts may belong to more 
than a single meta-class. Knowledge sources are functional descriptions of primitive inference steps. 
Each knowledge source refers to a relation defined at the domain layer. Meta-classes are used to 
provide a specification of the knowledge which is expected at the input and produced at the output 
of aknowledge source. Examples of knowledge source applications are the operations match, select, 
or classify. 
Task Layer: At the task layer, knowledge encoded at the inference layer is coordinated in order to 
achieve a defined goal. The three basic objects on the task layer are goals, tasks and control 
elements. Goals are states the system strives to reach. The representation of goals takes the form of 
concepts with specific attributes. Procedures used to achieve a goal are called tasks. According to 
the systematic ordering and decomposition of goals and subgoals, which result in a goal-
tree, tasks are classified by building task-structures. Control elements are data structures 
representing information that dynamically changes during the process of problem-solving. 
Strategic Layer: The pursuit of goal achievement by carrying out, or rearranging a sequence of tasks 
is done at the strategic layer. Knowledge at the strategic layer is used to model features of expertise 
like early recognition of problems and dead ends for a particual strategy chosen, change of the 
strategy in problem solving, and other aspects that pertain to the control of problem solving. Goal-
trees and task-structures are used to represent the strategies required to model these properties of 
expertise. 
In KADS, the notion of a conceptual model refers to the task-oriented description of expertise at all 
four layers. KADS-models may be used in two directions: Following the constructive direction, a 
conceptual model is an intermediary between the elicitation and interpretation of knowledge in the 
analysis phase and the design model and implementation of the design phase. If the domain layer 
is separated from a conceptual model, the resulting knowledge structure is called interpretation 
model. Thus an interpretation model is a domain-independent abstraction of a domain-dependent 
conceptual model. Interpretation models are representations of generic types of problem solving and 
can be collected in libraries. Following the selective direction, the addition of concepts and relations 
from a new domain to some suitable interpretation model chosen from a library results in a new 
conceptual model. In a word, knowledge engineering according to the modeling view provides the 
feasibility of taking either a bottom-up or a top-down approach. Apart from coarse-grained methods 
(e.g., some expert-system shells), which proceed in a top-down fashion, knowledge engineering 
according to the mining view is mostly dedicated to bottom-up approaches. Methodologies and 
methods developed with the modeling view in mind are doubtlessly more sophisticated and less 
prone to problems. However, important features of human expertise, which give flexibility and 
efficiency to human problem solving, are currently out of the scope of research within the modeling 
view. In the next section, we shall portray some of these features within the framework of case-
based reasoning. Experience and Episodic Knowledge 
The ability to accumulate experience available for subsequent use in problem solving is still one of 
the most striking differences of reasoning in man and machine. The endeavor to endow computers 
with that faculty is an ongoing research enterprise, with a growing influence on knowledge engineer­
ing. Case-based reasoning is a research paradigm that addresses the issue of experience and its 
impact on reasoning (Slade, 1991). In particular, case-based reasoning aims at pursuing two goals 
for research, the description and explanation of the development and use of experience in humans, 
and the design of computer programms that are equipped with the same talents. 
Remindings of previous cases can be used to solve current problems (e.g., Kolodner, 1991). This 
is not surprising as we use this kind of reasoning every day. Recent years have witnessed a growing 
interest in this type of problem solving in cognitive science and artificial intelligence (Caplan & 
Schooler, 1990, Strube & Janetzko, 1990). The knowledge used in case-based reasoning can usually 
be traced back to specific episodes, i.e., to the temporal and local conditions under which the 
relevant facts occured (e.g., 'I came across a similar problem in Detroit last summer, when I was...') 
This kind of knowledge has been termed episodic knowledge (Strube, 1989). Alternatively, but with 
more emphasis placed on the use in problem solving, this knowledge is referred to as special 
purpose knowledge (Shavlik, DeJong, & Ross, 1987). 
In contrast, the type of knowledge typically used in knowledge-based systems is or pretends to be 
universally valid (e.g., 'A robin is a bird'). Common ways to represent semantic knowledge are 
production rules, frames, semantic networks, and constraints. When used in problem solving the 
advantages and disadvantages of episodic and semantic knowledge are quite different. Semantic 
knowledge provides the advantage of universal application and the disadvantage of costly 
adjustments to solve a specific problem. That is, reasoning from first principles when coping with 
a simple or well-known problem may become an expensive expedition into the problem space. 
Episodic knowledge, on the other side, has the disadavantage of being useful only within a narrow 
range of problems and the advantage of shortcutting processes of reasoning. This relationship has 
been termed the operationalitygenerality trade-off '(Shavlik, DeJong, & Ross, 1987). A lesson lerned 
from this comparison is that the use or non-use of either general or episodic knowledge is not in 
itself an advantage or disadvantage for the process of problem solving. High performance with 
regard to the speed, accuracy and reliability of results calls for the retrieval and use of the 
appropriate type of knowledge. In the next section, we will pursue the question in which way models 
of case-based reasoning make use of episodic knowledge. In subsequent sections we shall return to 
the issue of when to use episodic and semantic knowledge in order to combine the advantages of 
both. The Basic Model of Case-Based Reasoning 
There is general agreement among researchers in case-based reasoning that the feasibility of this 
kind of knowledge-based problem solving is tied to a number of computational steps, which outline 
the current topics of research in case-based reasoning: 
• How are cases represented, i.e., how to find adequate representations of episodic knowledge, 
and interfaces with general knowledge? 
• How are cases retrieved from memory (e.g., from a case library)? 
• What measure of similarity is appropriate to select a suitable case in memory (= source case) 
given a current case (= target case)? 
• How is (partial) carry-over of the solution of a source case done to the target case? 
• How may (parts of) the solution of a source case have to be adapted to the constraints of the 
target case? 
• How can general knowledge be automatically acquired from episodic knowledge? 
• How should a case library be organized (and re-organized)? 
Most of these issues have already become the subject of research in other areas of artificial 
intelligence and cognitive science. Therefore, case-based reasoning provides a framework of 
research to link those areas and their results. This is especially true for KADS, which places 
emphasis on different aspect of knowledge engineering in comparison to case-based reasoning 
(Bartsch-Sporl, 1991). KADS belongs essentially to the province of software engineering, but aims 
at a specification of a set of epistemological primitives in order to improve knowledge engineer­
ing. The long-term perspective of KADS is to become the accepted industrial standard for the 
development of knowledge-based systems. Rooted in cognitive science, case-based reasoning is a 
unifying approach of theories of knowledge representation, learning and problem solving. A possible 
way of uniting both approaches through re-specifying case-based reasoning in KADS is presented 
in Figure 1. This results in an inference structure of case-based reasoning, which integrates meta-
classes (e.g., problem statement) and knowledge sources (e.g., reorganize). Strategies of turn taking 
are both failure-driven and achievement-driven. A similar inference structure for KADS has been 
suggested by Bartsch-Sporl (1991). assessment of 
solution 
assess 
solution 
target & 
solution 
apply solution to 
target 
modified solution 
& target 
I 
modify solution 
of source 
I 
source & 
target 
I 
select appropriate 
case 
set of sources & 
target & 
assessments store target & 
solution in 
case-library I 
case-library 
assess similarity 
source - target 
assess previous 
success of 
^ source 
assess computatio-] 
nal costs of case-
modification problem statement 
I 
reorganize 
i 
target-case 
i 
select features/ 
indices 
i 
retrieval frame & 
target case I 
assess retrieval 
frame 
I 
assessments 
I 
retrieve source 
I 
set of sources & 
target 
target & 
no solution target & 
no solution 
Figure 1: Case-Based Reasoning Inference Structure From Case-Based Reasoning towards a Theory of Problem Solving 
Our framework for case-based reasoning has the power to encompass a number of different models 
of tasks (retrieval, representation, similarity judgements, etc.) and types of problem solving (case-
based, rule-based, constraint-based etc.). The combination of a case-based reasoner with a rule-
based orfrom-scratchreasoner (Carbonell, 1983, Hinrichs, 1988, Koton, 1988, Rissland & Skalag, 
1989) has drawn attention to the fact that high performance in problem solving depends on selecting 
the appropriate strategy of reasoning. Hence, the potential flexibility of hybrid expert systems is 
available only if they are equipped with facilities for demand-driven selection of the problem 
solving strategy. Which conditions and criteria make a particular one the strategy of choice depends 
on the theoretical perpective taken. We shall discuss two perspectives on turn taking, the cognitive 
scientist's perspective and the knowledge engineer's perspective. Both these perspectives yield 
answers that differ more in emphasis than in principle. In fact, results of cognitive science may serve 
to design a conceptual framework for turn taking, and the same conceptual framework may stimulate 
further empirical research in cognitive science. We return to that issue later. 
Turn Taking in Problem Solving: The Cognitive Scientist's Perspective 
From the perspective of the cognitive scientist, the question when to use a certain strategy of 
problem solving is answered by searching for conditions which systematically correspond with the 
preferred use of that strategy in natural problem solving. We now discuss some of the conditions that 
have been under study. 
Conditions of Learning. In a series of experiments, Caplan & Schooler (1990) looked for the 
conditions that make a human problem solver use either episodic knowledge via episode-
based processing, or semantic knowledge via rule-based processing. Their subjects first learned to 
use a microcomputer drawing package ('Fullpaint'), and then had to cope with a series of drawing 
tasks. Caplan & Schooler concentrated on the influence of different conditions of training schedules 
and complexity of training examples. Training instructions were provided in a random or an 
organized order and with or without an analogical model. The analogical model introduced the 
various functions of Fullpaint with the help of icons. Practise trials differed in visual and logical 
complexity both of which varied high and low. Performance was measured by using paper-and -
pencil tests and recording actual use of Fullpaint. In paper-and-pencil and problem-solving tests 
theanaiogical model condition yielded better performance than the no-model condition when 
practise trials were logically complex. When practise trials were logically simple the no-
model condition yields better performances than the model condition both in paper-and-
pencil- and problem-solving-tests. Caplan & Schooler dicussed their results in light of the encoding 
specifity hypothesis (Tulving & Thompson, 1973), which states that performance is best when the 
kind of processing used at encoding and retrieval are identical. If conditions of learning encouraged 
case-based reasoning, performance was best when problem solving in the tests was also of a case-
base type (e.g., working with examples or cases). The reverse was true for rule-based learning. Level of Expertise. Do experts prefer one way of problem-solving over another? Experimental 
findings indicate that the relationship between the level of expertise and the preferred mode of 
problem-solving depends on a number of different conditions. In fact, this relationship is by far more 
intricate than the question seems to presuppose. First of all, we may assume that experts have a great 
number of episodes or cases at their disposal. Additionally, experts have been shown to actually use 
episodic knowledge, i.e. cases, in problem solving (Voss, Greene, Post & Penner, 1983). But 
growing expertise is also accompanied by the acquisition of rules (Glaser, 1986). However, this is 
far from being necessary (Brehmer, 1980). Formation of rules becomes more likely if the expert has 
to give formal accounts of his activities regularly. Having acquired abody of rules, experts predomi­
nantly engage in rule-based processing, unless problems come up that are extraordinarily hard or 
exceptional. In contrast, several investigators have demonstrated that novices make extensive use 
of examples (e.g., Anderson, Farrell & Sauers, 1984, Reed, Dempster & Ettinger, 1985). Ross 
(1984) suggests that preference of case-based over rule-based reasoning results from the extent of 
similarity between the current and a previously solved problem. 
Peculiarities of the domain. Rule-based problem solving has met with remarkable success in a 
number of domains, especially technical ones. Hence, rule-based reasoning is usually regarded as 
a universal way of problem solving, some domains like history are ill suited for this method. It is 
therefore not possible to grasp the essentials of each and every potential domain of application and 
transform it into a body of detailed and efficient rules. In some domains instable causal relation­
ships, or a vast number of variables make rule-based simulations and predictions nearly impossible. 
Case-based reasoning is therefore of special importance for both problem solving and decision 
making in the domains of law, history, or politics (Neustadt & May, 1986). A recent example has 
been analyzed by Enzensberger (1991), who points out how Saddam Hussein was compared (via 
case-based reasoning) to Hitler in the Gulf War, and that the comparison suggests that measures that 
would have been the right answer to Hitler probably were the right reaction against Saddam. At the 
same time, domains like history and politics are rich in hypothetical reasoning. This is an important 
feature of human reasoning which only tentatively has been subject to modeling in knowledge-
based systems (e.g. Rissland, 1986): 
Previous Case: "We know Israel's reaction in similar situations in the past." 
Hypothetical Case: "But what about Israel's reaction nowadays, if..." 
Summing up. Which kind of strategy is adopted for the solution of a given problem, depends not on 
a single factor, but upon a variety of influences. Obviously, expertise does not call for a particular 
problem solving strategy. Case-based reasoning, for instance, may become a last resort to the novice 
who has no rules available, or it may be the hallmark of expertise when tackling exceptional and 
highly complex problems. Experts and novices alike, however, need to have access to both case-
based and rule-based knowledge, and their use of a strategy depends (at least partly) on the effort 
required to access that knowledge. Another relevant factor in deciding which problem solving strategy to use is the prognostic 
evaluation of the strategies at hand. To our knowledge, no experimental evidence exists how human 
experts proceed, i.e., do they access both rules and cases, and evaluate both these approaches 
afterwards, or do they access only the knowledge that promises a solution readily? The majority of 
existing case-based reasoning systems attempt a two-step procedure. First, they select in a combined 
step of selection/evaluation a set of cases from a case-library that are similar to the problem at hand. 
In a second step, the cases in the selection set are evaluated with respect to their computational costs, 
etc. (Bareiss & King, 1989). Retrieval of information from long-term memory has been framed in 
a similar way (e.g., Raaijmakers & Shiffrin, 1981). 
Turn Taking in Problem Solving: The Knowledge Engineer's Perspective 
Decision making with respect to selection of a problem-solving strategy, i.e., turn taking, seems to 
be a function of the whole system of problem solving and not tied to a single facility or "turn-
taking-decision-maker." The complexity of the issue calls for a careful treatment of results from 
cognitive science. Basically, we have two options: A direct way of using results from cognitive 
science is to look for results that can be translated in a straight-forward manner into heuristics. An 
indirect way of using results from cognitive science is to think about architectural options for 
knowledge-based systems that allow for flexible turn taking behavior as observed in humans. Thus, 
knowledge engineering might be inspired by cognitive science as far as heuristics and architectural 
options is concerned without necessarily striving for cognitive modeling of turn taking in natural 
problem solving. 
Turn Taking in Problem Solving: Heuristics 
Clear-cut criteria, or at least heuristics that motivate the preference of one type of problem-
solver over an alternative one, are highly desirable for designers of knowledge-based systems. Turn-
taking heuristics might be understood as conflict resolution strategies analogous to those used in 
rule-based systems (Jackson, 1986). Typically, conflict resolution strategies are used within the 
framework of a rule-based problem solver; their task is to decide which rule, i.e., step of reasoning, 
will be given priority when two or more rules could be applied. In a hybrid system, however, 
conflict resolution strategies are required as well between different problem solvers to organize turn 
taking, e.g., between case-based and rule-based reasoners. The following sample of heuristics should 
be judged with care, however, because only the interplay between the heuristics and the architecture 
of the system will suffice to motivate turn taking. 
Specifity. If a case (source case) has initially been retrieved from the case library that is equivalent 
to the problem (target case) to a large extent, the likelihood for case-based reasoning rises. 
Weighting of features may be introduced to account for the fact that both corresponding and non-corresponding features of source and target case differ in their importance for problem solving and 
in the computational costs spent when adapting the source case to the target case. 
Efficiency. The solutions of the cases that have been retrieved must be evaluated. If the source cases 
have not met with success, case-based reasoning should only be used if the prospect of using rule-
based reasoning is even worse. Of course, measures of efficiency may be extended to aspects like 
processing time, neglected conditions, financial consequences, etc. 
Domain. If the target case belongs to a domain or subdomain where case-based reasoning has 
formerly been applied successfully the probability to use case-based reasoning rises. 
Turn Taking in Problem Solving: Architectural Options 
Conflict resolution strategies between problem solvers vary according to a number of aspects related 
to the architecture of a system that integrates several problem solvers. 
• Turn taking may be failure-driven or achievement-driven. 
• Strategies of conflict resolution may be realized in a hierarchical architecture with a higher, 
or meta-problem solver excercising control over the other, basic ones (case-based, rule-
based, etc.). Alternatively, conflict resultion strategies may be realized in a heterarchical 
architecture, where the different problem solvers settling turn taking among themselves. This 
way of controlling the behavior of the system is akin to Selfridge's (1959) pandemo-
model (albeit not to its implementation in those days), or to Minsky's (1985) society of mind. 
• Either division of labor is possible among the various problem solvers, or each problem 
solver works on each step of a problem on its own. 
• We need a set of defined points of turn taking. These are points in the flow of control of 
problem solving where the result of a test is used to decide whether or not the mode of 
problem solving has to be changed. Points of turn taking have to fulfil three requirements: 
- They have to be distributed across the whole process of problem solving. 
- It is not useful to install too great a number of points of turn taking. This would result 
in high computational costs, similar to someone who solves a problem and spends a lot 
of time and energy thinking about possible alternatives to solve the problem all the while. 
- To keep computational costs low, the test used by points of turn taking should be a 
natural part of the process of problem solving (e.g. computation of the degree of 
specifity, similarity, estimated distance to the solution, etc.) control scheme could possibly be implemented, where all the basic problem solvers take turns 
regularly, and the meta-problem solver pops up and decides upon turn taking among them only when 
too much computing resources have been used. 
A combined scheme seems also possible for failure-driven and achievement-driven control of turn 
taking. In fact, only an intelligent combination of both options allows for a sufficiently flexible 
behavior. A purely failure-driven approach is doomed to fail, because each basic problem solver 
may face problems and - consequently - no problem solver would attempt to solve the problem. This 
might be called giving up too early. Strict failure-driven turn taking will rule out every problem 
solver, unless it takes into consideration both the extent of the problem and the expected achieve­
ments of each basic problem solver. Likewise, a purely achievement-driven approach will not 
succeed, if a problem solver runs into lengthy computations and does not take into account to change 
the mode of problem solving. This might be labeled giving up too late. 
Cognitive Science and Knowledge Engineering 
We hope to have shown how methods and results from cognitive science may be applied to the field 
of knowledge engineering. We have done so, of course, in a selective way. Cognitive science has 
produced a multitude of papers that might be applied to knowledge engineering for case-
based expert systems. Issues of assessing similarity between cases have been treated by Tversky 
(1977), or Osherson (1987). Modification of cases might profit from research on analogical mapping 
(Falkenhainer, Forbus & Gentner, 1990; Holyoak & Thagard, 1989). Turning around, we recognize 
that research in cognitive science is instigated in many ways by 'applied* problems, e.g., in 
knowledge engineering. Transfer in both directions is the essence of the interplay between applied 
and basic research. 
One of the peculiarities of knowledge engineering as an applied science is that it draws on a variety 
of basic fields of research in addition to cognitive science, notably computer science/AI and 
mathematical logic. From the perspective of knowledge engineering, therefore, any offerings from 
cognitive science have to stand their ground against the alternatives offered by those other basic 
disciplines. Cognitive science is pressed by that to develop its own profile of research. Although 
similar in many ways to Al because of the programming techniques used in cognitive modeling, 
cognitive science is still distinct from Al because it focuses on modeling human information 
processing and experimental methods for testing its models, thus becoming a natural science rather 
than a branch of engineering. Bibliography 
Anderson, J. R., Farrell, R„ & Sauers, R. (1984). Learning to program LISP. Cognitive Science, 8, 
87-129. 
Bareiss, R., & King, J. A. (1989). Similarity assessment in case-based reasoning. In DARPA (Ed.), 
Proceedings of a Workshop on Case-Based Reasoning (pp. 67-71). Holiday Inn, Pensacola 
Beach, Florida, May 31 - June 2,1989. San Mateo, CA: Morgan Kaufmann. 
Bartsch-Sporl, B. (1991). KADS for (all) Cases. In B. Ueberreiter & A. VoB (Eds.), Materialien 
KADS Benutzer Trejfen. Miinchen: Siemens. 
Becker, B. (1991). Gibt es eine Methodologie der KI? Vortrag gehalten aufder 10. Friihjahrsschule 
Kunstiche Intelligent 23-10.3.1991. Gunne. 
Boose, J. & Bradshaw, J. (1987). Expertise transfer and complex problems: using AQUINAS as a 
knowledge acquisition workbench for knowledge-based systems. International Journal of 
Man-Machine Studies, 26, 3-28. 
Brehmer, B. (1980). In one word: Not from experience. Acta Psychologica, 45,223-241. 
Breuker, J., & Wielinga, B. (1987). Use of models in the interpretation of verbal data. In A. L. Kidd 
(Ed.), Knowledge acquisitionfor Expert Systems: A practical handbook (pp. 17-44). Pitman: 
London. 
Breuker & Wielinga (1989). Models of expertise in knowledge acquisition. In G. Guida, & C. Tasso 
(Eds), Topics in expert system design (pp. 265-295). Amsterdam: North-Holland. 
Breuker, J., Wielinga, B., van Someren, M., de Hoog, R., Schreiber, G., de Greef, P., Bredeweg, B., 
Wielemaker, J. Billaut, J.-P., Davoodi, M, & Hayward, S. (1987). Model-driven knowledge 
acquisition: Interpretation models. Deliverable task Al, Esprit Project 1098, Memo 87, VF 
Project Knowledge Acquisition in Formal Domains. 
Breuker, J., & Wielinga, B. (1989). Models of expertise in knowledge acquisition. In G. Guida, & 
C. Tasso (Eds.), Topics in expert system desing: Methodologies and Tools. Amsterdam: 
North-Holland. 
Brooks, L. R. (1987). Decentralized control of categorization: The role of prior episodes. In U. 
Neisser (Ed.), Concepts and conceptual development (pp. 141-174). New York: Cambridge 
University Press. 
Caplan, L. J., & Schooler, C. (1990). Problem solving by reference to rules or previous episodes: 
The effects of organized training, analogical models, and subsequent complexity of 
experience. Memory & Cognition, 18,215-227. 
Carbonell, J. (1983). Learning by analogy: Formulating and generalizing plans from past experience. 
In R. Michalski, J. Carbonell, & T. Mitchell (Eds.), Machine Learning: An Artifi­
cial Intelligence Approach (pp. 137-161). Palo Alto: Tioga Press. 
Ceci, S. J., & Liker, J. K. . (1986). A day at the races: A study of IQ, expertise, and cognitive 
complexity. Journal of Experimental Psychology, 115, 255-266. 
Christaller, T., Gusgen, H.-W., Hertzberg, J., Linster, M„ VoB, A., & VoB, H. (1988). Expertise und 
ihre Modellierung auf dem Rechner. etz, 109,1002-1006. 
DARPA (Ed.). (1989). Proceedings of the Second Workshop on Case-Based Reasoning, Holyday 
Inn, Pensacola Beach, Florida. San Mateo, CA: Morgan Kaufmann. 
di Piazza, J. S., & Helsabeck, F. A. (1990). Laps: Cases to models to complete expert systems. Al 
Magazine, 11 (3), 80-107. 
Enzensberger, H. M. (1991). Hitlers Wiederganger. Der Spiegel, 45 (6), 26-28. Falkenhainer, B., Forbus, K. D., & Gentner, D. (1990). The structure-mapping engine: Algorithm 
and examples. Artificial Intelligence, 41, 1-63. 
Feigenbaum, E. A. (1984). Knowledge engineering: The applied side of artificial intelligence. 
Annals of the New York Academy of Science, 246, 91-107. 
Glaser, R. (1986). On the nature of expertise In F. Klix, & H. Hagendorf (Eds.), Human memory and 
cognitive capabilities (pp. 915-928). Amsterdam: Elsevier. 
F. Hayes-Roth, D. Waterman, & D. Lenat (Eds.). (1983). Building expert systems. Reading, MA: 
Addison-Wesley. 
Hinrichs, T. R. (1988). Towards an architecture for open world problem solving. In Proceedings of 
a Workshop on Case-based Reasoning, Holy day Inn, Clearwater, Florida (182-189). San 
Mateo, CA: Morgan Kaufmann. 
Hoffman, R. R. (1987). The problem of extracting the knowledge of experts from the perspective 
of experimental psychology. Al Magazine, Summer 1987, 53-67. 
Holyoak, K. J., & Thagard, P. (1989). Analogical mapping by constraint satisfaction. Cognitive 
Science, 13,295-355. 
Jackson, P. (1986). Introduction to Expert Systems. Reading, MA: Addison-Wesley. 
Karbach, W. (1989). Modellbasierte Wissensakquisition. KI, 4, 13. 
Karbach, W., Linster, M., VoB, A. (1989). OFFICE-Plan: Tackling the synthesis frontier. In 
Proceedings ofGWAI(pp. 379-387). Berlin: Springer. 
Karbach, W., VoB, A., & Tong, X. (1988). Filling in the knowledge acquisition gap: Via KADS's 
models of expertise to ZDEST-2's expert systems. In Proceedings of the Second Euro­
pean Knowledge acquisition workshop, 31:1-17. Bonn: Gesellschaft fur Mathematik und 
Datenverarbeitung. 
Kolodner, J. L. (Ed.). (1988). Proceedings of a Workshop on Case-based Reasoning, Holydaylnn, 
Clearwater, Florida. San Mateo, CA: Morgan Kaufmann. 
Kolodner, J. L., & Simpson, R. L. (1986). Problem solving and dynamic memory. In J. L. Kolodner 
& C. K. Riesbeck (Eds.), Experience, memory and reasoning (pp. 99-114). Hillsdale, N.J.: 
Lawrence Erlbaum. 
Koton, P. (1988). Reasoning about evidence in causal explanations. In Proceedings of a Workshop 
on Case-based Reasoning, Holydaylnn, Clearwater, Florida (pp. 260-270). San Mateo, CA: 
Morgan Kaufmann. 
Kaplan, D., Leslie, J., & Schooler, C. (1990). Problem solving by reference to rules or previous 
episodes: The effect of organized training, analogical models, and subsequent com­
plexity of experience. Memory & Cognition, 18, 215-227. 
Kurbel, K. (1989). Entwicklung und Einsatz von Expertensystemen. Berlin: Springer. 
Minsky, M. (1985). The society of mind. New York: Simon & Schuster. 
Musen, M. A. (19&9). Automated Generation of model-based knowledge acquisition tools. Pitman: 
London. 
Neustadt, R., & May, E. (1986). Thinking in time: The use of history for decision makers. New 
York: The Free Press. 
Norman, D. A. (1983). Some observations on mental models. In D. Gentner, & A. Stevens (Eds.), 
Mental Models. Hillsdale, NJ: Lawrence Erlbaum. 
Osherson, D. N. (1987). New axioms for the contrast model of similarity. Journal of Mathematical 
Psychology, 31, 93-103. 
Pfitzner, K. (1990). Die Auswahl von Bibliothekslosungen mittels induzierter Problemklassen. In 
Beitrage zum 4. Workshop Planen und Konfigurieren, Vim, Mai 1990. Pressman, R. S. (1987). Software engineering. New York: McGraw-Hill. 
Raaijmakers, J. G. W., & Shiffrin, R. M. (1981). Search of associative memory. Psychological 
Review, 88, 93-134. 
Reed, S. K., Dempster, A., & Ettinger, M. (1985). Usefullness of analogous solutions for solving 
algebra word problems. Journal of Experimental Psychology: Learning, Memory and 
Cognition, 11,106-125. 
Riesbeck, C. K. (1988). An interface for case-based knowledge acquisition. In J. L. Kolodner (Ed.), 
Proceedings of a Workshop on Case-based Reasoning, Holyday Inn, Clearwater, Flori­
da (pp. 312-326). San Mateo, CA: Morgan Kaufmann. 
Riesbeck, C. K., & Schank, R. C. (1989). Inside case-based reasoning. Hillsdale, NJ: Erlbaum. 
Rissland, E. L. (1986). Learning how to argue: Using hypotheticals. In J. L. Kolodner & C. K. 
Riesbeck (Eds.), Experience, memory and reasoning. Hillsdale, NJ: Lawrence Erlbaum. 
Rissland, E. L., & Skalag, D. B., (1989). Combining case-based and rule-based reasoning: A 
heuristic approach. In Proceedings of the International Joint Conference on Artificial Intel­
ligence 1989, August 1989, Detroit. 
Ross, B. H. (1984). Remindings and their effects in learning a cognitive skill. Cognitive Psychology, 
16, 371-416. 
Selfridge, O. G. (1959). Pademonium. A paradigm for learning. In The mechanisms of thought 
processes. London: H. M. Stationery Office. 
Shavlik, J. W., DeJong, G. F., & Ross, B. H. (1987). Acquiring special case schemata in 
explanation-based learning. Proceedings of the Ninth International Annual Conference of 
the Cognitive Science Society (pp. 851-860). Hillsdale, NJ: Erlbaum. 
Schmalhofer, F. & Bergmann, R. (1990). Case-based knowledge acquisition: Extending expert 
problem solutions for technical planning tasks. Paper presented at the Gl-Workshop 
Knowledge Engineering. Berlin 26.4-27.4.1990. 
Schulz, A. (1989). Software-Lifecycle und Vorgehensmodelle. Angewandte Informatik, 4, 137-
142. 
Shavlik, J. W., DeJong, G. F., & Ross, B. H. (1987). Acquiring special case schemata in 
explanation-based learning. In Proceedings of the Ninth Annual Conference of the Cognitive 
Science Society (pp. 851-860). Hillsdale, NJ: Lawrence Erlbaum. 
Strube, G. (1989). Episodisches Wissen. Arbeitspapiere der GMD, 385,10-26. 
Strube, G., & Janetzko, D. (1990). Episodisches Wissen und fallbasiertes SchlieBen: Aufgaben fur 
die Wissensdiagnostik und die Wissenspsychologie. Schweizerische Zeitschrift fur 
Psychologie, 49,211-221. 
Thagard, P., & Holyoak, K. J. (1989). Why indexing is the wrong way. In DARPA (Ed.), 
Proceedings of a Workshop on Case-Based Reasoning (pp. 36-40). Holiday Inn, Pensacola 
Beach, Florida, May 31 - June 2,1989. San Mateo, CA: Morgan Kaufmann. 
Tulving, E., & Thompson, D. M. (1973). Encoding specifity and retrieval processes in episodic 
memory. Psychological Review, 80, 352-373. 
Ueberreiter, B. & VoB, A. (1991). (Eds.) Materialien KADSBenutzer Treffen. Miinchen: Siemens. 
Voss, J. F., Greene, T. R., Post, T. A., & Penner, B. C. (1983). Problem solving in the social. In G. 
Bower (Ed.), The psychology of learning and motivation: Advances in research theory. New 
York Academic Press. 
VoB, A., Karbach, W., Drouven, U., Lorek, D„ & Schukey, R. (1990). Operationalization of a 
synthetic problem. Task 1.2.1 Report. ESPRIT Basic Research Project P3178 REFLECT. 
Bonn: Gesellschaft fur Mathematik und Datenverarbeitung. Wielinga, & Breuker (1984). Interpretation models for knowledge acquisition. In T. O'Shea (Ed.), 
Advances in Artificial Intelligence. Amsterdam: North-Holland. 
Wielinga, B., & Breuker, J. (1986). Models of expertise In Proceedings of the European Conference 
on Artificial Intelligence, Brighton, 1986 (pp. 306-318). 
Wippich, W. (1982). Erinnerungen erinnern: Ein automatischer Vorgang? Zeitschrift jur 
experimented und angewandte Psychologie, 29, 517-530. 
Dipl. Psych. Dietmar Janetzko 
Prof. Dr. Gerhard Strube 
Institut fur Informatik und Gesellschaft 
Abteilung Kognitionswissenschaft 
Universitat Freiburg 
Friedrichstr. 50 
D-7800 Freiburg i.Br. The Refitting of Plans by a Human Expert 
Franz Schmalhofer, Christoph Globig, Jorg Thoben 
German Research Center for Artificial Intelligence 
University Bldg 57 
Erwin-Schroedinger Str. 
W-6750 Kaiserslautern 
email: schmalhc>@informatik.uni-kl.de 
Abstract. During the course of the development of a Case-Oriented Expert 
System for situated applications additional cases were needed. The required cases 
were obtained by having a human expert refit old solutions to new problems and the 
structural relations between source and target cases were analyzed: A higher degree 
of reuse of the old cases was found when the expert could apply derivational 
reasoning and a uniform design rationale (i.e. the solution of the source was 
generated by the expert himself) than when the expert could only analyze structural 
relationships (i.e. the source solution was constructed by some one else). Except 
with very obvious cases, it was also found, that different experts perceive different 
cases as the most similar source to a given target problem. The results also indicate 
for user-situated applications of expert systems. 
1. Introduction 
In order to overcome the brittleness of first generation expert systems, it has recently been 
proposed to develop Case-Oriented Expert Systems (COEx-Systems), which allow situated 
applications (Schmalhofer & Thoben, 1992). One prerequisite for developing such a system is 
that a sufficient number of prototypical cases are available for the desired competence of the 
system. Since originally we had only very few cases, we had an expert generate solutions to 
additional prototypical problems by having him refit old solutions, so that they would become 
solutions for those problems. 
The current paper first reviews the integrated knowledge acquisition method (Schmalhofer, 
Kuhn & Schmidt, 1991) for COEx-Systems together with their general characteristics. We then 
present a structural analysis of the refitted plans. Finally several conclusions with respect to the 
development of expert systems and the situated applications of old cases are drawn. 
2. Case-Oriented Expert Systems for Mechanical Engineering Planning Tasks 
In the knowledge acquisition phase for such COEx-Systems, model-based abstractions are 
formed from concrete past experiences, so that they can be reused in novel situations. Human 
expert judgments concerning the classification and similarities of the concrete past experiences 
are applied to obtain an abstraction hierarchy of problem classes (Bergmann & Schmalhofer, 
1991; Schmalhofer, Reinartz & Tschaitschian, in press) and supplementary knowledge from written materials is used to obtain explicit operator definitions (Schmidt, 1992) so that 
associated skeletal plans can be constructed (Bergmann, 1992; Friedland, 1985). 
The knowledge acquisition for such systems thus yields an abstraction hierarchy of problem 
classes with associated skeletal plans which allow for a situated utilization of past experiences 
in future tasks. During the knowledge acquisition phase, these past experiences have been 
interpreted by one or several experts within some uniform rationale. More details about such 
systems can be found in Schmalhofer & Thoben (1992). The respective knowledge acquisition 
procedures and tools were summarized by Schmalhofer, Bergmann, Kiihn & Schmidt (1991). 
The model of expertise or problem solving model (Breuker & Wielinga, 1989), which underlies 
COEx-Systems for planning tasks has been described by Kiihn & Schmalhofer (1992). 
Our research group has recently been developing such a system for production planning 
problems in mechanical engineering. Without going into any details of this application domain, 
we can state that production planning is a typical planning problem: For example, the mold of 
the workpiece defines the given state and the goal workpiece defines the goal state of the 
manufacturing problem. A number of different types of operations (chucking, unchucking, 
cutting operations) are available for transforming the mold (given state) to the goal workpiece 
(goal state). The operations themselves are quite complex requiring the specification of a 
number of different parameters (such as cutting path specification, specific cutting parameters, 
toolholders, etc.). It is therefore very useful to classify and abstract operations to different types 
of macro-operators. 
Workpiece 
Geometry Drive Shaft Drive Shaft Pinion Shaft Axle Shaft Axle Shaft Geometry 
gi g2 g3 g4 gs 
Vorkpiece 
material w1 w2 w3 w4 W1 w2 w3 w4 w1 w2 w3 w4 w2 w3 w4 w1 w2 w3 w4 
d1 3 3 1 3 3 1 4 4 2 4 4 2 
* • * * * 
m3 
d2 5 5 7 9 5 5 7 9 5 5 7 9 6 6 8 10 6 6 8 10 
* * * 
ml m4 m5 
d3 5 5 7 9 5 5 7 9 5 5 7 9 6 6 8 10 6 6 8 10 
* * * * * * 1 
C 
(0 
2 
Table 1 (after Schmalhofer & Thoben, 1992): A number of specific problems are used in order 
to delineate the competence of the future expert system. From the factorial combination of three 
types of manufacturing machines (dj,d2, and di), and workpieces with five different types of 
geometries (gj, g2, g3, g4, and gs) and materials (wj, W2 wj, and W4) fifty-two problems were 
identified as meaningful. The numbers 1 to 10 indicate the abstract problem classes to which a 
specific prototypical problem belongs. An abstraction hierarchy for these problem classes is 
shown in Figure 1. See text for further explanation. Since a production plan strongly depends upon the specific geometry of the workpiece (g), the 
workpiece material (w), and the particular machine (d), which are to be used when 
manufacturing the workpiece, we denote production problems with the descriptors g, w, and 
d. By using different indices with these descriptors we can thus refer to a given manufacturing 
problem. 
In Table 1 sixty production problems are specified through the factorial combination of 3 
manufacturing machines (di, d2, and d3), five different geometries (gi, g2, g3, g4> and gs) and 
four different workpiece materials (wi, W2, W3, and W4). Fifty-two of these problems (all 
problems whose cells are marked by a number between 1 and 10) are the prototypical 
problems, which delineate the desired competence of the future expert system. Problems with 
the same number were assigned to the same abstract problem class. The abstraction hierarchy of 
these ten abstract problem classes is shown in Figure 1. 
Since only five production plans were originally available for the 52 prototypical problems, i.e. 
the cases mi, m2,1113,1114, and ms (see Table 1), an expert refitted these plans (refitting roots) 
and his subsequently generated plans (refitting children) for 16 of the 52 prototypical problems. 
He also constructed one production plan from scratch (g5W4di). In Table 1, the problems with 
associated refitted plans are indicated by the asterisks. 
3. Plan Refitting 
Figure 2 identifies the different source cases which the expert used for finding solution plans 
for the 16 target problems: The source-target case relation is indicated by an arrow. Whereas 
case m3 was five times used as a direct source, the cases mi, m2, and 1114 were each only used 
once as a direct source and case ms was never used as a source. On 8 occasions one of the 
cases which had already been tested in the real world (tested source case or refitting root) were 
used as source and 8 times a solution plan which the expert had generated himself (i.e. a 
refitting child) was used as source (self-generated source case). / (5) 
\ 
r 
g ^3 (5) 
r 9 iw,d3 (5) 
y 
r g ^^j. (5) 
J 
9lW4d2 (9) 
t12: 5 tvMa (9) /• 
m2 (5) 
V g 2wid3 (5) 
ma (5) 
(6) 
9 SW3d2 (8) 
r 
g 5w3d3 (8) g 3w3di 0) 
V g 3w4di (0) 
g 3^3 (5) 
t,«: 9 3W2d2 (5) t2: g ^d. (3) 
> 
(3) 
9 2*4d3 (9) - t17: g 2^3^ (7) 
t3: g 5^di 
Figure 2 (after Schmalhofer & Thoben, 1992): The source case - target case relation is shown 
for the 17 tasks (tj to tjj) which were solved by the expert. In parentheses the abstract problem 
class that a specific task is associated with (see Figure 1) is noted. Task was solved from 
scratch, so that there is no source case associated with it. Whereas the cases mi, m2, mj, and 
m4 served as refitting roots, the other cases are denoted as refitting children. 
The task numbers ti to t\-j indicate the temporal order in which the 16 refitting and the one plan 
construction task (t3) were performed by the expert. These numbers show that the immediately 
preceding target solution was very often used as the source for the next target problem. For 
example the solution to task t£ was used as a source for t*j and the solution to t\4 was used as 
the source for tis. On other occasions somewhat earlier preceding target solutions were used as 
the source for the current target problem. For example, the last but one target solution was used 
as the source for task ti3. These temporal relationships indicate that for the refitting of old 
plans, the expert tried to maintain a fresh memory of the modification processes by which he 
constructed the old plan. 
When the expert remembers his reasoning (i.e. the derivations), by which he constructed or 
modified the old plan, he can perform derivational refitting processes (Carbonell, 1986). When 
the old cases was generated by somebody else, as for example the tested source cases mi, m2, 
m3, rri4 and ms (i.e. the refitting roots), the expert is more likely to perform only structural 
refitting processes (Hammond, 1989). Another important observation was: The plans which 
were obtained by modifying an already existing plan were completed by an order of magnitude 
faster than the plan which was produced from scratch (t3). 
3.1 Different Types of Modifications between Source and Target Plans 
We also analyzed more detailed structural relations between the source and target plans. 
Thereby it was distinguished between the refitting of tested source cases (i.e. refitting roots), 
where the expert was very likely to only use structural analogies and the refitting of self-generated source plans (i.e. refitting children), where the expert could at least to a certain degree 
also apply derivational analogies. 
zznrr i— 
c 
™3 g3w2d2 
g3wld2 
Figure 3: Shown are the structural relations between the tested source case msfor problem 
g3wjd2 and the resulting target plan for problem g3W2d2. See text for further explanation. 
Figure 3 shows the structural relationships between the operators of the tested source case m3 
and the refitted plan for the manufacturing problem g3W2d2. The Figure shows the structural relationships between corresponding operators of the source plan m3 for problem g3Wid2 and 
the target plan for problem g3W2d2 at the macro level. The ovals represent chucking and 
unchucking operations. All cutting (macro-)operations are indicated by rectangles. Within these 
rectangles, 1) the toolholder together with cutting tool, 2) the cutting path, and 3) the cutting 
parameter vc are symbolically represented from left to right. Shaded symbols in the target plan 
indicate changes from the source to the target. The solid lines with arrows indicate which 
operations of the source were reused in the target plan. The dashed lines indicate substantial 
changes in the individual operations themselves. 
The first two cutting operations of the source plan (see left side of Figure 3) were splitted apart 
and the resulting components were rejoined across the original operations of the source plan. 
Two new operations were thus created, which differ in all three parameters from the operations 
in the source case (see right side of Figure 3). As a consequence, the third cutting operation of 
the source was completely eliminated from the target. 
The execution order of the fourth and the fifth cutting operation of the source was also changed 
in the target. While the cutting path remained identical, cutting tool and cutting parameters were 
adjusted to the new workpiece material. The same modification was performed for the sixth and 
the seventh operation, except that these operations were not reordered in the target plan. 
( unchuck j •p 
c 
g5w3d2 unchuck 
g5w3d3 
Figure 4: Shown are the structural relations between a self-generated source and the resulting 
target case. See text for further explanation. Figure 4 shows the structural relationships between the self-generated source plan g5W3d2 and 
a target plan which is refitted for a machine which allows parallel processing. Whereas the 
chucking operations as well as the first two cutting operations remain identical, the third and 
fourth operations of the source are now executed in parallel in the target plan. In addition, one 
of the toolholders is changed. This source-target pair thus shows a large degree of reuse of the 
operations and the execution sequence of the old plan. 
3.2 Comparison of Structural Relations among Four Different Plan Pair 
Groups 
We compared the structural relations among four different groups of plan pairs. The first group 
consisted of the 8 pairs, which contained tested source cases (ti, ty, t6, ts, tn, ti4, tis, and 
ti6). The second group of plan pairs contained the 8 pairs with self-generated source plans (t2, 
The 16 actual modification tasks The 1 lmost similar case pairs 
type of change source is 
refiffing root source is 
refitting child source is 
refiffing root source is 
refitting child 
additional chuckings 0.25 0.13 0.00 0.14 
eliminated chuckings 0.00 0.13 0.00 0.14 
new parallel executions 0.13 0.25 0.25 0.29 
new serial executions 0.13 0.25 0.25 0.14 
splitted operations 0.75 0.00 0.50 0.00 
joined operations 1.13 0.00 1.25 0.00 
reordering of operations 0.75 0.50 1.00 0.43 
cutting path changes 3.75 0.25 6.75 1.43 
cutting parameter changes 5.63 4.25 8.75 2.86 
toolholder changes 4.88 2.50 8.50 1.57 
cutting tool changes 5.25 4.00 8.00 2.86 
total number of case pairs 8 8 4 7 
total number of cuts 
in source 61 57 34 46 
total number of cuts 
in target 53 55 36 41 
Table 2: Average number of different types of changes from the source to the target case for the 
16 performed modification tasks (see Figure 2) and 11 most similar case pairs from the 
abstraction hierarchy. t5> *7, t9> tio» ti2, ti3, and tn). The abstraction hierarchy of problem classes (see Figure 1 and 
for more details Schmalhofer & Thoben (1992)) was used for defining the third and fourth 
group of plan pairs. More specifically, for each of the 16 target plans, the most similar plan 
according to the abstraction hierarchy was selected as a hypothetical source case and the 
structural relations of these case pairs were analyzed. Group 3 contains the plan pairs, where 
the source plan was mi, m2,1113, or 1114 (i.e. the refitting roots): 1113 - giwid3, mi - giwid3, m2 
- g2Wid3, m3 - g3W2d2- Group 4 contains the plan pairs, where the source plan was a self-
generated plan: g3W3di - g3Widi, g3W3di - g3W2di, giwid3 - giwid2, givM2 - giW4d3, 
g5W3d2 - g5W3d3, g5W3d2 - g2W3d2, glW4d3 - g2W4d3. 
The results of this analysis are shown in Table 2. In general, fewer structural changes were 
observed between the (real or hypothetical) source and the target case, when the source case 
was also self-generated (i.e. a refitting child) than when the source case was generated by 
somebody else (i.e. a refitting root). And as expected, changes of the operations themselves 
occurred less frequendy than parameter changes (e.g. cutting parameter changes). 
3.3 Assessing the Expert's Consistency in the Source-Case Selections 
In order to assess the expert's consistency in selecting the same source case as the most similar 
one to a given target problem, further data were collected from the expert who had performed 
the 16 refitting tasks (HW). In addition an additional expert (RL) had to perform the same task. 
The task consisted in selecting the most similar source from the cases mi, m2, m3,1114 and ms 
to each of the 16 target problems, for which a plan modification was performed. In addition, 
the similarity between the source and target problem had to be estimated by a number between 1 
and 7. Whereas 1 meant the lowest similarity, 7 indicated the highest possible similarity. Table 
3 shrjws the results in comparison to the actually used source case. For self-generated source 
cases, the refitting roots (see Figure 2) were also determined. 
From Table 3 it can be seen that the cases which were identified as most similar by HW 
correspond in only 50 percent to the actually selected source case or root of the source case (i.e. 
the refitting root) in the refitting task. There is also only a 47 percent consistency between the 
two experts. However, when only those cases, which were identified as most similar with a 
similarity rating of 7 are considered, the two experts agreed in 100 percent of the cases. More 
details have been reported by Thoben, Schmalhofer & Reinartz. 
4. Conclusion 
Our main purpose for having an expert refit old plans to new problems was to obtain a 
sufficient number of cases for developing a Case Oriented Expert System for production 
planning in mechanical engineering. Although there is now a sufficient number of cases 
available for constructing skeletal plans for the important set of medium level problem classes 
(i.e. for all classes with a solid node in Figure 1), further prerequisites must be satisfied. 
Unlike case-based reasoning which does not make such strong prerequisites, Case Oriented 
Expert Systems require that all prototypical cases follow the same design rationale. This 
requirement arises from the fact, that several layers of more and more abstract skeletal plans are 
to be constructed from these cases, so that deductive justifications will exist for the resulting 
state and operator sequence abstraction mappings (Bergmann & Schmalhofer, 1992). We will 
consequently have to test, whether the cases of the refitting roots (mi, m2, m3,1114, and tn$) 
follow the same design rationale as the cases generated by the expert HW. problem HW RL 
task target case actually used 
source with 
(refitting root) most similar 
case identified most similar 
case identified 
ti g3w3dl m3 m5 : 5 m3:l 
t2 g3wldl g3w3dl (m3) m4: 6 m3:2 
U g3w4dl m3 m4 3 m3 :1 
t5 g3w2dl g3w3dl (m3) m4 4 m3:3 
t6 glw2d3 ml ml 3 m2:6 
h glwld3 glw2d3 (ml) ml 4 m2:6 
t8 g2wld3 m2 m2 7 m2:7 
t9 glwld2 glwld3 (ml) ml .4 m2:6 
Wo glw4d2 glwld2 (ml) ml :6 ml :5 
til g5w3d2 m4 m5 :7 m5:7 
tl2 glw4d3 glw4d2 (ml) ml :7 ml :7 
*13 g5w3d3 g5w3d2 (m4) m5 :7 m5:7 
tl4 g3w2d3 m3 m3 :7 m3 :7 
tl5 g3w2d2 m3 m3 :7 m3 :5 
tl6 g2w4d3 m3 m2 :3 m2:3 
t 17 g2w3d2 g2w4d3 (m3) m2 :4 m5 :4 
Table 3: Consistency assessment between two experts (HW and RL) and two different tasks: 
Actual source selection and most similar case identification with similarity judgement. 
Our study also yielded a typology for the structural relations between the old and the refitted 
plans. In some situations refitting purely consisted of small scale modifications (e.g. parameter 
changes) of the building blocks (i.e. macrooperators) of a plan, while the global structure of the 
plan (e.g. a complete or partial execution order) was maintained. Under other circumstances, 
the global structure of the plan was modified according to some well justifiably rationale. In still 
other situations, rather creative processes were applied: Operations were splitted and rearranged 
in different ways and the execution order was changed in a quite unpredictable way (see Figure 
3). Such changes may be an indication for different underlying design rationales. The 
inconsistencies between different experts are another indication for idiosyncratic planning 
rationales. 
The expert's refitting task is also similar to the task a user would perform with the expert 
system. As the expert in our study, the user (or the system) has to select the most similar abstract (or concrete) plan and refine (or refit) it to the problem at hand. From the observation, 
that different experts preferred different plans to be most similar to a given problem, we may 
conclude that expert systems should accommodate such differences in personal user 
preferences. In other words expert systems should be more user-oriented and user-situated 
applications should also be possible in expert systems. 
Acknowledgments 
This research was supported by grant ITW 8902 C4 from BMFT (German Ministry for Science 
and Technology) and by grant Schm 648/1 from DFG (German Science Foundation). We 
would like to thank Dipl.-Ing. Ralf Legleitner and Hans-Werner Hoper for serving as experts in 
this study. Thomas Reinartz helped in analyzing the recorded materials. The comments of Angi 
Voss and Stefan Wess on a previous version of this paper are also much appreciated. 
References 
Bergmann, R. (1992). Knowledge Acquisition by generating skeletal plans from real world 
cases. In Schmalhofer, F., Strube, G., & Wetter, T. (Eds.), Contemporary Knowledge 
Engineering and Cognition (pp. $pages). Berlin/Heidelberg: Springer-Verlag. 
Bergmann, R. & Schmalhofer, F. (1991). CECoS: A case experience combination system for 
knowledge acquisition for expert systems. Behavior Research Methods. Instruments. & 
Computers, 21 142-148. 
Bergmann, R. & Schmalhofer, F. (1992). Learning Plan Abstractions: Formal Model and 
Method. In Biundo, S. & Schmalhofer, F. (Eds.), Proceedings of the DFKI Workshop 
on Planning. DFKI-Document D-92nn, pp. 20-24. 
Breuker, J. & Wielinga, B. (1989). Models of expertise in knowledge acquisition. In Guida, G. 
& Tasso, C. (Eds.), Topics in expert system design, methodologies and tools (pp. 265 -
295). Amsterdam, Netherlands: North Holland. 
Carbonell, J.G. (1986). Derivational analogy: A theory of reconstructive problem solving and 
expertise acquisition. In Michalski, R.S., Carbonell, J.G., & Mitchell, T.M. (Eds.), 
Machine Learning: An artificial intelligence approach (Vol. 2, pp. 371-392). Los Altos, 
CA: Morgan Kaufmann. 
Friedland, P.E. & Iwasaki, Y. (1985). The concept and implementation of skeletal plans. 
Journal of Automated Reasoning. 1. 161-208. 
Hammond, K. (1989). Case-based planning. London: Academic Press. 
Kiihn, O. & Schmalhofer, F. (1992). Hierarchical skeletal plan refinement: Task-and inference 
structures. In Bauer, C. & Karbach, W. (Eds) Proceedings of the 2nd KADS User 
Meeting (pp. 201-210) Miinchen: Siemens AG. 
Schmalhofer, F. & Bergmann, R. (1992). Plan Recognition by Constructing Skeletal Programs 
as Abstractions from Symbolic Execution Traces, manuscript, DFKI Kaiserslautern. 
Schmalhofer, F., Bergmann, R., Kiihn, O., & Schmidt, G. (1991). Using integrated 
knowledge acquisition to prepare sophisticated expert plans for their re-use in novel 
situations. In Christaller, T. (Ed.), GWAI-91: 15. Fachtagung Kunstliche Intelligenz 
(pp. 62-73). Berlin: Springer-Verlag. 
Schmalhofer, F., Kiihn, O., & Schmidt, G. (1991). Integrated knowledge acquisition from text, 
previously solved cases, and expert memories. Applied Artificial Intelligence. 
311-337. 
Schmalhofer, F., Reinartz, T. & Tschaitschian, B. (in press). Intelligent documentation as a 
catalyst for developing cooperative knowledge-based systems. In Wetter, Th., Althoff, 
K.D., Boose, J., Gaines, B. Linster, M. & Schmalhofer, F. (Eds) Current 
Developments in Knowledge Acquisition: EKAW-92 Heidelberg: Springer-Verlag. 
Schmalhofer, F. & Thoben, J. (1992). The model-based construction of a case oriented expert 
system. Al-Communications. 5, 1, 3-18. 
Thoben, J., Schmalhofer, F., & Reinartz, T. (1991). Wiederholungs- Varianten- und 
Neuplanung bei der Fertigung rotationssymmetrischer Drehteile (DFKI-Document No. 
D-91-16). Kaiserslautern, Germany: German Research Center for Artificial Intelligence. Knowledge Acquisition by Generating Skeletal Plans 
from Real World Cases 
Ralph Bergmann 
German Research Center for Artificial Intelligence 
University Bldg 57 
Erwin-Schroedinger Str. 
D-6750 Kaiserslautern 
email: bergmann@informatik.uni-kl.de 
Abstract. Although skeletal plan refinement is used in several planning systems, a 
procedure for the automatic acquisition of such high-level plans has not yet been 
developed. The proposed explanation-based knowledge acquisition procedure 
constructs a skeletal plan automatically from a sophisticated concrete planning case. 
The classification of that case into a well-described class of problems serves as an 
instrument for adjusting the applicability of the acquired skeletal plans to that class. 
The four phases of the proposed procedure are constituted as follows: In the first 
phase, the execution of die source plan is simulated, and explanations for the effects 
of the occurred operators are constructed. In the second phase, the generalization of 
these explanations is performed with respect to a criterion of operationality which 
specifies the vocabulary for defining abstract operators for the skeletal plan. The 
third phase, a dependency analysis of the resulting operator effects, unveils the 
interactions of the concrete plan which are substantial for the specified class. In the 
forth phase, the concept descriptions for the abstract operators of the skeletal plan 
are formed by collecting and normalizing the important constraints for each 
operation that were indicated by the dependencies. With this procedure sophisticated 
planning solutions from human experts can be generalized into skeletal plans and 
consequently be reused by a planning system in novel situations. 
1. Introduction 
Many planning problems can be subdivided into more or less specific classes of problem 
types, in which each class has its own general solution plan (Tu, Kahn, Musen, Ferguson, 
Shortliffe & Fagan 1989). All plans of one class can thus be said to use the same solution idea, 
viewed from a higher level of abstraction. Such an abstract plan is called skeletal plan by 
Friedland and Iwasaki (1985) and is defined as follows: 
A skeletal plan is a sequence of abstract and only partially specified steps which, when 
specialized to specific executable operations, will solve a given problem in a specific problem 
context. Skeletal plans exist at many levels of generality and are applicable to different classes 
of planning problems. They capitalize on beneficial interactions and avoid detrimental 
interferences between the single operations of a plan. Skeletal plan operations just need the right 
level of generality in the respective situation, to maintain and replay important interactions and 
eliminate irrelevant details which are easy to reconstruct. Classical planning mechanisms such as specialization rules, (Shortliffe, Scott, Bischoff, 
Campbell, Melle & Jacobs 1981) or heuristic approaches (Friedland & Iwasaki, 1985) may be 
applied for obtaining a concrete plan from a good skeletal plan. A skeletal plan is refined to a 
concrete plan by specializing abstract operators independent of each other, so that the search in 
the space of concrete operators becomes feasible. This requires that the important interactions 
between operators, which always occur even in simple realistic planning tasks, must be taken 
into account during the construction of the skeletal plans. Therefore, the quality of the skeletal 
plan data base mainly determines the quality of the results of such a planning system. 
Nevertheless, the problem of the acquisition of skeletal plans has not yet been solved. In 
OPAL (Musen, Fagan, Combs & Shortliffe 1987), the knowledge acquisition system for the 
ONCOCIN expert system, oncological therapy protocols, which function as skeletal plans in 
this domain, must be constructed and entered manually with support of a graphical editor. For 
Friedlands MOLGEN planner the situation is similar. The acquisition and debugging of skeletal 
plans has been identified as a major problem (Stefik, 1981). This is because constructing 
skeletal plans is a modelling task which requires the definition of a terminology sufficiently 
abstracting from details which are irrelevant for the planning task. Usually neither an abstract 
planning terminology nor skeletal plans described in terms of such a terminology are directly 
available in real world domains. Schmalhofer and Thoben (this volume) have studied domain 
experts who are requested to construct skeletal plans for mechanical engineering. Their 
approach to the manual construction of skeletal plans seems to be successful but is quite time-
consuming for the expert. 
This current paper investigates explanation-based learning (EBL) (Mitchell, Keller, Kedar-
Cabelli 1986; DeJong & Mooney 1986) in order to establish an automatic knowledge 
acquisition method for obtaining skeletal plans from real world problem solutions for a given 
class of problems. A theory that describes important aspects of the operator effects is used to 
simulate the execution of the plan and to derive an explanation of how the plan solves those 
problem features that define the problem class. 
In the following sections, the real world application domain of mechanical engineering is 
introduced together with an example, and the representation of skeletal plans is discussed. The 
proposed method for an automatic knowledge acquisition is subdivided into four phases: a plan 
simulation and explanation phase, a generalization phase, a dependency analysis, and a 
normalization into the skeletal plan representation. In the final discussion the benefits and the 
limitations of this approach are evaluated in a general manner. 
2. Production Planning in Mechanical Engineering 
The planning domain used as the field of demonstration is mechanical engineering, more 
specifically the production of rotational parts on a lathe. Presently, the design of mechanical 
work pieces is widely supported by CAD systems, and computer controlled lathes (CNC-
machines) are used for manufacturing such parts. The planning process itself cannot be 
performed automatically since a lot of different kinds of domain knowledge are required for the 
construction of good plans. The characteristics of the complexity of this domain and the 
planning process are presented in detail by Schmalhofer and Thoben (this volume) through a set 
of planning tasks from a catalogue of a supplier of machining centers and tools. For the 
demonstration of the automatic approach to the acquisition of skeletal plans, only a simplified 
version of an already existing real world planning problem is introduced for clarity. Figure 1 
shows an example of the work piece to be produced together with a production plan which 
consists of one chucking and four cutting operations. Example Plan 
movement route cutting tool 
o 
a CSSNL 
CCLNL 
removed material Formalization of (he plan; 
chuck (lathe__dog, 30Kg\ left) 
cutCCSSNL', 
form(linear,(64t17), (21,17)), 
speed(450,0.45)) 
cutCCCLNU, 
foim(linear,(64,14),(39,14))f 
spced(450,0.45)) 
cutCCCLNU, 
form(linear,(64.10),(39,10)), 
speed(450.0.45)) 
cutCCCLNL', 
form(linear,(39,10),(39,17)), 
speed(l50,0.25)) 
Formalization of the desired workpiece 
surface(#l,form(linear, (3,0), (0,3))) A 
centerholc(# 1,7^n3mm') A 
surface(#2,form(linear, (0,3), (0,20))) A 
facing_area(#2) A 
surface(#3,form(linear,'(0f20), (18,20))) A 
surface(#4,form(linear, (18,20), (21,17)))A 
surface(#5,form(linearf (21,17), (39,17)))A 
surface(#6,form(linear, (39.17), (39,10)))A 
tolcrancc(#6,low) A 
surface(#7,form(Hnear, (39,10), (64,10)))A 
surface(#8,form(linear, (64,10), (643))) A 
facing_area(#8) A 
surface(#9,form(linear, (64,3), (61,0))) A 
centerhole(#9,'Zen3mm',) A 
Figure 1: 
In this example, a mold is fixed with a lathe dogf and material is removed in four steps, 
in which two cutting tools with different shapes are used. The material that is removed 
by each step is indicated by the shaded areas on the sketch of the mold. Note that this 
graphical sketch is only a two-dimensional sectional drawing of the 
three-dimenstional rotational part. desired |*—18 |-*_ 18 
workpiect •25 dimensions 
The formal description of the example is presented on the right side of this Figure. In this 
representation, the plan is defined as a sequence of operators together with their parameters. 
The workpiece is represented, similar to world states in STRIPS (Fikes & Nilsson, 1971), as a 
conjunction of predicate facts, in which each fact expresses one isolated attribute of the 
workpiece, such as a coherent surface area or a technological feature (centerhole, facing area or 
material) (Bergmann, Bernardi, Klauck, Kiihn, Legleitner, Schmalhofer & Schmidt 1990). 
This representation of cases, which can easily be derived from the data representations CAD 
systems employ, is used as the input for the skeletal plan construction procedure. 
2.1 Skeletal Plans in Mechanical Engineering 
A skeletal plan consists by definition of a sequence of abstract operators or operator classes. 
Extensionally, an operator class is formed by grouping some concrete operators together. 
Intentionally, an operator class needs to be described by a combination of relevant attributes that 
all operators of that class have in common. A conjunction of constraints to some operator 
features, such as inductive methods of concept formation usually construct, are a useful manner 
for defining operator classes for a skeletal plan. In mechanical engineering, for example, a 
subclass of chucking operators may be formed by the following conjunctive description of three 
operator features: chucking with centerholes and 
chucking position on the left side and 
two fixations. 
2.2 Acquisition of Skeletal Plans 
Automatic acquisition of skeletal plans by analysis of cases is itself a knowledge-intensive 
process. Knowledge is required to explain the functioning of the problem solution, to identify 
interactions between the operators, and a terminology is needed to construct the descriptions of 
operator classes. Therefore this automatic process is embedded into an integrated knowledge 
acquisition method, which has been described by (Schmalhofer, Schmidt & Kiihn, 1991b; 
Schmalhofer, Bergmann, Kiihn & Schmidt, 1991a). 
Within this integrated knowledge acquisition method, an interactive tool named COKAM 
(Case-Oriented Knowledge-Acquisition Method from Text) (Schmidt & Schmalhofer, 1990) is 
used to extract information from text and the expert's common sense knowledge, guided by 
cases of problem solution. The formalization of this elicited knowledge serves as a model of 
operators which already contains the basic terms on which operator abstractions can be 
composed. This operator theory is required to be mostly complete and tractable to enable the 
application of the explanation-based learning procedure. 
Another interactive knowledge acquisition tool named CECoS (Case-Experience Combination 
System) (Bergmann & Schmalhofer, 1991) yields a hierarchically structured set of problem 
classes from a set of prototypical cases through human expert judgements. The expert 
judgements are obtained so that a useful skeletal plan exists for each of the classes. An 
intensional definition of this class hierarchy, together with the classification of the origin case 
used for skeletal plan generation, is used to adjust the level of generality for the generated 
skeletal plan. 
3. The Generation Procedure 
The automatic generation of skeletal plans is based on an understanding of how a specific plan 
solves the given problem, and on recognizing those dependencies between the actions of the 
plan that are significant for a general solution for the whole problem class. A sequence of 
operator classes is constructed, so that the significant dependencies are maintained. The 
following detailed description of this approach is divided into four distinct phases. 
3.1 Phase-I: Simulation and Explanation 
This phase uses a domain theory that describes the applicability and the effects of operators for 
simulating the execution of the target plan. This theory formally represents each operator as a 
set of rules, in which the successor world state is created by the execution of the STRIPS like 
add- and delete actions of the rules' consequences. For example, two rules like Rl and R2 
describe two effects of chucking operators with different generality. Rl models the general 
effect of the execution of a chucking operator, whereas R2 models the more specific 
consequence of a special chucking operator that requires centerholes. 
Rl: IF operator(chuck(xt0ol,xl,x2))THEN R2: IF operator(chuck(xtoohxl,x2)) A 
DELETE(unchucked), 
ADD(chucked) requires_centerholes(xtool) A 
two_centerholes THEN 
ADD(chuck_precision(high)) Additionally, a set of axioms is provided to infer the conditions of these rules from the 
descriptions of the world states. With the axioms Al and A2 the condition of rule R2 can be 
inferred to be true for the first operator in the example in Figure 1. 
Al: (3cl,c2,xl,x2,x3,x4. (centerhole(cl,xl,x2) A centerhole(c2,x3,x4) A cl * c2)) -> 
two_centerholes. 
A2: requires_centerholesOathe_dog). 
With a complete theory for all operators in the target plan, its execution is successfully 
simulated by sequentially applying all rules for the operators Opi,...,Opn of the plan: 
Opi Op2 Opn 
Sini •Si •S2.... Sn-1 • Sgoal 
From the initial state Sini (the mold in mechanical engineering) all intermediate states that result 
after the execution of each operator, and the final state Sgoal (the target workpiece) are 
computed. The proofs that exists for the applicability of each operator rule can now be seen as 
an explanation of each effect, which depends on operator attributes as well as on world state 
attributes. 
3.2 Phase-II: Generalization 
These proofs are generalized using EBL (Mitchell, Keller & Kedar-Cabelli, 1986; DeJong & 
Mooney 1986), which yields separate and more general concepts for the produced effects. The 
operationality criterion for EBL determines the vocabulary for expressing the generalized 
concepts and establishes the constraints from which the operator classes are constructed. These 
terms are initially provided through the application of COKAM. As a result of this 
generalization, a set of conditions is found which ensures more generally that the situations 
Sl,...,Sn-l and Sgoal are created in the same manner. Look at the following example of four 
EBL generalized concepts for the effects of the first operator from Figure 1. 
Cl: chucked <-
operator(chuck(xtool» xl,x2)). 
C3: chucking_precision(high) <— 
operator(chuck(xtool» xl,x2)) A 
requires_centerholes(xtool)A 
centerhole(cl,x3,x4) A 
centerhole(c2,x5,x6) A 
cl *c2. C2: chucking_jposition(xpos) <— 
operator(chuck(xtool»x 1 ,xpos)X 
C4: chucking_fixations(xn) <— 
operator(chuck(xtool»xl,x2)) A 
number_of_fixations(xtool,xn)-
3.3 Phase-Ill: Dependency Analysis 
The task of the dependency analysis is to identify those effects of the operators which are 
necessary to guarantee that those features of the workpieces which are named as relevant for the 
classification of the workpiece, are created by every specialization of the skeletal plan. 
Therefore, the interconnections between the separate concepts which were identified in the 
second phase are determined and analyzed. A directed graph is constructed in which all existing dependencies between the concepts are explicidy noted as arcs. A dependency arc between two 
concepts Cx and Cy exists, if the concept Cx describes an effect which is a necessary condition 
which occurs in the formation of concept Cy. Figure 2 shows a graphical representation of the 
dependency graph that results from the analysis of the example in Figure 1. For example, the 
dependency of the concept Mtolerance(#6,low))" on the concept "chucking_precision(high)" 
states that it is necessary to have a high chucking precision in order to produce surface #6 with 
a low tolerance. Note that all the concepts are always related to one operator and usually require 
certain constraints on them. Thereby, the dependencies between two concepts also express 
dependencies between two operators. In the example mentioned above, the cut-operation which 
creates the surface #6 is dependent on the chucking operator. 
Figure 2: Dependency Graph for example case 
For generating a skeletal plan, which is tailored to a definite problem class specified by 
important features of workpiece and mold, all concepts on which the class-relevant features are 
dependent, have to be identified. This is achieved by computing the least subgraph which 
contains all relevant features of the problem class, and in which all dependency predecessors of 
the concepts in the subgraph are themselves part of the subgraph. In Figure 2, the gray marked 
concepts, together with their links, form the subgraph, which results from a description of the 
classification of the examples in Figure 1. An overgeneralization that may have resulted from 
the independent treatment of the operators must be avoided in order to ensure that the operator 
classes which are to be generated for the skeletal plan can in fact be specialized independently 
for the solution of a new planning problem. Therefore, the concepts of the determined subgraph 
are unified along their dependency arcs, which yields one general concept of the plan for the 
whole problem class. For the example in Figure 1 a fragment of this concept is sketched as 
follows: surface(#6,....) A surface(#7 ) A low_tolerance(#6) <— 
operator(l,chuck(xtooll»xl,x2)) A ; From concept Cl: "chucked" 
operator( 1 ,chuck(xtool2>*3,left)) A ; From concept C2 : "chuckingposition" 
operator(l ,chuck(xtool3> x4,x5)) A ; From concept C3: "chuckingprecision" 
requires_centerholes(xtool3) A ; From concept C3: "chuckingprecision" 
centerhole(cl,x6,x7)) A ; From concept C3: "chuckingprecision" 
centerhole(c2,x8,x9)) A ; From concept C3: "chuckingprecision" 
cl T£ C2 A ; From concept C3: "chuckingprecision" 
operator(2,cut(...)) A \ From concept "surface(#5...)" 
3.4 Phase-IV: Normalization into Skeletal Plan Representation 
This phase builds the skeletal plan in its final representation by identifying independently 
solvable sub-formulas from the concept of the plan which expresses only local constraints on 
one operator. By analyzing the occurrence of variables in the conditions of the plan concept, all 
conditions are separated into: 
• one set J^Enable which collects all conditions that only relate to features of the problem 
description, 
one set &Opi for each operator Opi where the conditions only specify parameters which 
directly correspond to one operator. 
The set of constraints ^Enable formally describes the class of problems for which the skeletal 
plan can be used, and thus functions as an application condition for the skeletal plan. The 
skeletal plan itself is built of the sequence of constraints ROpi> which exactly describe the 
required classes of operators. A further simplification of the constraint set is performed by the 
application of some rewrite reduction rules. Thereby, more operational descriptions of the 
operators classes are obtained. 
For the example in Figure 1 the following skeletal plan with application conditions is generated. 
1. Skeletal plan: operator(l,chuck(xtool, xl,left)) A requires_centerholes(xtooO 
operator(2,cut(...)) A ... 
operator(5,cut(...)) A ... 
2. Application condition: centerhole(cl,x3,x4)) A centerhole(c2,x5,x6)) A cl * c2 A .... 
A prototype of the described method was implemented on a Apple-Macintosh-II computer using 
the LPA-PROLOG environment (Bergmann 1990). This prototype creates skeletal plans for 
cases like the one in Figure 1. 
4. Discussion 
The automatic knowledge acquisition approach presented in this paper makes use of the idea to 
automatically prepare large amounts of already formally available knowledge for further use in 
an expert system. Especially for real world planning tasks such as mechanical engineering, the 
reuse of manually optimized plans in a more general way becomes possible without involving a 
domain expert in a time-consuming knowledge acquisition process. Qualitatively high skeletal 
plans can be generated if the origin plans are qualitatively good. Because of the explanation of the goal achievement, the beneficial interactions between operators are discovered and can be 
maintained for the abstract solution. 
Since a knowledge-intensive learning paradigm such as explanation-based learning is the core 
of this method, a large amount of knowledge has to be provided to enable its application. The 
requirements on the available domain theory are very high in the sense that a correct and 
tractable theory is needed which is complete enough to allow the simulation of the full plan. It 
seems hopeless to acquire such a theory automatically, even if inductive learning methods were 
applied. Therefore the skeletal plan generation procedure has to be integrated with other, non-
automatic methods such as COKAM and CECoS and works well if the requirements mentioned 
on the theory can be fulfilled in the application domain at hand. 
Another question is concerned with the usefulness of the skeletal plans that are automatically 
acquired by this procedure. A skeletal plan is useful if it provides an abstraction that reduces the 
computational complexity of a planning process (Korf, 1988), and if it can be applied to a large 
class of problems. Since complexity reduction and wide applicability are somehow competing 
properties for a single abstraction we are engaged to find a hierarchy of skeletal plans. 
Therefore the utility based on the generality of a skeletal plan should be judged with respect to 
the problem class, for which the skeletal plan is constructed. If automatically obtained skeletal 
plans are compared with those that were acquired manually with considerable effort, a major 
weakness of generalization procedure can be identified. The described procedure is able to 
construct skeletal plans by abstracting single operations of a plan as far as the domain theory 
contains abstract descriptions of operator effects. If such a sufficient operator model can be 
supplied, the automatically generated operations can compete in utility with those constructed 
manually. If only a shallow operator theory such as in the case of the STRIPS domain is 
provided, the resulting skeletal plan for the most specific problem classes is the same as a 
macro-operator composed of all operators in the plan (Fikes, Hart & Nilsson, 1972). 
Another kind of abstraction that appeared important for planning could not be performed by the 
proposed skeletal plan generation procedure. It is unable to collapse sequences of concrete 
operations into one single abstract operation. The bounds of the operations are always 
transfered from the concrete plan to the skeletal plan. Knoblock^ (1990) approach to operator 
abstraction shows exactly the same deficits while Madler (1991) tries to find "eyes of a needle" 
in the state space to combine sequences of operations into one single abstraction. 
Further research to improve this automatic knowledge acquisition should consequently deal 
with the problem of finding more appropriate abstractions, for example by changing the plan 
representation language. Since this seems to be a knowledge-intensive process that cannot be 
applied in a isolated fashion, the interactions between such automatic and manual knowledge 
acquisition methods must be further examined and developed. 
Acknowledgments 
I would like to thank Franz Schmalhofer for many helpful discussions and for significantly 
contributing to this paper. 
This research was supported in part by grant ITW 8902 C4 from the BMFT (German Ministry 
for Science and Technology) and by grant Schm 648/1 from the DFG (German Science 
Foundation). 
References 
Bergmann, R. (1990). Generierung von Skelettplanen als Problem der Wissensakquisition. 
Unpublished masters thesis. Universitat Kaiserslautern. Bergmann, R., Bernardi, A., Klauck, C, Kiihn, O., Legleitner, R., Schmalhofer, F., & 
Schmidt, G. (1990). Formulierung von Anforderungen zur Darstellung von Werkstucken 
und Spezifikation einer Makroreprasenation. Internes ARC-TEC Diskussionspapier Nr. 
8. 
Bergmann, R. & Schmalhofer, F. (1991). CECoS: A case experience combination system for 
knowledge acquisition for expert systems. To appear in: Behavior Research Methods, 
Instruments and Computers. 
DeJong, G., & Mooney, R. (1986). Explanation-based learning: An alternative view. Machine 
Learning, 1, pp. 145-176. 
Fikes, R.E., & Nilsson, N.J. (1971). STRIPS: A new approach to the application of theorem 
proving to problem solving. Artificial Intelligence, 2, pp. 189-208. 
Fikes, R.E., Hart, P.E., & Nilsson, N.J. (1972). Learning and executing generalized robot 
plans. Artificial Intelligence, 3, pp. 251-288. 
Friedland, P.E., & Iwasaki, Y. (1985). The concept and implementation of skeletal plans. 
Journal of Automated Reasoning, 1, pp. 161-208. 
Knoblock, C. A. (1990). Learning abstraction hierarchies for problem solving. Proceedings 
Eight National Conference on Artificial Intelligence, 2. 
Korf, R.E. (1988). Optimal path-finding algorithms. In: Search in Artificial Intelligence. 
Kanal, L., Kumar, V. (eds.), Springer, NY. 
Madler, F. (1991). Problemzerlegung als optimalitatserhaltende Operatorabstraktion. In: 
GWAI-91, 15. Fachtagung fiir Kiinstliche Intelligent Th. Christaller (ed.). Springer, 
Berlin. 
Mitchell, T.M., Keller, R.M., & Kedar-Cabelli, S.T. (1986). Explanation-based 
generalization: A unifying view. Machine Learning, 1, pp. 47-80. 
Musen, M.A., Fagan, L.M., Combs, D.M., & Shortliffe, E.H. (1987). Use of a domain 
model to drive an interactive knowledge editing tool. International Journal of Man-
Machine Studies, 26, 1, pp. 105-121. 
Schmalhofer, F., Bergmann, B., Kuhn ,0. & Schmidt, G. (1991a) Using integrated 
knowledge acquisition to prepare sophisticated expert plans for their re-use in novel 
situations. In: GWAI-91, 15. Fachtagung fiir Kiinstliche Intelligenz. Th. Christaller 
(ed.). Springer, Berlin. 
Schmalhofer, F., Kiihn, O., & Schmidt, G. (1991b). Integrated knowledge acquisition from 
text, previous solved cases and expert memories. Applied Artificial Intelligence , 5, pp. 
311-337. 
Schmalhofer, F., & Thoben, J. (this volume). The model-based construction of a Case-
Oriented Expert System. Contemporary Knowledge Engineering and Cognition. F. 
Schmalhofer, G. Strube & Th. Wetter (eds.). 
Schmidt, G., & Schmalhofer, F. (1990). Case-oriented knowledge acquisition from text. In: 
Current Trends in Knowledge Acquisition, Wielinga, B., Boose, J., Gaines, B., 
Schreiber, G., van Someren, M. (eds.), IOS Press, May 1990, pp. 302-312. 
Shortliffe, E.H., Scott, A.C. Bischoff, M.B., Campbell, A.B., Melle, W., & Jacobs, CD. 
(1981). ONCOCIN: An expert system for oncology protocol management. Proceedings 
of 7th International Joint Conference on Artificial Intelligence, Vancouver, Canada, pp. 
878-881. 
Stefik, M. (1981). Planning with constraints (MOLGEN: part 1). Artificial Intelligence 16, 
pp.111-139. 
Tu, S.W., Kahn, M.G., Musen, M.A., Ferguson, J.C., Shortliffe, E.H., & Fagan, L.M. 
(1989). Episodic skeletal-plan refinement based on temporal data. Communications of the 
ACM, 32, 12, pp. 1439-1455. Knowledge Acquisition from Cases 
Sonja Branskat 
Institute for Applied Information Technology 
German Research Center for Mathematics and Data Processing 
PO Box 1240 5205 St. Augustin 1 
Abstract. This paper presents a hypermedia, domain independent system 
supporting the acquisition, formalization, and representation of cases. The system 
assists a knowledge engineer in a step by step transformation of an informally 
represented case into a formally represented one. Each case consists of five compo­
nents: context, task, solution trace, solution and evaluation of the solution. The 
knowledge engineer builds a formalization sequence of case descriptions. An 
attribute/value description is considered to be formal. The data structure is derived 
from Memory Organization Packages. The sequence itself documents the formal­
ization process. 
1. Introduction 
Many techniques for knowledge elicitation are based on protocoling the expert when he works 
on a case (Neale, 1988). A knowledge engineer (KE) analyses the transcript and transforms it 
into a formal knowledge base. While the KE creates different knowledges bases (protocols on 
video- or tape-records, transcripts and models of expertise on paper and a knowledge base in a 
computer-based programming-language), only the formalized knowledge base on a computer is 
accessible by a user (expert, KE or customer). Due to the media break during the formalization 
the process itself is not transparent and cannot be re-examined. There is no access provided 
from formal knowledge elements to any underlying informal elements. As a consequence a user 
cannot look up the meaning or the context of the concepts that the interviewed expert had in 
mind (Karbach and Linster, 1990). 
The system UFA, presented in this paper, promises to substantially reduce if not discard the 
media break. UFA, implemented in HyperCard, is designed to systematically support the 
formalization of cases. The support facilitates integrated documentation, transparency of the 
formalization process, and avoids any media break. 
Further approaches using hypermedia for knowledge acquisition are described in (Gaines and 
Linster, 1990), (Linster and Gaines, 1990). 
2. Representation of Cases in UFA 
In UFA the KE represents cases in a formalization sequence analogous to the generalization and 
specialization hierarchy of the Memory Organization Packages (MOPs) (Schank, 1985). MOPs 
are widely used to represent cases in case-based reasoning systems. While Schank's norm-
based MOP-hierarchy consists of formal cases in different generalization states, a sequence in 
UFA consists of different formalization states of the same case. In the formalization sequence 
successive descriptions of the same case explanation link 
evaluation 
solution 
solution trace 
problem 
context 
caseC 13> explanation link formalization link 
formalization link 
scene-
links caseC informal. 1 
exemplar link o 
S 
© 
3 
o 
3 
formalization link 
caseC. informal 
Figure 1: Case model and formalization sequence. 
C: case Cinformai caseCinformaij, case Cinforma\k, ... , case Cforma\ (see Figure 1) 
have an increasing degree of formalization. This sequence documents the formalization process. 
The sequence starts with an informal case description and ends with a complete formal 
description. 
A case model is proposed, in which each case consists of five components: context, task, 
trace of the problem solving process (called solution trace), solution and evaluation of the solu­
tion as shown in Figure 1. Each case is represented in a MOP. Scene-links connect the com­
ponents of a case. Formalization links connect a case description (e.g. case ^informalk) w*tn 
its more formal case description (e.g. case Cformal.k+1 )• Explanation links connect a case de­
scription (e.g. case ^informal.k) w^tn *ts next *ess formal description (e.g. case 
^informal.k-1)- Exemplar links connect the case description of the first formalization state (case 
^informal.l) wi*h tne initial informal description (case Cinformai). 
3. Formalization of Cases in UFA: an Example 
To demonstrate UFA an example is given in the domain of business graphics. In business 
graphics one visualizes quantitative relations between data with graphical techniques to obtain 
optimal information presentation. For the case air pollution the scanned graphic is a first 
informal description of the component solution. Four circular charts present the contribution of 
cars to air pollution differentiated into four different groups of chemicals. In the first step the KE identifies the components: context, problem, solution trace, solution 
and evaluation and generates a new MOP in UFA (case Cinformaj) to describe the components. 
He describes the component solution with the scanned graphic as an informal description. 
In the second step the KE generates a new MOP (case Cinforma[ j) and connects it with a 
formalization link to the initial case description (case ^informal)- On the first formalization layer 
he differentiates the domain independent structural component solution into domain dependent 
structural subcomponents: the charttype that is used, the colors that are used and the labelling of 
the chart. Using the function add subcomponent (see Figure 2) he generates new fields to 
describe these subcomponents. Using the function add annotation (see Figure 2) he creates an 
annotation card for the selected subcomponent of the component solution and describes it with a 
picture and free text. With the function go back UFA returns from the annotation card to the 
component solution. Buttons labelled A (see Figure 2) access any annotation card for this 
subcomponent. 
case: air polution component: solution 
[ add subcomponent ) [ delete component ) [ moue subcomponent j 
( add annotation ] [goto newt component] ( attribute / ualue ] 
charttype: look at picture 
choose-of-colors: It is a black and white diagram. The part belonging to the motor 
vehicles is black. This s\rtssts the portion of the cars compared to the rest. 
label: look at picture 10 
CD 
CD 
order-of-data: the order stresses the importance of the toxin substances: The 
last chart, the significant substance, looks like a full stop. 
Figure 2: The component solution of the case air pollution: the formalization links and the 
textfields for the already generated subcomponents. 
At the outset of the third step the KE creates a new MOP (case Cinformai2 ) and connects it 
with an explanation link to its more informal version (case Cinforma[ j). He uses a formal­
ization link to connect (case Cinforma[ j) to its more formal version (case C(nformai 2 )• He 
decides to copy the subcomponents of the old description (case ^informal.l)to ^c components 
of the more formalized case description (case C-informal.2)- Tne KE creates a new 
subcomponent of the component solution, in order to describe the solution in a more formal 
way by adding the order the data are presented. He describes the new structural component 
order-of-data directly on the solution card using the text field popping up with the creation of a 
new subcomponent. 
A few formalization steps later the KE describes the subcomponent charttype with circle-
chart and decides, that multiple circle-chart should be a value of the attribute charttype. He 
declares this in UFA using the function att/value (see Figure 2) and expanding the range of 
values of this attribute. The case is formalized if all components are described with attributes and values (see Figure 3). The formalization process is finished, as now all relevant knowledge 
enclosed in graphics and text has been transformed into a formal, computer accessible and 
retrievable notation. 
air pollution I 
case: air polution component: solution 
[ add subcomponent ) ( delete component ) (moue subcomponent) 
( add annotation ) (goto newt component) ( attribute / ualue J 
charttype: (chart multiple-circle) 
choose-of-colors: (main-statement black) (subordinate-statement white) 
label: (circle-segments relative-values) (additionar-to-circles 
glossary-entry-below) 
order-of-data: (order Increasing-importance) 
Figure 3: The component solution of the case air pollution; a complete formalized version. 
Evolving a linear sequence of case descriptions the KE uses further functions to develop a 
domain dependent model for cases. He can: 
explain the concepts and formalizations he has used in a hypermedial terminological 
dictionary, 
define terms of concepts as synonym and replace them by a term he chooses, 
define domain dependent subcomponents as attributes and define values for them; he can 
use functions to check, if no attribute is associated with a value that is not allowed, 
determine the order of all domain dependent characteristics of a component, 
get an overview over the degrees of formalization that the system provides. 
4. Conclusion 
UFA shows a way how to bridge the media break. It supports the KE by stepwise transforming 
a "natural" representation of cases into a formal MOP like representation using only one media. 
The formalization process is completely documented within UFA. UFA can easily be extended 
to handle video or tape-records. However, it is necessary to gain more experience using UFA to 
give a more detailed evaluation. 
5. Acknowledgement 
I am grateful to Marc Linster, Barbara Becker, Thomas Christaller and all the other members of 
the Artificial Intelligence Research Division of the German National Research Centre for 
Computer Science (GMD) and Brian Gaines for making valuable comments on a previous 
version of this article. Literature 
Karbach W., Linster M..; Wissensakquisition fiir Expertensysteme; Carl Hanser Verlag, Bonn 
(1990). 
Linster, Marc and Gaines, Brian (\990)Supporting Acquisition and Interpretation of 
Knowledge in a Hypermedia Environment, Tech. Rept. 455, Arbeitspapiere der GMD, 
GMD, July 1990 
Gaines, Brain and Linster, Marc (1990) Development of Second Generation Knowledge 
Acquisition Systems; Introduced to EKAW90. 
Neale, I.M. (1988) First generation Expert Systems: a review of Knowledge Acquisition 
Methodologies; The Knowledge engineer review 3 , pp. 105-146. 
Schank, R.C.; Dynamic memory: A theory of reminding and learning in computers and people; 
Cambridge University Press (1985). Transforming examples into cases 
Peter Reimann & Thomas J. Schult 
Psychologisches Institut der Universitat 
Niemensstr. 10 
D-7800 Freiburg 
e-mail: reimann@cogsys.psychologie.uni-freiburg.dbp.de 
Abstract: Our work is concerned with a central process in knowledge acquisition 
for case-based expert systems: understanding examples of expert's problem 
solving traces, in our case worked-out examples in physics textbooks. Based on 
evidence from psychological research, an active, expectation-driven strategy for 
example processing is developed. The strategy deals with the initial phases of 
learning, exploiting case knowledge before relying on general knowledge. We 
report on first attempts to realize this strategy as a cognitive computer model. 
1. Introduction 
Knowledge acquisition was and will for some time be the major impediment to the wide-spread 
application of knowledge-based systems in real-world domains. Several ways were suggested 
to automate this task using machine learning techniques. The traditional approach uses induc­
tion to abstract principles out of a large set of examples (e.g., Quinlan, 1983). Recently, it has 
been suggested to circumvent the knowledge acquisition bottleneck to a certain extent by 
equipping expert systems directly with one of the main aspects of expertise: experience in form 
of cases. Case-based expert systems do not need difficultly to acquire rules, whether they stem 
from interviews with an expert or from the application of machine learning techniques. Instead, 
they attempt to solve new problems by mapping to solutions of former problems and adapting 
the old solution to a new problem. First commercial expert system shells including CBR com­
ponents are, for example, ART-IM and ReMind. 
The power of a case-based expert system lies mainly in the amount and quality of the cases 
stored as well as in the indexing of the cases. The knowledge acquisition process for a CBR 
expert system amounts to provide the cases and fine-tune the indexing. This task is not neces­
sarily easier than pressing an expert for the verbalization of rules. The complexity of this kind 
of knowledge acquisition is directly proportional to the structure of the cases one wants to store 
in case memory. If this structure is simple - for example, flat attribute-value lists - then case ac­
quisition is almost trivial. If the structure is more complex - for example, if cases contain de­
scriptions of causal relations - then case acquisition is more demanding and may require 
application of knowledge acquisition and machine learning techniques similar to those that 
have been developed for rule-based expert systems. 
In any case, it is desirable to automate the case acquisition process as much as possible. This is 
a challenging problem if cases have a complex structure. For instance, Redmond (1989a) de­
scribes the difficulties involved in transforming records of experts' troubleshooting behavior into a case description that is useful for a case-based troubleshooting expert system. In our re­
search, we study how humans acquire case knowledge not from observing an expert directly 
but from solution traces as they are provided by worked-out examples in textbooks. Insights 
into the process of transforming worked-out solution examples into cases is relevant for re­
search in knowledge acquisition for case-based expert systems to the extent that (a) learning 
from experts and learning from solution examples impose similar problems (Redmond 1989b), 
or (b) that leaiTiing from solution examples is a component of the knowledge acquisition task. 
2. Models of Example Elaboration Strategies 
In order to shed light on the acquisition of cases from solution examples, we consider how hu­
mans tackle this learning task. We identify successful human strategies and model components 
of these strategies with computer simulations. The examples are solutions to mechanics prob­
lems as they appear in a textbook on college-level physics widely used in the US. Successful 
learning from these examples can best be understood as an active elaboration of the solutions 
steps, as an empirical study revealed (Chi, Bassok, Lewis, Reimann & Glaser, 1989). 
Worked-out examples may be difficult to understand because they do not contain the necessary 
information to perform sensible generalizations. In particular, examples do often not contain 
the reasons for why a certain step in the solution is performed. Consider the example solution 
provided in Figure 1. In statement 6, it says that the forces shown in the force diagram are "all 
the forces acting on the body". Yet, the example contains no procedure that describes how this 
could be determined by the student. In the absence of rationales for problem solving decisions, 
it is hard to decide what the essential features of an example are and what the superficial ones 
are, i.e., those one can generalize over. 
(1) The left figure shows an object of weight W hung by massless strings. (2) Consider the knot at the 
junction of the three strings to be "the body". (3) The body remains at rest under the action of the three 
forces shown in the right figure. (4) Suppose we are given die magnitude of one of the three forces. (5) 
How can we find the magnitude of the other forces? (6) FA, FB and Fc are all the forces acting on the 
body. (7) Since the body is unaccelerated, FA + FB + Fc = 0. (8) Choosing the x- and y-axes as shown, 
we can write this vector equation as three scalar equations: (9) FAx + FBx = 0, and (10) FAy + FBy + FCy 
= 0. (11) The third scalar equation for the z-axis is simply: (12) FAz = FBz = FCz = 0 (...) 
Figure 1: A worked-out example for a mechanics problem (Halliday & Resnick, 1985) 
Even so this learning problem is typically one of novices (students) who want to acquire prob­
lem solving knowledge by studying a textbook, a knowledge engineer may encounter similar 
problems when observing an expert's problem solving behavior without being able or willing 
to interrupt and ask for the reasons for decisions taken by the expert, or when studying written materials such as textbooks to become more familiar with a domain. On the following pages, 
we will outline a computational model of how human problem solvers analyze worked-out ex­
amples so that they can use them effectively for problem solving by analogical transfer to sim­
ilar problems. To a certain extent, this model can also be seen as a method for automatic 
knowledge acquisition from text. It should be mentioned that as these pages are written this 
model of apprenticeship learning is yet a conceptual one, only parts of which are in the stage of 
becoming implemented. 
The general design of the strategy is based on insights into the process of text comprehension 
and understanding (e.g., Brown, Collins, & Harris, 1978) as well as on research concerning the 
dynamic character of human memory and its influence on problem solving and understanding 
(Schank, 1982). To account for the specific sort of text on which learning is based in our case, 
worked-out examples in mechanics, we further rely on observations made by Chi et al. (1989). 
There it was analyzed how students acquire problem solving knowledge concerning mechanics 
by studying worked-out examples. The study revealed important differences between success­
ful and not so successful students, success measured in terms of correct solutions to problems. 
Successful students mentioned more often that they didn't understand a certain part of the 
worked-out example. Besides this difference in monitoring understanding of the example text, 
successful students also engaged in a series of activities to overcome their problems: They elab­
orated on the relations between a particular step in the example and the goals behind that step. 
They further attempted to come up with a specification of conditions that could explain why the 
operator under question was applied. Finally, they elaborated on the effects the application of 
an operator had beyond those mentioned in the example. The not so successful students dis­
played either none or considerably less of these elaborative inferences. 
To capture the essential differences between successful and less successful learning from ex­
amples in form of computational models, we treat the process of example elaboration as a plan 
recognition task where the understanding system has to encode a given example solution in 
terms of problem solving goals, operators, and relations to domain concepts. Figure 2 shows 
the main components of the model. 
Example • Internal 
Format Example 
Understander 
Solution \ 
Model r Rule Interpreter -±.f Abstract 
' Domain 
Knowledge 
Problem 
Solver 
Indexer/ 
Retriever Case 
Memory — Problem 
Solution 
Figure 2: Model Components 
It comprises modules for example comprehension and problem solving. The problem solving 
component demonstrates the competence of the system. It attempts to solve new problems 
building on knowledge mainly acquired by learning from examples. Both the problem solving 
module and the example understanding module are hybrid systems, combining a case-based ap-proach with general but weak inference methods. The abstract domain knowledge comprises 
an object hierarchy for problems concerning particle dynamics and a rule-based problem solver 
for this domain. 
Most important for the current discussion is the example understander. This component takes a 
worked-out example as input (in a propositional format) and generates a model of the example 
solution that is then stored into case memory. To construct the example solution model, the sys­
tem calls up case knowledge about former examples and abstract domain knowledge. The gen­
eral control decision is to rely on case knowledge first. Two strategies are contrasted: an active, 
expectation-driven one that leads to a hierarchical representation of an example and allows for 
problem solving by derivational analogy (Carbonell, 1986), versus a passive example process­
ing strategy leading to a flat representation that enables the problem solver to use an example 
by transformations based on syntactical similarities. We want to illustrate the details of how 
these knowledge sources are used in the context of the active comprehension strategy. 
3. Active Example Elaboration Strategy 
In order to motivate our active elaboration strategy, we have to introduce the case structure that 
the elaboration strategy has to acquire from examples. We adopt the case model as described in 
Alterman (1988) because this case representation supports flexible analogical problem solving. 
If cases are represented in such a form and if corresponding retrieval and adaptation processes 
are defined, the case-based reasoner can deal with problems such as Steps-Out-Of-Qrder, Fail­
ing-Preconditions, Failing-Outcome, and Differing-Goals. In Figure 3, the main features of this 
case structure are depicted. The task of the elaboration strategy is to build a case representation 
that contains as many as possible of the nodes and links which are part of the fully developed 
case structure from a solution example . Many of these features have to be inferred by the ex­
ample understander since they are not mentioned explicitely in the example text. Under the active strategy, the system monitors its understanding of the example by checking 
for the steps in the example solution whether they cohere with its knowledge about problem 
solving in the domain. It tests its understanding in an active, expectation-driven manner: The 
system predicts the next example step and compares its prediction with the step actually appear­
ing in the example. Problems in understanding are identified by false or missing predictions. 
Having identified an understanding problem the system tries to explain the offending part of the 
example, thus resorting to a cognitively more demanding mode of example processing. This in­
cludes attempting to derive the action(s) observed in the example from its concrete and abstract 
knowledge about the domain as well as to come up with alternate solution plans. In summary, 
this strategy reflects many aspects of the performance of successful subjects from the study of 
Chi et al. (1989) and incorporates general principles of comprehension (e.g., Brown et al., 
1978). 
We are currently implementing this strategy in a system called AXE, the Active eXample Elab­
orate, using KEE and CommonLisp. AXE "reads" an example statement by statement and at­
tempts to formulate expectations about the content of the next statement. A statement 
comprises propositions describing either (a) a goal selection, or (b) an operator selection (law, 
equation, inference rule), or (c) a problem solving state. Thus, AXE tries to predict (a) which 
goal will be worked on next, (b) which operator will be applied, or (c) what effects a goal or 
operator selection will have on the solution. These predictions - or expectations - are generated 
as follows. 
Step 1 - Case-based expectation formation: After having read the first lines of an example 
where the problem is described in terms of objects and their relations, values given, and values 
desired, AXE performs a look-up in its case memory to find out whether it has encountered a 
similar example before. Case memory has the structure of a MOP-based memory (Schank, 
1982) that indexes problem statements and solution steps as they occur in worked-out exam­
ples. If AXE finds a similar example, it will be used to derive expectations for the new one, e.g., 
to predict the next state of the current example. For instance, after statement (7) in the example 
in Figure 1, AXE might expect a rotation of the reference frame, if in a similar case the frame 
did not remain in the default orientation. Later this expectation would turn out to be inappro­
priate. 
Step 2 - Rule-based expectation formation: If no similar example is found, AXE attempts to for­
mulate a prediction by using its abstract domain knowledge in a forward-reasoning mode. AXE 
can at times predict the class of operators instead of a specific operator since its knowledge 
about domain operators is organized in a rule hierarchy. After Step 1 or 2, AXE does either have 
an expectation about the next state of the example solution or has failed to derive an expecta­
tion. Having no expectation is considered a failure and the system continues with Step 5, trying 
to repair this failure. If an expectation has been derived, it continues with Step 3. 
Step 3 - Comparison: In the next step, the expectations about the example solution state are 
compared with the actual situation as described in the example statement. This comparison 
leads to an evaluation of the expectation. It may be (a) specific, (b) too general, or (c) insuffi­
cient, depending on whether a specific state can be predicted (a), or whether only a general state 
of affairs can be predicted (b), or whether the expectation covers only part of the effects result­
ing from the application of the operator (c). It is further determined whether the expectation is 
correct or wrong. For the example mentioned in step 1, a specific but wrong prediction would 
be identified. Step 4 - Continuation: In the case of a specific and correct hypothesis, AXE goes on and tries 
to formulate expectations for the next example state. Before that, the correctly predicted oper­
ator instantiation is stored into case memory in terms of its differences to the operator instanti­
ation retrieved. 
Step 5 - Case-based repair: In all the other cases (no expectation, wrong, insufficient or too gen­
eral expectation) AXE tries to learn from the expectation failure. The failure-driven repair is 
performed first in a case-based manner, relying on case knowledge before attempting to use 
more general, but search-intensive repair strategies. AXE looks into case memory to see wheth­
er it has made a similar expectation failure before and has encountered a similar example state 
before. This information is used to update the example solution model. The now successfully 
explained solution step is stored into case memory, and the system goes on predicting the next 
example statement. 
Step 6 - Rule-based repair: If an expectation failure cannot be explained using case memory 
content, the system attempts to reconstruct the plan that might underlie the actions taken in the 
example part. This mode of example processing is particularly important during initial learning, 
since in this stage the learner possesses only a few cases and therefore cannot always recognize 
an example as the "same old story". During plan reconstruction, the system resorts to its ab­
stract domain knowledge and tries to find an operator or a sequence of operators that could have 
generated the problem solving state as displayed in the current example statement. If AXE finds 
a sequence that connects the statement in the example with a currently active problem solving 
goal, it stores the sequence into case memory. In effect, the system can from now on substitute 
matching for search when encountering similar example parts. 
For the example mentioned in steps 1 and 3, AXE would learn that the default frame must not 
necessarily be rotated in order to fulfill the goal to orientate the reference frame so that the re­
sulting equations become simplified. 
Step 7 - Copying the example solution step: If the plan reconstruction attempts fail, the system 
will store the specific representation of the example statement (an operator and its bindings) un­
der the recent goal it was working on. Although AXE does not understand why this step was 
taken, it can at least copy the solution step when encountering a similar situation in a problem 
solving context. 
An additional step - counter/actual reasoning: Whenever AXE succeeds either in predicting a 
part of the example solution (Step 1 or 2) or in recovering from an expectation failure (Step 5 
or 6), it engages in a kind of counterfactual reasoning. After having established goal-action 
links by solution step recognition or reconstruction, stalling from the goal it is asked: What oth­
er means exist to realize this goal? And why were they not chosen in the example solution? To 
come up with answers to these two questions, alternative solution paths are generated by work­
ing forward form the respective goal and applying domain rules. Doing that, AXE looks for 
problems with the alternative solution plans, problems that can be classified into categories 
such as failed constraint, excessive costs, failed result, or bad side effect (Collins, 1987). The 
first step of the alternative sequence with the explanation of why it encountered a problem is 
stored in case memory. By learning from counterfactual reasoning, the system acquires decision 
heuristics, thus enriching the necessary conditions of operators with further justifications. Evi­
dence for reasoning similar to this counterfactual inference component was found in the most 
successful student of the Chi et al. (1989) study. 
Having implemented the central components of an active example processing strategy, we hope to be able to demonstrate in the next step that a passive strategy can be modeled as a subset of 
the components making up the current learning model and that these differences in learning will 
lead to expected differences in analogical problem solving. That is, the active learner when 
working on a new problem should be able to retrieve from its case memory example solutions 
or parts of them based on features that go beyond literal similarity and should be able to adapt 
the example steps in a more flexible and correct manner than it is possible for the passive learn­
er. 
Acknowledgement: The research reported here is supported by a grant from the Deutsche For-
schungsgemeinschaft to the first author (Re 814/1-1). 
4. References 
Alterman, R., Adaptive planning. Cognitive Science, 11, (1988), 393-421. 
Brown, J.S., Collins, A., & Harris, G., Artificial intelligence and learning strategies. In Learn­
ing Strategies, H. O'Neill (Ed.), New York, Academic Press, 1978, pp. 107-139. 
Carbonell, J.G., Derivational analogy: A theory of reconstructive problem solving and 
expertise acquisition. In Machine Learning. An artificial intelligence approach, Vol. 
2, R.S. Michalski, J.G. Carbonell, & T.M. Mitchell (Eds.), Los Altos, CA, Kaufmann, 
1986, pp. 371-392. 
Chi, M.T.H., Bassok, M., Lewis, M., Reimann, P., & Glaser, R., Self-explanations: How 
students study and use examples in learning to solve problems. Cognitive Science, 
73,(1989), 145-182. 
Collins, G.C., Plan creation: Using strategies as blueprints. Ph.d. thesis, Yale University, 1987. 
Halliday, D., & Resnick, R., Fundamentals of Physics. New York, John Wiley & Sons, 1985. 
Quinlan, J.R., Learning efficient classification procedures and their application to chess end 
games. In Machine Learning, R.S. Michalski, J.G. Carbonell, & T.M. Mitchell 
(Eds.), Palo Alto, Tioga Press, 1983, pp. 463-482. 
Redmond, M., Learning from others' experience: Creating cases from examples. In Proceed­
ings Case-Based Reasoning Workshop, San Mateo, CA, Morgan Kaufmann, 1989a. 
Redmond, M., Combining explanation types for learmng by understanding instructional 
examples. In Proceedings of the 11th Annual Conference of the Cognitive Science 
Society, Hillsdale, NJ., Erlbaum, 1989b. 
Schank, R.C., Dynamic memory. A theory of reminding and learning in computers and people. 
New York, Cambridge University Press, 1982. Case-Based Reasoning and Expert System 
Development1 
Klaus-Dieter Althoff, Stefan WeB 
University of Kaiserslautern 
Dept. of Computer Science 
P.O. Box 3049, D-6750 Kaiserslautern 
Federal Republic of Germany 
e-mail: /a5rnam^@informatik.uni-kl.de 
Abstract. As a supplementation to other papers within this chapter on case-based 
approaches to Knowledge Engineering, we discuss some general aspects of case-
based reasoning. We differentiate it from other case-using approaches and argue for 
the use of case-based reasoners within integrated knowledge engineering environ­
ments. 
1. Introduction 
Developing expert systems which can solve complex real world problems is still a difficult task. 
Therefore, knowledge engineering people need flexible methods and powerful tools which 
support them in doing this hard work. Within this paper we give a short introduction to such a 
flexible method, namely case-based reasoning, which might be one key issue in building, e.g., 
integrated knowledge engineering environments to offer the support needed. Cases are exam­
ples which have occurred in reality and consist of a problem description, a solution, and the 
underlying justification (derivation) for that solution. From a simplifying point of view, case-
based reasoning means solving novel problems based on the adaptation of already known simi­
lar problem solutions. For being able to improve the problem solving capabilities of a system, 
cases must be memorized and integrated with already available empirical knowledge. 
As concerned with problem solving, learning, and the acquisition of cases, case-based reason­
ing is within the focus of different fields of research, e.g. Cognitive Psychology, Machine 
Learning, and Knowledge Engineering. Apart from these strong commonalities, all those fields 
have their own view on the case-based reasoning approach. From a Cognitive Psychology 
point of view, it can be seen as a model of human problem solving. Within the Machine 
Learning community, case-based learning means an inductive learning method with a special 
kind of hypotheses generation. Verbatim examples are collected to learn (mainly) implicit con­
cept descriptions which are then processed by the use of analogical reasoning. For the field of 
Knowledge Engineering, case-based reasoning implies a dynamic view on knowledge model-
1 The work presented herein was partially supported by the Deutsche Forschungsgemein-
schaft, SFB 314: "Artificial Intelligence - Knowledge-Based Systems", projects X6 and 
X9. ing which overcomes the strict distinction between knowledge acquisition and application 
which, actually, is the underlying assumption of the model-based approach to knowledge ac­
quisition. The automation of the knowledge acquisition and adaptation processes is the transi­
tion to learning. In the sense of automatic knowledge modeling, this has already been suggested 
by Morik [Mor87]. Thus, the case-based reasoning approach can be roughly characterized by 
the notions of learning ability, adaptation, and integration of knowledge acquisition and appli­
cation. Case-based reasoning is a well-suited method for dealing with any kind of inhomoge-
neous solution spaces. 
In this paper, we discuss some general aspects of case-based reasoning. Since case-based rea­
soning is a hot research topic many scientific contributions within this field have to be consid­
ered. Many different research communities have, at least partially, similar interests and/or 
methods, e.g. Machine Learning, Cognitive Psychology, Statistics, Pattern Recognition, 
Neural Networks, and Knowledge Engineering. In the next section we summarize the basic 
characteristics of case-based reasoning. Commonalities and important distinctions between 
case-based reasoning and other approaches are presented in section 3. Finally, we argue for the 
use of case-based reasoning within integrated knowledge engineering environments. 
2. Case-Based Reasoning 
Introduced to the community by Kolodner [Kol80, KSS85] and Schank [Sch82], the basic 
problem solving model of case-based reasoning grew out of several projects at Yale University. 
There exists a strong overlapping with research work done so far in the field of analogical rea­
soning. In its simplest form, case-based reasoning is similar to approaches known from statis­
tics and pattern recognition (e.g. nearest neighbor classification) [cf. e.g. Tou81]. A general 
overview of case-based reasoning is given in [Sla91] and [RS89]. Important research goals 
concerning case-based reasoning from a Cognitive Psychology point of view are presented in 
[SJ90]. 
2.1. Cases 
What is meant by the notion of 'case' is one of the central questions in case-based reasoning. 
From a psychological point of view, cases are abstractions of events or processes which can be 
limited within space and time. Such knowledge is also known as episodic knowledge [cf. 
Str89]. Once the abstraction mapping is fixed, cases are often identified with their underlying 
events or processes. 
For us, a case is an "example which has occurred in reality", i.e. a problem that occurred and 
has been solved by a certain kind of problem solving mechanism (human expert, expert system 
etc.). Therefore, the "observed" solution is empirically justified. Such cases are then mapped 
onto the respective case representation which, of course, reflects only a part of the "problem 
solving reality". In this sense, cases include implicit problem solving heuristics which can be 
interpreted with respect to different purposes. For being able to describe cases in more detail, at least three different levels of abstraction 
should be differentiated [cf. Ric89 and And89]: 
• a cognitive level (knowledge level) 
• a representational level (algorithmic level) 
• an implementational level 
Within the context of diagnosing an engineering system, a case is the behavioral result of pro­
cesses that have their origin on the cognitive level. On the representation level, this could be 
abstracted into a sequence of attribute-value pairs. Finally, on the implementation level a case is 
implemented using lists, structured objects, or a special subgraphs. 
Since there is no general agreement concerning formal descriptions of cases, we give a defini­
tion which is very general but, nevertheless, sufficient for our purposes here [cf. also VC89]. 
Definition 
A case is a triple (P,S,J) where P is a problem description, S the solution for the de­
scribed problem, and J the justification of the solution. A case corresponds to a real event 
or process which can be limited within space and time. 
Justifications are an explicit representation of the problem solving process. They can be more or 
less complex. The simplest kind of justification is an "empty" one resulting in a case-based rea-
soner which could only find solutions for problems it has "seen" before. For classification 
tasks this approach is often sufficient and known as case-matching (classification! interpre-
tive/precedent-based) case-based reasoning [cf. Ham89a]. E.g., in a simple diagnostic situation 
a case might read as follows: the problem is described by means of the observed symptoms, the 
solution is the achieved diagnosis, and the justification is empty. 
If more than transfer of unmodified solutions is needed, justifications, as an additional knowl­
edge source, must be available. They can range from a simple problem solving trace to a com­
plete explanation using some kind of deep reasoning model. Thus, a justification always in­
cludes a procedure or a theory which allows the interpretation of the (static) trace. This ap­
proach is often called case-adaptation (problem solving) case-based reasoning [cf. Ham89a]. 
For a diagnostic task, a justification could be the temporal order by which the symptoms have 
been ascertained, and for a planning task, a more or less complete dependency graph. 
2.2. Problem Solving 
We now describe the basic problem solving cycle which characterizes the case-based reasoning 
paradigm (retrieve, compare, adapt, repair, generalize; cf. [Syc91]). Cases are knowledge 
sources as well as rules or deep models and, therefore, have to be considered during expert 
system development, too. Once a case has been acquired, it is stored in a case library (case 
memory). During problem solving it might be retrieved from the memory if its problem de­
scription is similar (enough) to the actual problem at hand. If the case can be applied to the cur­
rent problem its solution must be adapted based on some simple strategies (identical solution 
transfer, "patching", etc.), or on a more complex underlying domain theory using the available justifications. If the adaptation has been successful the completed case can be incorporated into 
the case memory. Thus, if the same problem occurs again it can be directly solved by retrieving 
this case and applying its stored solution. If the adaptation process has not been successful this 
case can be stored as a negative example to warn the problem solver not to go this direction if, 
e.g., the same problem has to be solved again. Additionally, if the system can find out the 
cause of the failure (explain the failure), it might be able to correct (repair) the wrong solution. 
Both the adaptation and the repair processes require a problem solver of their own. Such prob­
lem solvers can use general strategies and a more or less complex domain theory to reach their 
respective goals. In the worst case, they must be as powerful as from-scratch problem solvers. 
Therefore, the integration of case-based reasoners into broader problem solving architectures is 
an important research goal (cf. section 4). 
2.3. Similarity and Retrieval 
Besides the underlying case representation, storage and retrieval of cases are of fundamental 
importance for the quality and efficiency of case-based problem solving mechanisms. Cases 
should be stored in memory such that fast retrieval of sufficiently similar cases is possible. 
They can be organized using a simple list, a data base, a discrimination [cf., e.g., Kol83a+b], 
or dependency graph. Similar cases can then be found by means of a similarity measure. This 
could be realized as an explicit mathematical function, as a pair of insert and retrieval proce­
dures for the case memory, or as a combination of both. 
2.4. Learning 
A case-based reasoning system has to handle, at least, three different learning tasks. This en­
compasses learning from positive examples which might have been presented by an expert, 
learning from its own problem solving success, as well as from failure. Within the case-based 
reasoning community many different learning strategies have been used to handle these tasks. 
This includes rote learning for the integration of new cases or problem solving experiences into 
the case memory, explanation-based generalization to single out relevant features to be used as 
indices [RS89, Ham89b, BM88], generalization of implicit concept descriptions by means of 
partial matching (indexing, similarity functions) [Kol83a+b, PBH90, PG91], specialization of 
implicit concept descriptions (forgetting of cases according to certain selection criteria 
[AKA91], or competitive learning of feature relevances [AW91]), and generalization of feature 
values [Sal91]. 
3. Other Case-Using Approaches 
Up to now, cases as a knowledge source for solving certain kinds of problems have been used 
in many different fields. We want to give an overview together with a rough classification of 
the respective approaches. This allows for an easy differentiation between them. Since many 
underlying notions of and connections between these approaches are not well understood up to 
now, we will not introduce a formal framework. Here, much work is still to be done. Additionally, we do not want to differentiate between the case-based approach and approaches 
known as exemplar-based or instance-based. 
One main aspect of case-based reasoning is that the underlying basic problem solving method is 
analogical reasoning. In general, analogical reasoning means transforming and extending exist­
ing domain knowledge to solve a similar task within another domain using similar methods. 
The known domain is often called base and the new one target. Fundamental characteristics of 
the analogical process are the mechanisms which determine the similarity of the tasks and trans­
fer the methods and/or features from the base to the target domain, respectively. In principle, 
case-based reasoning can be seen as a special kind of analogical reasoning. 
Historically, different research communities have concentrated on these inference mechanisms. 
For instance, Kolodner [cf. Kol89] points out that the focus within case-based reasoning has 
been mainly on case representation and retrieval, whereas within analogical reasoning the solu­
tion transfer has been treated in more depth. This is due to different basic assumptions concern­
ing base and target domain. For case-based reasoning, they are normally identical, for analogi­
cal reasoning, on the other hand, it is mostly an essential feature to have different base and tar­
get domains [cf. Bur89, SD90]. For the rest of the paper we will not differentiate between these 
two approaches. 
X Real-Life 
Connection Kind of Heuristic Interpretation 
Rules Abstract Explicit Single 
Cases Concrete Implicit Multiple 
Fig.l - Contrasting Cases and Rules 
Case-based reasoning and inductive reasoning have in common that both reason from cases, 
and that the conclusions achieved are normally uncertain. Case-based, inductive, and explana­
tion-based learning all learn from cases and can use preexisting domain knowledge for hy­
potheses generation. For the pure form of explanation-based learning the domain theory is as­
sumed to be complete and correct. Here cases are used to focus the deductive process. Case-
based reasoners mainly learn from the comparison of two cases (i.e. the learning procedure is 
fundamentally incremental) whereas inductive learners often compare several cases during one 
learning step. Some inductive learning systems are also able to learn incrementally. While most 
case-based reasoners store all the cases verbatim within an abstraction hierarchy (case memory) 
[cf. Sal91], most inductive learners forget all the cases which have been the basis for the gen­
erated hypotheses. Other machine learning approaches do both the learning of explicit concept 
descriptions, and the verbatim storing of cases [cf., e.g., SS88, Fis89]. Additionally, some case-based reasoning approaches try to improve their implicit concept descriptions by selec­
tively removing cases from the case library [cf. KA88, AKA91]. 
Cognitive Level Protocol of a process 
Diagnostic process of Friday, the 6th of August, 
to find out why the lamp in our living-room was 
not shining. 
Representation Level Sequence of attribute-value pairs 
lamp-12 <- off 
switch-3 <- on 
bulb-7 <- okay 
voltage <- not available 
defect <- short-circuit-6 
Implementation Level List of the respective implementation language 
((lamp-12 off) (switch-3 on) (bulb-7 okay) 
(voltage none) (defect short-circuit-6)) 
Fig.2 - An Exemplary Case 
From a Machine Learning point of view, case-based reasoning is not so well understood as, 
e.g., inductive learning. Up to now, there is no general agreement concerning the overall 
learning task which is addressed by case-based reasoning. Rather, there is a focus on defining 
and understanding particular mechanisms like reasoning by analogy and reasoning from cases. 
As a reason for this, Shavlik and Dietterich point out in [SD90] that research work in the field 
of case-based reasoning has been mainly motivated by concerns for cognitive plausibility rather 
than by a desire to construct practical systems. 
Another reason is that most machine learning systems make a (strong) separation between 
learning and problem solving [cf. SD90]. Learning involves analyzing training examples or 
problem solving experiences to extract functions or rules, problem solving involves applying 
the learned functions or rules to solve new problems. In case-based reasoning, by contrast, 
problem solving is performed by directly inspecting the training examples (cases) and solving 
new problems by analogy with these past cases. This appears to be a major distinction of case-
based reasoning and other machine learning approaches. However, there are also strong simi­
larities between case-based problem solving and the well-known rule-based approach, because 
often it is not possible to differentiate between cases and rules (including their processing) on the levels of representation and implementation. Therefore, we suggest to define on a cognitive 
level what should be the difference between cases and rules. This allows some simple classifi­
cations which, as we hope, are helpful to answer some basic questions. 
Cognitive Level Rule of Thumb 
If you turn on a lamp and it does not shine, 
probably the bulb is defect. 
Representation Level Sequence of attribute-value pairs 
lamp <- off 
switch <- on 
defect <- bulb 
probability <- high 
Implementation Level List of the respective implementation language 
((lamp off) (switch on) (defect bulb) (probability 
high)) 
Fig.3 - An Exemplary Rule 
A production rule is a well-known knowledge representation scheme and most implemented 
systems within the Artificial Intelligence community have used it. We will give a very general 
definition of what a rule (of thumb) is, because we need it for contrasting purposes only. In 
section two, cases have been defined as episodic knowledge which consists of a problem de­
scription, a solution, and a justification for that solution. Normally, rules do not appear to be 
episodic knowledge but, rather, have been extracted from such knowledge, i.e. rules are more 
general than cases. Thus, a rule does not necessarily have a direct correspondence to one spe­
cific event, but is the result of a generalization process based on a number of different events. 
Definition 
A rule is a pair (C,A) where A is an action and C a condition which must be fulfilled to 
do action A. 
Compared to the definition of a case, there is a correspondence between C and A, on the one 
hand, and problem description P and solution S, on the other hand. From another point of 
view, a rule could be described as an explicit kind of problem solving heuristic which can be 
contrasted by the more implicit heuristics being included in a case. Thus, the intended use of a rule (normally) is clear whereas a case can be applied in many different ways to solve similar 
problems. The reason for this is that a case includes a justification which can be interpreted with 
respect to a current purpose whereas rules (normally) have lost their justification. All these 
aspects are summarized in figure 1. 
Though cases and rules differ concerning their complexity on the cognitive level this is not nec­
essarily reflected on the representation and implementation levels. Therefore, figures 2 and 3 
present an exemplary case as well as an exemplary rule which, in principle, differ on the cogni­
tive level only. 
Of course, case representations are often much more complex (cf., e.g., [Ber91]) and, addi­
tionally, other representational and implementational descriptions would have been possible. 
Based on the above definitions, figure 4 gives a rough classification of methods which use 
cases and/or rules. Apart from the differentiation between cases and rules, we think that the 
distinction of exact and partial matching is of importance as well. An underlying assumption is 
that the analogy-based approach applies reasoning between different domains and, therefore, 
needs more general knowledge than it is offered by cases. For instance, the approach Michalski 
describes in his paper on two-tiered concept meaning [Mic89] would be classified as an anal­
ogy-based (matching) approach. 
X Exact Matching Partial Matching 
Rules Standard Rule-Based 
Approach Analogy-Based 
Approach 
Cases Standard 
Data Base Approach Case-Based 
Approach 
Fig.4 - Matching of Cases versus Matching of Rules 
Using the table given in figure 4, an inductive learning system could be classified as a standard 
rule-based or analogy-based approach (we do not want to differentiate between the processing 
of decision trees and rules here). Additionally, approaches known as instance- or exemplar-
based as well as those known from statistics, pattern recognition, or neural networks would be 
classified as case-based approaches. 
The above classification can be refined by differentiating between two kinds of partial match­
ing, namely matching based on generalized indices (as it is used in most case memories [cf. 
Sch82, Kol83a+b, RS89]) and graded matching based on similarity measures [cf. SW88, 
AKA91, AW91]. While the motivation for the indexing approach is more oriented to cognitive 
psychology, the second one has its roots in mathematics/statistics. It applies to both approaches that one part of important information is represented explicidy, and another part not (cf. Fig. 5). 
Thus, their transparency and understandability cannot be evaluated independent from the used 
application. 
X Similarity of 
Cases Computation of 
Similarity 
Case 
Memory Explicit 
Neighbors are similar Implicit 
By insert and retrieval 
procedures 
Similarity 
Function Implicit 
By computed value Explicit 
By used Function 
Fig.5 - Similarity: Computation versus Representation 
In the past, many statistical and pattern recognition procedures have been developed which use 
similarity functions, as well as instance- and exemplar-based (case-based) reasoning ap­
proaches, but only apply pure syntactical methods for clustering or classification tasks. For a 
closer inspection of the relations between similarity, uncertainty, and case-based reasoning cf. 
[RW91]. 
4. Conclusions 
Case-based reasoning represents a specific method for solving a certain class of problems, 
especially for the treatment of inhomogeneous solution spaces. Within such solution spaces, 
cases correspond to homogeneous (i.e. "small" changes of the problem descriptions result in 
"small" changes of the solutions/justifications) subspaces. 
Case-based reasoning is a well-suited approach if cases are an important knowledge source 
within the underlying domain, and the available experts reason from cases (even a formal dis­
cipline as mathematics uses case-based reasoning, e.g. to find a certain proof [Ker89]). In ad­
dition, many domains are "case-based" in their overall structure, e.g. law, medicine, economy. 
Within these domains often a lot of "softcases" exist which can be easily adapted to solve novel 
problems. On the other hand, case-based reasoning is not well-suited in domains mainly con­
sisting of "hardcases" (cases which can only be treated by heavily using common sense knowl­
edge, or a huge amount of domain knowledge). 
Partly in response to this problem, it is now widely recognized that a case-based reasoner can 
"play" different "roles" (the added lists of implemented systems are not intended to be com­
plete, rather they represent an exemplary selection and classification) within a knowledge engi­
neering environment: • Case-based reasoning can be used as a stand-alone problem solver (no cooperation, e.g. 
CYRUS [K0I8O], MEDIATOR [Sim85, Kol89], PROTOS [Bar89, PBH90], CASEY 
[Kot88], CHEF [Ham89b], PATDEX [AdM+89, WeB91, AW91]) 
• Case-based reasoning can be combined with several other separate problem solvers 
(input-output cooperation, e.g. GREBE [BP91], JULIA [HK91]) 
• Case-based reasoning can be one among several cooperating completely integrated prob­
lem solvers (cooperation at all levels of problem solving, e.g. PRODIGY (?) [VC91a,b], 
CABARET (?) [RBD+91], CREEK (?) [Aam90,91], D3 (?) [PG91, Pup90], MOLTKE 
(?) [AMR90, AW91,Alt91]) 
The first role reflects the early phase of case-based reasoning research where a lot of stand­
alone systems have been implemented. Those systems cannot meet all the requirements which 
normally are posed by real world applications. For overcoming these shortcomings, actually the 
combination with other problem solving mechanisms (reasoning from rules, constraints, deep 
models etc.) is a hot research topic ("mixed paradigm reasoning", cf. [RSk89]). Up to now, 
such combinations are normally restricted to cooperations in an input-output manner. A deeper 
integration is an important research goal of many groups but, currently, no completely inte­
grated systems are available. All the systems within the third list are only examples which try to 
achieve this goal (and, therefore, are (question-) marked). Thus, Knowledge Engineering re­
searchers are asked to develop integrated architectures which make use of case-based reason­
ing. 
A first suggestion for the integration of case-based reasoning and model-based knowledge ac­
quisition is given in [JS91], whereas an overview of the integration of case-based, model-
based, and compiled knowledge is given in [SZP90]. Schmalhofer et al. make a suggestion 
concerning the use of cases within an integrated knowledge acquisition process for the prepara­
tion of expert plans which can be reused in novel situations [SBK+91]. The MOBAL system is 
an interesting example for the integration of manual and automatic knowledge acquisition meth­
ods [Mor90]. In [dl091] de la Ossa presents an approach for the automatic adaptation of a 
given diagnostic knowledge base with respect to changes in the physical system which is to be 
diagnosed. A case-based approach to theory revision using self-questions and experiments has 
been suggested by [Oeh91]. 
We mentioned above that, from a Machine Learning point of view, it is difficult to classify 
case-based reasoning, because its learning task is not well-defined. Shavlik and Dietterich 
[SD90] argue that the reason for this has been the motivation of case-based reasoning by con­
cerns for cognitive plausibility rather than by a desire to construct practical systems. However, 
from a Knowledge Engineering point of view, case-based reasoning has some important advan­
tages over standard Machine Learning approaches, namely, apart from a strong focus on cogni­
tive plausibility, the overcoming of the separation of learning and problem solving. 
5. Acknowledgement 
Thanks go to especially to Prof. Dr. Michael M. Richter, and our research group at Kaiserslau­
tern, for many very engaged discussions which have been very helpful for our work on case-based reasoning. Alvaro de la Ossa, Dietmar Janetzko, and Franz Schmalhofer have given help­
ful comments to earlier versions of this paper. Additional insights have come from discussions 
with Dieter Fensel, Katharina Morik, Stefan Wrobel, and Angi Voss. 
6. References 
[Aam90] Aamodt A. A Computational Model of Knowledge-Intensive Learning and 
Problem Solving. In: [WBG+90], pp 1-20 
[Aam91] Aamodt A. A Knowledge-Intensive, Integrated Approach to Problem Solving and 
Sustained Learning. PhD. Thesis, University of Trondheim, 1991 
[AdM+89] Althoff K-D, De la Ossa A, Maurer F, Stadler M, WeB S: Adaptive Learning in 
the Domain of Technical Diagnosis. Proc. Workshop on Adaptive Learning, FAW 
Ulm,1989 
[Aha91] Aha DW. Case-Based Learning Algorithms. In: [Bar91], pp 147-158 
[AKA91] Aha DW, Kibler D, Albert MK. Instance-Based Learning Algorithms. Machine 
Learning , 6, pp 37-66, 1991 
[Alt91] Althoff K-D. Eine fallbasierte Lernkomponente als integrierter Bestandteil der 
MOLTKE-Werkbank zur Diagnose technischer Systeme. Dissertation, University 
of Kaiserslautern (forthcoming) 
[AMR90] Althoff K-D, Maurer F, Rehbold R. Multiple Knowledge Acquisition Strategies in 
MOLTKE. In: [WBG+90], pp 21-40 
[And89] Anderson JR. A Theory of the Origins of Human Knowledge. Artificial 
Intelligence - Special Volume on Machine Learning -, 40, pp 313-352, 1989 
[AW91] Althoff K-D, WeB S. Case-Based Knowledge Acquisition, Learning and Problem 
Solving for Diagnostic Real World Tasks. Proc. EKAW-91,1991 
[Bar89] Bareiss R. Exemplar-Based Knowledge Acquisition. Academic Press London, 
1989 
[Bar91] Bareiss R (ed). Proc. 3rd DARPA Workshop on Case-Based Reasoning, 1991 
[Ber91] Bergmann R. Knowledge acquisition by generating skeletal plans from real world 
cases. In this volume 
[BM88] Barletta R, Mark W. Explanation-Based Indexing of Cases. Proc. AAAI-88, 1988 
[BP91] Branting LK, Porter BW. Rules and Precedents as Complementary Warrants. 
Proc AAAI-91, pp 3-9 
[Bur89] Burstein MH. Analogy versus Case-Based Reasoning. In: [Ham89a], pp 133-136 
[dl091] de la Ossa A. Integrating Strategic Knowledge Acquisition and Reasoning About 
Change for Knowledge Adaptation. Proc. KAW-91, 1991 
[Fis89] Fisher D. Noise-tolerant conceptual clustering. Proc. IJCAI-89, pp 825-830. 
Morgan Kaufmann, 1989 
[Ham89a] Hammond K (ed). Proc. of the 2nd DARPA Workshop on Case-Based 
Reasoning. Holliday Inn, Pensacola Beach: Morgan Kaufmann, 1989 
[Ham89b] Hammond K. Case-Based Planning. Academic Press London, 1989 
[HK91] Hinrichs TR, Kolodner JL. The Roles of Adaptation in Case-Based Design. In: 
[Bar91],pp 121-132 
[JS91] Janetzko D, Strube G. Case-based Reasoning and Model-based Knowledge 
Acquisition. In this volume 
[Ker89] Kerber M. Some Aspects of Analogy in Mathematical Reasoning. SEKI-Report 
SR-89-12, University of Kaiserslautern, 1989 
[KA88] Kibler D, Aha DW. Learning Representative Exemplars of Concepts: An Initial 
Case Study. Proc. Fifth International Workshop on Machine Learning, Morgan 
Kaufmann, 1988 [K0I8O] Kolodner JL. Retrieval and organizational strategies in conceptual memory: A 
computer model. PhD. Thesis, Yale University, 1980 
[Kol83a] Kolodner JL. Maintaining Organization in a Dynamic Long-Term Memory. 
Cognitive Science, 7, pp 243-280, 1983 
[Kol83b] Kolodner JL. Reconstructive Memory: A Computer Model. Cognitive Science, 1, 
pp 281-328, 1983 
[Kol84] Kolodner JL. Retrieval and organizational strategies in conceptual memory. 
Hillsdale, NJ: Lawrence Erlbaum Associates, 1984 
[K0I88] Kolodner JL (ed). Proc. of a DARPA Workshop on Case-Based Reasoning. 
Morgan Kaufmann Palo Alto, 1988 
[Kol89] Kolodner JL. The Mediator: Analysis of an Early Case-Based Problem Solver. 
Cognitive Science, 13, pp 507-549, 1989 
[Kot88] Koton P. Reasoning about evidence in causal explanations. Proc. AAAI-88, 
1988, pp 256-261 
[KSS85] Kolodner JL, Simpson RL, Sycara KP. A process model of case-based reasoning 
in problem solving Proc. IJCAI-85, pp 284-290. Los Angeles, CA: Morgan 
Kaufmann, 1985 
[Mic89] Michalski RS. Concept meaning, matching and cohesiveness. In: [V089] 
[Mor87] Morik K. Sloppy Modeling. In: [Mor89], pp 107-134,1987 
[Mor89] Morik K (ed). Knowledge Representation and Organization in Machine Learning. 
Springer Berlin Heidelberg New York, 1989 
[Mor90] Morik K. Integrating Manual and Automatic Knowledge Acquisition - BLIP. In: 
McGraw & Westphal (eds). Readings in Knowledge Acquisition - Current 
Practices and Trends, pp 213-232, Ellis Horwood, 1990 
[Oeh91] Oehlmann R. Case-Based Theory Revision: Learning from Self-Questions and 
Experiments. Talk given at the University of Kaiserslautern, August 1991 
[PBH90] Porter BW, Bareiss R, Holte RC. Concept Learning and Heuristic Classification 
in Weak-Theory Domains. Artificial Intelligence, 45,1990 
[PG91] Puppe F, Goos K. Improving Case Based Classification with Expert Knowledge. 
Proc. GWAI-91, Springer, 1991 
[Pup90] Puppe F. Problemlosungsmethoden in Expertensystemen. Springer Verlag, 1990 
[RBD+91] Rissland EL, Basu C, Daniels JL, McCarthy J, Rubinstein ZB, Skalag DB. A 
Blackboard-Based Architecture for Case-Based Reasoning: An Initial Report. In: 
[Bar91],pp 77-92 
[Ric89] Richter MM. Prinzipien der Kunstlichen Intelligenz. Teubner Verlag, 1989 
[RS89] Riesbeck CK, Schank RC. Inside Case-Based Reasoning. Lawrence Erlbaum 
Associates, 1989 
[RSk91] Rissland EL, Skalag DB. Combining Case-Based and Rule-Based Reasoning: A 
Heuristic Approach. Proc IJCAI-89, pp 524-530,1989 
[RW91] Richter MM, WeB S. Similarity, Uncertainty and Case-Based Reasoning in 
PATDEX. Festschrift for Woody Bledsoe, Kluwer Academic Publishers, 1991 
[Sal91] Salzberg S. A Nearest Hyperrectangle Learning Method. Machine Learning, 6, pp 
251-276, 1991 
[SBK+91] Schmalhofer F, Bergmann R, Kiihn O, Schmidt G. Using Integrated Knowledge 
Acquisition to Prepare Sophisticated Expert Plans for Their Re-Use in Novel 
Situations. Proc. GWAI-91, 1991 
[Sch82] Schank RC. Dynamic Memory: A Theory Of Learning in Computers and People. 
Cambridge, UK: Cambridge University Press, 1982 
[SD90] Shavlik JW, Dietterich TG (eds). Readings in Machine Learning. San Mateo: 
Morgan Kaufmann, 1990 [Sim85] Simpson RL. A Computer Model of Case-Based Reasoning in Problem Solving. 
PhD. Thesis, Techn. Rep. GIT-ICS/85/18, Georgia Inst, of Technology, 1985 
[SS88] Sharma S, Sleeman D. REFINER: A Case-Based Differential Diagnosis Aide for 
Knowledge Acquisition and Knowledge Refinement. Proc. EWSL-88, pp 201-
210, 1988 
[Sla91] Slade S. Case-Based Reasoning: A Research Paradigm. Al Magazine Spring 1991 
[Str89] Strube G. Episodisches Wissen. Arbeitspapiere der GMD, 385, pp 10-26, 1989 
[SW88] Stanfill C, Waltz D. The memory based reasoning paradigm. In: [K0I88], pp 414-
424 
[SJ90] Strube G, Janetzko D. Episodisches Wissen und fallbasiertes SchlieBen: Aufgaben 
fiir die Wissensdiagnostik und die Wissenspsychologie. Schweizerische 
Zeitschriftfur Psychologie, 49 (4), pp 211-221, 1990 
[Syc91] Sycara KP. Case-Based Reasoning. Overview of an course in case-based reason­
ing at the European Summer School on Machine Learning, 1991 
[SZP90] van Someren MW, Zheng LL, Post W. Cases, Models or Compiled Knowledge: a 
Comparative Analysis and Proposed Integration. In: [WBG+90], pp 339-355 
[Tou81] Tou JT. Application of pattern recognition to knowledge system design and diag­
nostic inference. Pattern Recognition - Theory and Application. Reidel D 
Publishing, 1981 
[VC89] Veloso M, Carbonell JG. Learning Analogies by Analogy - The Closed Loop of 
Memory Organization and Problem Solving. In: [Ham89a] 
[VC91a] Veloso M, Carbonell JG. Learning by Analogical Replay in PRODIGY: First 
Results. In: Proc. EWSL-1991, pp 375-390 
[VC91b] Veloso M, Carbonell JG. Variable-Precision Case Retrieval in Analogical Problem 
Solving. In: [Bar91] 
[V089] Vosniadou S, Ortony A (eds). Similarity and Analogical Reasoning. Cambridge 
University Press, 1989 
[WBG+90] Wielinga BJ, Boose J, Gaines B et al. Current Trends in Knowledge Acquisition 
(Proc. EKAW-90). IOS Press Amsterdam, 1990 
[WeB91] WeB S. PATDEX/2: ein System zum adaptiven, fallfokussierenden Lernen in 
technischen Diagnosesituationen. SEKI Working Paper SWP-91-01, University 
of Kaiserslautern, 1991 Part 3: 
Cognitive Adequacy of 
Expert Systems The Role of Cognitive Science 
in Knowledge Engineering 
Gerhard Strube 
Department of Cognitive Science 
Institute of Computer Science and Social Research 
Albert Ludwig University, Freiburg 
Friedrichstr. 50, D-7800 Freiburg i.Br., Germany 
strube @ cognition, iig. uni-freiburg. de 
Abstract. It is argued that knowledge engineering should take a cognitive 
stance, i.e. it should aim for cognitively adequate systems. The notion of 
cognitive adequacy is unfolded from an idealized, absolutely strong meaning 
(i.e., a complete model of a human expert) down to the very weak notion of 
conforming to recognized ergonomic standards. Various ways are proposed 
to enhance cognitive adequacy in the model-based framework of knowledge 
engineering. Finally, the relevance of these concepts for expert system 
application is discussed. 
Knowledge engineering, as the name says, is an engineering science. As such, the 
objective for knowledge engineering is to develop techniques to elicit knowledge from ex­
perts, or to acquire knowledge from texts, cases, and other sources (automatically, or 
through the mediating work of a knowledge engineer), to organize it and thus render 
domain-specific knowledge ready for knowledge-based systems, to validate the knowl­
edge-base, and to maintain its usability and integrity over the years. 
This task is certainly demanding enough to stifle ideas that would lead to further 
degrees of complexity. To insist that knowledge engineering should aim for a (psychologi­
cally valid) cognitive model of expert knowledge therefore seems an altogether unsane 
recommendation. Cognitive modeling should be left to psychology, and to cogni-tive science in general, unless - well, unless it could be shown to significantly enhance the 
quality of knowledge engineering with respect to its own genuine objective. Costs have 
to be compared, of course, to the return on investments that can be expected. More­
over, the question of the feasibility of cognitive modeling, by itself and in the context of 
knowledge engineering, has to be adressed. Loosely spoken, why bother with cogni­
tive modeling? 
I believe that there are three good reasons to give knowledge engineering a 'cogni­
tive* orientation, namely, because it provides knowledge-based systems with 
• enhanced validity, 
• added flexibility and stability, and 
• better security and ease of use. 
Validity. It is important to recognize that validity refers to both declarative and procedural 
knowledge, i.e., to the facts and rules that comprise the knowledge base as well as to the 
kind of reasoning employed in the inference component of the system. To my mind, 
knowledge engineering has concentrated too much on the content of knowledge, leaving 
the reasoning process to the programmers. (Take conflict resolution in production-
rule systems as an example. Because it is a technical problem, it has received much 
attention in computer science. But no one, to my knowledge, has ever demonstrated that 
kind of conflict, let alone the resolution strategies, in human experts.) On the other hand, 
we know far too little about the kind of reasoning employed by experts. Still, we have 
evidence for mental simulation (Stigler, 1984), evidence of experts running scenarios in 
their heads (Ceci & Liker, 1986). I believe that knowledge engineering must try to 
understand which ways of reasoning are employed by experts in the field, and when. 
Hybrid systems may encompass different problem solvers as well as different representa­
tions of domain knowledge, and the question is which kind of reasoning is to be applied 
at a given time (Janetzko & Strube, this volume). 
Therefore, validity refers to the content and representational format of domain-
specific knowledge, to the strategies of reasoning employed, and to the conditions of use 
for both. In today's reality, however, knowledge bases are often confined to only a narrow facet of domain-relevant knowledge. The usual perspective taken by the knowledge 
engineer, which is shaped by the tools available, as well as by the means provided for 
knowledge representation, is prone to ignore relevant areas of expert knowledge altogeth­
er. Conceptual models and a cognitive modeling approach, on the other hand, ensure an 
initial broadness that, although it must be narrowed down eventually, helps to think about 
which areas might be left out, and which are indispensable. Even where relevant aspects 
have to be omitted, the cognitive modeling approach ensures that this will not go unno­
ticed. In short, the cognitive stance in knowledge engineering is bound to yield systems that 
are more valid with respect to real, i.e., human expert knowledge, and whose 'blind zones' 
are known and can be taken into account when putting the system to use. 
Flexibility & stability. Human experts command a variety of ways to reason from a given 
set of facts, rules, and the like. The flexibility gained through selective application of 
different reasoning strategies, which in turn utilize different parts of knowledge - e.g., 
general rules, or specific cases - is one of the hallmarks of human cognition and perhaps 
the most important single cause of our success in thinking and problem solving. It follows 
that the cognitive approach to knowledge engineering must strive for variety in reasoning, 
and must likewise try to embody domain-specific knowledge in multiple representation 
formats in the knowledge base. This is certainly demanding, and costly as well. But it 
pays by providing a means to the solution of a problem where simple, single-minded 
approaches get stuck. Modem expert system technology aims for enrichment of systems 
through 'deep' models of parts of the domain (e.g., models of apparatus), and through 
incorporation of examples or libraries of cases already solved. 
Expert system technology, at this point, is more cognitive than knowledge engineer­
ing. Take CBR (case-based reasoning) as an example: There is already a considerable 
literature on representation of cases, and on reasoning from cases (e.g., DARPA, 1988; 
Riesbeck & Schank, 1989), but we lack techniques for assessing and structuring episodic 
knowledge. We even lack systematic evaluation of well-known methods like proto­
col analysis when applied to episodic knowledge.1 
To name but one of the cognitive problems that are involved, remembering a case results in a verbal 
protocol that looks like a protocol from 'thinking aloud' during solving an actual problem. But 
memory may act as a filter, or worse, may give rise to reconstruction, thus mixing general knowledge 
with specific recall. Therefore, a memory protocol may provide even more indirect evidence as the 
thinking-aloud technique (Ericsson & Simon, 1984). In addition to the flexibility gained through enrichment by CBR and the like, 
enhanced stability can be the result. (It need not be so, because added complexity may 
bring along problems of coordination, arbitration, etc.) Multiple knowledge enables not 
only us humans, but any system to cross-check results, which amounts to an evalutation 
of the solution proposed, and makes the system more robust and more dependable. 
Ease of use. A system that models the reasoning of real experts provides a sound base for 
giving explanations to its user, explanations that are both correct and understandable. By 
contrast, it is difficult to see how a system whose reasoning is utterly un-human might 
arrive at explanations that fulfil both requirements. (It is, however, easy to conceive that 
a system of that kind could give 'intuitive' explanations that are fictitious, or correct ones 
that are cryptic.) Researchers agree that giving adequate explanations is perhaps the crucial 
feature of a good expert system2. A cognitive approach is necessary, at least to some 
degree, to make the explanation component of a system useful. Of course, explanation 
makes further requirements, like natural language interaction (which in turn gains much 
from a cognitive approach). 
Although I would agree that consistency is the key feature to user-friendliness in 
knowledge-based systems, I feel that cognitive adequacy brings us a step further. The 
added security and ease of use that result from a cognitively adequate explanation facility 
are well worth the effort. 
It would be wrong to conclude that the cognitive stance in knowledge engineer­
ing is all pro's without the con's. Here are some cautions against: 
• Cognitive modeling is an extremely expensive endeavour, costly in time, and hence, 
money. The gains must be substantial indeed to warrant the effort. Yet my opinion 
is that there is much to be gained: theoretically, because a cognitive model helps us 
to understand the expert, and human expertise in general, and practically, because a 
valid model of expertise is bound to score better in solving real-world problems. 
2 'Explanatory capabilities are crucial to the ultimate acceptance of expert systems' (Buchanan & 
Shortliffe, cited after Swartout, 1990, p. 298). • It may simply be impossible to construct a cognitive model in many projects. We 
still know very little about the nature of expertise, and practical reasons may often 
forbid the attempt to construct a full model. Sometimes, however, much is won by 
even little steps in the right direction. Whenever we know that we cannot obtain the 
ultimate goal, it may be advisable to take a step or two in that direction. 
• A cognitive model may not be necessary, indeed, it may turn out to be adverse, or 
harmful, to the task. Of course, that might come true for very specialized, very 
technical domains only. Still, we must remember that our prime objective is not to 
model the expert, but to provide expert system tools for novices and experts alike. 
Therefore, diagnosing faults in a complex device may be more efficiently done by 
means of a (correct) engineering model than by modeling the cognitive process­
es of human experts. Still, I believe that modeling the expert is a commendable 
strategy in most of the cases. 
To sum up, it costs, but (at least usually) pays, to strive for a system that employs 
cognitively adequate representations of knowledge and problems, and equally adequate 
ways of processing that knowledge. Yet if we make a stand for cognitive modeling in 
knowledge engineering, we must try to define what it is that we aim at. Therefore: 
What does it mean to be cognitively adequate? 
There is certainly room for disagreement when answering the above question. For 
instance, does the term cognitive pertain to human thinking exclusively, or not? Whole 
wars of definition could be envisaged. Yet my objective in this chapter is much more 
humble than that. The following is no more than an attempt to scale different degrees of 
'nearness' to human cognition, and to explore the consequences of each, if attained in 
knowledge engineering and expert systems. 
Strong and weak cognitive adequacy. If strong adequacy is claimed, the system is 
supposed to function like a human expert, at least in a circumscribed domain. In short, 
strongly adequate systems employ the very same principles of cognitive functioning as 
human experts do. If this is not the case, but the system has been carefully built with the human user in mind, it may be credited with weak cognitive adequacy. In this sense, 
'weakly adequate' means the same as 'well-adapted to the user', or 'easy to use'. 
Weak cognitive adequacy. This is, or rather, should be, the trivial case. A system so 
characterized is ergonomic and user-friendly. Consistency of the user interface, recognized 
graphical standards, a clear language for commands, help texts, etc., make the system easy 
to use. Note, however, that the system may differ considerably from the experts (whose 
knowledge it attempts to represent) and from its users (if those are different from the expert 
group). Still, the systems tries to give users a comfortable feel, which may be achieved 
through symbols or words familiar to the user. The drawback is that the user's understand­
ing of system messages may be at odds with what the system actually does. 
Remember the Apple Macintosh's first version of the trash can, for instance. Its 
behavior - it worked simply as a delete command, so files could not be regained - irritated 
many users and was corrected in later versions. In expert systems, the possible damage is 
vastly greater. Users may interpret the systems explanations wrongly; they may, for 
instance, arrive at inadequate degrees of confidence in the system. Users of expert systems, 
if they are not experts themselves, might be lured by 'intuitive' surfaces into unwarranted 
conclusions about the scope of the system's knowledge. Indeed, the more 'intelligent' a 
system behaves on the outside, the less it is expected to lack common sense. 
Much the same difficulties that emerge in the translation process from system 
operations to explanations given to the user, also arise in the process of knowledge 
engineering, i.e., translation of human expert knowledge into the formalism used by the 
system to represent and utilize it. If the basic notions of the expert are incompatible with 
those that the system can represent, knowledge engineering becomes complicated, to say 
the very least. In addition, verification of the knowledge base is next to impossible, 
because the system's knowledge entities cannot be compared with those of the expert. 
My opinion is that weak cognitive adequacy is something every good program attains 
to, and that it is definitely not enough for expert systems. Indeed, I believe that to remain 
at that very low level of cognitive adequacy would become an obstacle to knowledge 
engineering, and a source of severe errors to the user. Strong cognitive adequacy, absolute and relative. Strong cognitive adequacy comes in 
(at least) two degrees, absolute and relative. This is to mean, that an absolutely adequate 
system of the 'strong' family claims to be a model of the human expert's knowledge and 
way of reasoning in every relevant aspect. For relatively strong adequacy, that claim is 
reduced to the assertion that the kind of knowledge representation and reasoning as used 
in the system can be found in human experts, too. 
The notion of absolute cognitive adequacy is, of course, an ideal not even fulfilled 
in the most ambitious projects of cognitive science, for which it amounts to the long-
range strategic goal of cognitive modeling. But it is not an objective for Al systems, 
although some authors, like Charniak and McDermott in their well-known textbook, claim 
that for the ultimate achievement of Al. To the contrary, it is obvious that expert systems 
need not (nor should they) have all human faculties at their disposal, not even all cognitive 
abilities. It would be nice if expert systems had some common sense, however, and the 
little we know about that suggests that we would need all human knowledge, and, per­
haps, the human body even, to arrive at common sense, therefore it does not seem feasible 
to have common sense (which we would like) without all the complexity of human 
cognition (which is a highly impractical, if not impossible task to do). This line of 
argument serves to reduce the quest for cognitively adequate systems to what I have termed 
relatively strong cognitive adequacy above. 
To construct cognitively adequate systems in the strong sense, then, amounts to 
employ knowledge representations and methods of reasoning that can be demonstrated to 
be used by human experts, too. As we have argued, this will not amount to a complete 
model of the human expert, and it need not amount to that. Still, we should take care not 
to ignore kinds of reasoning and forms of representation that seem essential for natural 
expertise. 
As far as we know, human experts have insight into the causal relationships in the 
domain, and they use it mainly when other, easier approaches fail, or when they are asked 
to explain their results. This kind of knowledge may be captured by causal, or 'deep' 
modeling. Apart from that, their experience yields heuristic rules that may reflect statisti­
cal properties, e.g., the relevance and typicality of certain symptoms with respect to some 
diagnostic category. Rule-based systems usually represent this kind of expert knowledge. But experts, as far as we know, often and in many domains, like to think 'analogically', i.e., 
use visual imagery. This aspect is not easily captured in technical systems, which by 
definition lack the sophisticated visual apparatus shaped in billions of years of evolution. 
Expert systems, as far as I know, have not yet been enriched with this kind of representa­
tion, although there are quite a few experimental approaches to imagery. Finally, experts 
often work along the same lines as they did when they solved a similar problem, and they 
like to refer to specific case studies in knowledge engineering interviews in order to explain 
procedures, or to illustrate certain principles. This characteristic trait of human exper­
tise has attracted much attention recently, and given rise to the construction of systems that 
employ case-based reasoning. 
An important, though mostly neglected, characteristic of natural expertise is meta­
knowledge, or reflection. Human experts generally differ from the inexperienced, because 
they are always 'oriented' during their work. They do not lose sight of the solution they 
seek, as novices often do, they seldom make errors, and they are quite good at estimating 
how near they are to a solution (Gruber & Strube, 1989). Meta-knowledge also lets them 
switch between strategies, an ability that is highly desirable in technical systems, too. 
To sum up, I believe that being cognitively adequate in the weak sense is mandatory, 
yet not sufficient, for modern expert systems. We must strive to use representations of 
domain-specific knowledge that are both practical and understandable to human users. 
Not surprising, it turns out that this is what human experts usually have arrived at. We 
should further try to implement kinds of reasoning that are used by human experts. At least 
some combinations, like using a case library of previously solved problems together with 
case-based reasoning techniques, promise to cut down the potentially enormous amount 
of search space and time, required by purely rule-based systems. We should also try to 
include at least a moderate degree of reflection, or meta-knowledge, in the system, which 
is both a hallmark of human expertise and a necessity for control in hybrid systems. 
Admittedly, we cannot make an expert system into a complete model of the human expert, 
but we should attempt to include representations and reasoning strategies that are essential 
to human experts. Towards cognitively adequate systems 
Starting with ergonomics. Aiming at weak adequacy first, let us start with ergonomic 
design of the knowledge base and its user interface. It needs consistency. It also needs 
flexibility, which in turn requires 'intelligent' adaptation of the system, hence, a certain 
degree of Al. User modeling, in the meantime, has become a field of its own (e.g., Kobsa, 
1985), but the approach of classifying users and designing the interface according to some 
typology is not sufficient and in need of enhancement. - Modern user interfaces are also 
characterized by windowing and graphics. Pen-based input will spread rapidly, while I 
doubt whether speech output will ever become popular. (Hearing a soft female voice saying 
'Your prin-ter ist out of pa-per' repeatedly is noi going to make your computer interac­
tion more pleasant, after the initial surprise wears off.) The use of natural language, 
however, is bound to increase as even portables include the high-performance processors 
and huge amounts of memory needed for the task. Here is another field where Al comes 
in. 
'Knowledge ergonomics9. The next step will not take us further from weak cognitive 
adequacy, yet it is a crucial step because it will bring us to knowledge. Naturally enough, 
the kind of knowledge used by an expert system, even if its machine representation may 
differ from the one we use in our heads, must be linked to our way of thinking. There are 
many means that together serve to accomplish this task, although no single one is suffi­
cient. 
The basic requirement is that the terminology used by the system must be consistent 
(see above) and should agree as well as possible with the terminology used by experts in 
the field. The usual techniques of extracting terminological knowledge from textbooks 
fulfil that requirement only partially. For instance, important concepts may lack verbal or 
formal definition, as in the field of interpreting aerial pictures a 'gently rolling plain' 
(Hoffman, 1987). The basic problem, however, applies to the status of terms or concepts 
with respect to their role in problem solving. Here is where proposals like KADS come in 
(Breuker & Wielinga, 1985; KADS-II, 1990). Although KADS is not primarily aimed at 
being cognitively adequate, its meta-terminology serves to structure a given domain and 
define the functional role of the concepts, too. In addition, tools should be available to make the system understandable to its users. 
This concerns mostly what is called the explanation facility of an expert system. Explana­
tion, of course, should not be limited to a 'rule 463 fired' style of system messages. Use 
of the domain terminology in explanations is important. The system's line of reasoning 
should be displayed, as well as the causal dependencies in the knowledge base used by 
the system. Graphical components may enhance that facility through visualization of causal 
dependencies, and can in turn be integrated with browsers. 
A further tool to enhance the cognitive ergonomics and general usefulness of an 
expert system is an' as if mode of functioning. This function is common in spreadsheets, 
where you can run simulations, and compare the effects of certain modifications. The 
same should be extremely useful for knowledge-based systems in order to assess the effects 
of certain pieces of knowledge, of different kinds of reasoning, or just different preference 
orders in reasoning. An 'as if mode opens a system to the user for exploration. It also 
enhances a system's usefulness in training. 
What else? In order to make a system approximately cognitively adequate, we must embed 
the characteristics of human problem solving in its knowledge base and inference engine. 
• Deep modeling. Pure surface modeling, i.e., reliance on statistical rules, is not 
enough. Although this kind of rules is used by experts, and serves to get quickly at 
'most probable' diagnoses, etc., human experts also have the capability to generate 
causal explanations for any problems using 'deep', functional models of the domain. 
Especially for untypical problems, deep modeling is a must for expert systems. Apart 
from considerations of efficiency, functional models of the domain are a necessary 
part of cognitively adequate systems, because only functional models serve to 
generate true explanations of a problem and its solution. 
• Episodic knowledge and case-based reasoning. Human knowledge acquisition 
makes good use of examples, and learning proceeds fastest when rules and examples 
are combined (Schmalhofer & Kiihn, 1988). Human experts are known to rely on 
episodic knowledge almost exclusively in domains like law and judgement, and 
substantially in domains like medical treatment (Strube & Janetzko, 1990). Case-
based reasoning has therefore become a major field of research, as conferences (e.g., DARPA, 1988) and recent textbooks (Schank & Riesbeck, 1989) show. Integration 
of CBR into traditional, rule-based approaches to expert system construction has 
become an important step in the direction of systems that are cognitively more 
adequate (see Janetzko & Strube, this volume). 
• Meta-knowledge. Cognitive control of one's strategies of reasoning, monitoring of 
one's own way toward a solution, and careful deployment of cognitive resources is 
one of the hallmarks of human experts (Gruber & Strube, 1989). The present state 
of expert system technology, however, largely ignores this aspect. Still, I hope that 
recent developments in software engineering, above all the debate on OOP and multi-
agent systems, may stimulate discussion of intelligent distribution of resources. 
• Learning. Although machine learning and expert systems have remained separate 
fields up to now, automatic knowledge acquisition, and hence, incorporation of 
components for learning, has become a hot issue for expert system technology. I'll 
focus on automatic knowledge acquisition during the system's active life time, i.e., 
on acquiring new and additional knowledge. 
Human experts continuously change and amplify their knowledge. This is done 
via two different processes: (1) Explicit communication (mainly verbal) of rules or 
equivalent (i.e., general) knowledge, and (2), learning from experience through re­
use of solution paths already successful in previous cases, and through generalization 
from specific experience. Much the same classification holds for machine learning. 
Modern expert systems should provide means for learning directly from the user. 
That feature should also include checking procedures for integrability of new rules 
into the knowledge base, i.e., checks for logical consistency (still a hard problem), 
and in case of inconsistencies, prompting the user to specify conditions of applicabili­
ty in order to circumvent inconsistency. (I am assuming here that AI's means of 
dealing with inconsistency do not yet approach the power of our abilities to be both 
rational and inconsistent.) In addition, an expert system should include facilities to 
generalize from experience (at least some simple, well-known algorithm, like EBG), 
and the capability to adapt and re-use previously computed solutions to similar 
problems, in other words, case-based reasoning. The German Ministry of Research and Technology's project FABEL (1991) is an example of building systems in that 
spirit. 
Approximating strong cognitive adequacy in knowledge-based systems, as may be 
guessed from the paragraphs above, is neither easy, nor is it cheap. Even those factors 
necessary for weak cognitive adequacy, i.e., the ergonomic features, are on a par with the 
most ambitious features to be found in other software with respect to complexity and 
demands on the hardware. So my message is once more to take at least some steps into the 
direction of greater cognitive adequacy, for this will enhance usability, and therefore 
acceptance. This brings us to our last issue, viz., concepts of expert systems usage that fit 
with the design goal of cognitive adequacy. 
How to use cognitively adequate expert systems 
Design of information-processing systems must include a philosophy of its application. 
This means that the designer must consider the role of the system in the organization of 
work on the problems the system is intended to help with. In other words, the system's 
integration into the greater mixed man-machine system has to be thought about. 
Expert systems technology started out with the goal of systems that could replace 
costly experts. While this is still a valid conception in certain environments (e.g., for doing 
routine diagnostics in computer-integrated manufacturing), the general trend has shifted 
to emphasize expert systems as general tools in the hands of experts (Becker & 
Paeteau,1991). This philosophy of expert system application makes communication 
between man and machine much easier, since the gap in terminology and knowledge 
between expert and non-expert is eliminated. In addition, non-expert users are usually not 
able to assess the limitations and constraints of an expert system's reasoning. Experts, 
however, to have the necessary prerequisites to evaluate systems, and to take their basic 
presuppositions into account. An expert system is a complex tool that needs sophistication 
on the side of the user, too. Seen as sophisticated tools, expert systems are also more 
benevolent in social perspective, since they do not aim at replacing highly skilled workers, 
but enhance the quality of work. Still, in order to become a tool as good as possible, expert 
systems must be given the ability to explain their ways of reasoning and, most importantly, evaluate alternative solutions (see the 'as if-mode discussed earlier in this chapter). 
Cognitively adequate systems will simply be the better tools. 
Goals for knowledge engineering 
How can knowledge engineering help us to get systems that are cognitively more 
adequate? I believe that we should follow three promising lines at least: 
• continue to follow the modeling approach (e.g., KADS), which are not implementa­
tion-specific, in order to provide a framework for terminological classes that could 
perhaps be modified to become cognitively more adequate in itself - a cognitve 
orientation need not be adverse to enginnering needs (see Linster, this volume), 
• enrich present-day knowledge engineering techniques with methods specifically 
devised or adapted for episodic knowledge, thereby integrating case-based ap­
proaches with more traditional lines of expert systems design, and finally, 
• integrate knowledge engineering and machine learning. This is not meant to take a 
stance in the long-standing discussion about the benefits and shortcomings of 
automatic knowledge engineering v. knowledge acquisition as mediated by 
knowledge engineers. Indeed, I believe that machine learning methods are a desirable 
complement to work that can and should be done by knowledge engineers. 
All this, and still more so the issues to be tackled along the way to strongly adequate 
cognitive systems, is expensive with respect to both effort and costs. But the benefits to be 
expected from those efforts might be even greater: It pays to have cognitively adequate 
systems. I wish knowledge engineering would follow that direction. References 
Becker, B., & Paeteau, M. (1991). Von der kognitiven zur interaktiven Adaquatheit? In: 
T. Malsch (Ed.), Informatisierung und gesellschaftliche Arbeit Berlin: Edition 
Sigma. 
Breuker, J.A., Wielinga, B. W., et al. (1987). Model-driven knowledge acquisition: 
Interpretation models. Deliverable task Al, Esprit Project 1098. University of 
Amsterdam. 
Charniak, E., & McDermott, D. (1985). Introduction to Artificial Intelligence. Reading, 
MA: Addison-Wesley. 
DARPA (1988). Case-Based Reasoning. Proceedings of the 1st Workshop on Case Based 
Reasoning. Clearwater, FL. 
Ericsson, K. A., & Simon. H.A. (1984). Protocol analysis. Verbal reports as data. Cam­
bridge, MA: MIT Press 
[FABEL](1991). FABEL: Intergration von modell- und fallbasierten Entwicklungs-
ansatzen filr wissensbasierte Systeme. Antrag fur ein Verbundvorhaben an den 
Bundesminister fur Forschung und Technologie (BMFT), Bonn. 
Gruber, H., & Strube, G. (1989). Zweierlei Experten: Problemisten, Partiespieler und 
Novizen beim Losen von Schachproblemen. Sprache und Kognition, 8, 72-85. 
Hoffmann, R. (1987). The problem of extracting the knowledge of experts from the 
perspective of experimental psychology. Al Magazine, 8 (2), 53-67. 
Janetzko, D., & Strube, G. (this volume). Case based reasoning and model-based knowl­
edge acquisition. 
[KADS-II] (1990). KADS-II. Espiit II Technical Annex for Project P5248. Meylan, 
France: Cap Gemini. 
Kobsa, A. (Ed.)(1985). Benutzermodellierung in Dialogsystemen. Berlin: Springer. 
Riesbeck, C, & Schank, R.C. (1989). Inside case-based reasoning. Hillsdale, NJ: Erlbaum. 
Schmalhofer, F., & Kuhn, O. (1988). Acquiring computer skills by exploration versus 
demontration. 10th Annual Conference of the Cognitive Science Society. Montreal. 
Hillsdale, NJ: Erlbaum. 
Stigler, J. F. (1984). 'Mental abacus': The effect of abacus training on Chinese children's 
mental calculation. Cognitive Psychology, 16, 145-175. 
Strube, G., & Janetzko, D. (1990). Episodisches Wissen und fallbasiertes SchlieBen: 
Aufgaben fiir die Wissensdiagnostik und die Wissenspsychologie. Schweizerische 
Zeitschriftfur Psychologic 49,2\\-221. 
Swartout, W. (1990). Explanation. In S.C. Shapiro (Ed.), Encyclopedia of Artificial 
Intelligence (vol. 1, pp. 298-300). New York: Wiley. 
Acknowledgements. I wish to thank all the participants of the KEKOG workshop for an 
inspiring discussions of the ideas presented here, and especially Angi Vofi, who commented 
on the first draft of this paper. Knowledge Acquisition as an Empirically 
Based Modelling Activity 
Beate Schlenker and Thomas Welter 
IBM Germany, Scientific Center 
Institute for Knowledge Based Systems 
Wilckensstr. la 
W-6900 Heidelberg 
Germany 
WETTER@DHDIBM 1 .bitnet 
When knowledge engineering is consequently looked upon as scientific 
discovery, certain prescriptions can be derived about how to observe ex­
pertise and how to deal with intermediate stages of the modelling process. 
These prescriptions concern roles played by underlying assumptions, theo­
ries and paradigms of cognitive science and their respective implications 
upon conduction and interpretation of individual experiments, and the 
language(s) used in the modelling process. Most essentially, scientific dis­
covery is characterized by planned feedback, including theory based iden­
tification of contradicting or falsifying evidence. Some of these 
characteristics clearly differ from principles suggested in KADS for the 
respective modelling activities. 
1.0 Considerations on Theory of Science and Underlying 
Assumptions 
The process of knowledge acquisition can be characterized as the knowledge engi­
neer's attempt to get to know the epistemological structure of the domain to be mod­
elled (Woodward, Shaw, and Gaines 1991). To this end the knowledge engineer has 
to learn something about the concepts and problem solving strategies used by the ex­
pert in the respective domain. One problem is that the knowledge engineer is a novice 
in the respective domain of expertise. Furthermore the expert's concepts and problem 
solving strategies are not directly observable, but have to be derived from (mainly 
verbal) data. In cognitive psychology (and Al) it is widely respected (e.g. Gigerenzer 
1981; Clancey 1989) that modelling empirical phenomena requires both, specific as­
sumptions about mechanisms underlying the phenomena and a foundation of the modelling process in theory of science. If the knowledge engineer does not make these 
theoretical concepts explicit, he will interpret the expert's observable behavior with 
uncontrolled bias and his interpretations will not be reproducible by others. 
The specific assumptions we make draw upon specific theories or paradigmata from 
cognitive psychology as background knowledge. This knowledge is not further put 
under question during the modelling process. One possible background or basic as­
sumption (the one we use) may be to describe higher cognitive processes such as 
learning as information processing behavior (information processing paradigm). 
Methods used in cognitive psychology to elicit data and to infer knowledge about the 
underlying cognitive processes are considered as background knowledge as well. The 
methods come along with their respective refinements of the information processing 
paradigm. Concurrent think aloud protocols are e.g. in accordance with the "working 
memory'-refinement of the information processing paradigm. In the framework of the 
so defined knowledge the knowledge engineer constructs models about entities whose 
existence "on the expert's mind" would explain his behavior. 1 
Concerning the foundations of the process to arrive at such a model one possibility 
to reduce the knowledge engineer's bias and to make the modelling process more ob­
jective is to apply the general "empirical method" (cf. Schulz, Muthig, and Koeppler 
1981) to knowledge acquisition. This implies the use of Popper's falsification strategy 
(1966), according to which hypotheses or theories have to be formulated in a way that 
they can be falsified in principle. The knowledge engineer's misconceptions can thus 
be identified during the phase of model construction. The proposed "empirically 
based method" of knowledge acquisition consists of a sequence of operations and re­
lated rules that are applied to test the appropriateness of models used to reconstruct 
part of the perceivable reality. The process of model construction can be character­
ized as a process of incremental refinement from a rough general to a complex and 
more detailed and accurate one. 
Since model development includes correction by falsifying evidence, the knowledge 
engineer has to define observable evidence against his hypothesis before having any 
data extracted from the expert. This implies both the selection of a data extraction 
method and the a priori definition of an encoding schema. An encoding schema is 
principially determined by the observation method used and the theoretic foundation 
which it is built upon. Contrary to methods like KADS, where the static models are 
1 We might speak of concepts, relations, processes ... instead of entities. But this would 
constrain the variation of model structures admitted in our approach more than necessary 
(given the information processing paradigm) and desirable given the subsequent consid­
erations about language. imported in order to form the model of expertise, using the strategy of planned feed­
back means dynamic model development. 
2.0 Planning the Process 
We don't treat a requirements analysis as part of the following process. Given the 
requirements the following activities occur several times in a cyclic process until a re­
sult meets external criteria, which we assume to have been specified as part of the 
requirements. Furthermore the information processing paradigm and the relatedness 
of observation and underlying theory are not put under question. As to the rest, the 
strategy of planned feedback means dynamic model development. In the following 
the modelling steps are listed. 
1. Construction of hypotheses forming a theory that explains the knowledge under­
lying the expert behavior. This can be knowledge concepts as well as knowledge 
structures or problem solving strategies. 
2. Formulation of a discriminating experiment. This includes the derivation of ob­
servable behavioral consequences that would be supported by the hypothesized 
underlying cognitive structures or processes as well as the behavior representing 
the falsification of the hypothesis. It also includes how to encode the raw obser­
vations. 
3. Definition of a criterion for empirical evidence to distinguish between support and 
falsification of hypotheses. 
4. Evaluation of a hypothesis according to empirical data. That is rejection of the 
hypothesis in case of falsification. 
a. In case of falsification: Continue with 1; construct new hypothesis 
b. In case of support: Continue with 6 
5. Check whether the model meets external criteria. 
a. If not: Continue with 1; refine hypotheses 
b. If yes: stop. 
2.1 Falsification 
We suggest to distinguish essential falsification from incidential falsification. We 
suggest to call Popper's falsification in the strict sense using only one contradicting 
evidence incidential falsification. Following Popper in a strict sense would mean that 
a hypothesis or theory would have to be given up in case of only one unexpected item 
occurring in an observation. For reasons which are probably inherent to the field of 
human cognition, incidential falsification will always occur. That is, theories will 
permanently be falsified before they can be applied. Therefore criteria have to be es­
tablished according to which sets of observations are considered as falsifications. Such an essential falsification provides the feedback that the model has to be rejected or 
modified. The term essential falsification means that a considerable amount of falsi­
fying items is needed in order to reject a hypothesis or theory. Statistical evidence 
cannot be used in the usual sense of observing a random sample of subjects, since the 
investigations made are mainly single case studies aiming at modeling individual ex­
pertise. Hence, criteria have to be established which arc plausible with respect to the 
method applied (e.g. its characteristic noise). One could also think of external criteria 
for essential falsification, such as ratings of another expert or consent from the party 
that ordered the knowledge acquisition activity. The empirical data are then com­
pared to the evidence criterion and the hypothesis or theory is refuted in case of es­
sential falsifying evidence. Otherwise the hypothesis or theory can be kept until a 
better one is found. 
It should be pointed out that our strategy of planned feedback and the incorporation 
of falsifying experiments can be seen as competing with an approach which might be 
derived from Woodward et al. (1992), where the observer considers possible causes 
of misconception, bias, etc. and tries to avoid them by compensating means either in 
planning investigations or interpreting observations. This anticipatory attitude or 
procedure is, however, hypothesis-driven. Hypotheses are not made about the model 
itself but about factors influencing the construction of the model. And hypotheses are 
never systematically challenged or tested in the process; they are just imported. 
3-0 Development of the Modelling Language 
It has turned out as another essential side condition of such an approach that the 
language used for denoting the model must not be prescribed in the beginning. As 
Heisenberg (1989) has pointed out, the need for the means to describe the phenomena 
under study emerges from the process of investigating them. E.g. quantum mechanics 
would not have reached its present state of cognition, if the term "impulse" would not 
have been discovered and understood as being applicable also in situations where 
mass, speed, or both don't make sense2 . In our case this means that we must avoid 
to prescribe a formal language for the model of the phenomenon to be investigated. 
We must even try to be mentally independent of such languages (to the extent that 
humans are capable of thinking independently of any language). It must be part of 
the investigator's attitude and awareness that situations will occur where he has to 
overcome the language he used in former iterations. On the other hand, a formal 
language must be arrived at in the end. For the model to be executed on a computer, 
2 i.e. essentially in such situations where the wave aspect is more useful than the particle 
aspect and the impulse of a wave emerges more or less naturally from its spectrum. it must be written in a language whose formal semantics is known. For the present 
purpose this should be operationalized in two ways. A denotational semantics allows 
to argue about properties such as truth conditions of sentences that might be part of 
the model. A procedural semantics allows to map the elements of the model onto op­
erations offered by some abstract machine, which then simulates the behavior of the 
model. Agreement between denotational and procedural semantics in the example of 
truth conditions means that the simulation supplies "true" exactly for the sentences 
found true on the basis of the denotational semantics. 
For practical purposes the direction into which the language should be developed, or 
maybe better, the sector in which the intermediate languages should lie, is thus con­
strained by the need for formal semantics. 
4.0 Skill Requirements for Empirically Based Modelling 
Reviewing all these requirements, it becomes obvious that the task we are describing 
is nothing less than scientific discovery. That means that the knowledge engineer 
doing his work according to these suggestions needs to have at least the following 
skills: 
• sufficient psychological knowledge to design experiments for a large number of 
purposes 
• some basic knowledge of theory of science and theory of cognition to do observa­
tion, interpretation, hypothesis formation, and experiment planning in a princi­
pled way 
• sufficient knowledge in semantics of formal languages to proceed from informal 
first notations to full fledged formal languages. 
In any case it is obvious that the suggested approach needs both very high skills and 
very much effort. So it may be asked whether less expensive methods should be gen­
erally preferred. In practice many projects which can be carried through by straight­
forward use of KADS would never have been approved on the basis of cost estimate 
of what we are suggesting. There are, however, general considerations and specific 
situations where our suggestion should nevertheless be applied. 
5.0 Comparison of Empirically Based Modelling and Modelling in 
KADS 
In the following the "empirically based method" as described above is compared with 
some modelling aspects of KADS. Although in KADS no explanation of the under-lying cognitive structures but functional models are intended, Breuker & Wielinga 
(1987) claim to explore the nature of expertise. Another intention of KADS is to 
support systematic knowledge engineering. Since data used for model verification are 
extracted by psychological methods, a critical analysis of some specific assumptions 
and actions in KADS seems helpful. The general consideration addresses the re­
striction by language. If we understand the KADS layers as constituents of a lan­
guage and the interpretation models (IMs) as building blocks for future applications, 
then the scope of what can be expressed is definitively fixed. Whatsoever detail iden­
tified as characterizing an application under study has to be expressed in KADS 
layers and I Ms. Details not fitting into this frame can either not be expressed or have 
to be deformed. 
5.1 Variety of Models in the Approaches 
The process of empirically based modelling may in some case well arrive at a model 
which replicates the KADS layers (domain, inference, task, strategy), because these 
are not implausible and definitely within the information processing paradigm. They 
do, however, not exploit its variation - frames with their tight coupling of entity and 
inference would not be covered by the KADS layers. Hence the empirically based 
modelling might arrive at models outside the scope of KADS. 
Obviously the IMs further constrain the search space (which is the intention in 
KADS and not per se a deficiency). However, among those empirically developed 
models which do replicate the KADS layers, only some will also replicate one of the 
existing IMs. 
5.2 Aspects of Validation and Control in the Process 
What is more severe than the limitation of expressiveness is the principle lack of 
feedback indicating that something has been missed. I.e. KADS representations 
which look appealing may be essentially deficient without any systematic chance to 
detect the deficiency. 
5.2.1 Model selection vs. model construction 
In contrast to the dynamic process in the empirically based approach with its means 
of feedback, modelling in KADS is intended as top-down process driven by the in­
terpretation models. Interpretation models are matched with verbal data gathered by 
psychological methods in order to find the appropriate interpretation model for the 
domain. This is critical since the method of verification does not imply any possibility 
to control the knowledge engineer's bias in interpreting the verbal data. Plausibility 
is used as the only test criterion for the appropriateness of the constructed conceptual 
model. Therefore model selection according to plausibility can lead to a situation 
where different knowledge engineers select different interpretation models for the same domain with every knowledge engineer having a plausible explanation for his 
decision. This reveals a certain arbitrariness of model selection in KADS. 
5.2.2 Observation and interpretation 
First of all it has to be clarified that elicited data are directly used in KADS, while 
encoded items are generated in the empirically based approach. The KADS view im­
plies that verbal data have an inherent truth. It contradicts the empirical view of data 
as verbalizations interpreted on the predefined background knowledge (cf. Clancey 
1989). 
5.2.3 Justification of elicited material 
For the selection of methods to elicit verbal data, Breuker & Wielinga (1987) have 
developed a table (p.23) which statically assigns methods to required data types such 
as procedural or static knowledge. This again reflects the attitude that elicited data 
can be both collected from the expert and applied in selecting or filling an IM irre­
spective of their context of elicitation and without need of theory and situation based 
interpretation. In other words: elicited data are justified in themselves and as they 
occur. 
In the empirically based approach elicitation methods and their respectice encoding 
prescriptions are dynamically designed or selected according to the needs to support 
or falsify an actual hypothesis. Each method entails guidelines to remove subjectivity 
and bias as far as possible based on known characteristics of the methods. As an 
example of such criteria, Ericsson & Simon (1984) name protocol segmentation and 
separate encoding of the segments in arbitrary order as a means to achieve objectivity. 
An encoded item is hence justified by having been produced from an authentic re­
cording of verbal data by means of theoretically justified procedure. As the next step 
will show, this does not mean that such an item is never put under question again. 
5.2.4 Justification of the use of elicited material in model formation 
Use of verbal data in KADS means model matching as long as an IM still has to be 
selected or slot filling once the decision for an IM has been made. In the latter case 
the attitude is confirmatory, i.e. the use of verbal data is justified when the knowledge 
engineer managed to use it for his model construction. 
In the empirically based approach the use of encoded items is never finally justified. 
The use of items to develop a theory respectively its model formulation in some di­
rection may be hypothetized at some stage in the development and may have to be 
withdrawn at later stages by falsifying evidence concerning those hypotheses of the 
model, to which the items contributed. Therefore, the use of items can be described 
as temporarily justified as long as no falsifying evidence has occured. 5.2.5 Justification of models of expertise 
Principally, in both approaches the justification of a full model is the sum of the in­
dividual justifications outlined above. This does not yet entail that a justified model 
is correct or useful. There is a weak notion of correctness in KADS, of the kind that 
all verbal data can be explained by a model. In the empirically based approach there 
is no claim that a model will ever be correct. It would only be used as an acceptable 
approximation as long as there is no falsifying evidence. Usefulness has to be judged 
on the basis of requirements, which have not been part of this outline. 
Breuker, J.A. & Wielinga, B.J. (1987) Use of models in the interpretation of verbal 
data. In A. Kidd (ed.), Knowledge Acquisition for Expert Systems: A Practical 
Handbook, (p. 17-44). New York: Plenum. 
Clancey, W. (1989) The frame of reference problem in the design of intelligent ma­
chines. To appear in K. van Lehn & A. Newell (eds.), Architectures for Intelligence: 
The Twenty-Second Carnegie Symposium on Cognition. Hillsdale: LEA. 
Ericsson, E.A. & Simon, H.A. (1984) Protocol Analysis. Verbal reports as data. 
Cambridge, Mass.: The MIT Press. 
Gigerenzer, G. (1981) Messung und Modellbildung in der Psychologic Miinchen: 
Reinhardt. 
Heisenberg, W. (1989) Ordnung der Wirklichkeit. Miinchen: Piper. 
Popper, K.R. (1966, 1989) Logik der Forschung. 9. Auflage. Tubingen: Mohr. 
Schulz, Th., Muthig, K-P., Koeppler, K. (1981) Theorie, Experiment und 
Versuchsplanung in der Psychologic Stuttgart: Kohlhammer. 
Woodward, B.J., Shaw, M.L.G., Gaines, B.R. (1992) The Cognitive Basis of Know­
ledge Engineering, in this volume Shifting Positions: Moving From a Cognitive Science 
Point of View to a Knowledge Engineering Stance 
Marc Linster 
Al Research Division 
GMD 
P0 Box 1240 
D-5205 St. Augustin 1 
Abstract 
After a short overview of knowledge acquisition highlights, we review experiences that we 
had in our knowledge acquisition project. We conclude that automated knowledge acquisition 
does not work without a documentation of the purpose that the knowledge will fulfill once it is 
acquired. This can be done for example through a description of a method of problem-solving. 
The remainder of the paper gives a more detailed account of the motives (outside the actual 
experiences with KRITON) that lead to these conclusions. After outlining several requirements, 
we delineate the role of cognitive science research in our current approach. 
1 A Review of Work in Knowledge Acquisition that 
Influenced Us 
We give a short overview of developments in knowledge acquisition that we consider as cornerstones. 
There is no intention of completeness. We only want to situate our work, and explicate our bias. 
The systematic construction of knowledge-based systems—henceforth called knowledge engineering— 
started out from early work on TEIRESIAS [Davis, 1982] and first developments of engineering 
guidelines for knowledge-based systems, for example [Buchanan et a/., 1983]. A considerable effort 
has been invested to obtain systematic support for the development of knowledge-based systems. 
To widen the knowledge-acquisition bottleneck researchers have investigated a multitude of methods: 
cognitive-science oriented techniques for the systematic elicitation of knowledge, for example protocol 
analysis, or personal-construct analysis; dedicated tools for the acquisition of knowledge for special 
tasks, for example OPAL [Musen et a/., 1987]; method-specific knowledge-acquisition tools such 
as ETS [Boose, 1985]; workbenches to support conceptualization, such as ProtoKEW [Shadbolt, 
1992]; learning systems to develop domain-models, for example, BLIP [Morik, 1987]; and knowledge 
acquisition methodologies, such as KADS [Wielinga et a/., 1992]. 
In the course of these developments, a change of focus occurred. Whereas early work viewed the 
development of knowledge-based systems as a transfer activity from a human mind into a computer-
accessible representation—this is best illustrated by Feigenbaum's bottleneck metaphor or Boose's 
expertise transfer system ETS—current work views knowledge engineering as a process of construct­
ing models [Clancey, 1989]. At the same time, the mining view of knowledge acquisition, whose 
optimistic variant states that extensive and deeper knowledge elicitation provides the key to exper­
tise, gave way to a more pragmatic view, stating that the knowledge acquisition process is guided 
by the requirements of system building. // we were able to obtain the detailed data of the knowledge that drives human expert behavior, we would not know how to handle it [Breuker and Wielinga, 1989, 
p. 267]. 
2 Experiences in Our Knowledge Acquisition Project 
This shift of positions can be illustrated very well using GMD's knowledge-acquisition project. Ini­
tially we focussed on automated knowledge-elicitation tools that relied heavily on cognitive-science 
motivated techniques. The first such tool that we developed at GMD was called KRITON. KRITON 
made it obvious that automated knowledge acquisition without an underlying method of problem-
solving does not work, no matter how pretty and elaborated the knowledge-acquisition techniques 
or interfaces are. 
The next sections give a more detailed account of the reasons and intermediary steps which lead to 
that conclusion. 
2.1 The Approach Taken in KRITON 
The knowledge acquisition tool KRITON [Diederich and Linster, 1989; Linster, 1989] includes several 
knowledge elicitation techniques that cooperate with the goal to acquire a large coherent body of 
knowledge: (1) TEXT ANALYSIS and INTERVIEW for the acquisition of static domain features; 
and (2) PROTOCOL ANALYSIS for the acquisition of procedural and associative knowledge. 
The TEXT ANALYSIS component reads texts from a file. Nouns are highlighted and made mouse-
sensitive. The user can include them into a hierarchy describing the way the text presents the 
organization of the concepts of the domain, or the way the expert sees their organization. The text 
analysis is a computer support to get started in a knowledge acquisition process. 
The INTERVIEW component edits and completes the initial information gathered by the TEXT 
ANALYSIS. Relying heavily on the repertory-grid technique [Gaines and Shaw, 1981], it acquires 
attributes describing the concepts of the domain. 
Several other techniques are used in the INTERVIEW to complete the description of the concepts, 
such as explicit inheritance, explicit generalization, and explicit completion. 
PROTOCOL ANALYSIS helps acquire procedural and associative knowledge. It is based on the 
work of Ericsson and Simon [1984]. It concentrates on the analysis of transcripts of recordings of 
thinking-aloud protocols. To analyze protocols in KRITON, they are transcribed with the pauses of 
speech. Using the assumption that pauses of speech represent delimiters that separate the transcript 
into coherent segments, these segments are transformed into operator-argument structures that are 
then combined into rules to make the inference steps, implicitly contained in the protocol, explicit. 
TEXT ANALYSIS and INTERVIEW work on a common data structure: a semantic net consisting 
of labeled edges and attributes. The user has total freedom to define relationships as edges or as 
attributes. PROTOCOL ANALYSIS produces rules describing how the concepts in the semantic net 
are used in a problem-solving process. 
In a prototypical acquisition session the user starts out with analyzing a background text and building 
a taxonomy representing the most important and frequently used concepts of a domain. These will 
then be attributed and described in the INTERVIEW. The taxonomy can be edited to fit new facets of 
the domain unraveled during the INTERVIEW. After this one can go back to the TEXT ANALYSIS 
to complete the taxonomy or go directly into the PROTOCOL ANALYSIS. New concepts appearing 
in rules are introduced into the taxonomy in the INTERVIEW. The result of an acquisition process 
with KRITON is a dense network describing structural relations (stemming from INTERVIEW and 
TEXT ANALYSIS) or associative relations (stemming from PROTOCOL ANALYSIS). 2.2 Consequences and Subsequent Work 
KRITON provides good analysis facilities to define an initial vocabulary and to get a first start when 
building a knowledge-based system. 
Experience though showed that to move beyond the initial knowledge acquisition, a tool needs strong 
guidance—it must be goal driven. In KRITON it is not clear in which directions the taxonomies and 
the protocols have to be elaborated, as no meta-knowledge about the purpose of the knowledge is 
available to the system. As the system has no information about the role an element of the semantic 
net plays, it cannot inquire about the typical relations that are needed between problem-solving 
concepts of certain roles. 
In subsequent developments we focussed on making KRITON knowledge-based, to give it explicit 
information about which kind of knowledge to acquire [Diederich and Linster, 1989]. We introduced 
pre-defined template-like domain structures, which had to be filled by the knowledge acquisition pro­
cess. The process is guided by a program called WATCHER, using meta-knowledge about expected 
domain structures to trigger elicitation tools. For example, the WATCHER uses the rule "classes and 
instances must be discernible on the basis of their attributes" to trigger a differentiating repertory-
grid-based interview for elements of the knowledge base requiring attributes or values. However, a 
reconsideration showed that the previous rule expresses knowledge structures needed by a classifi­
cation problem-solving method. This led us to an analysis of the relation between problem-solving 
method and knowledge-acquisition tool. For example we analyzed the combination of the problem-
solving method heuristic association with the repertory-grid based interview tool of KRITON. It 
showed that strong guidance for the acquisition tool emanates from the problem-solving method. 
First experiences with KADS [Karbach et al., 1989] led to the assumption that flexible, configurable 
problem-solving methods are needed, instead of pre-defined, selectable ones. The knowledge-needs, 
derived from the methods of problem-solving, seemed a good basis to guide the knowledge acquisi­
tion tools in their task. This set the stage for the research questions that we are exploring today: 
the exploitation of the properties of explicit problem-solving methods for (automated) knowledge 
acquisition. 
3 How Does This Relate to Our Positions on Cognitive 
Science? 
Initially, we aimed at developing descriptions of human reasoning, and transform these into opera­
tional systems. This was the purpose of the elicitation tools in KRITON. It left us with unstructured 
heaps of knowledge bits that each, in context, made a lot of sense. However, as decontextualized 
knowledge units, they are worthless. 
Thus, we changed our positions, and moved from a cognitive-science oriented point of view to a 
stance that emphasizes the engineering aspect of the development of knowledge-based systems. We 
decided not to view the knowledge of a system as consisting of discrete, atomic elements that each 
had the property of being knowledge. We assumed Clancey's point of view, stating that knowledge 
is something that an observer ascribes to a human agent in order to describe and explain recurring 
interactions that the agent has with its environment [Clancey, 1989, p. 288]. For us this meant that 
the results of knowledge acquisition systems must be viewed in the context of the purpose of the 
agent. We are fully aware that we cannot capture the totality of this purpose. However, we can try 
to define certain frameworks explicating those parts of the purpose that we are aware of and that we 
can express. 
The decision that we took, to frame the acquisition of knowledge with methods of problem-solving, is thus motivated by three arguments: 
1. Knowledge is not composed of discrete elements that each on their own, independently of 
purpose, context, or interpreter are knowledge. 
2. It is pointless to try to acquire knowledge that one cannot represent in a machine [Breuker and 
Wielinga, 1989, p. 267]. 
3. Knowledge elicitation, without the documentation of a purpose, gets stuck all too soon. 
This entailed a change in focus in our project. We are now using the framework of a problem-solving 
method to give meaning to knowledge-bits such as rules, clauses, or frames; and we are using that 
framework to guide automated knowledge acquisition. 
3.1 Where Does that Put Cognitive Science? 
At first sight, one may think that cognitive science is out of the ball park now. This is not true 
at all. Even if today we focus on the engineering of intelligent systems, and even if we say that we 
are building these systems in ways that must not necessarily coincide with human mechanisms of 
intelligence, then the process of building these systems is nevertheless a process of construing the 
ways one or several intelligent agents solve the problems of a real-world task. 
Thus our engineering implements must satisfy requirements from two sides: (1) they must be com­
puter accessible, so that we can closely link their role in the process of modeling-to-make-sense 
[Clancey, 1989, p. 289] with their task in modeling-to-implement-systems; and (2) they must make 
sense as mediating devices in the social interaction processes between knowledge engineer and human 
expert (s). 
We discuss the second point to explain in some detail why we still need cognitive science, even if we 
tackle the problem from an engineering point of view. 
In a knowledge-acquisition situation observed behavior (e.g., thinking-aloud protocols, video footage, 
repertory grids or answers to focussed questions) is being analyzed and rationalized using knowledge-
structuring primitives, such as rules, frames, knowledge sources, or tasks. This is a constructive 
process of building a model. The model is the result of construing behavior with the help of primitives 
aiming at making sense. Knowledge acquisition is a creative process of discussion between knowledge 
engineer and expert. The model is the result of this interaction. 
To enable this process, the meaning of the knowledge-structuring primitives must be accessible to all 
participants of the discussion. Especially, they must be able to see that a model accounts for observed 
behavior. This is easier for the knowledge engineer than for the domain expert, as the primitives do 
have well-defined semantics in terms of the underlying operational knowledge-representation system. 
One cannot expect the domain expert to be a programmer. This implies that the semantics of the 
terms of the model must be intuitively accessible to the expert, that is, they must be adequate for 
the task. 
To make the discussion more proficient, the meaning of the primitives should be such that a model 
can be tested against new situations, that is, that it can be validated. Potential contradictions 
between the model and a new situation lead to reconsideration, extension or structural changes of 
the model. 
Furthermore, an ideal set of primitives should be such that a model provides guidance in the discus­
sion, just as an agenda keeps track of unresolved issues. A formal analysis of a model might point 
to open ends, ambiguous or underconstrained decision points. This implies that even though the model must not necessarily be a truthful reconstruction of human 
reasoning processes, the primitives that are used to build the models must in a certain sense be 
cognitively adequate to mold human knowledge and problem-solving processes, even if these are 
created in the actual knowledge engineering process. 
3.2 Today's Situation 
Today we are analyzing modeling frameworks like KADS, to find out whether they suffice these 
requirements. For example when re-modeling the cancer-chemotherapy administration task of 0N-
COCIN with KADS [Linster and Musen, 1992] we analyzed how KADS knowledge-structuring prim­
itives (i.e., concept, instance, and relation on the domain layer; meta class, knowledge source, and 
inference structure on the inference layer; task and task structure on the task layer) can be used 
as rationalization tools for observed behavior (i.e., medical documents of cancer chemotherapy and 
knowledge structures of ONCOCIN). Furthermore we examined how KADS models guide the acqui­
sition process, and in how far they support a constructive discussion between expert and knowledge 
engineer. An analysis of this modeling process, the cognitive processes and potential biases involved 
is presented in [Woodward, 1991]. 
To close the loop of conceptual modeling a la KADS (i.e., modeling to make sense) and modeling to 
implement systems several implementations have been developed, such as MODEL-K [Karbach et al., 
1991] or OMOS [Linster, 1991]. The operational modeling language OMOS bridges the gap between 
modeling as a process of making sense and modeling as a process of implementing knowledge-based 
systems, by providing KADS-oriented knowledge-structuring primitives for methods and domains. 
They are operational and provide immediate feedback for automated knowledge acquisition tools 
[Kuhn et al, 1991] (see Figure 1). 
Disambiguation, complete 
definition in a formal 
language 
Model building, Implementing 
making sense svstems 
Testing of knowledge bits 
in context, formal analysis 
Figure 1: The interaction between model building to understand ill-structured situations and behaviors, and modeling 
to implement systems. 
Our recent work on OMOS and MODEL-K has emphasized the system-building aspect of knowledge 
engineering. We have been looking for knowledge-structuring primitives that are operational and 
that provide guidance in the construction process. We now must analyze these results again from 
the cognitive science point of view. We need to know in how far our terms are cognitively adequate 
building blocks for the modeling of expertise. Furthermore we want to analyze the knowledge- creation 
aspect of our current view on knowledge engineering. We want to know in how far a knowledge 
engineering process relying on these primitives creates the knowledge for a knowledge-based system, 
and how these knowledge-structuring primitives influence the discussion process between expert and 
knowledge engineer. This involves the analysis of the cognitive activities of knowledge engineering 
[Woodward et al., 1992] in KADS-like frameworks, and the role of epistemic knowledge-structuring 
primitives in frameworks that were devised to analyze the model-building processes (e.g., [Wetter and Woodward, 1990]). This will help us answer questions like: What kind of knowledge can we 
acquire into our models? How will our models be biased? How can we use well-defined techniques 
from cognitive science to support model-building and the creation of operational systems? 
Acknowledgements 
This work has been influenced strongly by discussions led in the Al Research Division of GMD. 
Barbara Becker, Werner Karbach, and Angi VoC were involved in the development of many of the 
arguments of this paper. Thomas Christaller and Angi VoC commented on earlier versions. 
References 
[Boose, 1985] John H. Boose. A knowledge acquisition program for expert systems based on personal 
construct psychology. International Journal of Man-Machine Studies, 23:495 - 525, 1985. 
[Breuker and Wielinga, 1989] Joost Breuker and Bob Wielinga. Models of expertise in knowledge 
acquisition. In Giovanni Guida and Carlo Tasso, editors, Topics in Expert System Design, Method­
ologies and Tools, Studies in Computer Science and Artificial Intelligence, pages 265 - 295. North-
Holland, Amsterdam, 1989. 
[Buchanan et al, 1983] B. G. Buchanan, D. Barstow, R. Bechtal, J. Bennett, William Clancey, 
Casimir Kulikowsky, Tom Mitchell, and Donald Waterman. Constructing an expert system. In 
F. Hayes-Roth, D. Waterman, and D. Lenat, editors, Building Expert Systems, pages 127 - 167. 
Addison Wesley Publishing, London, 1983. 
[Clancey, 1989] William J. Clancey. The knowledge level reinterpreted: Modeling how systems in­
teract. Machine Learning, Special Issue on Knowledge Acquisition, 4(3, 4):285 - 292, 1989. 
[Davis, 1982] R. Davis. Application of meta-level knowledge to the construction, maintenance. In 
R. Davis and D.B. Lenat, editors, Knowledge Based Systems in Artificial Intelligence. McGraw-
Hill, New York, 1982. Doctoral dissertation, Computer Science Department, Stanford University. 
[Diederich and Linster, 1989] Joachim Diederich and Marc Linster. Knowledge-based knowledge elic­
itation. In Giovanni Guida and Carlo Tasso, editors, Topics in Expert System Design, pages 323 
- 352. North-Holland, Amsterdam, 1989. 
[Ericsson and Simon, 1984] A. Ericsson and Herbert Simon. Protocol-Analysis - Verbal Reports as 
Data. MIT Press, Cambridge, 1984. 
[Gaines and Shaw, 1981] Brian R. Gaines and Mildred L.G. Shaw. New directions in the analysis 
and interactive elicitation of personal construct systems. In Mildred L. G. Shaw, editor, Recent 
Advances in Personal Construct Technology, Computers and People, pages 147 - 182. Academic 
Press, London, 1981. 
[Karbach et al., 1989] Werner Karbach, Marc Linster, and Angi VoC. OFFICE-Plan, tackling the 
synthesis frontier. In Dieter Metzing, editor, Proceedings of GWAI89, volume 216 of Informatik 
Fachberichte, pages 379 - 387, Heidelberg, Septembre 1989. Gesellschaft fuer Informatik, Springer 
Verlag. Also published as WEREX-Bericht nbr. 23. 
[Karbach et al., 1991] Werner Karbach, Angi VoC, Ralf Schukey, and Uwe Drouven. MODEL-K: Pro­
totyping at the knowledge level. In Proceedings of the First International Conference on Knowledge 
Modeling and Expertise Transfer, Sophia Antipolis, France, 1991. [Kiihn et al., 1991] Otto Kiihn, Marc Linster, and Gabi Schmidt. Clamping, COKAM, KADS and 
OMOS. In Duncan Smeed, Marc Linster, John H. Boose, and Brian R. Gaines, editors, Proceedings 
of EKAW91. University of Strathclyde, 1991. Also published as Technical Memo TM-91-03 of 
DFKI, Kaiserslautern. 
[Linster and Musen, 1992] Marc Linster and Mark Musen. Use of KADS to create a conceptual 
model of the ONCOCIN task. Knowledge Acquisition, Special Issue on KADS, 1992. 
[Linster, 1989] Marc Linster. Towards a second generation knowledge acquisition tool. Knowledge 
Acquisition, 1(2):163 - 183, 1989. 
[Linster, 1991] Marc Linster. Knowledge acquisition based on explicit methods of problem-solving. 
PhD thesis, University of Kaiserslautern, Kaiserslautern, 1991. Submitted. 
[Morik, 1987] Katharina Morik. Acquiring domain models. International Journal of Man-Machine 
Studies, 26:93-104, 1987. 
[Musen et al., 1987] Mark A. Musen, L. M. Fagan, D. M. Combs, and E. H. Shortliife. Use of 
a domain-model to drive an interactive knowledge-editing tool. International Journal of Man-
Machine Studies, 26:105 - 121, 1987. 
[Shadbolt, 1992] Nigel R. Shadbolt. Facts, fantasies and frameworks: The design of a knowledge 
acquisition workbench. In Franz Schmalhofer, Gerd Strube, and Thomas Wetter, editors, Con­
temporary Knowledge Engineering and Cognition, Lecture Notes in Computer Science. Springer, 
Heidelberg, 1992. 
[Wetter and Woodward, 1990] Thomas Wetter and Brian Woodward. Towards a theoretical frame­
work for knowledge acquisition. In John H. Bosse and Brian R. Gaines, editors, Proceedings of the 
5th Banff Knowledge Acquisition Workshop, pages 35/1 - 35/25, Calgary, 1990. AAAI, University 
of Calgary. 
[Wielinga et al., 1992] Bob Wielinga, Guus Schreiber, and Jost Breuker. KADS: A modelling ap­
proach to knowledge engineering. Knowledge Acquisition, Special Issue on KADS, 1992. 
[Woodward et al., 1992] Brian Woodward, Mildred Shaw, and Brian Gaines. The cognitive basis of 
knowledge engineering. In Franz Schmalhofer, Gerd Strube, and Thomas Wetter, editors, Con­
temporary Knowledge Engineering and Cognition, Lecture Notes in Computer Science. Springer, 
Heidelberg, 1992. 
[Woodward, 1991] Brian Woodward. Developing K-ONCOCIN: A case study in the cognitive pro­
cesses of knowledge engineers. In John II. Boose and Brian R. Gaines, editors, Proceedings of the 
Knowledge Acquisition Workshop 91 (KAW91), Calgary, 1991. AAAI, University of Calgary. Two Questions from Expert System Developers to 
Cognitive Scientists 
Frank Puppe, Ute Gappa 
University Karlsruhe 
Institut fiir Logik, Komplexitat und Deduktionssysteme 
Postfach 6980, W-7500 Karlsruhe 
Germany 
Abstract: (1) Are the well-known "strong" problem solving methods in 
expert systems cognitively adequate enough for the experts which have to 
formalize their knowledge accordingly? and (2) How significant are adequate 
graphical representations offered by some knowledge acquisition tools for the 
internal model of the experts? 
In this paper we concentrate on one of the many forms of cooperations between the displicines 
of knowledge engineering and cognition: Proposals from the knowledge engineering field 
should be evaluated with respect to their cognitive validity. In particular, we exemplify this 
approach by asking two questions: about the cognitive significance of strong problem solving 
methods and of graphical display forms of the knowledge. Both questions reflect basic 
approaches of our research. We also give some reasons, why we are asking these questions. 
However, we don't provide any answers. 
There are three basic knowledge acquisition types for expert systems: 
• Indirect knowledge acquisition: A "knowledge engineer" acquires the knowledge from 
experts ("knowledge holders") and formalizes it for the expert system. 
• Direct knowledge acquisition: The knowledge holders formalize their knowledge by 
themselves. 
• Automatic knowledge acquisition: The knowledge is transformed automatically from 
already existing knowledge (e.g. from the literature or from cases). 
Indirect knowledge acquisition is susceptible to communication problems and quite expensive, 
in particular with respect to the maintenance of the knowledge bases. High quality automatic 
knowledge acquisition is impossible with the current state of the art. Therefore, direct 
knowledge acquisition seems to be the most promising path. It has also the motivational 
advantage, that the knowledge holders are given complete authorship and reputation for their 
knowledge bases. However, direct knowledge acquisition can only succeed, if the expense of the knowledge holders for learning and using given knowledge acquisition and maintenance 
facilities remains acceptable low. This requires: 
• cognitive adequacy of the underlying problem solving methods, 
• simple to use and effective knowledge acquisition tools. 
Although it is not necessary, that the problem solving methods of expert systems are those of 
the experts, the methods should be at least easily understandible. Otherwise experts will have 
much difficulty to express and formalize their knowledge. Therefore, an exchange with cogni­
tive science could be quite useful: On the one hand, the existing problem solving methods 
should be tested for their cognitive plausibility, and on the other hand, constructive results from 
cognitive science could lead to the development of easy to understand problem solving 
methods. 
classification 
certain 
classification 
decision 
tables classification with certain 0->S knowledge 
and riven observations 
decision 
trees classification with certain 
0->S knowledge 
heuristic 
classification classification with 0->S knowledge 
model based 
classification 
set covering 
classifcation classification with S->0 knowledge 
functional 
classification classification based on a 
functional system model 
statistical 
classification classification with knowledge derived from a 
large representative collection of cases 
case based 
classification classification with case collections and 
additional knowledge for similarity measure 
Fig. 1: Overview on well known classification problem solving methods (from [Puppe 90]). 
Abbrevations: O = Observations; S = Solutions. Hierarchy i 
ObjecHi 
[ Object 111 
106J112 | 
Object 1121 
(0 113K 
Object 114 
'—• Object 1141 I 
1 Object 115 =•1 Graph 
I OblectT 
| Object 5 j I ObiectT 
t Object 4 I 
Hierarchy and Graph 
Object Form 
Attribute 1 Nemeofthe object J ( ) [ OK : 
r Cancel : 
Attribete 2 O elternetive 1 O elternetive 3 O elternetive 5 
<§> alternative 2 O elternetive 4 O elternetive 6 
Attribute 3 Thl31s en example text. 6 
I 
iii 
Attribute 4 
Attribute 5 
Attribute 6 
Attiibvte 7 I pop-up- menu option"! 
| number 1 | until 
| Object 1 
Object 2 
Object 3 
Object 4 
Additional attribute 1 DC Additional attribute 2 
Object Form 5EI 
"I Object 2 "1 
si 1 Table Ul= 
Rovs ' —, Columns Object 10 Object 20 Object 30 
j Section i iljiliHiM^^ lillllillijijlliiliillp lijiliillilijiiljljljij lijlll&lliiliiilil iiiifiijiiilHiiiiiiiji 
Object 1 
Attribute Value 11 X X 
Attribute Value 12 X lilli 
Attribute Value 13 iii:; 
Object 2 m 
W-
W«l Attribute Value 21 10 12 m 
W-
W«l [Section 2 lii'&i.iMiUi! hwwi "a M&iYi'.'ifriii;.! iw '!'$! liLi't!' | I'M1' fiV m 
W-
W«l 
Object 3 
Object 4 ! i 
Attribute Value 41 high j i i ffi|l{ 
Attribute Value 42 low 
Object 5 1 i ! !i 
Attribute Value 51 i j j O 
01 liiiiiipiiiiiiiijiiiiiiiii! Niii>Hi«iiiiiiiHiii|it£iiIiliItiiIiiEiiiifiNfi i|t|iiriz|iiiHitIiIiIfiiiiifiIisicHeis§ 
Table Uiili 
Fig. 2: Generic graphical primitives (from [Gappa 91]) Examples for such an exchange are the psychological studies about the decision making process 
of physicians [Elstein 78, Kassirer 78, 82, Feltovich 84]. Among other things, they revealed 
the consistent use of the hypothesize-and-test strategy (early generation and goal-directed 
evaluation of diagnostic hypotheses) and of the differential-diagnosis strategy (taking into 
account always several competitive hypotheses simultaneously) by physicians. In accordance 
with the progress of expert system research, it would be desirable to test complete problem 
solving methods, of which the above mentioned strategies are only parts. An overview on well 
known classification problem solving methods is shown in Fig. 1 from [Puppe 90], where the 
methods also are described in detail. 
One precondition for examining hypothesized cognitive models of experts is the 
formalization and implementation of their problem solving models, which we tried in our 
classification expert system shells MED2 nesp. D3 [Puppe 87, D3 91]. 
However, adequate problem solving methods are only part of the story. In addition 
adequate, maybe graphical knowledge acquisition tools are needed in order to help domain 
experts to understand the underlying problem-solving model and to structure and formalize their 
knowledge by themselves with only limited assistance from "knowledge engineers". Graphical 
knowledge acquisition environments should allow to directly enter knowledge in specialized 
versions of basic graphical primitives like hierarchies, graphs, forms and tables shown in Fig. 
2 from [Gappa 91], where the primitives are also described in more detail. Our other question 
to cognitive scientists concerns the significance of such graphics for the internal model of the 
experts. Do they normally visualize their knowledge in such graphical representations and are 
there other basic primitives than those in Fig. 2? 
References: 
[D3 91] Bamberger, S., Gappa, U., Goos, K., Meinl, A., Poeck, K., and Puppe, F.: The Diagnostic Expert 
System Shell D3, Manual, Version 1.0 (in German; translation to English in preparation), Universitat 
Karlsruhe, Institut fur Logik, Komplexitat und Deduktionssysteme, 1991. 
Elstein, A., Shulman, L., and Sprafka, S.: Medical Problem Solving, Harvard Univ. Press, 1978. 
Feltovich, P., Johnson P., Moller, J., and Swanson, D.: LCS: the Role and Development of Medical 
Knowledge in Diagnostic Reasoning, in Clancey, W. und Shortliffe, E.(eds.): Readings in Medical 
Artificial Intelligence, Addison-Wesley, 1984 (1980). 
Gappa, U.: A Toolbox for Generating Graphical Knowledge Acquisition Environments, in: Proc. of The World 
Congress of Expert Systems, Vol. 2, 797-810, Pergamon Press, 1991. 
Kassirer, J., Kuipers, B., and Gorry, G.: Towards a Theory of Clinical Expertise, American Journal of Medicine 
73: 251-259, 1982. 
Kassirer, J. and Gorry, G.: Clinical Problem Solving: a Behavioral Analysis, Annals of Int. Med. 89, 245-255, 
1978. 
Puppe, F.: Requirements for a Classification Expert System Shell and Their Realization in MED2, Applied 
Artificial Intelligence 1: 163-171,1987. 
Puppe, F.: Problem Solving Methods in Expert Systems (in German; translation to English in progress) 
Springer, 1990. The Cognitive Basis of Knowledge Engineering 
J. Brian Woodward, M.L.G. Shaw, and B.R. Gaines 
Knowledge Science Institute 
Department of Computer Science 
University of Calgary 
2500 University Dr. NW. 
Calgary, Alberta 
CANADA, T2N1N4 
e-mail: woodward®cpsc.ucalgary.ca 
Abstract. The goal of knowledge engineering is to create an artificial system 
which reflects knowledge-like qualities. Current tools, techniques and procedures 
in knowledge engineering concentrate on the elicitation and representation of 
knowledge structures. This concentration of effort reflects the current emphasis on 
the epistemological and computational/representational characteristics of knowledge 
engineering. A different, yet complementary perspective is offered in this paper. 
Knowledge engineering is defined as a human activity system, characterized as a 
cognitive environment or network which deals with complex epistemological 
domains. Rather than viewing knowledge engineering as entirely concerned with 
knowledge content, those processes which produce the knowledge in the 
knowledge engineering environment are viewed as the focus of attention. Memory, 
judgment and choice, text comprehension, and social cognition and communication 
represent a selection of cognitive science domains which offer research findings of 
importance for knowledge engineering. Based on these research findings, the 
groundwork is laid for the development of cognition-support tools for 
knowledge engineering. 
1. Introduction and Purpose 
The domain of knowledge engineering has focused on the acquisition and modelling of 
knowledge. This is not a surprising statement but it raises an important issue which this 
paper addresses. The terms 'acquisition' and 'modelling' would first suggest an emphasis 
on the processes and procedures related to knowledge engineering but a closer look 
suggests that the focus is rather on the outcome of these processes. The 'knowledge' focus 
dictates that whatever processes are developed, or adapted, they must result in structures 
which somehow 'capture' or 'represent' identified forms of 'knowledge'. The tools, 
techniques and procedures developed to acquire knowledge are classified as either 'domain-
specific' or as 'generic'. The term 'domain- specific' denotes a tool, technique or procedure 
which has developed out of knowledge engineering within a defined domain (eg. Opal: see 
Musen, 1989). The content, or the specific knowledge structures of the domain, guide and 
determine the tool, technique or procedure to be used. Generic tools (eg. KSSO: Gaines, 
1988) are those which are considered to be useful and appropriate in a wide variety of 
domains. In this case, the tool guides the type of content which will be specified from the 
domain. The results of these tools are knowledge-specific structures. This paper presents arguments and research support for the development of a different 
category or class of knowledge engineering tool: cognition-based tools. This type of tool is 
characterized by a de-emphasis of the 'knowledge' itself with a greater emphasis on the 
cognitive activities abundant in knowledge engineering activity which produce knowledge 
structures. These cognitive processes inherent in the knowledge engineering process 
establish the cognitive basis for knowledge engineering. The selection is based on sites of 
cognitive activity not solely on domain tasks or knowledge centred generic processes. 
Rather than emphasizing processes which identify domain tasks and knowledge structures, 
this approach emphasizes the support for the cognitive process activity in knowledge 
engineering. 
Knowledge engineering is viewed as a body of activity which is characterized by intense 
cognitive activity as well as a strong emphasis on knowledge structures. Section 2 presents 
the main premises for viewing knowledge engineering from a cognitive processing 
perspective rather than from a knowledge perspective. Knowledge engineering is defined 
as a human activity system consisting of different sites and levels of cognitive activity. The 
cognitive processes occurring at each site and between sites constitutes part of the cognitive 
environment. The focus of this paper is on the cognitive processes of the knowledge 
engineer. Section 3 presents research on the processes involved in understanding new, 
complex domains (a usual experience for knowledge engineers). Sections 4 through 7 
present cognitive research relevant for knowledge engineering: memory, text 
comprehension, judgment and choice, and social cognition. 
2. Knowledge Engineering Viewed as a Collection of Cognitive Processes 
2.1 The Current Focus in Knowledge Engineering 
The domain of knowledge engineering reflects a strong emphasis on the acquisition and 
modelling of knowledge structures and the processes to manipulate and transform them. 
These aspects may be categorized as the epistemological nature of the domain. 
Epistemological and accompanying ontological discussions are concerned with the concepts 
selected to express the various structures for acquisition and modelling. This emphasis 
reflects the importance of concepts like the 'knowledge level' (Simon, 1988) and the 
research on 'expertise' and expert knowledge (eg. Johnson, 1986). Results of these studies 
centre predominantly on expert-novice differences in conceptualizing the domain problem, 
in the knowledge structures used in finding a solution, and, to a lesser extent, in the 
different methods used to solve the problems. 
Discussions concerning the nature of knowledge also characterize the domain of knowledge 
engineering. These discussions include the role of different types or forms of knowledge in 
meeting tasks demands. The KADS (Wielinga, et al, 1989) methodology in knowledge 
acquisition is based on a strong epistemological foundation. A parallel approach to the 
development of re-useable problem solving structures and processes is represented by role-
limiting methods (see Marcus, 1988) and generic tasks (Chandrasecaran, 1988). This 
research points to the need for identifying usable knowledge structures and the processes 
by which these structures are manipulated. 
A related issue is that of knowledge representation. Representation frameworks abound and 
they will not be discussed here (see Brachman and Leveque, 1985). The discussion of 
forms and structures of knowledge logically lead to the issues of how to represent that 
knowledge in a form useful for modelling. Selected epistemological constructs act as the 
basis for knowledge structures and are used to impart meaning. Formal languages and 
representational frameworks determine what is computationally tractable. Epistemic concepts are used to develop formal structures and these structures are used to order and 
arrange the knowledge of a given domain. 
2.2 The Cognitive Aspect of Knowledge Engineering 
Knowledge engineering activity has been largely focused on the epistemological and 
representational aspects of knowledge. A different, yet complementary view would identify 
knowledge engineering as the interaction of cogniting agents with the emphasis on the 
cognitive processes used in knowledge engineering. The knowledge-intensive approach in 
current knowledge engineering forces us to view the outcome or the product of our efforts 
as most important. The 'knowledge* is 'acquired', 'transferred', 'captured' and 
'modelled'. This emphasis directs our attention away from the processes that 'produce', 
'organize', and 'represent' the knowledge: away from notions of learning, comprehending, 
and communicating. Perhaps these processes are taken for granted; perhaps they are 
considered invariable. This emphasis is clearly displayed in our tools, techniques and 
procedures in that they focus on structures of knowledge and the processes used to 
manipulate them. Few of our tools address the complexity of the cognitive processes used 
to develop the knowledge structures although many of our knowledge acquisition tools 
support them. 
Perhaps another reason for our present emphasis is the inconclusive nature of cognitive 
processes. Perhaps we understand these processes far too little or view them as problems 
and sources of noise to use them effectively in our work in knowledge engineering. We try 
to avoid the biasing nature of the knowledge engineer's presence in the development of 
knowledge-based systems. Our approach is to develop methods to 'capture' the expert's 
knowledge automatically and directly, thus eliminating the influence of the knowledge 
engineer. This view might alter if cognitive processes were viewed as a necessary 
component to knowledge engineering in that we cannot fully understand knowledge and its 
re-creation if we fail to understand those cognitive processes involved. 
Wetter and Woodward (1990) have identified the importance of incorporating an 
understanding of cognitive processes in the development of a theory of knowledge 
acquisition. In order to develop a method of knowledge acquisition in a principled manner, 
the epistemic concepts must be clearly defined, the epistemic concepts must have a basis in 
psychological occurrence, and the representational formalism must reflect the intended 
epistemic and psychological meaning. 
Rather than viewing the domain of knowledge acquisition from the epistemological or 
representation perspective, this paper addresses the domain from the cognitive perspective. 
This perspective is viewed as complementary to the epistemological and representational 
ones in that it helps to complete the picture. The cognitive perspective is characterized by an 
emphasis on cognitive processes, their identification and support, rather than on the 
identification and production of knowledge structures. The question here is not 'Where's 
the knowledge ?' but 'What cognitive processes produce the knowledge?' 
2.3 Knowledge Engineering as a Set of Cognitive Processes 
This section sets out the premises for viewing knowledge engineering as a set of cognitive 
processes rather than as a set of activities which acquires and models knowledge in one 
form or another. 
First, knowledge engineering represents an ordered collection of cognitive activities. From 
an information processing point of view, cognitive activities are explained as processes of a 
"physical symbol system (eg. human brain) consisting of a representation system and the processes to manipulate it" (Aitkenhead and Slack,1985). This is a broad definition which 
includes human cognition as well as physical symbol systems which display cognition-like 
processes. Cognitive activities may be ordered and contained within one physical symbol 
system or shared between interacting physical symbol systems. 
Second, the purpose of knowledge engineering is to use cognitive processes to produce a 
model which demonstrates cognitive properties. This premise suggests a comparison point 
between the goals of cognitive psychology and those of artificial intelligence in that the 
outcomes of each arc considered models. Aitkenhead and Slack (1985) point out that in 
those disciplines concerned with cognitive processing and modelling the formulation of 
information processing models are evaluated against a body of experimental data. In 
artificial intelligence, the goal is to build computer-based models of performance which are 
evaluated against criteria such as computational efficiency and logical coherence. The focus 
of the former is on the final model and its ability to account for evidence of cognitive 
structures and processes. The focus of the latter is on the computational attributes and 
usefulness of the final model. Viewing knowledge engineering as a set of cognitive 
processes means that we emphasize the information processing activities which lead to a 
final model rather than the characteristics and content of the final model. 
Third, knowledge engineering is done in a cognitive environment. The environment defines 
a set of situations in which information is generated and produced, provided, manipulated, 
organized, re-organized and re-represented. The environment also defines those processes 
and structures which are used to generate, manipulate, comprehend, and organize the 
information. These processes are concerned with meaning, not just information. In this 
sense, the environment we are defining here might be called a 'knowledge' environment 
but that would suggest we are interested more in the outcome of the meaning or in asking 
'what is the meaning' of a certain piece of information. However, we are more interested in 
asking 'how is the meaning' of a certain piece of information determined. Consequently, 
our interest is on the cognitive processes of developing meaning. In this environment, 
humans are involved and they supply many of he cognitive processes. 
Fourth, the cognitive environment is characterized by sites and levels of cognitive activity. 
The environmental structure determines the locations of activity and the types of cognitive 
activity which occur at those locations. Sites refer to those clearly distinguishable points of 
cognitive activity (eg. the knowledge engineer, expert). Level refers to various combination 
of sites which are in interaction. Interaction of cognitive processing sites entails another 
level of cognitive activity which is distinguishable from cognitive activity occurring at a 
single site. For example, single site activity (eg. the expert) may reflect processes leading to 
making a choice about a course of action. Interaction of sites requires the use of social 
cognition processes. Presently, our field is much more fascinated with the cognitive 
processes of the expert but less so with those of the knowledge engineer and the user. In 
fact, we try to eliminate the cognitive effect of the former and ignore that of the latter. Why 
do we wish to develop our understanding of cognitive processes at one significant site in 
knowledge engineering and not others ? The expert's cognitive processes as well as those 
of the knowledge engineer and the user are open for our observation and study because 
they a part of the knowledge engineering environment. Addressing cognitive processes 
separately and in interaction may bring us better understanding of the knowledge 
engineering process. 
Fifth, an understanding of the cognitive environment acts as a basis for developing tools, 
techniques, and procedures to support and/or replace these processes. With our emphasis 
on the cognitive aspects of knowledge engineering, the focus of potential support processes 
is based on our understanding and careful elucidation of those cognitive processes which 
are characteristic of sites and levels of cognitive activity. Rather than emphasize knowledge support we are suggesting a complementary view of cognition support as a basis for tool, 
technique, and procedure development For example, in addition to asking questions based 
on the domain content and problem-solving heuristics, it is useful to ask questions which 
are formed to reflect different cognitive processes (eg. LaFrance, 1986). 
3. Cognitive Characteristics of the Domain of Knowledge Engineering 
3.1 Understanding Complex Domains 
The role of the knowledge engineer is cognitively demanding in that the knowledge 
engineer is expected to enter into the epistemological structure of a domain far enough to 
build a model of that domain. Even if the domain is restricted in scope, the knowledge 
engineer must become familiar with the problem-solving structures in the domain and the 
underlying assumptions of the domain of interest. These demands are similar to expert 
development within that domain in that the knowledge engineer, as a novice, must quickly 
become familiar with the necessary epistemological concepts and problem-solving 
processes in order to adequately model the domain or part thereof. When a knowledge 
engineer enters a new, complex, epistemological domain, he/she uses a variety of cognitive 
processes to first understand and comprehend the basic structures and assumptions of the 
domain and second, to model this understanding. It is these cognitive processes which 
require identification and description so that tools, techniques, and procedures may be 
developed as aids and supports to the knowledge engineer alone or in interaction with the 
expert. 
The development of problem-solving skill in medicine acts as a case in point. Research on 
how medical students and interns come to develop expertise in a complex epistemological 
domain parallels the cognitive demands placed on the knowledge engineer when he/she 
enters a new domain. Evans (1989) stresses that addressing the complex domain of medical 
problem-solving reflects four critical issues: 
"l)the relation between domain knowledge and problem-solving behaviour, 2) the 
sources of bias and misconception in problem representation and decision-making; 
3) the role of discourse in structuring the acquisition of data..., and 4) the 
identification of formal methods for the evaluation of performance" (p. 2). 
These same issues are of importance to the knowledge engineer and to knowledge 
engineering activity. The point here is that the knowledge engineer is required to learn 
something about the domain in order to make knowledge engineering decisions. The 
cognitive demands placed on a knowledge engineer when he/she enters a new domain are 
similar to novices in that domain who are in the process of developing a greater 
understanding of the domain. The rest of this section presents research support for a 
number of cognitive processes involved when an individual is developing understanding in 
a complex domain. 
3.2 Cognitive Processes in Understanding 
The biomedical domain is characterized as epistemologically rich. Many of the concepts and 
families of concepts are highly interdependent, forming extremely complex epistemological 
networks. Meaning and understanding of the concepts alone as well as the interactions of 
these concepts are necessary for understanding the domain. Understanding is often 
hampered not only by the complexity but by the ill-structured nature of the domain. A 
number of researchers have reported the difficulties of understanding complex systems (Flood, 1987) as well as the requirements and methods for overcoming these difficulties 
(Cairns and Woodward, 1988). The novice and the knowledge engineer are similar when 
they embark on conceptual analyses of clusters of complex concepts. 
Patel et al (1989) identified a number of cognitive processes used in developing a functional 
relationship between biomedical knowledge and clinical reasoning. The following cognitive 
processes were viewed as necessary for developing an understanding of how to incorporate 
models of biomedical knowledge in diagnosing clinical cases. These processes included 
comprehension of text-based propositions, construction of consistent and coherent 
explanations, and selection of relevant and irrelevant facts based on clear discrirrination 
principles. These findings are based on novice-expert interactions and they represent points 
of distinction between novice and experts in the domain. Knowledge engineers resemble 
novices (or even sub-novices) when they first enter a new domain. The cognitive processes 
used by novices, which distinguish them from experts, are identified as correlates of poor 
understanding, diagnostic performance, and explanatory performance. 
Another set of studies (Feltovich et al, 1989,1984; Spiro et al, 1988,1990) addresses the 
nature of conceptual understanding in biomedicine. More specifically, the studies address 
how deep models of complex ideas develop and how problems may arise with these 
models, during their development, which lead to misconceptions. The results have shown 
developmental patterns for a variety of biomedical misunderstandings. The results support 
three main reasons for misunderstandings: multiplicity, interdependency and 
oversimplification. The first refers to the fact that many influences contributed to the 
acquisition and maintenance of misconceptions such as learner style and ability, the 
methods of education, and practices of biomedical science research. The second 
characterizes misconceptions as reciprocating networks based on basic component ideas 
which are faulty. These networks tend to support each other or positively reinforce each 
other so that misconceptions are promulgated throughout the network. The final pattern 
suggests that due to human information processing capacity, the complexity of the concepts 
is reduced by selecting, collapsing or relating concepts in a way which changes the 
meaning. 
3.3 Cognitive Demands on the Novice 
Complex concepts make unusual cognitive demands on the novice. Feltovich et al (1989) 
describe four main demands on: 
1. working memory because of the large number of nested or looped steps or goals, 
or because of the large number of variables to be considered, processed, and/or 
reconciled; 
2. formal representation in the sense that the degree of abstraction necessary for 
understanding is often onerous (eg. representing the concept of rate or acceleration); 
3. intuition or prior knowledge in the sense that new concepts can conflict with 
prior meanings and/or relationships among concepts; and 
4. notions of regularity because many concepts can be ill-structured and highly 
variable in how they are used or they may be very strongly dependent on other 
concepts for their meaning. 
These same demands are placed on the knowledge engineer. These cognitive demands 
affect knowledge engineering activity by leading to misconceptions which may be 
perpetuated into the final system if not 'caught' by the expert (assuming there exist specific 
procedures for the expert to use to 'catch' the misconception). Also, when complex 
conceptual domains are addressed, complexity reduction occurs. Many researchers have 
studied the processes whereby humans use particular procedures to reduce the complexity of conceptual information. (Kahneman and Tversky, 1973; Kahneman et al, 1982; Coulson 
et al, 1986; Spiro, 1980). 
Feltovich et al (1989) distinguishes conceptual bias from those biases in judgment and 
decision-making (Kahneman et al, 1982) in that the family of 'reductive biases' are used 
solely to reduce complexity rather than reflect judgment processes. Six reductive biases 
were identified. 'Static' bias occurs when a dynamic system is viewed as a static model. 
'Step-wise' bias describes the situation where a continuous process is broken down into 
discrete, identifiable steps resulting in a loss of meaning. 'External agent' bias is the 
attribution of essential intrinsic characteristics of entities or processes to external influences. 
'Prior analogy' bias occurs when new concepts are given meaning based on already held 
concepts or simple models within the domain or from other domains. 'Common 
connotation' bias occurs when technical terms are reduced in meaning based on the day-to­
day use of the term or concept. Finally, 'restriction of scope' bias refers to the belief that 
general principles are taken to apply only under certain circumstances based on a repeated 
co-incident relationship. 
3.4 Methods of Information Presentation 
One important issue addressed in the research in the biomedical domain is the need to select 
the 'right' concepts for presentation (Feltovich et al, 1989). Due to the complex, ill-
structured nature of certain facts of a domain, the tendency is to present information initially 
in a simple overview format and then to incrementally increase the complexity so that 
models are built to a point of greater sophistication (Glaser, 1984). The reason to select the 
right concepts is due to a perceived failure of this incremental approach. These researchers 
found that students used the simplified models to 'filter-out' or arrange later material so that 
developing an understanding of the complexity did not occur. Also, they found that the 
initial simplification promoted further simplification of ideas. The tendency was to under-
dimensionalize. The recommendation is to select key concepts, to ensure that they are 
understood, and then to build around the key concepts. Criteria for selection of these key 
concepts are the perceived importance of the concept by the community, the degree of 
centrality of the concept in the literature and for clinical cases, a high degree of difficulty in 
understanding the concept, and the concept has cross-context application. 
A second issue emerging from the biomedical research suggests the use of abnormality 
models of information processing. This type of model is extremely useful in identifying, 
describing and correcting processes which result in some form of misunderstanding. These 
models frame a conceptual misunderstanding in terms of the processes used to develop it. 
These types of models appear to have a real application in knowledge engineering. Often 
the knowledge engineer needs to use the expert as the best guide to conceptual verification; 
however, only content is addressed. Procedural models of common processing biases or 
distortions (abnormalities) might be used by the knowledge engineer in verification of 
knowledge-based systems. 
3.5 Summary 
The biomedical research demonstrates the value of studying the cognitive processes of 
learning in the context in which they occur. In most cases the research was carried out in 
the classroom and teaching hospitals associated with medical schools. If we are to learn 
more about the cognitive processes involved in knowledge engineering, the actual context 
becomes the laboratory. 
Also, the research presented from the biomedical field provides an example of the 
importance of understanding the cognitive processes involved when an individual attempts to comprehend a complex epistemological domain. Knowledge engineers are constantly 
confronted with this task and the cognitive demands placed on them are onerous. This 
research has indicated that identifiable cognitive processes are involved in the 
understanding of complexity and has provided some direction for the development of 
cognition support tools, techniques and procedures for knowledge engineering. 
The following sections of the paper identify and describe other sets of cognitive processes 
apparent in knowledge engineering activity. 
4. Memory 
4.1 Types of 'Memory' 
Commonly agreed upon mechanisms of memory (Potter, 1990) support memory as a three 
phase phenomenon. Processes are postulated for information registration or encoding, for 
retrieval or remembering, and for forgetting. Temporal architectures have also been 
postulated (Crowder, 1982). Gorfein and Hoffman (1987), Klatzky (1980), and Crowder 
(1976) provide good evaluations of the memory research. For knowledge engineering, 
specific issues and questions arise from this research. 
Snodgrass (1989) asks 'How many memories?'. This question was originally asked by 
Tulving (1985) but Snodgrass developed a memory arrangement based on 3 types of 
memory tasks. The first is called 'episodic' memory which is essentially 'remembering 
that' something had occurred or that something was said. The memories reflect traces of 
autobiographical events, important to the individual and retained by the individual because 
of some relevance pointer. The second memory task is that of 'semantic' memory or 
'knowing that' something is true or false or something is known or believed to have 
occurred. This type of memory is more encyclopedic and is indexed by a number of 
conceptual pointers so information is retained because it points to some other piece of 
information. The third memory is that for 'knowing how'. Snodgrass describes this 
memory as a 'procedural' memory in that what is retained is a perceptual/motor sequence of 
actions which have been acquired and demonstrated by completing actions, by doing. 
The concept of multiple memories suggests that events, occurrences and information are, in 
some manner and form, 'retained' by the individual in different memory 'locations'. The 
processes of representing and retaining information in each of the three memories are 
considered different tasks, but Snodgrass admits to knowing little about the degree of 
independence or interdependence among the memory types. The concept of different 
memories also brings up other questions: under what circumstances are the different 
memory tasks activated (eg. recall, cued recall, recognition tasks), how is information in 
each memory represented, and what processes are used to access die different memories? 
Answers to these questions may lead directly to processes which may be supported in the 
knowledge engineering process. 
4.2 Environment X Memory Research 
Smith (1988) presented anecdotal evidence for episodic memories which were cued or 
triggered by environmental stimuli. Bjork and Richardson-Klavehn (1989) present research 
on the context-memory relationship drrecdy. They studied aspects of various contextual 
influences on how information is initially retained and how information is available, based 
on contextual retrieval cues. They contend that generating ideas or becoming aware of new 
information is done within a defined, current, episodic context Features of the context are incorporated with the new information in memory. Recall probabilities are seen as a 
function of the strength of association to the current context. Matching episodic contexts 
ensures better recall of information. 
It is necessary also to distinguish between aspects of the episodic context that are 
meaningfully related to the information retained and those aspects of the context which are 
independent and incidental to that information. Baddeley, (1982) distinguished 'interactive' 
and 'independent' forms of contextual information or patterns. 'Integrated' and 'isolated' 
distinctions were made by Eich (1985) and Hewitt (1980) categorized contextual patterns as 
'intrinsic' or 'extrinsic' to the meaning of the retained information. Bjork and Bjork (1988) 
proposed three aspects of context. That information which was clearly and strongly 
associated with the target information was labelled 'integrated' information. 'Influential' 
information was that which had strong associations to the target information in the sense 
that it influenced the degree of meaningfulness of the target information. "Incidental' 
information was not only independent or isolated from the target information but did not 
influence the interpretation of that information. These three distinctions are used to 
describe context-target relationships. Finally, these researchers postulate two types of 
cognitive processing during recall of information. The data driven process (also see 
Jacoby, 1983) posits a type of task which forces the individual to respond to perceptual 
information only (eg. identify words or word fragments or supply missing structures). 
This is usually considered an indirect form of memory. Conceptually driven tasks (see 
Roediger and Blaxton, 1987) demand constructive, semantically-based processes as a basis 
for direct recall of information. These three main dimensions (concept-target relationships, 
type of context, and type of recall task) form a taxonomy of recall which suggests 12 
different methods of structuring a recall situation to increase the effectiveness and efficiency 
of recall activity. 
Memory recall situations are common ones in knowledge engineering activity in the 
knowledge acquisition phase. The concept of multiple memories has application (eg. La 
France, 1986) in that structured recall situations established by the knowledge engineer 
affects the retrieval processes of the expert. Also, structures such as the Bjork and 
Richardson- Klavehn's taxonomy and can act as a basis for developing support tools and 
procedures to aid in recall for the expert, the knowledge engineer and even for the user. 
4.3 Retrieval and Organizational Strategies 
Kolodner (1984) outlines a number of principles of memory retrieval and storage. Those of 
particular interest to knowledge engineers are presented and discussed. The first principle 
states that human remembering is often a process of re-construction. An individual must re­
assemble or recreate the necessary structures and events which 'must have happened' rather 
than directly retrieving what actually did happen. The 'actual' event, with all its salient 
features, is not 'stored' in memory. When asked to recall a situation, an individual will 
produce a cogent assembly of plausible items. A second principle suggests that the process 
of remembering mirrors a progressive narrowing or focusing-in on a description of the 
recalled event. In other words, an individual begins with salient features (as perceived in 
the recall context) and begins a type of choice or selection task in order to eventually select 
the full event (for reconstruction) to meet the current recall demands (eg. E-MOPS are 
considered by Kolodner to be the basic structures which organize events in memory). 
The issue of contextual information is raised in a third principle. In order to retrieve the 
appropriate information from memory, an individual first requires some information or 
knowledge about the contexts associated with the target items. The original context is seen 
as having a powerful link with other salient features of information (previously discussed). 
A fourth principle suggests that retrieval often requires a search for something other than what was requested. In order to successfully retrieve and re-construct an event or episode, 
it is often necessary to locate intermediate or ancillary information which may act as indices 
for the required information. Finally, a fifth principle suggests that conceptual categories 
contain or hold generalized information only and that details may be generated at the time of 
recall. 
Each of these principles raise issues of interest for the knowledge engineering environment. 
Naive assumptions about what an expert offers in response to questions lull the knowledge 
engineer into believing that he/she has 'captured' the necessary knowledge from the expert. 
Tools, techniques and procedures based on these principles may aid the knowledge 
engineer to clearly identify what knowledge has been gained, what further knowledge is 
necessary and what knowledge is suspect The effects of the knowledge engineer's 
questions are not studied yet this process is the main method for gathering expert 
information. 
A more detailed analysis of retrieval processes is necessary to identify supportable 
processes. Kolodner (1984) suggests two main types of retrieval activity. Processes for 
constructing and further specifying the context for search represent the first type. She calls 
these processes 'executive' strategies. 'Instantiation' strategies represent the second type of 
process. These processes direct search and the application of constructive strategies. 
Elaboration, transformation, and relation have been identified. One event may enable 
another, act as a precondition for another, may result from another, act as a reason for 
another. An event can be linked to another event in a larger episode of events, related in a 
sequence of events, act as a preceding or following event or act as a standard event. Similar 
relationships have been distinguished by Schank (1975) and by Graesser and Clark (1985). 
Kolodner (1984) also discusses the important aspect of event 'features'. These features are 
aspects of events which are associated with the target information and unique features 
usually make the best indices for memory retrieval strategies. Also, good features are able 
to relate events and provide context relationships. The indices are then used for searches. 
Access to information retained by an individual can be more efficient through the use of key 
features. One method of retrieval is through responses to questions (eg. Lehnert, 1978) 
which are based on key features and which emphasize the appropriate format or form of the 
answer as opposed to the content of the answer. Good answers are seen as responding to 
the intent and content of the question. Using the model of instantiations (Kolodner, 1984), 
experts may be given tasks where they are required to identify a variety of relationships 
between events as well as the features of events. 
Transformational processes Kosslyn (1980) themselves deserve a closer look for their 
particular relevance to knowledge engineering. In contrast to comparison processes, which 
juxtapose memory structures and return a match/mismatch or measure of similarity, 
transformational processes change the contents of a structure. 'Alterations' are one form of 
transformation which add or delete structures or parts thereof or they act to reorganize the 
structure. 'Productions* use old data structures to generate new structures or replace old 
ones. 'Derivations' are new structures which have been generated by inferential processes. 
The preceding discussions of retrieval and organizational processes have reflected a set of 
'content-free' processes. Retrieval processes based on semantic meaning also provide a 
basis for understanding how some structures are implicitly activated. The basic assumption 
(Nelson, 1979) is that words are tokens which are connected to a larger number of related 
concepts but that these concepts can be connected to a larger number of related concepts. 
Hitting the trigger activates a network of concepts. The questions arise, however, as to 
what information is actually stored and can be activated quickly (implicitly) and what must 
be re-generated. Graesser and Clark (1985) have reported that some semantic forms of knowledge are not likely to be stored (see section on text comprehension). For implicitly 
activated concepts, the cue 'set' is of great importance. The larger, more diverse, the set of 
cues, the greater the network of related entities and the greater the time and confusion in re­
generating the unstored information. A complex stimulus triggering multiple 
representations in memory can lead to a wealth of information but the representations 
themselves may be of a different form and structure (Snodgrass, 1989; Collins and Loftus, 
1975). 
Do our knowledge engineering methods attempt to access different memory structures and 
do they support an understanding of how these structures may change during the 
knowledge engineering process ? Our borrowed tools, techniques and procedures were not 
generally developed with an understanding of memory structures and processes in mind. 
Also, the issue of context-memory relationships has been neglected even though its effect is 
of great importance to knowledge engineering. An understanding of how new structures 
are developed and incorporated into new episodic experiences or how related concepts 
become incorporated into representations is not supported in knowledge engineering tools. 
Determining which concepts are related directly and which indirectly to decision making are 
of concern to the knowledge engineer but the tools used are rudimentary and are focused on 
the knowledge, not the processes. 
5. Text Comprehension 
5.1 Cognitive Processes in Text Comprehension 
The reader appears an active participant in text comprehension. Perceptual processes 
(processes which react to visual stimuli and which reflect modular stimulus processing) 
occur during comprehension and are considered mandatory, automatic and immediate 
(Swinney and Osterhout, 1990). Cognitive processes, on the other hand, reflect inference 
processes based on prior world knowledge, plausibility, and pragmatics. The cognitive 
activity drives analysis and acts as the basis for viewing readers as constructive learners 
(Wittrock, 1979) or 'active agents' (Anderson, 1970). These cognitive processes and their 
influences are of interest to better understand the cognitive demands placed on the 
knowledge engineer. 
Inferential processes constitute a class of cognitive processes used in text comprehension to 
understand concepts, to differentiate between concepts, and to make connections between 
and among concepts. As someone reads, he/she attempts to make the text meaningful, 
usually by making predictions (Smith, 1982) and then by determining if the predictions 
hold true. The elimination of possible meanings until the correct one is found is a process 
of inferencing based on prior knowledge and on the goal of the reader (van Dijk and 
Kintsch, 1983; Lesgold and Perfetti, 1978). These types of processes are of interest to us 
because to make meaningful the information contained in domain textual material, onerous 
inferential demands are placed on the knowledge engineer. 
It is very common for readers to fail in making all the necessary inferences during text 
comprehension (Britton et al, 1990). Comprehension failure increases as the text becomes 
less of a narrative, with common inference patterns, and more expository. It is common for 
a knowledge engineer to read expository and or instructional texts. Making the appropriate 
inferences depends in large part on prior knowledge of the domain, so considerable 
inferential demands are placed on the knowledge engineer. The more inferences required of 
the reader by the text, the less likely the inferences will be made and the less likely the 
meaning captured. Making the necessary distinctions of concepts and making the necessary 
inferences of these concepts requires familiarity with the domain: something which the knowledge engineer does not initially have and may never develop sufficiently during the 
knowledge engineering activity of the project. 
5.2 Inference Making During Text Comprehension 
Content-based inferences are those which can be made on the basis of perceived word 
meaning (semantics), as the word is used in the sentence (syntax), or in the context of 
understanding (pragmatics). Seifert (1990) distinguishes three types of inferences: 
anaphoric inferences or inferences based on pronouns (see Clark and Haviland, 1977), 
instrumental inferences or inferences about action (see Singer, 1980), and pragmatic 
inferences or contextually driven connections (see Graesser, 1981). These three types of 
content inference are generated on the bias of the kind of world knowledge (and specific 
domain knowledge) needed to understand a certain kind of text. 
Thematic inferences (Seifert et al, 1986) and schematic structures (Schank, 1982; Schank 
and Abelson, 1977; and Rumelhart and Ortney, 1977) are those inferences generated less 
on the basis of content and more on the basis of abstract patterns of goals and plans 
suggested by the text (Seifert, 1990). The relations between concepts are implied more by 
an abstract conceptualization or structure than by the semantic links in the text. In many 
cases, these themes are determined by the processing goals (Holyoak and Novik, 1988) of 
the comprehender. Skimming a text vs searching a text for a specific point may lead to 
different inferencing paths (Seifert, 1990). The processing goals of the knowledge engineer 
may vary at different points in the knowledge engineering process (i.e. during each pass 
through the text). Misconstruing the meaning, generating incomplete or erroneous inference 
paths, and selecting or constructing partial or incomplete thematic and schematic structures 
hampers the knowledge engineering process. 
The difficulties of the knowledge engineer are further defined by Garner (1985) who notes 
that the comprehension of technical, expository prose is "little assisted by content-
schematic knowledge, in that the very novelty of the material ensures that the readers will 
be unable to fit much of the new information into their old in-head information"(p.l0). Van 
Dijk and Kintsch (1983) suggest a series of comprehension stages for technical prose. The 
first step involves a focus on specific content (based on single, individual propositions) to 
build a 'microstructure. Then, the task is to produce or derive a 'macrostructure' of 
important content through the application of three types of inferencing rules - deletion, 
construction and generalization. Procedures for supporting this set of inferencing activity 
may help overcome the need for domain knowledge in initial comprehension activity. 
Types of content-based inferences are more dependent on stored knowledge than on 
generated or infered knowledge. When reading technical prose, comprehension is strongly 
based the capacity of the reader to make the correct content-based inferences. Knowing 
what types of structures are usually stored versus those that are usually generated from 
stored information helps in determining what processes may be constructed to aid the 
knowledge engineer. Graesser and Clark (1985) have reported a systematic study of those 
structures which appear to be stored and those which are not. Those concepts that are 
known to be directly stored may be elicited. Those concepts can then act as anchors to 
derive those concepts which have not been directly stored. Knowing which concepts are 
likely derived gives the knowledge engineer the opportunity to challenge, test or validate 
them in another manner. Different processes are also necessary for eliciting different types 
of concepts (eg. causal, goal). 
Causal inferences are also of interest Myers and Duffy (1990) describe a popular 
mechanism for the development or generation of causal inferences during text 
comprehension. First, a network representation is constructed. This network results from those initially identified concepts in the text Tentative relationships are constructed on the 
basis of the surface meaning of the text. Sets of inferences are then generated from these 
antecedents and they become part of the network. More inferences are drawn as more 
related information becomes available through more text. In particular, causal antecedents 
are believed held in short term memory awaiting a link or connection to another concepts 
which satisfies causal chain properties (Fletcher and Bloom 1988; Trabasso and Sperry, 
1985). These causal inferences display four particular properties (Trabasso et al, 1989). 
The first property is 'temporal priority, in that a cause never happens after the effect. The 
property of 'operativity' states that the cause must be active at the time when the 
consequence occurs. The third property suggests that a cause must be seen as necessary for 
the event to occur. Finally, given a set of antecedent conditions, a cause must be 
'sufficient' for the consequent to occur. 
For text comprehension in knowledge engineering, it is important for the knowledge 
engineer to generate the appropriate causal inferences while reading the text. However, 
given the complexity and newness of the content in the domain, it is entirely likely that the 
knowledge engineer will fail to make the necessary causal inferences. Some research 
findings (Graesser and Clark, 1985; Liu, 1989; Trabasso et al, 1988) suggest that it is 
possible to improve text comprehension by inviting causal inferences by such procedures 
as systematically posing questions to the reader. Procedures which flag the need for a 
causal inference or to identify when one is missed would aid the knowledge engineer 
during comprehension. 
5.3 Role of Text Structures in Comprehension 
Grammatic and semantic structures in the text have been shown to influence text 
comprehension. Rumelhart (1980) suggests that when text comprehension is appropriately 
completed, it is done so because structures in the text have helped to organize knowledge 
into units which activate the required cognitive processes so that new information is easily 
processed. When comprehension fails, it is partially due to the reader not having the 
appropriate schemata for the text content; the reader having the appropriate schemata but no 
textual cues to trigger its generation or retrieval; or, the reader finds or constructs a 
consistent interpretation of the text but not the one which was intended (see also Spilich et 
al, 1979). 
Haberland and Graesser (1990) have found that when goal hierarchies were easily 
portrayed in the text passages subsequent statements were easier to comprehend. When 
readers are forced to assume or identify goal hierarchies on their own, there tends to be a 
greater failure rate in making the appropriate inferences. Poor text construction leads to 
poor text comprehension in a number of other ways as well (Garner, 1988): 
Expert generated text comes in many forms. Highly structured text often reflect 
assumptions that the reader has the necessary background knowledge for sufficient 
comprehension. In this type of text, goal hierarchies and cause-effect indicators may not be 
as prevalent as they may be in more introductory material. Procedures which flag these 
types of indicators in richly technical material (Woodward, 1990) support the knowledge 
engineer's inferencing processes. 
5.4 Meta-Cognition and Comprehension Failure 
Comprehension of technical, expository text can also be affected by factors or cues external 
to the text (Rothopf, 1982). Metacognition, or stable knowledge the reader has about 
him/herself, about the comprehension task, and about the strategies employed affect the 
success of comprehension. Flavell (1981) identified four metacognitive components and their relationships to success in text comprehension. Meta-cognitive experiences of the 
reader refer to those thoughts of how the reader is approaching the goal of reading, 
questions about how the comprehension is progressing, or the intrusion of other thoughts 
into the comprehension process. Cognition 'goals' refer to the variety of reasons for 
reading (eg. skimming, identifying assumptions). Finally, the cognitive strategies of the 
reader refer to the types of processes the reader uses while reading the text (eg. highlighting 
passages or writing down key words). Steinberg (1984) offers a compendium of cognitive 
processes which may be used during text comprehension. 
Another critical meta-cognitive process for a reader is that of detecting comprehension 
failures (see Baker and Brown, 1984a, 1984b; and Brown et al, 1986 for reviews). Often 
readers do not pay attention to all the words of a sentence or passage. Also, sentences are 
often mis-parsed due to a partial match between text-based terms and the relevant 
knowledge structures (Reder and Cleermans, 1990; and see also a summary of error 
detection research, Garner, 1988). Finally, the necessary inferences may not be made by 
the reader. Under these circumstances, comprehension will be incomplete and the reader 
may fail to note the discrepencies. Comprehension failure can only be rectified or corrected 
if the reader becomes aware of the errors or aware of the partial nature of the inferences. If 
detection fails as well,then few connections or inferences are likely to be made. Markman 
(1977) has called this state of affairs the 'delusion of comprehension'. 
5.5 Summary 
Most of the research in natural language processing in the field of knowledge engineering 
addresses the understanding and use of natural language as a basis for knowledge 
acquisition for knowledge-based systems. Wetter and Nuse (1992) have identified many 
conceptual and practical difficulties with this approach. Essentially, text-based information 
used for knowledge engineering purposes is removed from its pragmatic context. 
Knowledge structures present in the pragmatic context for writing the text may or may not 
relate directly with the knowledge structures in the domain or with the pragmatic context in 
the knowledge engineering domain. The resulting formal knowledge has experienced two 
critical transformations: the domain knowledge into natural language and the natural 
language into a formal language. This approach to natural language focuses on generated 
knowledge structures and their relationship to domain knowledge. 
This paper suggests an alternate, but complementary, approach to the used of text-based 
natural language by suggesting the development of tools, techniques and procedures which 
aid and/or identify the cognitive inferencing processes during text comprehension. Rather 
than focusing on 'extracting' the knowledge from natural language, the focus is on 
identifying and supporting the cognitive processes involved in knowledge engineering 
activity. There appear to be well defined and substantiated processes which can focus tool, 
technique and procedure development. 
6. Judgment and Choice 
6.1 Judgment and Choice in Knowledge Engineering 
Making judgments and choosing alternatives are complicated and complex processes. In 
many cases, humans complete these complex processes quickly, with less than complete 
information, and with little consideration of their own thought processes. Limited human 
information processing capacity requires that we reduce the complexity of the information 
environment by a number of methods, which consequently result in loss of information and 
meaning. The expert's domain displays its own inherent patterns but the knowledge engineer and the expert (and the user) structure their own patterns for the domain to make 
evaluative and predictive judgments and to deal with their own uncertainty about the 
patterns displayed by the domain. 
A common type of judgment made in knowledge engineering is that of 'probable cause'. 
Einhorn and Hogarth (1981) have identified a number of factors which affect this type of 
judgment Aspects of the context in which the judgment is made is considered a major 
factor. The causal context sets the stage for how various causal candidates emerge as 
distinct from the entire causal context Causal clues such as temporal order, co-variation, 
contiguity in time and space, and the similarity of cause and effect reflect a number of 
imperfect indicators of causal relations which influence judgment A third major factor is 
that of the judgmental strategy used to combine the information from the causal context 
with the causal clues. A final factor is the role of alternative explanations which may act to 
discount or reduce the strength of particular causal patterns. 
Probability theory has been used as a contrast point to human causal reasoning processes. 
Tversky and Kahneman (1980) have shown that 'base rate' information is often ignored 
when making predictive judgments unless it is seen as relevant in the causal context 
Judgment errors such as reversing the probabilities of two related events (Eddy, 1982) 
have also been identified. Issues of plausibility of combined, predicted events present the 
effects of taking multiple connected predictions into account (Kahneman and Tversky, 
1973; Slovic et al, 1976). Accurate judgments become scarce as multiple predictions are 
requested. Finally, research has shown how people update judgments upon receiving new 
information (Edwards, 1968). When new information forces a review of a prior judgment, 
it has proven difficult to evaluate the new evidence against two alternative hypotheses, 
especially when one has not been well specified. When updating beliefs (Fischhoff and 
Beyth-Marom, 1983), it is not common to make simultaneous evaluations of that evidence 
for all alternative hypotheses. 
The interaction effects between task demands and human judgment processes helps explain 
some of the difficulties individuals have with making judgments under certain 
circumstances. Cue availability constitutes one contextual variable reflecting aspects of task 
demand (Hogarth, 1987) which strongly influence predictive and evaluative judgments. In 
the knowledge engineering domain, judgments are made by the expert, knowledge engineer 
and the user. Availability of appropriate cues (Tversky and Kahneman, 1973) can affect 
judgments of relative frequency and information presentation patterns can affect the salience 
of context and cues. Often the degree of variability of a variable is strongly distorted by 
contextual effects, thus affecting resulting judgments. For example, people tend to base 
their estimates of frequencies and probabilities on absolute vs relative frequencies (Estes, 
1976) 
Within the knowledge engineering domain it is possible to identify and outline those 
judgment processes which are not normally completed effectively by the expert, knowledge 
engineer and the user. Statistical models have been developed or adapted (Meyer and 
Booker, 1991) but other types are possible. Hogarth (1987) has described different 
methods for combining information to stabilize judgment processes. Processes of 
judgment and choice are viewed as conflict resolution models in that the selection of 
competing alternatives requires a resolution of conflicting information. 'Compensatory' 
models directly address the conflict created by the differences by allowing trade-offs on the 
choice dimensions. Examples are the linear model, additive difference model, and the ideal 
point model. Each model sets a tactical approach to combining information. 'Non­
compensatory' approaches are those which do not allow trade-offs on choice dimensions. 
Examples of these types of models are the conjunctive, disjunctive, lexicographic, and 
ehmination-by-aspects models. The field of decision analysis focuses on identifying the intellectual tasks required for 
making decisions and the field has developed a variety of mathematically and logically 
based theories, procedures and techniques. This approach stresses the organization of 
information in a manner which is more conducive to making correct judgments and 
decisions. A main problem is figuring out what the information means and what relevance 
it has to the decisions under investigation. The intellectual tasks are identified as systems 
analysis or decision analysis. The main steps are to identify the problem (its overall analytic 
structure) and to formalize parts of it Secondly, the analyst picks an appropriate subset of 
analytic tools and structures. Finally, the elements and relations identified in the first step 
are refined (von Winterfeld and Edwards, 1986). 
Based on this approach, a number of techniques and procedures have been developed to aid 
decision making. Decision trees identify steps, sequences of judgment points, and the 
possible paths (see Holloway, 1979; Behn and Vaupel, 1982). Event and fault trees are 
similar techniques. Influence diagrams (Howard and Matheson, 1980) present a graphic 
picture of interactions of variables in a model without imposing a tree structure. The study 
of uncertainty measurement has also produced a number of models of how to model and 
represent uncertain information (eg. value and utility measurement, group probability 
assessments). Finally, this approach has spawned such techniques as multiattribute utility 
theory (Edwards and Newman, 1982) and sensitivity analysis (von Winterfeld and 
Edwards, 1986). Techniques in knowledge acquisition based on a decision analysis 
approach have been developed (Bradshaw and Boose, 1989a; Bradshaw et al, (1989b). 
6.2 Judgmental Bias and Decision Aids 
Hogarth and Makridakis (1981) have listed a number of sources of bias and have identified 
their effects on judgment and decision making processes. In the information acquisition 
stage of decision making, cue clarity and availability, selective perception, cue frequencies, 
concrete information domination, illusory correlations, data presentation form (primacy, 
recency, mode, mixture, display and context), and framing are listed as sources of bias. In 
the processing stage of decision making, inconsistency (of strategy application), 
conservatism (low revision), non-linear extrapolation, heuristics, anchoring and 
adjustment, representativeness, law of small numbers, justifiability, regression bias, 'best-
guess' strategy, complexity of relationships, emotional stress, social pressures, and 
inconsistent information sources are identified as sources of bias. In the output stage of 
decision making, question format, scale effects, wishful thinking and illusion of control are 
seen as biasers. Finally, in the feedback stage, incomplete outcomes, misperception of 
chance fluctuations, success/failure attributions, logical fallacies in recall and hindsight bias 
contribute to faulty judgments. 
Specifically in the knowledge engineering environment, Meyer and Booker (1991) and 
Cleaves (1986) have identified expert bias under two categories: motivational and 
conceptual. Cleaves developed a number of monitoring and corrective procedures for use 
with experts to reduce bias. 'Mechanical' procedures manipulate the task or adjust the 
judgments after knowledge elicitation. 'Behavioural' procedures use interviewing and 
group interaction techniques to encourage full understanding, identification and control of 
biasing processes. Visual props to emphasize visual patterns rather than verbal 
expressions, varying the format of the requested judgment, using differences between 
actual and essential values, and combining judgments from several individuals constituted 
mechanical means of bias reduction. Behavioural means included focusing on specific 
biases in interviews, breaking down complex relationships, training experts in die types of 
biases, exhorting experts to give reasons for judgments and using group settings for 
developing consensus. Meyer and Booker (1991) developed similar methods to reduce elicitation biases and they have developed a variety of statistical methods for combining 
judgments and for reducing biasing effects with groups of experts. 
7. Social Cognition and Communication 
7.1 Effects of Domain Content on Cognitive Processes 
In scientific and technical domains judgments are based on ideas, concepts and/or concrete 
objects. Using concepts and ideas about concrete entities or well established principles and 
laws requires a set of cognitive processes or inference processes as a basis of making 
decisions about something external to the decision maker him/herself. In social domains 
where decision making needs to include judgments about people a different set, or perhaps 
an additional set, of cognitive processes come into play. In domains such as law or 
banking, judgments are made by people about other people. In addition to inferences about 
ideas, concepts, concrete objects, inferences are made about intentions, characteristics, and 
motivations of people. This section presents a sampling of research from the areas of social 
cognition and communication and addresses its application to knowledge engineering. 
Knowledge engineers may make social decisions about the expert and/or the user. The 
expert makes social judgments about the knowledge engineer's abilities and extent of 
domain knowledge. If the expert's domain involves making judgments about people then 
social cognition processes also come into play in the expertise. In this sense, then, the 
social characteristics of the domain affect the types of cognitive processes involved in 
decision making. 
7.2 Causal and Dispositional Attributions 
This area accounts for a full 11% of all social psychological research (Kelley and Michela, 
1980), consequently, a wealth of models and a strong theoretical foundation exists for this 
area of social cognition. With respect to the knowledge engineering domain, this field of 
research addresses the question of how the expert, knowledge engineer and user infer a 
correspondence between observed behaviour and the intentions that produced it. 
Correspondent inference theory (Jones, 1985) provides a basis for understanding the 
cognitive processes involved in attributing intentions and dispositions. Attribution theory 
(Kelley, 1983) discusses those cognitive structures and processes involved in attributing 
cause. 
Kelley (1983) offers two sets of inference processes and structures which account for how 
people arrive at causal attributions. The first assumes there is information from multiple 
sources and that covariation of an observed effect and possible causes can be perceived. 
The second assumes that the perceiver is faced with a single observation and must then use 
the configuration of factors that are plausible causes of the observed effect This first set of 
processes is named the 'covariation' principle. Kelley likened this set of cognitive 
processes to an ANOVA analysis. 'Consensus' information is required in that the number 
of people who make similar decisions is taken into account in the decision making 
processes. 'Consistency' information about how often the single decision maker in 
question made similar judgments under similar conditions is used as well. Finally, 
information concerning the 'distinctiveness' of the choice which provides a sense of 
whether or not a different choice is made due to small changes in one factor. If each of 
these three factors is considered as having two values (high and low), then a 2 X 2 X 2 
ANOVA pattern helps explain this principle. Shaver (1981) points out however, that this 
approach does not help in distinguishing the truly causal from covarying but non-causal 
relationships and Langer (1978) has suggested that causal attributions are elicited by types 
of questions and context demands and are not usually emitted. The second set of processes describes the ' configuration' principle. This principle bases its 
strength on proposed underlying structures or causal schemata to help explain casual 
attributions. Fiedler (1982) points out that a causal schemata does not really exist but it acts 
as a useful psychological construct for explanation and prediction. One schemata is that of 
Multiple Sufficient Cause which is evoked when the decision maker can predict effects 
from the presence or absence of causal contenders. Depending on the presence or absence 
of multiple contenders, this schemata helps to sort out and order the various combinations 
of causes. Two important principles are connected with this particular schemata. The 
'discounting' principle outlines when one cause or set of possible causes has sufficient 
strength to discount others. The 'augmentation' principle outlines under what conditions 
causal contenders support one another. 
Schemata of this type support decision making when information is incomplete, when 
causal 'shorthands' are required for complex situations, and when similar decisions are 
required across different content areas (Fiske and Taylor, 1984). Reeder and Brewer 
(1979) postulate 'implicationaT schemata to refer to the perceiver's prior conceptions about 
categories of behaviours. 'Partially restricted' schemata reflect extremes of behaviour. If a 
person's behaviour is seen as an extreme type of behaviour then it is difficult for the person 
to be seen acting in the opposite extreme. 'Hierarchically restricted' schemata assign a 
behaviour or individual to a category which keeps the individual from being seen as better 
or worse that than category. For example very skilled people experience a range of 
outcomes depending on motivation and task demands but low skilled people are never seen 
as having greater aptitude or skill. 'Fully restrictive' schemas are categorical attributions 
which identify stable levels of an attribute (eg. neatness). 
Other research has supported the free occurrence of causal attributions. This research 
applies to knowledge engineering activity. Lau and Russell (1980) devised a method for 
identifying attributional structures through content analysis of newspaper stories. Think 
aloud protocols were analyzed for causal attributions from the responses of parole decision 
makers while they were reviewing cases (Carroll and Weiner, 1982). During problem 
solving, when prior expectations were not confirmed, the causal attributions made by 
subjects were identified and analyzed (Pyszczynski and Greenberg, 1981; Hamilton, 
1988). 
7.3 Cognitive Components of Causal Attribution 
In the previous section the structures of attribution were presented. This section reports on 
the research which identifies the processes of attribution. The logic of attributions is 
addressed, the inference processes as well as the knowledge used when making causal and 
dispositional attributions is summarized. 
Hansen (1985) distinguishes attributional content from process. Concepts like covariation 
and configuration (causal schemata) are considered aspects of logic. How the actual 
attributions are made constitute the process (see also Newscombe and Rutter, 1982a, 
1982b). The knowledge structures used when making attributions is seen as the content 
(see also Galambos et al, 1986). Kelley's (1983) covariation model has been contrasted 
with Hewstone's and Jaspars (1987) logical model for identifying necessary and sufficient 
conditions for causal attribution. The logical model helps delineate all possible combination 
of causes for consideration. The 'abnormal conditions' model identifies 'counterfactual' 
criteria and 'contrastive' criteria to establish an opportunity to compare the normal to the 
abnormal. 
Cognitive processes for causal attribution are based largely on the research exemplified by 
Tversky and Kahneman (1980). Attributional heuristics for causal information processing such as representativeness, anchoring and adjustment, and availability have been identified 
and are discussed in more detail in another section of this paper. The concept of 
attributional salience is addressed by Taylor and Fiske (1978). This work identifies the 
process by which causal attributions refute judgments based only on immediately 
perceivable information. The mechanism suggests that this type of information can become 
overly represented in subsequent explanations. In other words, new information which is 
drawn direcdy from the immediate context may, under certain circumstance, become over 
represented in subsequent explanations and render a prior attribution null and void. 
Causal schemata are assumed to affect perception, memory and inference (Fiske and 
Taylor, 1984). They are viewed as working in a top-down manner to simplify and/or 
amplify information and they act to identify salient information for attributional processes. 
Knowledge-based causal attribution (Abelson and Lalljee, 1988; Leddo and Abelson, 
1986) focuses on the content of causal schemata. This approach tries to draw distinctions 
between the process of explaining events and the process of understanding them. It has 
been found that this process requires an understanding of how an individual forms an 
action plan, the goals of the plan sequence, how that particular plan could be seen to 
achieve the goal and those particular conditions which initiate the goal (Read, 1987). 
7.4 Meaning Interpretation in Communication 
Most of the work on meaning interpretation emphasizes the semantic aspect of language 
generation and comprehension. This approach is based heavily on the semantic qualities of 
memory (Kolodner, 1984). Semantically-based categories hold conceptual items which are 
related through inferences that can be drawn about the categories and concepts. Semantic-
based meaning is coupled with words and phrases. In any context, meaning is generated 
through the use of stored concepts in interaction with a variety of inference processes. 
Frames (Minsky, 1975), scripts (Schank and Abelson, 1977), schemas (Rumelhart, 1980), 
mental models (Johnson-Laird, 1980,1983), causal memory (Genter and Stevens, 1983), 
situation models (van Dijk and Kintsch, 1983), and E-MOPS (Kolodner, 1984) are 
examples of semantic memory structures and their accompanying inference processes. 
These semantic approaches have influenced knowledge engineering computational and 
representational structures. 
However, Grice (1975) places the emphasis not on word meaning but on implications of 
utterances. In studying communication, linguistic mechanisms have remained front stage in 
the form of much research in phonology, syntax and semantics. These three fields have 
produced a number of approaches which concentrate on the meaning and knowledge 
structures in linguistic forms. The assumptions of these approaches state that knowledge is 
encoded in the utterances or with the utterances. However, semantic representations of 
sentences cannot be regarded as corresponding very closely to to thoughts because 
sentences can be used convey a great number of different thoughts (Sperber and Wilson, 
1988). Semantics provide a basis for understanding some of the message but little of the 
meaning. Semantics do not cover the time and place of the utterance, the identity of the 
speaker or the speaker's intention. Semantics, as studied by the grammar, only help define 
the range of possibilities of interpretation. The pragmatics of communication, the 
interpretation of utterances, helps to choose among the possibilities. 
Sperber and Wilson (1988) put forward the mutual knowledge hypothesis. In order to 
reduce misunderstandings and to increase the recreation of meaning between two people, 
the context must be shared. Does the expert assume that the knowledge engineer knows X? 
Does the knowledge engineer know that the expert knows that he knows X? To generate 
the correct interpretation of an utterance, the one intended by the speaker (Grice, 1975), 
every item of contextual information used in the interpretation of the utterance must be known by both individuals and each must know that they know. The speaker and the 
listener must use specific strategies to interpret utterances so to identify the intent of the 
communication. Misunderstandings commonly occur not only because of semantic 
confusion but because of pragmatic confusion as well. It is critical for the knowledge 
engineer to establish an environment with the necessary contextual attributes to ensure that 
meaning is not impaired. 
Without capturing the intention, mistaken facts are no different from true facts. The goal of 
communication, especially in a domain like knowledge engineering which demands that 
knowledge is understood, is to ensure that the intentions of statements are clear. When the 
expert gives an explanation of an event, what are his/her intentions? It is to inform, to 
teach, to impart understanding, is it to fill-in the background? Knowledge engineers 
attribute intentions to the experts and users and the experts attribute intentions to the 
knowledge engineers. A knowledge engineer may review written protocols for 
communication intentions or he/she can ask the expert for his/her intentions. In a shared 
cognitive environment like knowledge engineering, all participants have the capacity for 
making the correct assumptions about the meaning of utterances but they do not always do 
so. Sperber and Wilson (1988) remind us that the current task determines what information 
is used for communication. When situational demands trigger cognitive processes, stored 
information will only be used if it is necessary, the default is to use information directly 
inferable from the immediate context 
8. Elaborative Paths 
The purpose of this paper was to develop a view of knowledge engineering as a set of 
cognitive processes to complement the predominant 'knowledge' perspective. Knowledge 
engineering was characterized as a set of cognitive activities used to understand complex, 
epistemologically rich domains. Selected research in the fields of text comprehension, 
memory, judgment, and social cognition provided examples of a variety of cognitive 
processes which have a high probability of occurrence in knowledge engineering activity. 
The question now is how can this perspective be developed and tested? What are the 
promising paths? 
One of the basic premises outlined in section 2 of the paper described the concept of a 
cognitive environment Viewing knowledge engineering as occuring in such an 
environment requires much more elaboration as to what constitutes such an environment. 
Identifying and defining the various sites of cognitive activity and the manner in which 
these sites interact appears to be an essential component of the cognitive basis of 
knowledge engineering. One of the main issues reflects the need to develop a common 
understanding of how meaning is produced within, among, or between interacting sites. 
Also, within this framework, the user of the final system becomes a much more important 
factor. A more elaborate and systematic description of the cognitive network of knowledge 
engineering is necessary. 
A second path is suggested by the need to empirically identify the critical cognitive 
processes involved in knowledge engineering. Four likely candidates of cognitive research 
were presented in this paper but little evidence is available to substantiate the use of many 
of the cognitive processes suggested in the presented research. Also, this paper addressed 
only those processes attributed to the knowledge engineer, not the other cogniting agents 
involved in knowledge engineering. A much more systematic endeavour is required which 
will result in a system of cognitive processes for knowledge engineering. The various 
models of cognitive processes from the cognitive science research domain need to be applied in the knowledge engineering domain to determine those which best explain the 
cognitive processes of knowledge engineering. 
A parallel approach to the one mentioned in the previous paragraph is to test the utility of 
cognitve process models by developing tools, techniques, and procedures for use in 
knowledge engineering. This approach is a pragmatic one and may appeal more to 
knowledge engineers than to cognitive scientists. The development of this type of artefact 
would support (and may, in some cases, replace) the cognitive processes identified in 
knowledge engineering. The emphasis of these tools would centre on the cognitive activity 
in knowledge engineering rather than on the development of knowledge structures. 
Surveying the tools, techniques, and procedures already in use (formally and informally) 
by knowledge engineers and experts (even users) which could be classified as cognition-
support tools would aid in this approach. 
Another very critical issue raised in this paper focused on the very onerous tasks faced by 
the knowledge engineer when entering a new domain. The issue of how an individual deals 
with complexity, especially epistemological complexity, is central to the cognitive process 
view of laiowledge engineering. Research on expertise sheds some light on this issue for 
the expert, but we have very little information on how the knowledge engineer and the user 
deal with the same complexity from a cognitive process point of view. The biomedical 
research presented in this paper addresses some of the same issues for knowledge 
engineers. Among other ideas is the importance of understanding how misconceptions 
develop and spread. For example, the process of comprehending technical prose might be 
enhanced by structuring the prose in ways which reduce the inference demands on the 
reader. Some clear paths for research and study have been identified but what is needed is 
to view knowledge engineering as a laboratory for the study of applied cognitive processes. 
Finally, there appears a wealth of cognitive science knowledge available to knowledge 
engineers: however, challenging problems exist. The first observation is that few of the 
studies reported in cognitive science are in a form direcdy useful to knowledge engineers. 
This is not surprising but it is problematic for knowledge engineering. Methods are 
required for translating single cognitive science research results and for organizing and 
presenting a variety of studies into a form useful to knowledge engineers. Another related 
issue includes developing a valid approach to selecting relevant and promising cognitive 
science work for application in knowledge engineering. This development requires a 
thorough understanding of the theoretical foundations of cognitive science and the 
pragmatic demands of knowledge engineering. 
Once cognitive models have been identified for translation into knowledge engineering, 
another serious problem arises. How does a cognitive science model produce an 
operational tool, technique, or procedure in knowledge engineering? Does the model retain 
its meaning in the new context? Does the 'engineering1 alter or negate the model's meaning 
and application? Also, even if the result is successful, other issues arise. These issues are 
related to the unprincipled importing and combination of knowledge tools which may have 
contradictory or incompatible assumptions about knowledge. Engineering cognitive science 
models into knowledge engineering tools, techniques, and procedures requires a principled 
model or methodology of knowledge engineering. 
Cognitive Science offers a great deal of well-developed knowledge to the domain of 
knowledge engineering but a principled, coordinated framework for identifying, selecting, 
organizing, translating, engineering cognitive science results into useful knowledge 
engineering tools, techniques, and procedures is necessary for the full benefits to be 
realized. 9. References 
Abelson, R.P. and Lalljee, M. (1988). Knowledge structures and causal explanations. In 
D. Hilton (Ed.), Contemporary Science and Natural Explanation: 
Commonsense Conceptions of Causality. Harvester Press: Brighton. 
Aitkenhead, A.C. and Slack, J.M. (1985) Issues in Cognitive Modeling. Lawrence 
Erlbaum Associates: London. 
Anderson, R.C. (1970). Control of student mediating processes during verbal learning and 
instruction. Review of Educational Research, 40, 349-369. 
Baddeley, A.D. (1982). Domains of recollection. Psychological Review, 89, 708-729. 
Baker, L. and Brown, A.L. (1984a) Cognitive monitoring in reading. In J. Flood (Ed.) 
Understanding Reading Comprehension: Cognition, Language and 
the Structure of Prose (pp. 21-44). DE: International Reading Association: 
Newark. 
Baker, L. and Brown, A.L. (1984b) Metacognitive skills in reading. In P.D. Pearson (Ed.) 
Handbook of Reading Research (pp. 353-394). Longman: New York. 
Behn, R.D. and Vaupel, J.W. (1982) Quick Analysis for Busy Decision Makers. 
Basic Books: New York. 
Bjork, R.A. and Richardson-Klavehn, A. (1989) On the puzzling relationship between 
environmental context and human memory. In C. Izawa (Ed.) Current Issues in 
Cognitive Processes: The Tulane Flowertree Symposium on 
Cognition (pp. 313-344). LEA: Hillsdale, NJ. 
Bjork, E.L. and Bjork, R.A. (1988). On the adaptive aspects of retrieval failure in 
autobiographical memory. In M.M. Gruneberg, P.E. Morris, and R.N. Sykes 
(Eds.). Practical Aspects of Memory n. Wiley: London. 
Brachman, R.J. and Levesque, H.J. (Eds.). Readings in Knowledge 
Representation. Morgan Kaufman Publishers: Los Alto, California. 
Bradshaw, J.M. and Boose, J. H.(1989a) Decision analysis techniques for knowledge 
acquisition: combining information and preference models using Aquinas. 
International Journal of Man-Machine Studies, 32, 121-186. 
Bradshaw, J. M., Covington, S.P., Russo, P.J., and Boose, J.H., (1989). Knowledge 
acquisition techniques for intelligent decision systems: integrating Axod and 
Aquinas in DDUCKS. Proceedings of the AAAI Uncertainty Workshop, 
August 18-20, Windsor, Ontario, Canada. 
Britton, B.K.,Van Dusen, L., Glynn, S.M., and Hemphill, D. (1990) The impact of 
inferences on instructional text. In Graesser, A.C. and Bower, G.H. (Eds.), 
Inferences and Text Comprehension (pp. 53-70). Academic Press: New 
York. 
Brown, A.L., Armbruster, B.B., and Baker, L. (1986) The role of metacognition in 
reading and studying. In J. Orasanu (Ed.), Reading Comprehension: From 
Research to Practice (pp. 49-75). Erlbaum: Hillsdale, NJ. 
Cairns, K.V. and Woodward, J.B. (1988) Life Choices simulation: model and 
methodology. Systems Practice, 1(1), 47-64. 
Carroll, J.S. and Weiner, R.L. (1982) Cognitive social psychology in court and beyond. 
In A.H. Hastorf and A.M. Isen (Eds.), Cognitive Social Psychology. 
Elsevier/North-Holland: New York. 
Chandrasecaran, B. (1988) Generic tasks as building blocks for knowledge-based systems: 
the diagnosis and routine design examples. The Knowledge Engineering 
Review, Vol. 3(3), p 183-210. 
Clark, H.H. and Haviland, S.E. (1977) Comprehension and the given new contract. In 
R.O. Freedle (Ed.), Discourse Production and Comprehension (Vol. 1). 
Ablex: Norwood, N.J. 
Cleaves, D.A. (1986). Cognitive biases and corrective techniques: proposals for improving 
elicitation procedures for knowledge-based systems. In proceedings of the First AAAI Knowledge Acquisition Workshop for Knowledge-Based 
Systems: Banff, Canada. 
Collins, A.M. and Luftus E.F. (1975) A spreading activation theory of semantic 
processing. Psychological Review, 82, 407-428. 
Coulson, R.J., Feltovich, P.J., and Spiro, R.J. (1986) Foundations of a 
Misunderstanding of the Ultrastructural Basis of Myocardial Failure: A 
Reciprocating Network of Oversimplifications. Report #1 Conceptual 
Knowledge Research Project, Southern Illinois University School of 
Medicine, Springfield, IL 
Crowder, R.G. (1976). Principles of Learning and Memory. Erlbaum: Hillsdale, 
N.J. 
Crowder, R.G. (1982). The demise of short term memory. Acta Psychologica, 50, 
291-323. 
Eddy, D.M. (1982) Probabalistic reasoning in clinical medicine: problems and 
opportunities. In D, Kahneman, P. Slovic, and A. Tversky (Eds.). Judgment 
Under Uncertainty: Heuristics and Biases. Cambridge University Press: 
New York. 
Edwards, W. (1968) Conservatism in human information processing. In B. Kleinmuntz 
(Ed.) Formal Representation of Human Judgment. Wiley: New York. 
Edwards, W. and Newman, J.R. (1982). Multiattribute Evaluation. Sage: Beverly 
Hills, CA. 
Eich, E. (1985) Context ,memory, and integrated item/content imagery. Journal of 
Experimental Psychology: Learning, Memory and Cognition, 11, 764-
770. 
Einhorn, H.J. and Hogarth, R.M. (1981). Behavioral decision theory: processes of 
judgment and choice. Annual Review of Psychology, 32, 53-88. 
Estes, W.K. (1976) The cognitive side of probability learning. Psychological Review, 
83, 37-64. 
Evans, D.A. (1989) Issues of cognitive science in medicine. In D.A. Evans and V.L. Patel 
(Eds.), Cognitive Science on Medicine: Biomedical Modeling (pp. 1-16). 
MIT Press: London. 
Fiedler, K. (1982) Causal schemata: review and criticism or research on a popular 
construct. Journal of Personality and Social Psychology, 42, 1001-13. 
Feltovich, P.J., Spiro, R.J., and Coulson, R.L. (1989). The nature of conceptual 
understanding in biomedicine: the deep structure of complex ideas and the 
development of misconceptions. In D.A. Evans and V.L. Patel (Eds.), Cognitive 
Science on Medicine: Biomedical Modeling (pp. 113-172). MIT Press: 
London. 
Feltovich, P.J., Johnson, P.E., Moller, J.H., and Swanson, D.B. (1984) LCS: the role 
and development of medical knowledge in diagnostic expertise. In W. J. Clancey 
and E.H. Shortcliffe (Eds.) Readings in medical Artificial Intelligence: 
The First Decade. Addison-Wesley: Reading, MA. 
Fischhoff, B. and Beyth-Marom, R. (1983) Hypothesis evaluation from a Bayesian 
perspective. Psychological Review, 90, 239-260. 
Fischhoff, B., Slovic, P, and Lichtenstein, S. (1978) Fault trees: sensitivity of estimated 
probabilities to problem representation. Journal of Experimental 
Psychology: Human Perception and Performance, 4, 342-355. 
Fiske, S.T. and Taylor, S.E. (1984). Social Cognition. Random House: New York. 
Flavell, J.H. (1981) Cognitive monitoring. In W.P. Dickson (Ed.), Children's Oral 
Communication Skills (pp. 35-60). Academic Press: New York. 
Fletcher, C.R. and Bloom, CP. (1988). Causal reasoning in the comprehension of simple 
narrative texts. Journal of Memory and Language, 27, 235-244. 
Flood, R.L. (1987). Complexity: a definition by construction of a conceptual framework. 
Systems Research, 4(3), 177-185. Gaines, B.R. (1988) Knowledge acquisition systems for rapid prototyping of expert 
systems. INFOR, 26(4), 256-285 (Nov.). 
Galambos, J.A., Abelson, R.P., and Black, J.B. (Eds.) (1986). Knowledge 
Structures. Erlbaum: Hillsdale. 
Garner, R. (1985). Metacognition and Reading Comprehension. Ablex: Norwood, 
NJ. 
Garner, R. (1988). Strategies for reading and studying expository text. Educational 
Psychologist. 
Gentner, D. and Stevens, A.L. (1983) Mental Models. Erlbaum: Hillsdale, NJ. 
Glaser, R. (1984) Education and thinking: the role of knowledge. American 
Psychologist, 39, 93-104. 
Gorfein, D.S. and Hoffman, R.R. (1987). Memory and Learning: The Ebbinghaus 
Centennial Conference. Erlbaum Associates: Hillsdale, NJ. 
Graesser, A.C. and Clark, L.F. (1985) Structures and Procedures of Implicit 
Knowledge. Ablex: Norwood, NJ. 
Graesser, A.C. (1981) Prose Comprehension Beyond the Word. Springer-Verlag: 
New York. 
Grice, H.P. (1975). Logic and conversation. In P. Cole and J. Morgan (Eds.), Syntax 
and Semantics 3: Speech Acts. Academic Press: New York. 
Haberland, K, and Graesser, A.C. (1990) Integration and buffering of new information. In 
Graesser, A.C. and Bower, G.H. (Eds.), Inferences and Text 
Comprehension (pp. 71-88). Academic Press: New York. 
Hamilton, D.L. (1988). Causal attribution viewed from an information processing 
perspective. In D. Bar-Tal and A.W. Kruglanski (Eds.), The Social 
Psychology of Knowledge. Cambridge University Press: Cambridge, UK. 
Hansen, R.D. (1985). Cognitive economy and commonsense attribution processing. In 
J.H. Harvey and G. Weary (Eds.), Attribution: Basic Issues and 
Applications. Academic Press: Orlando, FL. 
Hewitt, C. (1985) The challenge of open systems. Byte, 10, 223-42. 
Hewstone, M. and Jaspars, J.M.F. (1987). Covariation and causal attribution: a logical 
model of the intuitive analysis of variance. Journal of Personality and Social 
Psychology, 53, 663-72. 
Hogarth, R. (1987) Judgment and Choice (2nd Ed). Wiley: New York. 
Hogarth, R., and Makridakis, S. (1981) Forecasting and planning: an evaluation. 
Management Science, 27(2). 
Holloway, CA. (1979). Decision Making Under Uncertainty: Models and 
Choices. Prentice-Hall: Englewood Cliffs, NJ. 
Holyoak, K. and Novik, L. (1983) Unpublished manuscript. Referenced in Seifert, 
C.M.Content-based inferences. In Graesser, A.C. and Bower, 
G.H.(Eds.),Inferences and Text Comprehension (p. 118). Academic Press: 
New York. 
Howard, R.A. and Matheson, J.E. (1980) Influence Diagrams. Stanford Research 
Institute International: Menlo Park, CA. 
Jacoby, L.L. (1983). Remembering the data: analyzing interactive processes in reading. 
Journal of Verbal Learning and Verbal Behaviour, 22, 485-508. 
Johnson, P.E. (1986) Specification of expertise: Knowledge acquisition for expert 
systems. In proceedings of the First AAAI Knowledge Acquisition 
Workshop for Knowledge-Based Systems: Banff, Canada. 
Johnson-Laird, P.N. 91983) Mental Models. Cambridge University Press: Cambridge, 
UK. 
Jones, E.E. (1985) Major developments in social psychology during the past five decades. 
In G. Lindzey and E. Aronson (Eds.), Handbook of Social Psychology 
(Vol.l)(3rd Ed). Random House: New York. Kahneman, D.L. and Tversky, A. (1973). On the psychology of prediction. 1 
Psychological Review, 80, 273-251. 
Kahneman, D.L., Slovic, P. and Tversky, A. (Eds) (1982) Judgment Under 
Uncertainty: Heuristics and Biases. Cambridge University Press: New 
York. 
Kelley, H.H. and Michela, J.L. (1980). Attribution theory and research. Annual Review 
of Psychology, 31, 457-503. 
Kelley, H.H. (1983). Perceived causal structures. In J.M.F. Jaspars, F.D. Fincham, and 
M. Hewstone (Eds.), Attribution Theory and Research: Conceptual 
Development and Social Dimensions. Academic Press: London. 
Klatzky (1980). Human Memory: Structures and Processes (2nd Ed.) W.H. 
freeman: San Fransisco. 
Kolodner, J.L. (1984) Retrieval and Organizational Strategies in Conceptual 
memory: A Computer Model. LEA: Hillsdale, NJ. 
Kosslyn, S.M. (1980) Image and Mind. Harvard University Press: Cambridge, MA. 
La France, M (1986) The knowledge acquisition grid: a method for training knowledge 
engineers. In proceedings of the First AAAI Knowledge Acquisition j 
Workshop for Knowledge-Based Systems: Banff, Canada. 
Langer, E.J. (1978). Rethinking the role of thought in social interaction. In J.H. Harvey, 
WJ. Ickes and R.F. Kidd (Eds.), New Directions in Attribution Research 
(Vol. 3). Erlbaum: Hillsdale, NJ. 
Lau, R.R. and Russell, D. (1980) Attributions in the sports pages. Journal of 
Personality and Social Psychology, 39, 451-63. 
Leddo, J., and Abelson, R.P. (1986). The nature of explanations. In J.A. Galambos, R.P. 
Abelson, and J.B. Black (Eds.), Knowledge Structures. Erlbaum: Hillsdale, 
N.J. 
Lehnert, W.G. (1978) The Process of Question Answering. LEA: Hillsdale, NJ. 
Lesgold, A.M. and Perfetti, C.A. (1978) Interactive processes in reading comprehension. 
Discourse Processes, 1, 323-336. 
Liu, L. (1989) Reading Between the Lines: The Assessment and Promotion of 
Comprehension by the Use of Questions. Unpublished Doctoral 
Dissertation: University of Chicago. 
Marcus, S. (Ed.)(1988) Automated Knowledge Acquisition for Expert Systems. 
Kluwar Academic Publishers: Norwell, Mass. 
Markman, E.M. (1977) Realizing that you don't understand: elementary school children's 
awareness of inconsistencies. Child Development, 50, 643-655. 
Meyer, M.A. and Booker, J.M. (1991). Eliciting and Analyzing Expert Judgment: 
A Practical Guide. Academic Press: New York. 
Minsky, M. (1975) A framework for representing knowledge. In P.H. Winston (Ed.), j 
The Psychology of Computer Vision. McGraw-Hill: New York. 
Musen, M.A. (1989) Conceptual models of interactive knowledge acquisition. 
Knowledge Acquisition, Vol. 1(1), p 73-88. 
Myers, J.L and Duffy, S.A. (1990) Causal inferences and text memory. In Graesser, A.C. ) 
and Bower, G.H. (Eds.), Inferences and Text Comprehension (pp. 159-
174). Academic Press: New York. 
Nelson, D.L. (1979) Remembering pictures and words: appearance, significance, and j 
name. In L. Cermak and F. Craik (Eds.). Levels of Processing in Human 
Memory. Erlbaum: Hillsdale, NJ. 
Newscombe, R.D. and Rutter, D.R. (1982a). Ten reasons why ANOVA theory and j 
research fail to explain attribution processes: 1 conceptual problems. Current j 
Psychological Reviews, 2, 95-107. 
Newscombe, R.D. and Rutter, D.R. (1982b). Ten reasons why ANOVA theory and ] 
research fail to explain attribution processes: 2 methodological problems. Current 
Psychological Reviews, 2, 153-70 Patel, V.L., Evans, D.A. ,and Groen, G J. (1989). Biomedical knowledge and clinical 
reasoning. In D.A. Evans and V.L. Patel (Eds.), Cognitive Science on 
Medicine: Biomedical Modeling (pp. 53-112). MIT Press: London. 
Potter, M. (1990). remembering. In D.N. Osherson and E.E. Smith (Eds.), Thinking 
(pp. 3-32). MIT Press: Cambridge, MA. 
Pyszczynski, T.A. and Greenberg, J. (1981). Role of disconfirmed expectancies in the 
instigation of attributional processing. Journal of Personality and Social 
Psychology, 40, 31-8. 
Read, SJ. (1987) Constructing causal scenarios: a knowledge structure approach to causal 
reasoning. Journal of Personality and Social Psychology, 52, 288-302. 
Reder, L.M. and Cleermans, A. (1990). The role of partial matches in comprehension: the 
moses illusion revisited. In Graesser, A.C. and Bower, G.H. (Eds.), Inferences 
and Text Comprehension (pp. 233-258). Academic Press: New York. 
Reeder, G.D. and Brewer, M.B. (1979). A schematic model of dispositional attribution in 
interpersonal perception. Psychological Review, 86, 61-79. 
Roediger, H.L. and Blaxton, T.A. (1987). Retrieval modes produce dissociations in 
memory for surface information. In D.S. Gorfein and R.R. Hoffman (Eds.). 
Memory and Cognitive Processes: The Ebbinghaus Centennial 
Conference, LEA: Hillsdale, NJ. 
Rothkopf, E.Z. (1982) Adjunct aids and the control of mathemagenic activities during 
purposeful reading. In W. Otto and S. White (Eds.) Reading Expository 
Material (pp. 109-138) Academic Press: New York. 
Rumelhart, D.E. and Ortney, A. (1977) The representation of knowledge in memory. In 
R.C. Anderson, RJ. Spiro, and W.E. Montague (Eds.), Schooling and the 
Acquisition of Knowledge. Erlbaum: Hillsdale, NJ. 
Rumelhart, D.E. (1980) Schemata: The building blocks of cognition. In RJ. Spiro, B.C. 
Bruce and W.F. Brewer, (Eds), Theoretical Issues in Reading 
Comprehension (pp. 33-58). Erlbaum: Hillsdale, NJ. 
Schank, R.C. (1975). Conceptual Information Processing. American Elsevier: New 
York. 
Schank, R.C. and Abelson, R.P. (1977) Scripts, Plans, Goals, and 
Understanding. Erlbaum: Hillsdale, N.J. 
Schank, R.C. (1982) Dynamic Memory: A Theory of Reminding and Learning 
in Computers and People. Cambridge University Press: Cambridge. 
Seifert, CM. (1990) Content-based inferences in text. In Graesser, A.C. and Bower, 
G.H. (Eds.), Inferences and Text Comprehension (pp. 103-122). Academic 
Press: New York. 
Seifert, CM., Abelson, R.P., and McKoon, G. (1986) The role of thematic knowledge 
structures in reminding. In J.A. Galambos, R.P Abelson, and J.B. Black (Eds.), 
Knowledge Structures. Erlbaum, Hillsdale, NJ. 
Shaver, K.G. (1981). Back to basics: on the role of theory in the attribution of causality. In 
J.H. Harvey, W.J. Ickes and R.F. Kidd (Eds.), New Directions in 
Attribution Research (Vol. 3). Erlbaum: Hillsdale, NJ. 
Simon, H.A. (1988) Cognitive architectures and rational analysis:Comment. In K. 
vanLehn (ed), Architectures for Intelligence: The Twenty-Second 
Carnegie Symposium on Cognition. Lawrence Erlbaum Associates: 
Hillsdale. 
Singer, M. (1980) The role of case-filling inferences in the coherence of brief passages. 
Discourse Processes, 3, 185-201. 
Slovic, P. Fischhoff, B., and Lichtenstein, S. (1976). Cognitive processes and societal 
risk taking. In J.S. Carroll and J.W. Payne (Eds.) Cognition and Social 
Behavior. Erlbaum: Hillsdale, NJ. 
Smith, F. (1982). Understanding Reading (3rd Ed.). Holt, Rinehart, and Winston: 
New York. Smith, S.M. (1988). Environmental context-dependent memory. In D.M. Thomson and 
G.M. Davies (Eds.) Memory in Context: Context in Memory. Wiley: New 
York. 
Snodgrass, J.G. (1989). How many memory systems are there really?: some evidence 
from the picture fragment completion task. In C. Izawa (Ed.) Current Issues in 
Cognitive Processes: The Tulane Flowertree Symposium on 
Cognition (pp. 135-174). LEA: Hillsdale, NJ. 
Sperber, D. and Wilson, D. (1988) Relevance: Communication and Cognition. 
Basil Blackwell: Oxford, UK. 
Spilich, G.J., Vesonder, G.T., Chiesi, H.L., and Voss, J.F. (1979). Text processing of 
domain-related information for individuals with high and low domain knowledge. 
Journal of Verbal Learning and Behaviour, 18, 275-290. 
Spiro, R.J., Feltovich, P.J., and Coulson, R.L. (1988) Seductive reductions: the hazards 
of oversimplification of complex concepts. Report #4 Conceptual Knowledge 
Research Project, Southern Illinois University School of medicine, Springfield, 
IL. 
Spiro, RJ. (1980) Constructive processes in prose comprehension and recall. In RJ. 
Spiro, B.C. Bruce, and W.F. Brewer (Eds.), Theoretical Issues in Reading 
Comprehension. Lawrence Erlbaum Associates: Hillsdale, NJ. 
Spiro, RJ., Feltovich, P.J., Coulson, R.L., and Anderson, D.K. (1990) multiple 
analogies for complex concepts: antidotes for analogy-induced misconceptions in 
advanced knowledge acquisition. In S. Vosniadou and A. Ortony (Eds.) 
Similarity and Analogical Reasoning. Cambridge University Press: 
Cambridge, UK. 
Stanovich, K.E. (1980). Toward an interactive-compensatory model of individual 
differences in the development of reading fluency. Reading Research 
Quarterly, 16, 32-71. 
Steinberg, RJ. (1984) What should intelligence tests test? Implications of a triarchic theory 
of intelligence for intelligence testing. Educational Researcher, 13, 5-15. 
Swinney, D.A. and Osterhout, L. (1990). Inference generation during auditory language 
comprehension. In Graesser, A.C. and Bower, G.H. (Eds.), Inferences and 
Text Comprehension (pp. 17-34). Academic Press: New York. 
Taylor, S.E. and Fiske, S.T. (1978). Salience, attention and attribution: top of the head 
phenomena. In L. Berkowitz (Ed.), Advances in Experimental Social 
Psychology (Vol. 11). Academic Press: New York. 
Trabasso, T, and Sperry, L.L. (1985) Causal relatedness and importance of story events. 
Journal of Memory and Language, 24, 419-427. 
Trabasso, T, van den Broek, P.W. and Suh, S.Y. (1989) Logical necessity and transitivity 
of causal relations in stories. Discourse Processes, 12, 1-25. 
Trabasso, T, van den Broek, P.W. and Liu, L. (1988) A model for generating questions 
that assess and promote comprehension. Questioning Exchange, 2, 25-38. 
Tulving, E. (1985). How many memory systems are there? American Psychologist, 4, 
385-398. 
Tversky, A. and Kahneman, D.L. (1973). Availability: a heuristic forjudging frequency 
and probability. Cognitive Psychology, 5, 207-232. 
Tversky, A. and Kahneman, D.L. (1980). Causal schemas in judgment under uncertainty. 
In M. Fishbein (Ed.) Progress in Social Psychology. Erlbaum: Hillsdale, 
NJ. 
van Dijk, T.A. and Kintsch, W. (1983). Strategies of Discourse Comprehension. 
Academic Press,: New York, 
von Winterfeldt, D. and Edwards, W. (1986) Decision Analysis and Behavioral 
Research, Cambridge University Press: Cambridge. UK. Wetter, Th. and Nuse, R. (1992) Use of natural language for knowledge acquisition: 
strategies to cope with semantic and pragmatic variation. IBM Journal of 
Research and Development: Special Issue on Al. 
Wetter, Th. and Woodward, J.B. (1990) Towards a theoretical framework for knowledge 
acquisition. Proceedings of the Fifth AAAI Knowledge Acquisition 
Workshop for Knowledge-Based Systems: Banff, Canada. 
Wielinga, B., Akkermans, H., Schreiber, G. and Balder, J. (1989) A knowledge 
acquisition perspective on knowledge-level models. Proceedings of the Fourth 
AAAI Knowledge Acquisition Workshop for Knowledge-Based 
Systems: Banff, Canada. 
Wittrock, M.C. (1979). The cognitive movement in instruction. Educational 
Researcher, 8, 5-11. 
Woodward, J.B. (1990) Knowledge acquisition at the front end: defining the domain. 
Knowledge Acquisition, 2(1), 73-94. Concluding Remarks A Comparative Assessment of Selected 
Approaches in the Focal 
Area of 
Knowledge Engineering 
and Cognition 
Thomas Wetter 
IBM Germany, Scientific Center 
Institute for Knowledge Based Systems 
Postfach 10 30 68 
W-6900 Heidelberg 
Germany 
WETTER@DHDIBMl.bitnet 
Introduction 
A workshop on knowledge engineering and cognition by its very nature brings to­
gether researchers and practitioners with quite different perspectives. Each of the 
perspectives sets the stage for a specific way to approach problems. The perspective 
taken may be as crucial for the solutions that are achieved as the problem itself is. 
As a result, it may turn out - as was the case concerning some of the approaches 
presented at the workshop - that different groups arrived at contradictory results for 
seemingly identical problems. 
It is the intention of this analysis to identify reasons for such apparent contradictions. 
I am not about to remove contradictions but hope to trace them back to their causes 
and hence clarify them. This basically involves an analytical approach, namely 
understanding why something happens. But it does also have a constructive facet: 
given a situation where a researcher or practitioner has to make a selection to inves­
tigate, refine, or apply an approach, the situation so far has shown that in many cases 
he will find positive and negative votes concerning whether the approach works jux­
taposed in an unrelated way. The aim of the analysis is to distinguish situations where 
an approach does or does not work. The prevailing structure of this article will be to examine a number of criteria and to 
characterize as many individual contributions according to as many criteria as seem 
to apply. 
Among the reasons for different researchers and practitioners to proceed fundamen­
tally differently and to arrive at different results that are hard to compare is that both 
knowledge engineering and the study of cognition are imbedded in a cluster of dif­
ferent disciplines and their respective methods and criteria. Among them are 
• informatics 1 ([Dallemagne], [Manago], [Branskat])2 
• artificial intelligence ([Bergmann], [Puppe]) 
• cognitive and social psychology ([Fensel], [Woodward] [Thoben], [Glowalla]) 
• cognitive modelling ([Reimann], [Janetzko]) 
This list is not exhaustive, since some of the disciplines tend to lie somewhere in be­
tween. According to their traditions, the approaches may be focussed upon 
• software development and its tools, re-usable building blocks and results, and the 
process of managing software projects, 
• sophisticated computational methods to solve complex problems, 
• individual and social processes and their impact upon applying and communi­
cating knowledge, 
• computational models of knowledgeable behavior. 
Consequently, the approaches may judge their respective results according to 
• exploitation of human and computational resources, 
• appropriateness of representation and inference, 
• assessment of processes and contents addressed by the involved agents, 
• plausibility of models, similarity with observed phenomena. 
Some readers may expect to find the term "computer science" here. But we are not really 
dealing with a science of computing machinery but with a science of the well founded use 
of information. Therefore, the term "informatics" that is more common in Europe has 
been preferred to the traditional American "computer science". 
Arguments presented in this text may draw upon oral contributions during the workshop 
in February, 1991 or papers in this textbook prepared after the workshop. In the latter 
case, the source is referenced as [First_author92]. In the former case, when there is no 
corresponding written contribution, or when it does not play a role, the respective work­
shop participant is referenced as [Participant]. Of course I take full responsibility for 
possibly having misunderstood or over-interpreted some detail of the oral presentations. If we consider this topic more generally again, some of the approaches are predomi­
nantly product oriented (informatics and artificial intelligence), others address the 
extent of capturing the processes involved in human knowers (cognitive and social 
psychology), and a third group mainly aims at abstract formal models of the real 
world (cognitive modelling). To label these approaches in a much too rough but still 
suggestive way, we could talk about engineering, formal theories, humanities, and 
science approaches. Formal theories may of course play a role in all four. In Al, 
their inherent properties are an essential point of interest. An engineer may use the­
ories for constructing objects in the real world with desired properties. In the hu­
manities and more so in science, the descriptive and predictive power of formal 
theories with respect to phenomena in the real world is the most essential criterion. 
Given this, it would be more of a surprise than an expected result that the different 
approaches arrive at comparable or even similar results. One of the major outcomes 
for the workshop participants, and hopefully for the readers of this book, might be 
to understand which role each of the disciplines plays with respect to the other disci­
plines involved. 
Consequently, the strategy of presentation in this chapter is to try and pick up one 
point of contrast after the other and to collect the controversial opinions and results 
from the individual contributions. This may draw upon oral contributions at the 
workshop or written material in the respective text in these proceedings. In the latter 
case I only briefly refer to a segment from the individual contribution. 
The different attitudes that the involved disciplines take and the methods they apply 
form one organizational scheme for the subsequent text. There is a second scheme 
orthogonal to this one, namely to identify objects and individuals playing certain 
parts in the process of knowledge engineering. It is their properties, activities, con­
tributions, etc. that can be analyzed from the different perspectives of the disciplines. 
Individuals obviously are experts and knowledge engineers but also researchers in 
knowledge engineering 3. Objects may be tools or methods that knowledge engineers 
use. 
Outline of Presentation 
The text is organized as follows: there will be two major parts centering around the 
two groups of human protagonists of the play: the experts and the knowledge engi­
neers. The part concerning the characteristics of the expert's role in the play will 
address the three aspects of: 
3 The role of users of the final systems has not been an issue at the workshop and is only 
addressed in [Strube]. Therefore a comparative assesssment is not required. 1. development of expertise, including the aspect of intermediate states 
2. change of expertise through the effects of knowledge acquisition activities 
3. representation of expertise, with special emphasis on 
• case vs. theory based representation 
• common sense foundation of expertise 
The part analyzing the knowledge engineer 's role has two sections: 
4. his possibilities, limitations, skills etc., including 
• details of how to play the role 
• the degree to which he must become a domain expert of his own 
• his biases 
5. methods and tools that can support his activity, with the models of problem solv­
ing as the most prominent examples: 
• their origins in artificial intelligence, psychology, or directly from knowledge 
engineering 
• their nature 
• the activities involved in using them 
• the required skill 
Finally, we will discuss shifts of positions that happened in long ranging projects and 
speculate about reasons why different approaches arrived at different and seemingly 
contradictory results. 
1. Development and Nature of Expertise 
A major group of arguments can be gathered around "encoding specifity" 
([Tulvi73]). Encoding specifity roughly means that memorizing some information 
cannot be separated from memorizing the context (place, mode of presentation, etc.) 
of the information. Encoding specifity has been used by [Glowalla] and can explicitly 
be found referenced by Janetzko and Strube ([Janet92]). Glowalla also draws upon 
neural architecture considerations in this context, which make it plausible that the full 
setting of the "information itself", the way it comes across the individual and the sit­
uation or context where it is embedded, influence the way it is encoded. 
[Woodward] also uses related arguments under the terminology "environment vs. 
memory" (also "ecologic memory"). 
Taking these arguments in their full strength would imply that the individual does 
not have much of a choice regarding the way his knowledge becomes represented in 
his mind - one might even use the term "brain" here. 
On the other hand, [Schmalhofer] reports experimental evidence for a learning proc­
ess of going gradually into the depth of a "body of knowledge" as opposed to going 
gradually into the breadth. Since there is no reason to believe that systematically 
"shallow details" from the full horizontal range of a domain precede "deep details", i.e. that all environments "choose" to "present" first shallow and then deep material, a 
second active mechanism (aside from the more passive specific encoding) may exist. 
According to [Schmalhofer], deep representations can arise from application of prior 
knowledge or from being confronted with a combination of material (cases, justifica­
tions, theory, etc.). The former does not lend itself for a systematic argument (we 
will return to this point when discussing common sense parts of expertise), because it 
leads to an infinite regress when trying to clarify the origin and nature of the prior 
knowledge. The latter is further specified by [Schmalhofer] as mechanisms for filling 
holes in representations. It remains open as to how much this process is active and 
how much is passive. [Schmalhofer] also encountered the correction of miscon­
ceptions which again can be interpreted as due to active or passive processes. 
For the focal area of cognition and knowledge engineering, two implications from the 
recent discussion are important. 
First, if only passive mechanisms occur, it is hard to argue that basic representations 
can change. This is crucial for the subsequent discussion as to whether cases or theo­
ries are the basic representation. If e.g. material only occurs in the form of cases, 
there is no reason to believe that the individual can arrive at a theory, as has been 
postulated in [Janct92]. 
Secondly, we can ask whether intermediate representations persist or are extinguished 
when more advanced ones emerge in the individual. If we have to be aware of active 
revisions, as postulated for the process of developing mental models, early represent­
ations may vanish. If a metaphor of adding and compiling applies, early represent­
ations would not be extinguished but just "buried", and could be retrieved by 
adequate "digging", as postulated in Kolodners models of memory and recall ( for 
references to both schools of thought, cf. [.Janet92]). 
2- Change of Expertise under Knowledge Acquisition Activities 
It has been claimed (e.g. by Becker and Bartsch-Sporl ([Becke90])) that being exposed 
to knowledge acquisition activities changes the knowledge in the expert. This aspect 
has not been addressed at the workshop. But a posteriori reflection of its contrib­
utions supplies systematic support for some of the above arguments namely about 
encoding specifity. If indeed the way how the material is presented to the knowledge 
engineer co-determines the representation, knowledge engineering as a discipline must 
be aware that its activities fundamentally determine its results. If certain predominant 
elicitation methods are applied over and over again, experts may be incapable of re­
sisting the format of presentation associated with the methods. In other words, the 
methods will start to encode into the expert that which they claim to elicit from him. nature of expertise 
development (1) cases vs theory common sense 
cogn Fiq. 2 coan Al 
passive active 
mechanisms mechanisms not-
Glowalla Pfeifer Bergmann ro 
CO o 
Glowalla Reimann Schmalhofer Woodward Janetzko 
(1) including "change of expertise under knowledge acquisition activities" 
Figure 1. Nature of expertise This is different and more critical here than in cognitive science because of its impact 
on software products. 
In Figure 1 and in the following figures, a topic of the size of approx. one section is 
broken down into several steps. Leaf notes of the resulting trees represent individuals 
and their contributions. Abbreviations in boxes (cognitive science), m/(ormatics), 
^l(rtificial) /(ntelligence), psychology) and soc(ia\) sciences) denote the disciplines 
from which the results have been derived. The depth of the tree may vary according 
to the degree of differentiation of the positions, "not" above Pfeifer denotes that 
common sense as part of expertise is refuted in the contribution of Pfeifer et al. (see 
below). 
3. Forms and Representation of Expertise 
Cases vs. Theories 
In this section, "theory" will be used in the sense of Janetzko's and Strube's 
([Janet92]) "semantic knowledge". For "case" we follow Althoff's and WeB' 
([Altho92]) definitions. Concerning cases, an important aspect of differentiation will 
be to what extent abstraction takes place; we will speak of authentic cases when no 
conscious or explicit reduction, omission, or comprehension of detail occurs. 
Apart from the transient role of cases or episodes during development of expertise (cf. 
section 1.) these same cases and episodes are also increasingly discussed as constitu­
ents or carriers of fully developed expertise. This is the subject of the present section. 
In a related and more fundamental text, Althoff and WeB ([Altho92]) provide the 
terminological basis for the distinction between case based reasoning and inductive 
learning, between case and rule based reasoning, and between case based and 
analogical reasoning. Their terminological and partly mathematical apparatus will be 
used here and there in the subsequent set of direct comparisons between approaches 
presented in this book. 
We follow the top level distinction of [Altho92] that cases can be used directly for 
reasoning (case based or analogical reasoning) or as material from which to construct 
general knowledge by induction. 
To continue with the distinctions that we encounter: 
• Case and rule based reasoning may occur in isolation or they can be combined 
or fully integrated in different ways. 
• Cases can be taken as such or as concrete exemplars from which abstractions are 
generated and used for reasoning. Let us set out with reasoning from authentic cases, i.e. a more or less pure approach 
according to the first two distinctions. Rcimann and Schult ([Rcima92]) supply con­
siderable detail from the cognitive psychology literature and from their own work 
about learning from the solutions of exemplary tasks. They have isolated those be­
havioral patterns that characterize successful learners. Their methodological contrib­
ution is to describe these patterns and to work towards formal and executable models 
and simulation of such models. 
Successful learning involves more elaboration, reflection, and self assessment of the 
learner and results in hierarchical arrangements of the example tasks or cases. These 
observations from mechanics resemble results known from text comprehension. In 
contrast, Reimann and Schult find that passive learning supplies flat case collections 
organized by similarity measures. 
While [Reima92] classify the form of learning that results in a hierarchical represen­
tation as successful, and the form resulting in flat collections as less successful, 
Althoff and We8([Altho92]) report other psychological investigations without any 
preference for one of the forms. This deviation between the findings reported in 
[Reima92] and [Altho92] can be interpreted in several ways but can probably not be 
decided at present. The hypothesis of [Reima92] in favor of hierarchical organization 
may turn out to be generally valid and happened not to be detected in the investi­
gations quoted in [Altho92]. In this case, cognitive science comes up with a well 
founded recommendation for architectures of case based reasoners. This architecture 
may resemble Kolodner's (for more detail and references, cf. Janetzko and Strube, 
[Janet92]) 
It may, however, turn out as well that there are fields where hierarchical organization 
is superior and others where similarity based retrieval should be preferred. This is not 
implausible, because in contrast to some other domains, the rather regular domain 
of classical mechanics used by [Reima92] may have intrinsic structures supporting the 
development of a hierarchical indexing structure. In this case, both types of archi­
tectures will coexist, and it now becomes part of the knowledge engineer's skill to de­
termine the demands of the domain he is encountering. [Rcima92]'s contribution to 
the resolution of this question could consist in a move towards testing (in domains 
other than mechanics) whether the cognitive models correctly predict the retrieval of 
cases. 
While Reimann and Schult ([Reima92]) are looking deeply into human cognition and 
how it acquires and represents cases, Branskat ([Brans92]) rather assumes the prac­
tical informatics perspective to provide a knowledge engineer's workplace. Her 
hypercard implemented tool supports the step by step transition from informal case 
descriptions to MOPs to be used in a case based rcasoner. In other words, 
[Branskat] sets out from one of the above plausible hypotheses how cases may be stored, namely through hierarchical indexing in Kolodner's sense, and supplies a tool 
for the generation of respective knowledge bases. 
The common feature of these three contributions, which also distinguishes them from 
all others presented aside from a short part of Manago and Conruyt ([Manag92] see 
below) is that they focus on sets of authentic cases, as opposed to abstractions from 
cases or theories derived from cases. 
We now vary the pure form of solely reasoning from non-abstracted cases. The first 
variation is brought into play in Janetzko and Strube ([Janet92]). They refer to re­
search results in cognitive psychology which imply that human individuals apply both 
cases or episodes and general knowledge, each at its respective time, and report about 
conditions when it is necessary to "take a turn" from case to theory based reasoning 
or vice versa. This contribution suggests both, a hybrid architecture where cases and 
a theory co-exist and a special form of inference is to take a turn from one form of 
reasoning to the other, and a KADS interpretation model of such a hybrid system, i.e. 
a form that has proved useful for the process of engineering systems. 
This theoretical concept for a hybrid architecture should not be mistaken for the fully 
implemented two systems KATE and CAS-SYSTEM in [Manag92]. KATE is an 
induction system enhanced by pre-structuring of the cases. CAS-SYSTEM is a case 
based reasoner based on authentic cases. KATE and CAS-SYSTEM exist next to 
each other, not as an integrated hybrid reasoner. But KATE has proved in large real 
applications (with certain drawbacks to be reported below), and CAS-SYSTEM has 
been created to overcome some of these practically relevant shortcomings. Both these 
systems in [Manag92] are precisely described in data processing terminology ("...dy­
namically builds a path...", "...computes the tree..."), as opposed to the psychological 
backbone of [Janet92]/s argumentation. 
The second variation away from pure case based reasoning does not change the ar­
chitecture but proceeds from authentic cases to forms of systematic useful ab­
stractions. The two contributions of Schmalhofer et al ([Schma92]) and Bergmann 
([Bergm92]) complement each other in isolating the essence of a case from incidental 
detail to support knowledge engineering for a system architecture that reasons from 
abstract cases, which in this case are skeletal plans. 
Let us start with [Schma92] who supply a frame in which [Bergm92] fills a certain 
slot. Starting point of the work of [Schma92] are concrete observations in a domain 
of mechanical engineering about how humans memorize problem categories and 
partly specified solution "skeletons". Concretely, a technical drawing of a rotational 
part is a problem description, for which the solution in form of a fully specified 
working plan to produce the part is required. A possibly surprising result is that problem classes and skeletal solutions are memo­
rized - or at least can be elicited - separately. One reason may be that the attribution 
of a skeletal solution plan to a problem definition is not just to match the visual pat­
tern of the drawing to an equally visual skeletal plan from a library but involves 
considerations about material properties, the number of pieces to produce, etc. 
Secondly, problem classes result in a hierarchical organization, which is, however, 
fully different from Kolodncr's indexing in both contents and method of construction. 
While Kolodner's indices are the extensions of concrete memorizing activities for in­
dividual events, [Schma92]/s classes and their distinctions each reflect whole catego­
ries of similar problems characterized e.g. by workpiece material or machine type. 
Furthermore, an index in Kolodner's sense comes into existence by having to distin­
guish a concrete new individual element from concrete known elements. The classes 
in [Schma92]are derived by psychometric methods, which determine distances be­
tween individual yet prototypical elements based on similarity assessments of pairs 
("hierarchical clustering"). As an independent criterion that the clusters in [Schma92] 
make sense, it turns out that production plans follow naturally (although not trivially) 
and exemplars from a library of solutions can be identified for classes that make 
sense, whereas such workplans do not exist for senseless classes. 
All activities described in [Schma92] are supported by dedicated tools. We are facing 
an informatics style workplace, whose only shortcoming might possibly be that it has 
so far only proven useful for the one application outlined, namely working plans for 
rotational parts. Generalization may be feasible, but there is no evidence yet. 
For the transition from expert generated concrete plans to abstract skeletal plans, 
Bergmann ([Bergm92]) adds an Al based set of automatic tools which realize expla­
nation based learning to derive abstract descriptions of operations concretely en­
countered in workplans and dependency graphs between the abstract operators. This 
allows for the final skeletal plans to have beneficial dependencies (e.g. required se­
quences in order to ensure preconditions for operators) preserved and detrimental 
ones removed. 
In this context, [Thoben] has pointed out at the workshop that the term "variant 
planning" from engineering sciences is, according to his observations, misleading for 
the description of the mental use of known cases. While variant planning in engi­
neering modifies fully worked out cases to satisfy a slightly deviating requirement, 
human use of cases more resembles the application of partially instantiated abstract 
patterns and their instantiation, as can be realized on the basis of Bergmann's ab­
stracted skeletal plans. 
Bergmann's tools draw upon theories from Al, including some common sense ideas. 
They provide results of well understood formal quality: for a generated skeletal plan cases vs theory 
theory 
authentic cases 
analogi cal 
reasonina casebased 
reasoni ng 
inf inf abstraction 
i 
cogn inf| cogn Al inf cogn mach. learning 
Althoff Branskat Manago Janetzko Reimann Schmalhofer Bergmann Manago inf cogn 
Janetzko 
••suggested architecture |* 
• imp!emented alternatives 
Figure 2. Cases vs. theory it can be guaranteed that all specific plans which it represents can be derived as spe­
cializations. At present, the automatic procedure cannot, however, "collapse se­
quences of concrete operations into a single abstract operation", a capacity which the 
human expert has. 
For the comparison with the next approach it makes sense to rephrase this work as 
follows: general theoretical considerations are applied to improve the usefulness of 
case collections. At first sight, the opposite seems to be true for the core part of 
Manago and Conruyt ([Manag92]): cases are used for automatic induction of general 
regularities. 
Practical experience with predecessors of KATE have, however, led to a transition 
from "blind" induction to the prestructuring of the case format. The major argument 
is a practical one: purely mechanical induction is not feasible beyond a certain num­
ber of features of the objects to be learned. Common sense is used to reduce the 
number of combinations that make sense. Although formally different, the ap­
proaches of [Bergm92] and [Manag92] hence share the strategy of reducing their re­
spective "search spaces" by making use of human knowledge about implausible or 
physically impossible combinations before the initiation of mechanical processes. 
The positions that the contributions in this book take about case vs. theory based 
reasoning are not as controversial as we will find later in this text concerning different 
model based approaches to knowledge acquisition. They nicely exploit large parts of 
the possible spectrum, but all except [Manag92] have not yet undergone sufficient 
comparative or large scale practical evaluation to reveal far ranging differences. 
Common sense 
Positions at the workshop widely diverged about the common sense contents of ex­
pertise. 
Fundamentally, [Glowalla] claimed that a considerable part of expertise is made up 
of common sense. This would be supported by [Bergmann]'s observation that the 
constraints superimposed on skeletal plans to transfer them into realistic, fully speci­
fied ones carry many traits regarding what has been discussed as formal theories of 
common sense ([Hobbs85]), such as that there can only be one form of fixture 
(chucking) at a time (in the domain of production plans for manufacturing rotational 
parts). 
Another support comes from observations of [Schmalhofer] already briefly discussed 
above that deep knowledge (inasmuch as expertise is deep) can develop on the basis 
of prior knowledge, which ends up in common sense sooner or later. I.e. expertise 
itself would at least be founded on common sense. And given the position of devel-opment as adding and compiling, presented above as one plausible alternative for 
development of expertise, common sense would remain present in expertise one way 
or the other. 
In contrast [Pfeifer] argued that expertise is rather specialized. It is acquired by fully 
developed cognitive individuals. Hence it is bound to be different than the normal 
repertory of cognitive science methods as applied, e.g. by [Glowalla]. 
Support for this position comes from the use of KADS [Schre88]. A survey of existing 
conceptual models, i.e. specifications of domain knowledge and problem solving 
processes occurring in an application, reveals that the strategic layer of KADS which 
may be understood as representing flexible human management of new or unexpected 
situations, is hardly ever required in KADS conceptual models, i.e. the comprehensive 
notation of a model of expertise. An interpretation by KADS researchers is that 
"expert - i.e. highly overlearned - problem solving skills compiles out the flexibility 
available as part of "'intelligence' in the psychometric sense" [Schre88]. 
Although it has proved successful in practical engineering domains, one should be 
careful about the latter argument as being valid as a scientific statement about the 
nature of expertise. For applying KADS means to assess human capacities from the 
informatics perspective, which is more or less negligent of the intellectual and exper­
imental repertoire of cognitive science. KADS has not claimed to be able to deal with 
expertise in the ways of psychological investigation - as would be required to prove 
[Pfeiferjs conjecture - but to strive for usefulness for the purpose of running know­
ledge acquisition projects. Although KADS models may well serve the purpose of 
being a humanly conceivable notation of how expert problem solving can take place, 
there is neither a claim nor a proof that these models describe the manner of problem 
solving that actually happens in the expert. 
4. Roles and Capacities of Knowledge Engineer and Expert 
As indicated we will organize this section by role and - inasmuch as they involve a 
knowledge engineer - by the capacities, limitations, and possibilities that support his 
activities. 
Roles 
Only in Puppe's and Gappa's ([Puppe]) approach do we find the position that a 
knowledge engineer is not required. This is based on the claim that CLASSICA is a 
tool that allows the expert to construct knowledge based systems on his own within 
the scope of the shell underlying CLASSICA. This does avoid serious questions and 
problems raised in the sequel of this text (e.g. concerning biases of the knowledge 
engineer). But [Puppe] himself raised the question of cognitive adequateness of CLASSICA as a tool for novice users of electronic data processing equipment. Fur­
thermore, the question must be raised whether an expert is able to (re-)structure his 
knowledge according to the fixed requirements of CLASSICA, with or without being 
educated in the theory underlying CLASSICA. This can be rephrased as to whether 
the expert has to become a knowledge engineer. This is rather like the mirrorimage 
of a question that we will have to discuss later, namely whether the knowledge engi­
neer has to become a domain expert. This makes evident why the usability question 
of CLASSICA and comparable tools is not the classical one of software ergonomics: 
it is not a question of whether an individual is efficiently supported in his normal job 
(namely acting as an expert) but in an unusual one, namely reflecting and formalizing 
his knowledge. To summarize, using tools of the type of which CLASSICA is an ex­
ample requires of the expert the coordination of different unfamiliar activities, namely 
using the tools and reflecting his knowledge under constraints of having to formalize 
it. 
All other approaches presented involve a knowledge engineer, and obviously all in­
volve experts. The expert can be more an actor or the object of observation, his role 
can be precisely specified in advance or may evolve in the process. In my opinion, the 
most liberal attitude towards the expert is the one of [Fensel], who refers to the in­
terpretative paradigm of qualitative social science. For initial phases of a knowledge 
engineering process communication and interaction between knowledge engineer and 
expert without restriction by any modelling or other paradigm is a prerequisite for 
guaranteeing (or at least for enabling) coverage of the full spectrum of expertise. Any 
early determination of having to model into some representation bears the risk of 
overseeing important details next to those that can be subsumed under the model of 
Strube (cf. [Strub92]). 
After this interaction and communication process, the knowledge engineer applies his 
data reduction and interpretation methods to discover as much as possible from the 
traces he has. In other words, both expert and knowledge engineer are first involved 
in the full spectrum of both their technical and communicational capacities. Then the 
knowledge engineer evaluates the interactions in the light of his methods. 
The next two approaches place the expert in two differently restricted roles in com­
parison to [Fensel] or [Puppe]. First, Schlenker and Wetter ([Schle92]) take a scien­
tific discovery perspective of the following form: there is a natural phenomenon 
named knowledge. Knowledge engineering means to proceed towards a model of the 
phenomenon through a cyclic process of experimentation, interpretation of observa­
tions, hypothesis forming, and designing of further experiments to support or refute 
the present state of cognition. In [Wetter]'s exposition of this approach, the expert is 
comparable to subjects in psychological experiments. He undergoes an experimental 
setting, is observed, and that's it. The attempt to apply this approach and to supply 
rationales for all its activities has meanwhile revealed to Schlenker and Wetter (cf. [Schle92]) that the pure objectivist observer - observed object relation cannot or 
should not be maintained between knowledge engineer and expert. Instead, the set­
ting of elicition co-determines the concretely observed (verbal) data. In analogy to the 
above mentioned encoding specifity, one might think of a "recall specifity". Never­
theless, the knowledge engineer controls the process and hence is as responsible for 
the result as is the natural scientist for his physical, etc. model. On the other hand, 
ideally he can draw upon all the methodological and theory of science knowledge that 
guards and criticizes the process of scientific progress in natural sciences. 
Apart from the possible criticism as to whether knowledge can be "delivered" in such 
a sterile situation, it is obvious that such a process is extremely time consuming. Both, 
[Fensel] and [Schlenker] may end up with highly detailed, well suited, and objec­
tively (in their respective theories of science traditions) justified results. Or they might 
end up running out of resources. Or they might not even get started because know­
ledge engineers with the required skills are not available and no one is willing to pay 
for their education except for within narrow academic environments. 
[Bergmann] defines a restricted role of the expert in a more realistic, practical, down 
to earth way. In his environment of specific plans being generalized by means of 
formal domain theories, it is a common observation that plans are over-generalized 
due to an underspecified domain theory. It is now the role of the expert to detect 
over-generalizations, to mend them, and to enlarge or modify the domain theories 
accordingly. In this case both the communicational role and the content related role 
of the expert is very narrowly restricted, but in accordance with what practical ex­
pertise really consists of. A question may arise as to whether the outlined role can be 
generalized to other areas of application. 
Does the knowledge engineer have to become a domain expert? 
[Woodward] takes a clear position in understanding knowledge engineering to be the 
study of a complex domain and draws analogies to learning biases etc. Studying the 
domain can almost be re-phrased as becoming an expert. 
Although [Woodward] differs from [Schlenker] in a number of other aspects, there 
is the following similarity: if we interpret the scientific discovery of [Schlenker] as a 
process at the end of which the scientist is an expert in his domain, the knowledge 
engineer in this framework would be an expert of the expertise that he has discovered. 
In the case of [Fensel], it is not so clear w7hether the knowledge engineer must become 
a domain expert. The first interaction phase seems to tend toward this direction, 
whereas the second (at least potentially) operationalized interpretation phase might 
have the character of mechanical labelling, ordering etc., based on linguistic markers. 
It appears that the second phase of [Fensel] can be labeled as "labelling" in which the 
individual labels arise from the domain under study, whereas the way in which a label rol es 
expert knowledge engineer 
i 
cogn psy/soc inf 
I Al 
I I 
Puppe Bergmann 
Fensel i nteract, i nterpret • 
Schlenker* •probe, observe* perspective 
upon role bias 
psy/soc j Icogn 
•Fensel psy/soc cogn cogn becomi ng 
expert 
Woodward Glowalla 
•Schlenker support 
Fig. 4 
ro 
o 
Figure 3. Human roles in knowledge engineering is attached to a recorded detail is prescribed by the method. Labelling principles 
immanent to a method might reduce the need to become a domain expert, but also 
the precision and security of match of the resulting model might be less in the method 
of [Fensel] than in the feedback controlled one of [Schlenker]. 
Biases of the knowledge engineer 
According to [Woodward], cognitive biases inevitably co-determine the work of the 
knowledge engineer. Starting from "knowledge engineering as learning a complex 
domain", we have to be aware of learning biases (e.g. an over-simplification bias), 
biases of memorizing and recall (e.g. the whole environment vs. memory types of 
problems) (cf. [Woodw92]). Further support for the existence of recall biases comes 
from [Glowalla], who reveals e.g. considerable omissions of presented detail in a way 
which can easily be identified in a controlled psychological experiment where the in­
vestigator typically knows all the detail of material presented. This does not, how­
ever, apply to knowledge engineering where the knowledge engineer typically docs not 
know the material to be elicited from the expert. 
Biases can be dealt with in a number of ways: 
1. trying to be aware of the mechanisms and to avoid or reduce them 
([Woodward]) 
2. trying to be aware of the context for possible intervention or compensation 
([Fensel]) 
3. hoping for laws of larger numbers, i.e. of a cancelling out when a large number 
of knowledge engineering efforts is taken (multiple knowledge engineers, 
elicitation methods, interpretation methods, experts, etc.) 
4. trying to detect the biases by means of feedback ([Schlenker]). 
The third of these is probably not really feasible. The first might appear most feasible, 
but it bears the risk of being trapped in wrong hypotheses about where the biases 
come in and by what methods they can be reduced or compensated for. The second, 
taken as a goal, reflects one of the strongest present trends in knowledge acquisition 
but lacks any operationalization in the approach of [Fensel], An instance of the 
fourth point is [Schle92], which may, however, not be feasible except for extremely 
critical bodies of knowledge. 
5. Supporting the Knowledge Engineer 
After having outlined the inevitable constraints under which knowledge engineering 
has to take place, we now describe cognitive and engineering aids in support of the 
knowledge engineer's work. By far the longest part of this section deals with ap­
proaches dominated by their respective types of pre-existing models and the ways in which they are used. But let us start with the few alternatives that have been ad­
dressed at the workshop. 
Fully bottom up approaches 
Principally, all knowledge engineering work involves some manifestation of know­
ledge and knowledgeable behavior and an executable formalization being in some re­
lation to the manifestation. Support can consist of the systematic treatment of 
manifestations (bottom up) or desired target structures of formalizations (top down). 
The first set of approaches will depend on interpretation aids, whereas the latter will 
depend on formalization aids. In practice, both aids will occur in both types of ap­
proaches, with stronger emphasis on one or the other. 
Obviously [Fensel] has a strong sense of how to interpret. His way of interpreting is 
not determined by the form of the model to be arrived at, but aims rather at making 
the best (justified) use of the detail. The same applies to [Schlenker], who takes dif­
ferent measures to make the best use of the detail (operationalized interpretation rules 
in the approaches of [Fensel] and [Schlenker] plus planned feedback by further ob­
servation in [Schlenker]). So primarily both approaches are open as to which kind 
of model is to be arrived at. 
[Fensel] aims at arriving at KADS conceptual models at a later stage. Although very 
critical about the present stage of KADS interpretation models, he makes the point 
that his highly consuming bottom up approach can profit from some top down guid­
ance. 
While [Fensel] assumes full spectrum communication as a starting point, 
[Schlenker] and [Woodward] share a view of reducing the communication spectrum. 
However, while this is done in the case of [Schlenker] specifically with an aim at 
modelling, in the case of [Woodward] it respects more the known phenomena about 
communication and its failures or biases. 
Libraries of models 
Models support communication and the mental capture of the details of a domain 
much better than isolated "pieces of knowledge". This applies to some extent to the 
previous approaches which develop new models, and probably more to the following 
ones that try to make use of existing models. It has also been claimed that the 
communicational value of models supports maintenance. Depending upon their 
provenance some classes of models may be better suited for capture and communi­
cation of expertise than others. Origins of models 
Models and libraries of models can originate from the disciplines involved in different 
ways. The usability of models for human users on one hand and as computational 
formalizations or preforms of such on the other hand can be expected to be highly 
correlated to the origin of the models. 
Puppe's and Gappa's ([Puppe92]) models e.g. of cover and differentiate (in diagnosis), 
etc. originate from artificial intelligence, more concretely from the discussion of mod­
els of problem solving and partly from naive physics or studies about the relation 
between structure and behavior. These models draw on first principles of reasoning 
and their respective knowledge structures. They are fully computationally imple­
mented templates, including a user interface or modelling workplace to be used by the 
domain expert for form filling. Aside from this, they exist without any further guid­
ance, instruction, etc. 
KADS also draws upon research about humanly comprehensible generic elements of 
representing knowledge and inference such as the KL-ONE research and Clancey's 
work. These latter roots rather emphasize characteristics of representation and infer­
ence whereas the ancestors of the models of [Puppe] are rather those of the charac­
teristics of the domains themselves. But this difference in origin is probably much less 
pronounced than the difference in what the two approaches did with the models: 
Puppe implemented them, whereas the KADS group elaborated on how to apply 
them. 
Coming back now to more detail on ancestry, the following is true for the spirit of the 
conceptual ("knowledge level") modelling: empirical evidence in a number of diverse 
fields of early KADS applications was reflected and cast into semiformal descriptions 
of the inference processes that had been encountered. This is not too far from the 
manner in which early expert system shells evolved; having modelled an application, 
its knowledge representation and inference technology were isolated and sold as a 
software product. The difference with KADS is that a software product is not sold 
but rather a methodology, which includes experiences that have been reflected and 
cast into some humanly conceivable notation of the abstracted models. 
The next prominent modelling approach represented at the workshop is McDermott's 
and coworkers' Spark-Burn-Firefighter (SBF) of Dallemagne et al. ([Dalle92], 
[Klinker]). At the beginning (see paragraph Are libraries and tools feasible and usa­
ble? below), the attempt was made to provide a meta-tool making available the tech­
nologies of approved knowledge acquisition tools, supporting the process of selecting 
among them in order to allow the non-programmer to write knowledge based systems. 
The starting point was empirical in the sense of collecting what had proved useful in 
practice and not theoretical in the sense of supplying the formal detail of theories of 
reasoning, etc. When models arise from cognitive psychology ([Schlenker], Janetzko and Strube 
([Janet92])), their intention is to best capture the processes in humans when solving 
a problem. They would, hence, not model domains as such but human conceptuali­
zations of domains. By this token they would have the best chances of being humanly 
conceivable (see Strube's discussion on cognitive adequacy in this book) but would 
offer the least chance of detecting and getting rid of individual misconceptions (bi­
ases) of the expert from whose conceptualization the model is derived. 
Nature of the models 
Axes of distinctions between the different approaches are 
• whether models are intended as mental tools or code fragments 
• which grain size they and their elements have 
• to what extent and detail they pre-exist, or rather how much refining and 
instantiating is required in the individual knowledge engineering process 
• which stages a model goes through. 
To start with the most obvious approach, Puppe's and Gappa's ([Puppe]) D3 offers 
executable versions of the artificial intelligence models of diagnostic problem solving. 
The individual engineering activities consist of selecting the most appropriate diag­
nostic model and supplying domain knowledge in the respective format. There is no 
operationalized process of supporting the selection, but once it has been arrived at, 
the rest is more or less routine form filling. The models are specified on all levels of 
grain size. Every model is a complete monolithic building block, which precisely 
complements a domain theory to be added by the expert. 
[Klinker] distinguishes two grain sizes of constituents. The smaller ones — mech­
anisms — serve as both mental and technical building blocks and are designed for 
re-use. The process of selecting and configuring the appropriate mechanisms is widely 
supported by analytic and attribution aids in the tool Spark. The tool Burn generates 
acquisition tools to elicit the required detail for customizing the configuration of 
mechanisms. The fully configured combinations — the methods or workflows — being 
the larger constituents in SBF are understood to be specific for the application and 
not intended for re-use. In other words, we find here no match for what will be in­
troduced as the interpretation model in KADS. 
The model structure and building block perspective of KADS plays different roles in 
the approaches of [Fensel], Pfeifer et al. ([Rothenfluh]), [Shadbolt], and 
4 For an introduction to the KADS terminology, cf. Janetzko and Strube ([Janet92]). The 
KADS expression "domain layer" should not be confused with the standard use of "do­
main" in contexts such as domain expert, complex domain, etc. where static and infer­
encing aspects are normally included. In some cases where confusion might easily occur, [Linster]4. In KADS, re-occurring patterns of inferencing (the larger units) have 
been abstracted from the domains they operate upon. They have been named inter­
pretation models and have been collected in a library. Typical elements of the library 
are methods of problem solving such as heuristic classification. Interpretation models 
have been provided in verbal and pseudocode notation including individual know­
ledge sources as smaller building blocks. In pure KADS, interpretation models are 
primarily meant — as the name already suggests — as mental help in interpreting data 
from knowledge elicitation activities. They help in constructing conceptual models, 
but they do not become parts of them. 
Activities using models and the required skills 
Having introduced the KADS modelling elements, we now begin the section on using 
models with KADS and will come back to the other types of models later, i.e. we will 
proceed in the reverse order now. 
[Fensel] takes KADS as such (at least in the oral presentation; in the text, the refer­
ence to KADS is only marginal and the modelling terminology is more like that of 
[Musen89]). After the initial steps of arriving at a common understanding between 
knowledge engineer and expert, he tries to come up with a principled formulation of 
the process model 5. Further processes use elicited data to add the domain informa­
tion to the selected process model. Models do not control the process but filter re­
corded interpretations in the end. The required skills for this final filtering are in 
understanding the model constituents as mental tools, in understanding results of in­
terpretation by means of methods from qualitative sociology, and relating them to 
each other. While the latter draws upon skills that are part of an education in the 
humanities, the former presently seems to be based on intuition. 
While [Fensel] is not fully determined about modelling in KADS but arrives at 
structures in a way which seems related to KADS, [Rothenfluh] explicitly refers to 
KADS but comes up with a different method of arriving at KADS conceptual mod­
els. Based on protocol analysis data, the method uses the degree of variation among 
knowledge engineers in phrasing their interpretations in terms of the KADS inter­
pretation models as a measure for the adequatencss of the individual interpretation 
model. Basically [Rothenfluh] understands the KADS conceptual models as compu­
tational, and his question is whether they are also mental tools. To answer this, he 
draws upon intuition just as [Fensel] did (besides the skill of experimental psychology 
of conducting think-aloud experiments), but at least he assesses the result by the un-
I use the expression "application" instead of "domain" to denote that all static and infer­
encing aspects are to be subsumed. 
5 It can be assumed that it roughly resembles the task and inference layer of a KADS con­
ceptual model. But to preserve authenticity, Fcnscl's [Fensc92] terminology is used. specific feedback of similarity or dissimilarity of the conceptual models produced by 
different knowledge engineers. He tests for inter-individual reliability among know­
ledge engineers, not for the validity of models. This is in good accordance with the 
communication role of models. 
[Shadbolt] works on a method of precisely determining the selection of an interpre­
tation model. Based on an informal task analysis, he offers elicitation tools whose use 
results in a suggested interpretation model. For this purpose he has complemented the 
set of interpretation models (for the selection of which in KADS there is only an in­
tuitively defined decision tree) by a set of formal criteria, whose evaluation is sup­
ported by the tools. 
[Linster] keeps the static knowledge / inference knowledge distinction, but starts from 
individually tailored models rather than from surrogates derived from pre-formed 
patterns. Essentially, the model serves as a medium of communication between hu­
mans. [Linster] has meanwhile extended his approach that starts with a thorough 
investigation of the structure of a new domain by meta-tools to generate tailor suited 
tools which enable the experts to enter the details of the model themselves. In the 
latter sense, the approaches of [Linster] and [Puppe] resemble each other. However, 
in Linster's case, a knowledge engineer is heavily involved in forming the model which 
then determines the functionality of the customized tool. No mental support other 
than the KADS recommendation of layers is given. In other words, skill in computer 
science and intuition or modelling skill form the intellectual bases of knowledge engi­
neering. 
Coming back now to the non-KADS building block approaches, the approach of 
[Klinker] may seem to be perfect: his process is widely operationalized, his smaller 
building blocks (mechanisms) are both mentally embedded and technically available 
as building blocks (code fragments) whose instantiation supplies operational code. 
However, obviously and admittantly the building blocks of Klinker's approach arc 
considerably simpler than those of KADS or in Puppc's case. Assuming that a well 
educated facilitator ( = knowledge engineer?) is involved, more or less standard pro­
cedures (or their abstractions in forms of the mechanisms) can be identified and 
instantiated, the result being a software module which has properties that enable easy 
integration in an environment of similarly produced modules. In summary, the ap­
proach has been discussed more as KASE (knowledge aided software engineering) 
than as knowledge engineering. The key role is that of the facilitator, who has to 
match abstract descriptions of mechanism functionality (provided by programmers) 
with the information processing needs of (non-programmer) users. If this succeeds, 
the users can influence their own application programs to a considerable extent 
without requiring essentially different skills than they need for their normal profes­
sion. support i I 
bottom up restricted communication 
|psy/sec j cogn psy/sec j 
Fensel Schlenker Woodward 
I ! 
oriain of models top down 
purpose of models 
Al cogn 
domain principals 
charac- of repres. 
teristies and inference Janetzko Schlenker software 
engi neeri ng capturing expertise 
Code ge­
neration Fensel Rothenfluh Shadtbolt 
Klinker Puppe Linster 
Puppe Klinker KADS 
KADS 
Figure 4. Ways of supporting the knowledge engineer To finish this reverse list of skills required to apply the models, we find the trivial fact 
that in the case of [Puppe], the knowledge engineer has finished his work when he 
has implemented the computational form of the model. In particular, he is not in­
volved in any interpretation activity and his biases (cf. [Woodward]) are not an issue. 
(However, what about expert biases?) The requirements on the domain expert have 
already been discussed above. Given the origin of the models from artificial intelli­
gence research, Puppe's observation is probably correct that it would be too time 
consuming for the domain expert to create them himself. 
Fig. 4 illustrates the diverse aids that are being supplied for knowledge engineering. 
For the top down part, KADS plays a distinguished role indicated by the connection 
of the four "tear nodes of authors Linster, Fensel, Rothenfluh, Shadbolt. That part 
of the figure is the only one where an attribution to one of the four disciplines does 
not seem appropriate. This may be an indication that we are operating in the genuine 
territory where knowledge engineering and cognition widely overlap. 
Discussion 
Since this text as a whole has the character of a discussion, this section will only be 
used as a reminder of the aspects of highest contrast encountered and to speculate 
about origins and the reasons for the divergences. 
Which criteria are met by the systems produced by our methods? 
Many of the contributions cannot undergo this comparison yet because they either 
do not cover the whole process up to running systems or are in a preliminary status. 
For the purpose of demonstrating extreme positions, we concentrate on Manago and 
Conruyt ([Manag92]) and Schlenker ([Schlc92]). 
[Manag92] clearly claim practical applicability of knowledge bases induced by KATE 
but equally clearly admit that induction algorithms supply sets of rules, some of 
which are hardly conceivable for the human user. Although this violates Strubc's 
([Strub92]) criterion of cognitive adequacy, it is not sufficient to fully disqualify 
KATE. Despite the inconceivability of some rules, the systems can be successfully 
used, which may mean that fields where KATE has been applied are factually char­
acterized by such rules that can by no means be expressed in a form that is more 
appealing. Then human expertise cannot really cover such fields. Another interpre­
tation is that inappropriate descriptions have been used as a starting point for KATE 
and that starting from different formalizations of the cases themselves or more ap­
propriate use of common sense prior to induction may lead to better conceivable 
knowledge bases. At the present state of cognition, a certain scepticism remains as 
to whether a system can be valid if it cannot be understood by a human. In the other extreme, Schlenker [Schle92] supplies some of the detail required to arrive 
at absolutely cognitive adequate systems in Strube's sense (cf. [Strub92]). We have 
argued above and also find support in [Strub92] that this is a cumbersome way to 
proceed, and indeed, in contrast to KATE there are no real size applications of 
Schlenker's method available yet. At present it is fair to say that large scale applica­
tion and strong cognitive adequacy are mutually exclusive. 
Are KADS conceptual models computational? 
From the perspective of the originators of KADS, KADS models are non-formal and 
non-computational models for making sense, whereas [Fensel] and [Rothenfluh] 
understand them to be computational models. This may be due to the different 
socializations of the groups. The original KADS formulation can be characterized as 
pseudocode which has a certain formal and computational flavor for researchers with 
a background in the humanities, whereas — for researchers in formal sciences — it 
does not satisfy the required standards of precision, rigorosity, and formal founda­
tion. 
Are libraries of tools feasible and usable? 
The Spark-Burn-Firefighter of [McDermott] has started as an attempt to collect 
knowledge acquisition tools in a library, to supply a meta-tool that guides the se­
lection of the best suited individual tool(s) from the libraries, and to generate know­
ledge based systems by means of these tools. This has failed in a certain sense: the 
meta-tool would have had to ask too many questions of a far too complex nature to 
allow efficient use. The scope and focus of Spark-Burn-Firefighter was consequently 
changed towards simpler building blocks (mechanisms instead of artificial intelligence 
inference methods), towards homogeneous foundations in the sense of standardized 
description of components ("process", "resources", "outcomes"), and towards far 
ranging tool support for early parts of the conceptualization process (dictionary, etc.). 
The present claim, supported by first experimental evidence, is that the effort in 
producing tailored application systems is considerably reduced for the users of 
Spark-Burn-Firefighter. 
Another claim is that the typical users of Spark-Burn-Firefighter would not be capa­
ble of applying KADS; e.g. they would not be able to make appropriate use of KADS 
interpretation models, while there is evidence that they make appropriate use of SBF 
mechanisms. 
On the other hand there is a considerable number of KADS success stories and the 
attempt of [Shadbolt] to provide a meta-tool (ProtoKEW) to systematically select an 
interpretation model and tools to syternatically fill a selected one. At least two considerations may help to resolve these apparent contradictions. The 
first draws upon the different foundations of early Spark-Burn-Firefighter and 
ProtoKEW. While [McDermott] attempted the technical integration of conceptually 
unrelated tools, [Shadbolt] aims at a workbench which supports one methodology of 
knowledge engineering (KADS). It has been a matter of intensive debate in recent 
workshops on knowledge acquisition whether a combination of inhomogenous tools 
is worth the effort, or whether a common theory as a basis of integration should 
precede the technicalities of software compatibility issues. The present trend seems to 
favor a precedence of theoretical foundation (cf. Wetter and Woodward, [Wette90]), 
for which [Shadbolt] is a representative. 
The second consideration takes the education and skills of users into account. The 
typical Spark-Burn-Firefighter user (not the facilitator) is a non-programmer em­
ployee in a non-EDP department (such as sales or accounting), who needs software 
support for some of his work. His skills arc those required in his department. The 
typical KADS "worker" has specific skills ranging from system analyst to Ph.D. in 
computer science with a KADS education. Needless to say, the latter may have wider 
ranging capacities in handling complex modelling approaches. 
Must the knowledge engineer become an expert? 
The bottom up approaches and [Woodward] agree upon "yes" while the modelling 
approaches do not make explicit claims but implicitly mean "no". This divergence has 
a shallow and a deep explanation. The superficial one is that the skill in applying 
modelling aids and model structures provides a means of penetrating a domain for the 
purpose of formalizing it without really understanding it: modelling skill as a shortcut 
towards learning complex subject si 
Though this may factually be true in the sense that knowledge based systems are 
generated in such a way, it may conceal a deeper aspect. Modelling within a modelling 
paradigm means capturing what is within the scope of the paradigm but deforming, 
deliberately omitting, or just overseeing what is outside. Taking up the above question, 
modelling skill does not enable the individual to really learn a complex subject but to 
get hold of those aspects of the subject that can be subsumed under some facet of the 
model structures or languages he handles. And these arc narrower and less flexible the 
higher the demands on precision and formality arc. This is one of the reasons why 
informality of conceptual models is defended by a number of KADS researchers who 
primarily see KADS as a research environment. 
And how about the personality of the knowledge engineer? 
Might it be true, as [Bartsch-Sporl] claimed, that, given all the intricacies revealed 
during the workshop and partly reported here, the ultimate determining success fac-tor is the intuition or other personal traits of the individual knowledge engineer? And 
if so, would this mean that projects beyond a certain size or complexity are not fea­
sible because there are not enough such geniuses available? Maybe it is permissible 
to finish this speculative outlook by paraphrasing M.M. Richter, head of the research 
group who hosted the workshop: Wc don't require methods for those who already 
know, but as a basis to teach those who also need to know. 
Acknowledgements 
Comments from Klaus-Dieter Althoff on earlier versions have helped me to see se­
veral details clearer. Angi Voss has very carefully studied the material. Her sug­
gestions have supported and motivated me very much to extend the scope of the text 
to its present size and to provide, although technically simple, graphic overviews of 
the approaches which may serve as an additional form of guidance into the highly 
diverse subject matter of the book. Last but not least, Mark Beers has supported 
me in writing it in English. 
References 
IAltho92] Althoff, K.D. and WeB, S. Case-Based Reasoning and Expert System De­
velopment in this volume 
|Becke90] Becker, B. and Bartsch-Sporl, B. Die Verdnderung von Expertenwissen 
durch den Prozefi der Wissensakquisition (Modification of expertise during 
the process of knowledge acquisition) Kl 2 (2) 31-36 (1990) 
[Bergm92] Bergmann, R. Knowledge Acquisition by Generating Skeletal Plans from 
Real World Cases in this volume 
[Brans92J Branskat, S. Knowledge Acquisition from Cases in this volume 
[Dalle92] Dallemagne, G., Klinker, G., Marques, D., McDermott, J., Tung, D. 
Making Application Programming More Worthwhile in this volume 
[Fense921 Fensel, D. Knowledge Acquisition and the Interpretative Paradigm in this 
volume 
(Janet92] Janetzko, D. and Strube, G. Case-based Reasoning and Model-based 
Knowledge Acquisition in this volume 
[Hobbs851 Hobbs, J.R. and Moore, R.C. Formal Theories of the Commonsense 
World Norwood, NJ 1985 (Ablex) 
[Linst92] Linster, M. Shifting Positions : Moving from a Cognitive Science Point 
of View to a Knowledge Engineering Stance in this volume 
|Manag921 Manago, M., Conruyt, N. Using Information Technology to Solve Real 
World Problems in this volume 
(Musen89] Musen, M. Conceptual Models of Interactive Knowledge Acquisition Tools 
Knowledge Acquisition 1, 73-98 (1989). [Pfeif92j Pfeifer, R., Rothenfluh, T., Stolze, M., Steiner, F. Mapping Expert 
Behaviour onto Task-Level Frameworks: The need for " Eco-Pragmatic" 
Approaches to Knowledge Engineering in this volume 
[Puppe92J Puppe, F. and Gappa, U. Two Questions from Expert System Developers 
to Cognitive Scientists in this volume 
[Reima92| Reimann, P. and Schult, T.J. Learning from Problem Solving Traces in 
this volume 
[SchIe92J Schlenker, B. and Wetter, Th. Knowledge Acquisition as an Empirically 
Based Modelling Activity in this volume 
[Schma92j Schmalhofer, F., Globig, C, and Thoben, J. Refitting of Plans by a Hu­
man Expert in this volume 
(Schre88) Schreiber, G., Breuker, J., Bredeweg, B., and Wielinga, B. Modelling in 
knowledge based system Development Boose, J., Gaines, B., and Linster, 
M. (eds.); Proc. European Knowledge Acquisition Workshop EKAW88, 
Bonn, July 88, GMD-Studien 143, St. Augustin 1988 
|Shadb92| Shadbolt, N. Facts, Fantasies, and Frameworks: The Design of a Know­
ledge Acquisition Workbench in this volume 
[Strub92] Strube, G. Different Types of Cognitve Adequacy, in this volume 
ITuIvi73| Tulving, E. and Thompson, D.M. Encoding Specifity and Retrieval Proc­
esses in Episodic Memory Psychological Reviews 80, 352-373 (1973) 
|Wette90| Wetter, Th. and Woodward, B. Towards a Theoretical Framework for 
Knowledge Acquisition in: Boose, J. and Gaines, B. (eds.) Proc. 5th AAAI 
Knowledge Acquisition for Knowledge Based-Systems Workshop, Banff 
(Canada) Nov. 1990 
[Woodw92J Woodward, J.B., Shaw, M.L.G., Gaines, B.R. The Cognitive Basis of 
Knowledge Engineering in this volume 
[Glowall| Dr. U. Glowalla Justus-Liebig-Universitdt Giefien, Fachbereich 06 Psycho­
logic, 6300 Giefien, Federal Republic of Germany About the Authors 
Klaus-Dieter Althoff studied mathematics and operations research at the Technical 
University of Aachen/Germany. From 1986 to 1990 he worked as a researcher in the German 
Sonderforschungsbereich 314 "Artificial Intelligence - Knowledge-Based Systems" in the 
projects X6 (knowledge acquisition workbench for the fault diagnosis of engineering systems) 
and X9 (learning and analogy for engineering expert systems). Since the beginning of 1991 he 
worked as a research assistant at Kaiserslautern University/Germany within the research group 
of Prof. Richter and is about to finish his doctoral dissertation on "A case-based learning 
component as an integrated part of the MOLTKE workbench for the diagnosis of engineering 
systems". 
Ralph Bergmann is research assistant at the German Research Center for Artificial 
Intelligence and is currently pursuing his dissertation (PhD). He received his diploma in 
computer science from the University of Kaiserslautern in 1990. His primary research interests 
are machine learning, especially explanation-based methods, knowledge acquisition and 
cognitive modelling. His current address is: German Research Center for Artificial Intelligence, 
University Bldg. 57, Erwin-Schrodinger-Str., D- 6750 Kaiserslautern, Germany. 
Sonja Branskat is currently pursuing her PhD at the Knowledge Science Institute of the 
University of Calgary. She received her B.Sc. in mathematics from the University of Freiburg, 
Germany in 1986 and her M.Sc. in computer science from the University Hagen, Germany in 
1991. Her research interests are in knowledge acquisition and computer supported cooperative 
work. Her current address is: Department of Computer Science, University of Calgary, 2500 
University Drive N.W., Calgary, Alberta, Canada, T2N 1N4. 
Noel Conruyt is a research engineer at Acknowledge corporation. He received his Master's 
in computer science at University of Paris VI (Jussieu) in 1988 and he is currently doing his 
PhD at the Institut National de Recherche en Informatique et Automatique (INRIA) in 
collaboration with the Museum of Natural History in Paris. His research interests are in using 
induction and case-based reasoning to solve problems in biology (identification of marine 
sponges), building interactive tools to collect case libraries, designing knowledge acquisition 
tools which assist the user in building domain theories for machine learning and case-based 
reasoning tools. His current address is: Acknowledge, 16 Passage Foubert, 75013 Paris, 
France. 
Geoffroy Dallemagne is a researcher at Digital Equipment Corporation's Al research 
group. He is currently pursuing a PhD at Ecole Centrale de Paris's Laboratory for Computer 
Science. He received his M.Sc. in applied mathematics from Ecole Centrale de Paris in 1988. 
His research interests are workplace analysis and computer supported cooperative work. His 
current address is: Paris Research Lab. Digital, 85 avenue Victor-Hugo, 92563 Rueil-
Malmaison Cedex, France. Dieter Fensel is research assistant at the University of Karlsruhe. He received a diploma 
degree in sociology from Freie Universitat Berlin and a diploma degree in computer science 
from Technische Universitat Berlin in 1989. His research interests are in shifting knowledge 
acquisition from an art to an engineering discipline and in developing and applying machine 
learning algorithms. His current address is: Institutfur Angewandte Informatik und Formale 
Beschreibungsverfahren, Universitat Karlsruhe, P.O.Box 6980, 7500 Karlsruhe, Germany, e-
mail: fensel@aifb.uni-karlsruhe.de. 
Dr. Brian R. Gaines is Killam Memorial Research Professor and Director of the 
Knowledge Science Institute at the University of Calgary. He received his B.A., M.A. and 
PhD from Trinity College, Cambridge, and is a Chartered Engineer, Chartered Psychologist, 
and a Fellow of the Institution of Electrical Engineers, the British Computer Society and the 
British Psychological Society. His research interests include: the socio-economic dynamics of 
science and technology; the nature, acquisition and transfer of knowledge; software engineering 
for heterogeneous systems; and expert system applications in manufacturing, the professions, 
sciences and humanities. His current address is: Knowledge Science Institute, University of 
Calgary, Calgary, Alberta, Canada T2N1N4. 
Christoph Globig is a student in computer science at the University of Kaiserslautern and a 
student researcher at the German Research Center for Artificial Intelligence. 
Ute Gappa is research assistent at the University of Karlsruhe and is pursuing her dissertation 
(PhD). She received her Diplom in Informatik (Masters in computer science) from the 
University of Kaiserslautern in 1988. Her primary research interests are automated knowledge 
acquisition from experts, graphical user-interfaces and tools for the generation of knowledge 
acquisition systems. Her current adress is: University of Karlsruhe, Institute of Logic, PO Box 
6980, D-7500 Karlsruhe, Germany. 
Dietmar Janetzko is a doctoral candidate at Bochum University and a research assistant in 
the department of cognitive science at the Institute for Computer & Social Sciences at Freiburg 
University. He obtained his diploma in psychology in 1987 from Bochum University. His 
research interests include case-based reasoning, analogy, metaphor and knowledge acquisition. 
His current address is: Department of Cognitive Science, Institute for Computer & Social 
Sciences, Friedrichstr. 50, D-7800 Freiburg, Germany. 
Georg Klinker joined Digital Equipment Corporation in 1988 as a member of the Al research 
group. His research interests focus on making it easier to create application programs through 
reuse of software artifacts. From 1984 to 1988 he was a research scientist at the Carnegie 
Mellon Computer Science Department. Georg Klinker received his M.B.A. from the University 
of Hamburg, Germany in 1983. While in Hamburg he worked for the software company IfaD. Dr. Marc Linster is a member of the Al Research Division of GMD (German National 
Research Center for Mathematics and Dataprocessing). His work is focussing on problem-
solving methods and their influence on (automated) knowledge acquisition. He obtained a 
doctorate —concerned with the above-mentioned problems— from the University of 
Kaiserslautern, where he had also received his diploma in computer science in 1987. His 
current address is: Al Research Division, GMD, PO Box 1240, 5205 St. Augustin, FRG. 
Dr. Michel Manago is the chief scientist at Acknowledge corporation. He received his B.Sc. 
in computer science from the University of Illinois in Urbana Champaign in 1983 and his PhD 
at University of Orsay (France) in 1988. His research interests include inductive learning and 
case-based reasoning technologies, integration of symbolic and numeric methods, theoretical 
foundations of machine learning and building computer tools for the analysis of the human 
genome (genetics). He is involved in using machine learning and case-based reasoning for 
applications which deal with finance (credit assessment), identification (photo-interpretation), 
analysis in clinical databases, telecommunications and technical maintenance (fault diagnosis, 
preventive maintenance). His current address is: Acknowledge, 16 Passage Foubert, 75013 
Paris, France. 
David Marques has been a research scientist in Digital Equipment Corporation's Artificial 
Intelligence Research Group for the past 5 years. His research interests are in enabling end-
users to do their own programming (through software reuse), and in understanding (through 
modeling) the context of end-user activities. He received his A.B. in psychology from Cornell 
in 1972, PhD in psychobiology from the University of Michigan in 1976, and did research in 
neuroscience before joining Digital Equipment Corporation in 1982 in the Software Services 
organization. 
Dr. John McDermott is a member of the technical staff at Digital Equipment Corporation. 
He has been a member of the faculty of the Computer Science Department at Carnegie-Mellon 
University since 1974. In 1983 he confounded the Carnegie Group, Inc. He received a PhD in 
philosophy from the University of Notre Dame in 1969. His research interests are the 
application of Al techniques to industrial problems and artificial intelligence in general. 
Dr. Rolf Pfeifer is a full professor of computer science and heads the Al Lab at the Institute 
for Informatics at the University of Zurich in Switzerland. After receiving his M.Sc. in physics 
and mathematics and his PhD (with a thesis on Al and psychological modeling) at the Swiss 
Federal Institute of Technology (ETH), he spent three years in the USA at Carnegy Mellon 
University in Herb Simon's group and at Yale in Bob Abelson and Roger Schank's cognitive 
science lab. His main research interests are foundations of Al and cognitive science, 
autonomous agents, and "situated design". His current address is: Al Lab, Institute for 
Informatics, University of Zurich, Winterthurerstrasse 190, CH-8057 Zurich. E-mail: 
pfeifer@ifi.unizh.ch. Dr. Frank Puppe is professor for informatics at Wurzburg University. He received his PhD 
1986 from Kaiserslautern University and his habilitation about "Problem Solving Methods in 
Expert Systems" 1991 from Karlsruhe University. His research interests include strong 
problem solving methods in expert systems, graphical knowledge acqusition, intelligent 
tutoring systems and machine learning as well as their evaluations in technical and medical 
applications. His current address is: Universitdt Wurzburg, Institut fur Informatik, Am 
Hubland, D-8700 Wurzburg. 
Dr. Peter Reimann is lecturer at the Department of Psychology, University of Freiburg, 
FRG. He received his M.Sc. in psychology in 1984 and his PhD in 1989 from the University 
of Freiburg. His research interests are the computational psychology of learning, memory and 
problem solving as well as applications in intelligent tutoring and testing systems and man-
machine interaction. His current address is: Dept. of Psychology, Univ. of Freiburg, 
Niemensstr. 10, D-7800 Freiburg, FRG. 
Dr. Thomas Rothenfluh is currently a Visiting Scholar at The Ohio State University in 
Columbus, Ohio. He received his PhD in psychology from the University of Zurich in 1988 
and is now supported with a 3-year scholarship from the Swiss National Science Foundation. 
His main research interests are in cognitive science, knowledge systems and psychological 
modelling. His current address is: Laboratory for Artificial Intelligence Research, Dept. of 
Computer and Information Science, The Ohio State University, 2036 Neil Ave. Mall, 
Columbus, OH 43210-1277. 
Beate Schlenker is currently pursuing her Diplom-Degree in psychology at the University of 
Freiburg. Her primary research interest is knowledge acquisition. Her current address is: Am 
langen Graben 41,5300 Bonn 3. 
Dr. Franz Schmalhofer is a senior scientist and leader of the knowledge acquisition group 
at the German Research Center for Artificial Intelligence (DFKI) in Kaiserslautern. He has 
studied mathematics, psychology and computer science at the University of 
Regensburg/Germany and the University of Colorado at Boulder, where he earned his PhD in 
1982. He has held academic positions at the Universities of Heidelberg/Germany, 
Freiburg/Germany and the McGill University of Montreal/Canada. His research lies mostly in 
the areas of cognitive modelling, knowledge acquisition, machine learning, expert systems, 
planning, human-computer interaction, text comprehension and decision research, email: 
schmalho@informatik.uni-kl.de. 
Thomas J. Schult is research scientist at the Department of Psychology, University of 
Freiburg, Germany. He received his B.Sc. in mathematics from the University of Freiburg in 
1988, and his diploma in computer science from the University of Hagen in 1990. His research 
interests are in case-based reasoning and in intelligent tutoring systems. His current address is: 
Dept. of Psychology, Univ. of Freiburg, Niemensstr. 10, D-7800 Freiburg, Germany. Dr. Nigel Shadbolt is professor of Intelligent Systems at the University of Nottingham, 
England. He received his B.A. in philosophy and psychology from Newcastle upon Tyne 
University in 1978 and his PhD from the Department of Artificial Intelligence at the University 
of Edinburgh in 1984. His research interests are in the areas of knowledge acquisition, and the 
foundations of agent design. His current address is: The Artificial Intelligence Group, 
Department of Psychology, University of Nottingham, University Park, Nottingham NG7 
2RD, England. 
Dr. Mildred L. G. Shaw is Professor of Computer Science at the University of Calgary. 
She received her B.Sc. and M.Sc. from the University of London, and her PhD from Brunei 
University and is a Chartered Mathematician and Chartered Psychologist. She is a Fellow of the 
Institute of Mathematics and its Applications and the British Computer Society and an Associate 
Fellow of the British Psychological Society. Her research interests include: human-computer 
interaction; the acquisition and transfer of knowledge; software engineering; and expert system 
applications. Her current address is: Knowledge Science Institute, University of Calgary, 
Calgary, Alberta, Canada T2N1N4. 
Felix Steiner is currently working on knowledge based systems for customer credit 
assessment at Credit Suisse, Zurich. He received his degree in clinical psychology from the 
University of Zurich, Switzerland in 1981. His interests are in knowledge engineering, 
knowledge acquisition, and connectionism. His current address is: Credit Suisse, P.O. Box 
590 I Okb39, CH-8021 Zurich. 
Markus Stolze is currently persuing his PhD at the Al Lab, University Zurich. He received 
his M.Sc. in computer science from the University of Bern, Switzerland in 1988. His primary 
research interests focus on the design of interactive knowledge based systems. His current 
address is: Institute for Informatics, Winterthurerstrasse 190, CH-8057 Zurich. E-mail: 
stolze@ifi.unizh.ch. 
Dr. Gerhard Strube is professor of cognitive science at the University of Freiburg, 
Germany. He received Mag.theol. and Dipl.-Psych, degrees in 1973 and 1974, Dr. phil. in 
cognitive psychology 1977, Dr. phil. habil. 1983 (University of Munich). 1982-1987 senior 
scientist at the Max Planck Institute for Psychological Research, Munich, 1987-1991 full 
professor of cognition and human information processing at the Ruhr University, Bochum. His 
research interests focus on computer models of language processes, acquisition and 
representation of knowledge, and reasoning. His current position is director of the department 
of cognitive science, Institut fur Informatik und Gesellschaft, Universitat Freiburg, 
Friedrichstr. 50, D-7800 Freiburg i. Br., Germany. 
Jorg Thoben is research assistant at the German Research Center for Artificial Intelligence 
and is currently pursuing his dissertation (PhD). He received his diploma in psychology from 
the University of Miinster in 1992. His primary research interests are mental models, cognitive 
modelling and knowledge acquisition. His current address is: German Research Center for 
Artificial Intelligence, University Bldg. 57, Erwin-Schrodinger-Str., D- 6750 Kaiserslautern, 
Germany. David Tung is a researcher/principal software engineer at Digital Equipment Corporation, Al 
Research Group. His research interests include knowledge acquisition, software reuse, and 
workplace analysis. He received his B.Sc. in electronic engineering from University of 
London, and M.Sc. in artificial intelligence from University of Edinburgh. He is a member of 
IEEE, AAAI, ACM, and EE. His current address is: DEC, 111 Locke Drive, LM02-1IK11, 
Marlboro MA 01752. 
Dr. Angi Voss is working at the Al Research Division of the German National Research 
Center for Computer Science (GMD), where she is responsible for a subdivision on knowledge 
modelling. She received her diploma in informatics at the University of Bonn, and her doctoral 
degree at the University of Kaiserslautern. Her current address is: GMD, Al Research Division, 
Schloss Birlinghoven, D-5305 St. Augustin 1, e-mail: avoss@gmdii.gmd.de. 
Stefan Wess studied computer science and operations research at the University of 
Kaiserslautern. From 1987 to 1990 he worked as a student researcher in the German 
Sonderforschungsbereich 314 "Artificial Intelligence - Knowledge-Based Systems" in the 
projects X6 (knowledge acquisition workbench for the fault diagnosis of engineering systems) 
and X9 (learning and analogy for engineering expert systems). He received his diploma in 
computer science in 1990. Since 1991 he is working as a researcher in a Case-Based Reasoning 
Project at the University of Kaiserslautern. 
Dr. Thomas Wetter studied mathematics and computer science at the Technical University 
of Aachen. He investigated several topics in mathematical modelling of physiological and 
medical phenomena and received his PhD in mathematics about logic based medical diagnosis 
in 1984. His affiliation now is the IBM Germany Scientific Center, Heidelberg. There he has 
been working in software ergonomics, expert systems, and knowledge acquisition. He teaches 
graduate courses in Al at the universities of Heidelberg and Kaiserslautern. 
Dr. Brian Woodward is a research associate with the Knowledge Science Institute in the 
Department of Computer Science at the University of Calgary. His research interests include the 
design and development of cognition support software, training and educational simulations, 
and management decision-making. He obtained his PhD in educational psychology from the 
University of Calgary in 1978 and works as an industrial/organizational psychologist 
specializing in management selection and development. 

